\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[11pt]{krantz}
\usepackage{lmodern}
\usepackage{setspace}
\setstretch{1.15}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmonofont[Mapping=tex-ansi,Scale=0.6]{Source Code Pro}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Predictive Soil Mapping with R},
            pdfauthor={Tomislav Hengl},
            colorlinks=true,
            linkcolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}
\usepackage{tabu}

% \setmainfont[UprightFeatures={SmallCapsFont=AlegreyaSC-Regular}]{Alegreya}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{255,255,255}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\ifxetex
  \usepackage{letltxmacro}
  \setlength{\XeTeXLinkMargin}{1pt}
  \LetLtxMacro\SavedIncludeGraphics\includegraphics
  \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
    \IncludeGraphicsAux{#1}%
  }%
  \newcommand*{\IncludeGraphicsAux}[2]{%
    \XeTeXLinkBox{%
      \SavedIncludeGraphics#1{#2}%
    }%
  }%
\fi

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\makeatletter
\@ifundefined{Shaded}{
}{\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}}
\makeatother

\newenvironment{rmdblock}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframe}
  \item
  }
  {
  \end{kframe}
  \end{itemize}
  }
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Predictive Soil Mapping with R}
\author{Tomislav Hengl}
\date{2018-10-16}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}
\begin{center}
% \includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{predictive-soil-mapping-for-advanced-r-users}{%
\chapter*{Predictive Soil Mapping for advanced R
users}\label{predictive-soil-mapping-for-advanced-r-users}}
\addcontentsline{toc}{chapter}{Predictive Soil Mapping for advanced R
users}

\begin{center}\includegraphics[width=0.33\linewidth]{figures/f0_web} \end{center}

This is the online version of the Open Access book:
\href{https://envirometrix.github.io/PredictiveSoilMapping/}{\textbf{Predictive
Soil Mapping with R}}. Pull requests and general comments are welcome.
These materials are based on the technical tutorials developed within
the \href{http://isric.org/}{ISRIC's} Global Soil Information Facilities
(GSIF) framework for automated soil mapping over the period 2014--2017.

\textbf{This website is under construction}. For news and updates please
refer to the
\href{https://github.com/envirometrix/PredictiveSoilMapping/issues}{github
issues}.

Hard copies of this book from will be made available in the second half
of 2018.

\hypertarget{editors}{%
\section*{Editors}\label{editors}}


\href{http://envirometrix.net/staff}{Tom Hengl} is a Senior Researchers
/ technical director at Envirometrix Ltd. He has more than 20 years of
experience as an environmental modeler, data scientist and spatial
analyst. Tom is a passionate advocate for, and supporter of, open data,
reproducible science and career development for young scientists. He has
designed and implemented the global
\href{http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0169748}{SoilGrids}
dataset, partially in response to other well known open data projects
such as OpenStreetMap, GBIF, GlobalForestWatch and global climate
mapping projects. He has been teaching predictive soil mapping at
Wageningen University / ISRIC within the ``Hands-on-GSIF'' block
courses. Video tutorials of the soil mapping with R can also be found at
\url{http://youtube.com/c/ISRICorg}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


Predictive Soil Mapping (PSM) is based on applying statistical and/or
machine learning techniques to fit models for the purpose of producing
spatial and/or spatiotemporal predictions of soil variables i.e.~maps of
soil properties and classes at different resolutions. It is a
multidisciplinary field combining statistics, data science, soil
science, physical geography, remote sensing, geoinformation science and
number of other sciences
\citep{Scul01, MCBRATNEY20033, Henderson2004Geoderma, Boettinger2010Springer, Zhu2015PSM}.
\emph{Predictive Soil Mapping with R} is about understanding the main
concepts behind soil mapping, mastering R packages that can be used to
produce high quality soil maps, and about optimizing all processes
involved so that also the production costs can be reduced.

The main idea behind Predictive vs traditional expert-based soil mapping
is that production of maps: (a) is based on using state-of-the-art
statistical methods to ensure objectivity of maps (including objective
uncertainty assessment vs expert judgment), and (b) is driven by
automation of the processes so that overall soil data production costs
can be reduced and updates of the maps implemented without a need for
large investments. R in that sense is a logical platform to develop PSM
workflows and applications, especially thanks to the vibrant and
productive R spatial interest group activities and also thanks to the
increasingly professional soil data packages such as the soiltexture,
aqp, soilprofile, soilDB and similar.

Book is divided into sections covering theoretical concepts, preparation
of covariates, model selection and evaluation, prediction and
visualization and distribution of final maps. Most of chapters contain R
code examples that try to illustrate main processing steps and give
practical instructions to developers and applied users.

\hypertarget{connected-publications}{%
\section*{Connected publications}\label{connected-publications}}


Most of methods described in this book are based on the following
publications:

\begin{itemize}
\item
  Hengl, T., Nussbaum, M., Wright, M. N., Heuvelink, G. B., and Gräler,
  B. (2018) \href{https://peerj.com/preprints/26693/}{Random Forest as a
  generic framework for predictive modeling of spatial and
  spatio-temporal variables}. PeerJ Preprints.
\item
  Sanderman, J., Hengl, T., Fiske, G., (2017)
  \href{http://www.pnas.org/content/early/2017/08/15/1706103114.full}{The
  soil carbon debt of 12,000 years of human land use}. PNAS,
  \url{doi:10.1073/pnas.1706103114}
\item
  Ramcharan, A., Hengl, T., Nauman, T., Brungard, C., Waltman, S.,
  Wills, S., \& Thompson, J. (2018).
  \href{https://dl.sciencesocieties.org/publications/sssaj/abstracts/82/1/186}{Soil
  Property and Class Maps of the Conterminous United States at 100-Meter
  Spatial Resolution}. Soil Science Society of America Journal, 82(1),
  186-201.
\item
  Hengl, T., Leenaars, J. G., Shepherd, K. D., Walsh, M. G., Heuvelink,
  G. B., Mamo, T., et al. (2017)
  \href{https://link.springer.com/article/10.1007/s10705-017-9870-x}{Soil
  nutrient maps of Sub-Saharan Africa: assessment of soil nutrient
  content at 250 m spatial resolution using machine learning}. Nutrient
  Cycling in Agroecosystems, 109(1), 77--102.
\item
  Hengl T, Mendes de Jesus J, Heuvelink GBM, Ruiperez Gonzalez M,
  Kilibarda M, Blagotic A, et al. (2017)
  \href{http://dx.doi.org/10.1371/journal.pone.0169748}{SoilGrids250m:
  Global gridded soil information based on machine learning}. PLoS ONE
  12(2): e0169748. \url{doi:10.1371/journal.pone.0169748}
\item
  Shangguan, W., Hengl, T., de Jesus, J. M., Yuan, H., \& Dai, Y.
  (2017). \href{https://doi.org/10.1002/2016MS000686}{Mapping the global
  depth to bedrock for land surface modeling}. Journal of Advances in
  Modeling Earth Systems, 9(1), 65-88.
\item
  Hengl, T., Roudier, P., Beaudette, D., \& Pebesma, E. (2015)
  \href{https://www.jstatsoft.org/article/view/v063i05}{plotKML:
  scientific visualization of spatio-temporal data}. Journal of
  Statistical Software, 63(5).
\item
  Gasch, C. K., Hengl, T., Gräler, B., Meyer, H., Magney, T. S., \&
  Brown, D. J. (2015)
  \href{https://doi.org/10.1016/j.spasta.2015.04.001}{Spatio-temporal
  interpolation of soil water, temperature, and electrical conductivity
  in 3D+ T: The Cook Agronomy Farm data set}. Spatial Statistics, 14,
  70--90.
\item
  Hengl, T., Nikolic, M., \& MacMillan, R. A. (2013)
  \href{https://doi.org/10.1016/j.jag.2012.02.005}{Mapping efficiency
  and information content}. International Journal of Applied Earth
  Observation and Geoinformation, 22, 127--138.
\item
  Hengl, T., Heuvelink, G. B., \& Rossiter, D. G. (2007)
  \href{https://doi.org/10.1016/j.cageo.2007.05.001}{About
  regression-kriging: from equations to case studies}. Computers \&
  geosciences, 33(10), 1301-1315.
\item
  Hengl, T. (2006)
  \href{https://doi.org/10.1016/j.cageo.2005.11.008}{Finding the right
  pixel size}. Computers \& geosciences, 32(9), 1283--1298.
\end{itemize}

Some other publications / books on the subject of Predictive Soil
Mapping include:

\begin{itemize}
\item
  Malone, B.P, Minasny, B., McBratney, A.B., (2016)
  \href{https://www.springer.com/gp/book/9783319443256}{Using R for
  Digital Soil Mapping}. Progress in Soil Science ISBN: 9783319443270,
  262 pages.
\item
  California Soil Resource Lab, (2017)
  \href{https://casoilresource.lawr.ucdavis.edu/software/}{Open Source
  Software Tools for Soil Scientists}, UC Davis.
\item
  McBratney, A.B., Minasny, B., Stockmann, U. (Eds) (2018)
  \href{https://www.springer.com/gp/book/9783319634371}{Pedometrics}.
  Progress in Soil Science ISBN: 9783319634395, 720 pages.
\item
  FAO, (2018)
  \href{https://github.com/FAO-GSP/SOC-Mapping-Cookbook}{Soil Organic
  Carbon Mapping Cookbook}. 2nd edt. ISBN: 9789251304402
\end{itemize}

Readers are also encouraged to obtain and study the following R books
before following some of the more complex exercises in this book:

\begin{itemize}
\item
  Bivand, R., Pebesma, E., Rubio, V., (2013)
  \href{http://www.asdar-book.org/}{Applied Spatial Data Analysis with
  R}. Use R Series, Springer, Heidelberg, 2nd Ed. 400 pages.
\item
  Kabacoff, R.I., (2011) \href{http://www.manning.com/kabacoff/}{R in
  Action: Data Analysis and Graphics with R}. Manning publications,
  ISBN: 9781935182399, 472 pages.
\item
  Kuhn, M., Johnson, K. (2013)
  \href{http://appliedpredictivemodeling.com/}{Applied Predictive
  Modeling}. Springer Science, ISBN: 9781461468493, 600 pages.
\item
  Lovelace, R., Nowosad, J., Muenchow, J., (2018)
  \href{https://geocompr.robinlovelace.net/}{Geocomputation with R}.
  forthcoming book with CRC Press.
\item
  Reimann, C., Filzmoser, P., Garrett, R., Dutter, R., (2008)
  \href{https://onlinelibrary.wiley.com/doi/book/10.1002/9780470987605}{Statistical
  Data Analysis Explained Applied Environmental Statistics with R}.
  Wiley, Chichester, 337 pages.
\end{itemize}

For the most recent developments in the R-spatial community refer to
\url{https://r-spatial.github.io} and/or the R-sig-geo mailing list.

\hypertarget{contributions}{%
\section*{Contributions}\label{contributions}}


This book is constantly updated and contributions are welcome (through
pull requests, but also through adding new chapters) provided that some
minimum requirements are met. To contribute a complete new chapter
please contact the editors first. Some minimum requirements to
contribute a chapter are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The data needs to be available in majority of chapter, best via some R
  package or web-source.
\item
  Chapter should focus on implementing computing in R (it should be
  written as R tutorial).
\item
  All examples should be computationally efficient with not more than 30
  secs of computing time per process on a single core system.
\item
  Theoretical basis for methods and interpretation of results should be
  based on peer-review publications. This book is not intended to host
  primary research / experimental results, but only to supplement
  existing research publications.
\item
  Chapter should consist of at least 1500 words and at most 3500 words.
\item
  The topic of the chapter must be closely connected to theme of soil
  mapping, soil geographical databases, methods for processing spatial
  soil data and similar.
\end{enumerate}

In principle, all submitted chapters should also follow closely the
\href{https://en.wikipedia.org/wiki/Wikipedia:Five_pillars}{five pillars
of Wikipedia}, especially: Verifiability, Reproducibility, No original
research, Neutral point of view, Good faith, No conflict of interest,
and no personal attacks.

\hypertarget{reproducibility}{%
\section*{Reproducibility}\label{reproducibility}}


To reproduce the book, you need a recent version of
\href{https://cran.r-project.org/}{R}, and
\href{http://www.rstudio.com/products/RStudio/}{RStudio} and up-to-date
packages, which can be installed with the following command (which
requires \href{https://github.com/hadley/devtools}{\textbf{devtools}}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\OperatorTok{::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"envirometrix/PredictiveSoilMapping"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To build the book locally, clone or
\href{https://github.com/envirometrix/PredictiveSoilMapping/archive/master.zip}{download}
the
\href{https://github.com/envirometrix/PredictiveSoilMapping/}{PredictiveSoilMapping
repo}, load R in root directory (e.g.~by opening
\href{https://github.com/envirometrix/PredictiveSoilMapping/blob/master/PredictiveSoilMapping.Rproj}{PredictiveSoilMapping.Rproj}
in RStudio) and run the following lines:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bookdown}\OperatorTok{::}\KeywordTok{render_book}\NormalTok{(}\StringTok{"index.Rmd"}\NormalTok{) }\CommentTok{# to build the book}
\KeywordTok{browseURL}\NormalTok{(}\StringTok{"docs/index.html"}\NormalTok{) }\CommentTok{# to view it}
\end{Highlighting}
\end{Shaded}

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}


The authors are grateful to numerous contributions from colleagues
around the world, especially for the contributions by the current and
former ISRIC --- World Soil Information colleagues: Robert MacMillan,
Gerard Heuvelink, Johan Leenaars, Jorge Mendes de Jesus, Wei Shangguan,
David G. Rossiter, and many others. ISRIC is a research foundation
funded primarily by the Dutch Government. The authors also grateful to
the support received via the \href{http://africasoils.net}{AfSIS
project}, which has been funded by the Bill and Melinda Gates Foundation
(BMGF) and the Alliance for a Green Revolution in Africa (AGRA). Many
soil data processing examples in the book are based on the R code
developed by Dylan Beuadette, Pierre Roudier, Julian Moeys, Brandad
Malone and many other developers. Author is also grateful to comments
and suggestions to the methods explained in the book by Travis Nauman,
Amanda Ramcharan, David G. Rossiter and
\href{http://julienmoeys.info/}{Julian Moeys}.

SoilGrids are based on numerous soil profile data sets that have been
kindly contributed by various national and international agencies: the
USA National Cooperative Soil Survey Soil Characterization database
(\url{http://ncsslabdatamart.sc.egov.usda.gov/}) and profiles from the
USA National Soil Information System, Land Use/Land Cover Area Frame
Survey (LUCAS) Topsoil Survey database \citep{Toth2013LUCAS}, Africa
Soil Profiles database \citep{Leenaars2012}, Australian National Soil
Information by CSIRO Land and Water
\citep{Karssies2011CSIRO, searle2014australian}, Mexican National soil
profile database \citep{INEGI2000} provided by the Mexican Instituto
Nacional de Estadística y Geografía / CONABIO, Brazilian national soil
profile database \citep{cooper2005national} provided by the University
of São Paulo, Chinese National Soil Profile database
\citep{shangguan2013china} provided by the Institute of Soil Science,
Chinese Academy of Sciences, soil profile archive from the Canadian Soil
Information System \citep{macdonald1992cansis} and Forest Ecosystem
Carbon Database (FECD), ISRIC-WISE \citep{Batjes2009SUM}, The Northern
Circumpolar Soil Carbon Database \citep{essd-5-3-2013}, eSOTER profiles
\citep{VanEngelen2012}, SPADE \citep{hollis2006spade}, Unified State
Register of soil resources RUSSIA (Version 1.0. Moscow --- 2014),
National Database of Iran provided by the Tehran University, points from
the Dutch Soil Information System (BIS) prepared by Wageningen
Environmental Research, and others. We are also grateful to USA's NASA,
USGS and USDA agencies, European Space Agency Copernicus projects, JAXA
(Japan Aerospace Exploration Agency) for distributing vast amounts of
remote sensing data (especially MODIS, Landsat, Copernicus land products
and elevation data), and to the Open Source software developers of the
packages rgdal, sp, raster, caret, mlr, ranger, h2o and similar, and
without which predictive soil mapping would most likely not be possible.

This book has been inspired by the
\href{https://geocompr.robinlovelace.net/}{the Geocomputation with R
book}, an Open Access book edited by Robin Lovelace, Jakub Nowosad and
Jannes Muenchow. Many thanks to Robin Lovelace for helping with
rmarkdown and for giving some initial tips for compiling and organizing
book. The author is also grateful to the numerous software/package
developers, especially Edzer Pebesma, Roger Bivand, Robert Hijmans,
Markus Neteler, Tim Appelhans, and Hadley Wickham, that have enabled a
generation of researchers and applied projects.

Every effort has been made to trace copyright holders of the materials
used in these materials. Should we, despite all our efforts have
overlooked contributors please contact the author and we shall correct
this unintentional omission without any delay and will acknowledge any
overlooked contributions and contributors in future updates.

\textbf{Data availability}: All data used in this book is either
available through R packages or is available via the github repository.
If not mentioned otherwise, all code presented is available under the
\href{https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html}{GNU
General Public License v2.0}.

\textbf{Copyright}: © 2018 Hengl et al.

This work is licensed under a Creative Commons Attribution-ShareAlike
4.0 International License.

\hypertarget{introduction}{%
\chapter{Soil resource inventories and soil maps}\label{introduction}}

\emph{Edited by: Hengl T. \& MacMillan R.A.}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

This chapter presents a description and discussion of soils and
conventional soil inventories framed within the context of Predictive
Soil Mapping (PSM). Soils, their associated properties, and their
spatial distribution are the central focus of PSM. We discuss how the
products and methods associated with conventional soil mapping relate to
new, and emerging, methods of PSM and automated soil mapping. We discuss
similarities and differences, strengths and weaknesses of conventional
soil mapping (and its inputs and products) relative to PSM.

The universal model of soil variation presented in detail in Chapter
\ref{statistical-theory} is adopted as a framework for comparison of
conventional and PSM. Our aim is to show how the products and methods of
conventional soil mapping can complement, and contribute to, PSM and
equally, how the theories and methods of PSM can extend and strengthen
conventional soil mapping. PSM aims to implement tools and methods that
can be supportive of growth, change and improvement in soil mapping and
that can stimulate a rebirth and reinvigoration of soil inventory
activity globally.

\hypertarget{soils-and-soil-inventories}{%
\section{Soils and soil inventories}\label{soils-and-soil-inventories}}

\hypertarget{soil-a-definition}{%
\subsection{Soil: a definition}\label{soil-a-definition}}

Soil is a natural body composed of biota and air, water and minerals,
developed from unconsolidated or semi-consolidated material that forms
the topmost layer of the Earth's surface
\citep{chesworth2008encyclopedia}. The upper limit of the soil is either
air, shallow water, live plants or plant materials that have not begun
to decompose. The lower limit is defined by the presence of hard rock or
the lower limit of biologic activity \citep{richter1995deep, SSDS1993}.
Although soil profiles up to tens of meters depths can be found in some
tropical areas \citep{richter1995deep}, for soil classification and
mapping purposes, the lower limit soil is often arbitrarily set to 2 m
(\url{http://soils.usda.gov/education/facts/soil.html}). Soils are
rarely described to depths beyond 2 m and many soil sampling projects
put primary focus on the upper (0-100 cm) depths.

The chemical, physical and biological properties of the soil differ from
those of unaltered (unconsolidated) parent material from which the soil
is derived over a period of time under influence of climate, organisms
and relief effects. Soil should show a capacity to support life,
otherwise we are dealing with inert unconsolidated parent material.
Hence, for purposes of developing statistically based models to predict
soil properties using PSM, it proves useful to distinguish between
\emph{actual} and \emph{potential} soil areas (see further section
\ref{soil-covariates}).

A significant aspect of the accepted definition of soil is that it is
seen as a \emph{natural body} that merits study, description,
\emph{classification} and interpretation in, and of, itself. As a
\emph{natural body} a soil is viewed as an object that occupies space,
has defined physical dimensions and that is more than the sum of its
individual properties or attributes. This concept requires that all
properties of soils be considered collectively and simultaneously in
terms of a completely integrated natural body \citep{SSDS1993}. A
consequence of this, is that one must generally assume that all soil
properties covary in space in lockstep with specific named soils and
that different soil properties do not exhibit different patterns of
spatial variation independently relative to a named soil.

From a management point of view, soil can be seen from at least three
perspectives. It is a:

\begin{itemize}
\item
  \emph{Resource} of materials --- It contains quantities of
  unconsolidated materials, rock fragments, texture fractions, organic
  carbon, nutrients, minerals and metals, water and so on.
\item
  \emph{Stabilizing medium / ecosystem} --- It acts as a medium that
  supports both global and local processes from carbon and nitrogen
  fixation to retention and transmission of water, to provision of
  nutrients and minerals and so on.
\item
  \emph{Production system} --- Soil is the foundation for plant growth.
  In fact, it is the basis of all sustainable terrestrial ecosystem
  services. It is also a source of livelihood for people that grow crops
  and livestock.
\end{itemize}

For \citet{frossard2006function} there are six key functions of soil:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{food and other biomass production},
\item
  \emph{storage, filtering, and transformation of water, gases and
  minerals},
\item
  \emph{biological habitat and gene pool},
\item
  \emph{source of raw materials},
\item
  \emph{physical and cultural heritage} and
\item
  \emph{platform for man-made structures: buildings, highways}.
\end{enumerate}

Soil is the Earth's biggest carbon store containing 82\% of total
terrestrial organic carbon \citep{Lal2004Science}.

\hypertarget{soil-variables}{%
\subsection{Soil variables}\label{soil-variables}}

Knowledge about soil is often assembled and cataloged through \emph{soil
resource inventories}. Conventional soil resource inventories describe
the geographic distribution of \emph{soil bodies} i.e. \emph{polypedons}
\citep{Wysocki2005Geoderma}. The spatial distribution of soil properties
is typically recorded and described through reference to mapped soil
individuals and not through separate mapping of individual soil
properties. In fact, the definition of a soil map in the US Soil Survey
Manual specifically \emph{``excludes maps showing the distribution of a
single soil property such as texture, slope, or depth, alone or in
limited combinations; maps that show the distribution of soil qualities
such as productivity or erodibility; and maps of soil-forming factors,
such as climate, topography, vegetation, or geologic material''}
\citep{SSDS1993}.

In contrast to conventional soil mapping, PSM is primarily interested in
portraying, in the form of maps, the spatial distribution of \emph{soil
variables} --- measurable or descriptive attributes commonly collected
through field sampling and then either measured \emph{in-situ} or
\emph{a posteriori} in laboratory. Soil variables can be roughly grouped
into:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{quantities of some material}
  (\(y \in [0 \rightarrow +\infty]\));
\item
  \emph{transformed or standardized quantities} such as pH
  (\(y \in [-\infty \rightarrow +\infty]\))
\item
  \emph{relative percentages} such as mass or volume percentages
  (\(y \in [0 \rightarrow 1]\));
\item
  \emph{boolean values e.g.~showing occurrence and/or non-occurrence} of
  qualitative soil attributes or objects (\(y \in [0,1]\));
\item
  \emph{categories} (i.e.~factors) such as soil classes
  (\(y \in [a,b,\ldots,x]\));
\item
  \emph{probabilities} e.g.~probabilities of occurrence of some class or
  object (\(p(y) \in [0 \rightarrow 1]\)).
\item
  \emph{censored values} e.g.~depth to bedrock which is often observed
  only up to 2 m.
\end{enumerate}

The nature of a soil variable determines how the attribute is modeled
and presented on a map in PSM. Some soil variables are normally
described as discrete entities (or classes), but classes can also be
depicted as a continuous quantities on a map in the form of
probabilities or memberships
\citep{DeGruijter1997Geoderma, McBratney2003Geoderma, Kempen2009Geoderma, Odgers201130}.
For example, a binary soil variable (e.g.~the presence/absence of a
specific layer or horizon) can be modeled as a binomial random variable
with a logistic regression model. Spatial prediction (mapping) with this
model gives a map depicting (continuous) probabilities in the range of
0--1. These probabilities can be used to determine the most likely
presence/absence of a class at each prediction location, resulting in a
discrete representation of the soil attribute variation.

In that context, the aims of most soil resource inventories consist of
the identification, measurement, modelling, mapping and interpretation
of soil variables that represent transformed or standardized quantities
of some material, relative percentages, occurrence and/or non-occurrence
of qualitative attributes or objects, and/or soil categories.

\hypertarget{primary-and-secondary-soil-variables}{%
\subsection{Primary and secondary soil
variables}\label{primary-and-secondary-soil-variables}}

Soil properties can be \emph{primary} or \emph{inferred} (see further
section \ref{soil-variables-chapter}). Primary properties are properties
that can be measured directly in the field or in the laboratory.
Inferred properties are properties that cannot be measured directly (or
are difficult or too expensive to measure) but can be inferred from
primary properties, for example through pedotransfer functions
\citep{Wosten2001JH, wosten2013soil}. \citet{Dobos2006digital} also
distinguish between primary and secondary soil properties and
\emph{`functional'} soil properties representing \emph{soil functions}
or \emph{soil threats}. Such soil properties can be directly used for
financial assessment or for decision making. For example, soil organic
carbon content in grams per kilogram of soil is the primary soil
property, while organic carbon sequestration rate in kilograms per unit
area per year is a \emph{functional} soil property.

\hypertarget{soil-mapping}{%
\section{Soil mapping}\label{soil-mapping}}

\hypertarget{what-are-soil-resource-inventories}{%
\subsection{What are soil resource
inventories?}\label{what-are-soil-resource-inventories}}

Soil resource inventories describe the types, attributes and geographic
distributions of soils in a given area. They can consist of spatially
explicit maps or of non-spatial lists. Lists simply itemize the kinds
and amounts of different soils that occupy an area to address questions
about what soils and soil properties occur in an area. The resulting
answer is often not highly specific in space but rather presents a
mainly non-spatial itemization of soils and soil attributes expected to
occur in a bounded area. Maps attempt to portray, with some degree of
detail, the patterns of spatial variation in soils and soil properties,
within limits imposed by mapping scale and resources.

According to the USDA Manual of Soil Survey \citep{SSDS1993}, a soil
survey:

\begin{itemize}
\item
  describes the characteristics of the soils in a given area,
\item
  classifies the soils according to a standard system of classification,
\item
  plots the boundaries of the soils on a map, and
\item
  makes predictions about the behavior of soils.
\end{itemize}

The information collected in a soil survey helps in the development of
land-use plans and evaluates and predicts the effects of land use on the
environment. Hence, the different uses of the soils and how the response
of management affects them need to be considered.

In conventional soil mapping, the objects of study, whose spatial
distributions are portrayed on any resulting map, are \emph{soil
individuals} that are assumed to possess and exhibit a unique set of
soil properties with a defined range of values. A fundamental assumption
of conventional soil mapping is therefore that, if one maps the pattern
of spatial distribution of uniquely defined \emph{soil individuals}, one
can infer the patterns of spatial distribution of the \emph{soil
properties} associated with each defined individual. Thus, conventional
soil maps must, by definition, only map soil individuals and not
individual soil properties \citep{SSDS1993} and then subsequently infer
the distribution of soil properties from the mapped distribution of soil
individuals.

This attribute of conventional soil mapping represents a significant
difference compared to PSM, where the object of study is frequently an
individual soil property and the objective is to map the pattern of
spatial distribution of that property (over some depth interval)
independently from consideration of the spatial distribution of soil
individuals or other soil properties.

Soil maps give answers to three basic questions: (1) what is mapped?,
(2) what is the predicted value?, and (3) where is it? Thematic accuracy
of a map tells us how accurate predictions of targeted soil properties
are overall, while the spatial resolution helps us locate features with
some specified level of spatial precision.

\begin{rmdnote}
The most common output of a soil resource inventory is a \emph{soil
map}. Soil maps convey information about the geographic distribution of
named soil types in a given area. They are meant to help answer the
questions \emph{``what is here''} and \emph{``where is what''}
{[}@Burrough1998OUP{]}.
\end{rmdnote}

Any map is an abstraction and generalization of reality. The only
perfect one-to-one representation of reality is reality itself. To fully
describe reality one would need a model at 1:1 scale at which 1 m\(^2\)
of reality was represented by 1 m\(^2\) of the model. Since this is not
feasible, we condense and abstract reality in such a way that we hope to
describe the major differences in true space at a much reduced scale in
model (map) space. When this is done for soil maps, it needs to be
understood that the map cannot describe all of the variation that is
present in reality. It can only describe that portion of the total
variation that is systematic and has structure and occurs over distances
that are as large as, or larger than, the smallest area that can be
feasibly portrayed and described at any given scale. Issues of scale and
resolution are discussed in greater detail in chapter
\ref{downscaling-upscaling}.

An important functionality of PSM is the production and distribution of
maps depicting the spatial distribution of soils and, more specifically,
soil attributes. In this chapter we, therefore, concentrate on
describing processes for producing maps as spatial depictions of the
patterns of arrangement of soil attributes and soil types.

\hypertarget{soil-mapping-approaches-and-concepts}{%
\subsection{Soil mapping approaches and
concepts}\label{soil-mapping-approaches-and-concepts}}

As mentioned previously, spatial information about the distribution of
soil properties or attributes, i.e.~soil maps or GIS layers focused on
soil, is produced through soil resource inventories, also known as soil
surveys or soil mapping projects
\citep{Burrough1971, Avery1987, Wysocki2005Geoderma, Legros2006SP}. The
main idea of soil survey is, thus, production and dissemination of soil
information for an area of interest usually to address a specific
question or questions of interest i.e.~production of soil maps and soil
geographical databases. Although soil surveyors are usually not
\emph{per se} responsible for usage of soil information, how soil survey
information is used is increasingly important.

In statistical terms, the main objective of soil mapping is to describe
the spatial variability i.e.~spatial complexity of soils, then represent
this complexity using maps, summary measures, mathematical models and
simulations. Some known sources of spatial variability in soil variables
are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Natural spatial variability in 2D (different at various scales),
  mainly due to climate, parent material, land cover and land use};
\item
  \emph{Variation by depth};
\item
  \emph{Temporal variation due to regular or periodic changes in the
  ecosystem};
\item
  \emph{Measurement error (in situ or in lab)};
\item
  \emph{Spatial location error};
\item
  \emph{Small scale variation};
\end{enumerate}

\begin{rmdnote}
In statistical terms, the main objective of soil mapping is to describe
the spatial complexity of soils, then represent this complexity using
maps, summary measures, mathematical models and simulations. From the
application point of view, the main application objective of soil
mapping is to accurately predict response of a soil(-plant) ecosystem to
various soil management strategies.
\end{rmdnote}

Soil mappers do their best to try explain the first two items above and
minimize, or exclude from modelling, the remaining components: temporal
variation, measurement error, spatial location error and small scale
variation.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_soil_crop_model_scheme} 

}

\caption{Inputs to soil-plant, soil-hydrology or soil-ecology models and their relationship.}\label{fig:soil-crop-model-scheme}
\end{figure}

From the application point of view, the main objective of soil mapping
is to accurately predict soil properties and their response to possible
or actual management practices (Fig. \ref{fig:soil-crop-model-scheme}).
In other words, if the soil mapping system is efficient, we should be
able to accurately predict the behavior of soil-plant, soil-hydrology or
similar ecosystems to various soil management strategies, and hence
provide useful advice to agronomists, engineers, environmental modelers,
ecologists and similar.

We elect here to recognize two main variants of soil mapping which we
refer to as \emph{conventional soil mapping} and \emph{pedometric} or
\emph{predictive soil mapping} as described and discussed below (Fig.
\ref{fig:comparison-dsm}).

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Table_comparison_DSM} 

}

\caption{Comparison between traditional (primarily expert-based) and automated (data-driven) soil mapping.}\label{fig:comparison-dsm}
\end{figure}

\hypertarget{soil-mapping-theory}{%
\subsection{Theoretical basis of soil mapping: in context of the
universal model of spatial variation}\label{soil-mapping-theory}}

Stated simply, \emph{``the scientific basis of soil mapping is that the
locations of soils in the landscape have a degree of predictability''}
\citep{Miller1979}. According to the USDA Soil Survey Manual,
\emph{``The properties of soil vary from place to place, but this
variation is not random. Natural soil bodies are the result of climate
and living organisms acting on parent material, with topography or local
relief exerting a modifying influence and with time required for
soil-forming processes to act. For the most part, soils are the same
wherever all elements of the five factors are the same. Under similar
environments in different places, soils are similar. This regularity
permits prediction of the location of many different kinds of soil''}
\citep{SSDS1993}. \citet{Hudson2000SSSAJ} considers that this
\emph{soil-landscape paradigm} provides the fundamental scientific basis
for soil survey.

In the most general sense, both conventional soil mapping and PSM
represent ways of applying the \emph{soil-landscape paradigm} via the
universal model of spatial variation, which is explained in greater
detail in Chapter \ref{statistical-theory}.
\citet[p.133]{Burrough1998OUP} described the universal model of soil
variation as a special case of the universal model of spatial variation.
This model distinguishes between three major components of soil
variation: (1) deterministic component (trend), (2) spatially correlated
component and (3) pure noise.

\begin{equation}
Z({\bf{s}}) = m({\bf{s}}) + \varepsilon '({\bf{s}}) + \varepsilon ''({\bf{s}})
\label{eq:univ-var}
\end{equation}

where \(\bf{s}\) is two-dimensional location, \(m({\bf{s}})\) is the
deterministic component, \(\varepsilon '({\bf{s}})\) is the spatially
correlated stochastic component and \(\varepsilon ''({\bf{s}})\) is the
pure noise (micro-scale variation and measurement error).

\begin{rmdnote}
The \emph{universal model of soil variation} assumes that there are
three major components of soil variation: (1) the deterministic
component (function of covariates), (2) spatially correlated component
(treated as stochastic) and (3) pure noise.
\end{rmdnote}

The deterministic part of the equation describes that part of the
variation in soils and soil properties that can be explained by
reference to some model that relates observed and measured variation to
readily observable and interpretable factors that control or influence
this spatial variation. In conventional soil mapping, this model is the
empirical and knowledge-based \emph{soil-landscape paradygm}
\citep{Hudson2000SSSAJ}. In PSM, a wide variety of statistical, and
machine learning, models have been used to capture and apply the
soil-landscape paradigm in a quantitative and optimal fashion:

\begin{equation}
S = f (cl, o, r, p, t)
\label{eq:clorpt}
\end{equation}

where \(S\) stands for soil (properties and classes), \(cl\) for
climate, \(o\) for organisms (including humans), \(r\) is relief, \(p\)
is parent material or geology and \(t\) is time. The Eq. \eqref{eq:clorpt}
is the CLORPT model originally presented by Jenny
\citeyearpar{jenny1994factors}.

\citet{MCBRATNEY20033} have further conceptualized the so-called
\emph{``scorpan''} model in which soil property is modeled as a function
of:

\begin{itemize}
\item
  (auxiliary) \textbf{s}oil classes or properties,
\item
  \textbf{c}limate,
\item
  \textbf{o}organisms, vegetation or fauna or human activity,
\item
  \textbf{r}elief,
\item
  \textbf{p}arent material,
\item
  \textbf{a}ge i.e.~the time factor,
\item
  \textbf{n} space, spatial conntext or spatial position,
\end{itemize}

Pedometric models are quantitative in that they capture relationships
between observed soils, or soil properties, and controlling
environmental influences (as represented by environmental co-variates)
using statistically-formulated expressions. Pedometric models are seen
as optimum because, by design, they minimize the variance between
observed and predicted values at all locations with known values. So, no
better model of prediction exists for that particular set of observed
values at that specific set of locations.

Conventional soil mapping has a long history of effective development
and application of empirical, knowledge-based, soil landscape models to
predict how soil classes vary spatially across landscapes. Such models
can be criticized, however, for being neither quantitative nor optimal.

Our essential point is that both conventional and pedometric soil
mapping use models to explain the deterministic part of the spatial
variation in soils and soil properties and these models differ mainly in
terms of whether they are empirical and subjective (conventional) or
quantitative and objective (pedometric). Both can be effective and the
empirical and subjective models based on expert knowledge have, until
recently, proven to be the most cost effective and widely applied for
production of soil maps by conventional means.

The spatially correlated part of the observed variation is that part
that shows spatial structure that lends itself to prediction through
interpolation but that is not explainable, or easily explained, through
use of a deterministic model that relates observed values to controlling
factors. This part of the variation is typically modeled in pedometric
mapping using geostatistics and kriging to interpolate, in an optimal
manner, between point locations with known values
\citep{goovaerts2001geostatistical, McBratney2003Geoderma}.

It can be argued that conventional soil mapping has an analogue to
kriging in situations where there is no clearly apparent relationship
between observed values and readily observable controlling environmental
variables. In such instances, conventional soil mappers typically resort
to an approach in which they make as many closely spaced observations as
feasible and then manually \emph{``interpolate''} between these
locations of known soils or soil properties to locate boundaries
indicative of locations of significant change in soils or soil
properties. In the vernacular of soil surveyors this is often referred
to as \emph{``digging it out''} in which a pattern that is not readily
apparent or visible is revealed through interpolation between closely
spaced observations. So, under some circumstances, conventional soil
surveyors do implement an analogue of spatial interpolation to describe
patterns of variation in soils where such patterns are not readily
related to a clear soil-landscape model.

\begin{rmdnote}
In its essence, the objective of PSM is to produce optimal unbiased
predictions of a mean value at some new location along with the
uncertainty associated with the prediction, at the finest possible
resolution.
\end{rmdnote}

There is one way in which PSM differs significantly from conventional
soil mapping in terms of the universal model of soil variation. This is
in the use of statistics and machine learning to quantitatively correct
for error in predictions, defined as the difference between predicted
and observed values at locations with known values. Conventional soil
mapping has no formal or quantitative mechanism for correcting an
initial set of predicted values by computing the difference between
predicted and observed values at sampled locations and then correcting
initial values at all locations in response to these observed
differences. PSM uses geostatistics to determine (via the
semi-variogram) if the differences between predicted and observed values
(the residuals) exhibit spatial structure (e.g.~are predictable). If
they do exhibit spatial structure, then it is useful and reasonable to
interpolate the computed error at known locations to predict the likely
magnitude of error of predictions at all locations
\citep{hengl2007regression}. This interpolated prediction error can then
be systematically subtracted from (or added to) the original predicted
value to correct for errors in the initial predictions that are
systematic and spatially correlated. This \emph{``after the fact''}
correction of initial predictions is an aspect of PSM that represents an
improvement over conventional soil mapping methods and that conventional
methods would do well to emulate.

Neither conventional soil mapping nor PSM can do more than simply
describe and quantify the amount of variation that is not predictable
and has to be treated as pure noise. Conventional soil maps can be
criticized for ignoring this component of the total variation and
typically treating it as if it did not exist. For many soil properties,
short range, local variation in soil properties that cannot be explained
by either the deterministic or stochastic components of the universal
model of soil variation can often approach, or even exceed, a
significant proportion (e.g.~30--40\%) of the total observed range of
variation in any given soil property. Such variation is simply not
mappable but it exists and should be identified and quantified. We do
our users and clients a disservice when we fail to alert them to the
presence, and the magnitude, of spatial variation that is not
predictable. In cases where the local spatial variation is not
predictable (or mappable) the best estimate for any property of interest
is the mean value for that local area.

\hypertarget{conventional-mapping}{%
\subsection{Traditional (conventional) soil
mapping}\label{conventional-mapping}}

Traditional soil resource inventories are largely based on manual
application of expert tacit knowledge through the soil-landscape
paradigm \citep{Burrough1971, Hudson2000SSSAJ}. In this approach, soil
surveyors develop and apply conceptual models of where and how soils
vary in the landscape through a combination of field inspections to
establish spatial patterns and photo-interpretation to extrapolate the
patterns to similar portions of the landscape (Fig.
\ref{fig:soilsurvey-scheme}). Traditional soil mapping procedures mainly
address the deterministic part of the universal model of soil variation.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_soilsurvey_scheme} 

}

\caption{Typical soil survey phases and intermediate and final products.}\label{fig:soilsurvey-scheme}
\end{figure}

Conventional (traditional) manual soil mapping typically adheres to the
following sequence of steps, with minor variations
\citep{McBratney2003Geoderma}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Specify the objective(s) to be served by the soil survey and
  resulting map};
\item
  \emph{Identify which attributes of the soil or land need to be
  observed, described and mapped to meet the specified objectives};
\item
  \emph{Identify the minimum sized area that must be described and the
  corresponding scale of mapping to meet the specified objectives};
\item
  \emph{Collate and interpret existing relevant land resource
  information (geology, vegetation, climate, imagery) for the survey
  area};
\item
  \emph{Conduct preliminary field reconnaissance and use these
  observations to construct a preliminary legend of conceptual mapping
  units (described in terms of soil individuals)};
\item
  \emph{Apply preliminary conceptual legend using available source
  information to delineate initial map unit boundaries (pre-typing)};
\item
  \emph{Plan and implement a field program to collect samples and
  observations to obtain values of the target soil attributes (usually
  classes) at known locations to test and refine initial conceptual
  prediction models};
\item
  \emph{Using field observations, refine the conceptual models and
  finalize map unit legends and boundaries to generate conventional
  area--class soil maps};
\item
  \emph{Conduct a field correlation exercise to match mapping with
  adjacent areas and to confirm mapping standards were adhered to};
\item
  \emph{Select and analyse representative soil profile site data to
  characterize each mapped soil type and soil map unit};
\item
  \emph{Prepare final documentation that describes all mapped soils and
  soil map units (legends) according to an accepted format};
\item
  \emph{Publish and distribute the soil information in the form of maps,
  geographical databases and reports};
\end{enumerate}

Expert knowledge about soil-landform patterns is generally used to
produce manually drawn polygon maps that outline areas of different
dominant soils or combinations of soils --- \emph{soil map units} (see
Figs. \ref{fig:smu-aggregation} and
\ref{fig:from-photointerpretation-to-soilmap}). Soil map units (polygons
of different soil types) are described in terms of the composition of
soil classes (and often also landscape attributes) within each unit,
with various soil physical and chemical variables attached to each
class. Most commonly, the objective of conventional soil mapping is to
delineate recognizable portions of a landscape (soil--landform units) as
polygons in which the variation of soils and soil properties is
describable and usually (but not always) more limited than between
polygons. Because most soil mapping projects have limited resources and
time, soil surveyors can not typically afford to survey areas in great
detail (e.g.~1:5000) so as to map actual \emph{polypedons}. As a
compromise, the survey team generally has to choose some best achievable
target scale ( e.g.~1:10,000 - 1:50,000). Maps produced at some initial
scale can be further generalized, depending on the application and users
demands \citep{Wysocki2005Geoderma}.

\begin{figure}[t]

{\centering \includegraphics[width=0.85\linewidth]{figures/Fig_SMU_aggregation} 

}

\caption{Three basic conceptual *scales* in soil mapping: (left) *most detailed scale* showing the actual distribution of soil *bodies*, (center) *target scale* i.e. scale achievable by the soil survey budget, (right) *generalized intermediate scale or coarse resolution maps*. In a conventional soil survey, soils are described and conceptualized as groups of similar pedons (smallest elements of 1–10 square-m), called *“polypedons”* — the smallest mappable entity. These can then be further generalized to soil map units, which can be various combinations (systematic or random) of dominant and contrasting soils (inclusions).}\label{fig:smu-aggregation}
\end{figure}

Where variation within a polygon is systematic and predictable, the
pattern of variation in soils within any given polygon is often
described in terms of the most common position, or positions, in the
landscape occupied by each named soil class. In other cases, soil
patterns are not clearly related to systematic variations in observable
landscape attributes and it is not possible to describe where each named
soil type is most likely to occur within any polygon or why.

Conventional soil mapping has some limitations related to the fact that
mapping concepts (mental models) are not always applied consistently by
different mappers. Application of conceptual models is largely manual
and it is difficult to automate. In addition, conventional soil survey
methods differ from country to country, and even within a single region,
depending largely on the scope and level-of-detail of the inventory
\citep{Schelling1970Geoderma, SSS1983USDA, Rossiter2001}. The key
advantages of conventional soil maps, on the other hand, are that:

\begin{itemize}
\item
  \emph{they portray the spatial distribution of stable, recognizable
  and repeating patterns of soils that occupy identifiable portions of
  the landscape}, and
\item
  \emph{these patterns can be extracted from legends and maps to model
  (predict) the most likely soil at any location in the landscape using
  expert knowledge alone} \citep{Zhu2001}.
\end{itemize}

Resource inventories, and in particular soil surveys, have been
notoriously reluctant, or unable, to provide objective quantitative
assessments of the accuracy of their products. For example, most soil
survey maps have only been subjected to qualitative assessments of map
accuracy through visual inspection and subjective correlation exercises.
In the very few examples of quantitative evaluation
\citep{Marsman1986ALTERRA, Finke2006Elsevier}, the assessments have
typically focused on measuring the degree with which predictions of soil
classes at specific locations on a map, or within polygonal areas on a
map, agreed with on-the-ground assessments of the soil class at these
same locations or within these same polygons. Measurement error can be
large in assessing the accuracy of soil class maps.
\citet{MacMillan2005CJSS}, for example, demonstrated that experts
disagreed with each other regarding the correct classification of
ecological site types at the same locations about as often as they
disagreed with the classifications reported by a map produced using a
predictive model.

Assessments of map accuracy that compare the ability of a map to predict
classes of soil at specific locations are insufficient to assess the
ability of a map to predict spatial variation in soil properties. Maps
are increasingly used to predict \emph{soil functional properties} at
specific (point) locations. In traditional soil mapping, all properties
are tied to soil classes and all properties are assumed to vary in
exactly the same manner as the observed variation in soil types. To
predict the value of a soil property at a location, one would first
predict the soil class most likely to occupy that location then infer
the soil property based on the predicted soil class. This has
disadvantages when soil properties do not covary exactly with soil
classes and when spatial variation in soil classes is difficult to
predict.

\hypertarget{variants-of-soil-maps}{%
\subsection{Variants of soil maps}\label{variants-of-soil-maps}}

In the last 20--30 years, soil maps have evolved from purely 2D polygon
maps showing the distribution of soil poly-pedons i.e.~named soil
classes, to dynamic 3D maps representing predicted or simulated values
of various primary or inferred soil properties and/or classes (Fig.
\ref{fig:soilmap-types}). Examples of 2D+T and/or 3D+T soil maps are
less common but increasingly popular (see e.g. \citet{Rosenbaum2012WRCR}
and \citet{Gasch2015SPASTA}). In general, we expect that demand for
spatio-temporal soil data is likely to grow.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_soilmap_types} 

}

\caption{Classification of types of soil maps based on spatial representation and variable type.}\label{fig:soilmap-types}
\end{figure}

\begin{rmdnote}
A soil map can represent 2D, 3D, 2D+T and/or 3D+T distribution of
quantitative soil properties or soil classes. It can show predicted or
simulated values of target soil properties and/or classes, or inferred
soil-functions.
\end{rmdnote}

The spatial model increasingly used to represent soil spatial
information is the \emph{gridded or raster data model}, where most of
the technical properties are defined by the grid cell size i.e.~the
ground resolution. In practice, vector-based polygon maps can be
converted to gridded maps and \emph{vice versa}, so in practical terms
there is really no meaningful difference between the two models. In this
book, to avoid any ambiguity, when mentioning soil maps we will often
refer to the spatio-temporal reference and support size of the maps at
the finest possible level of detail. Below, for example, is a full list
of specifications attached to a \emph{soil map} produced for the African
continent \citep{Hengl2015AfSoilGrids250m}:

\begin{itemize}
\item
  \emph{target variable}: soil organic carbon in permille;
\item
  \emph{values presented}: predictions (mean value);
\item
  \emph{prediction method}: 3D regression-kriging;
\item
  \emph{prediction depths}: 6 standard layers (0--5, 5--15, 15--30,
  30--60, 60--100, 100--200 cm);
\item
  \emph{temporal domain (period)}: 1950--2005;
\item
  \emph{spatial support (resolution) of covariate layers}: 250 m;
\item
  \emph{spatial support of predictions}: point support (center of a grid
  cell);
\item
  \emph{amount of variation explained by the spatial prediction model}:
  45\%;
\end{itemize}

Until recently, maps of individual soil properties, or of soil functions
or soil interpretations, were not considered to be true soil maps, but
rather, to be single-factor derivative maps or interpretive maps. This
is beginning to change and maps of the spatial pattern of distribution
of individual soil properties are increasingly being viewed as a
legitimate form of soil mapping.

\hypertarget{pedometric-mapping}{%
\subsection{Predictive and automated soil
mapping}\label{pedometric-mapping}}

In contrast to traditional soil mapping, which is primarily based on
applying qualitative expert knowledge, the emerging, \emph{`predictive'}
approach to soil mapping is generally more quantitative and data-driven
and based on the use of statistical methods and technology
\citep{grunwald2005environmental, Lagacherie2006Elsevier, Hartemink2008Springer, Boettinger2010Springer}.
The emergence of new soil mapping methods is undoubtedly a reflection of
new developing technologies and newly available global data layers,
especially those that are free and publicly distributed such as MODIS
products, SRTM DEM and similar (Fig. \ref{fig:new-technologies}). PSM
can be compared to, and shares similar concepts with, other applications
of statistics and machine learning in physical geography, for example
Predictive Vegetation Mapping \citep{Fran01, Hengl2018PNV}.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_new_technologies} 

}

\caption{Evolution of digital soil mapping parallels the emergence of new technologies and global, publicly available data sources.}\label{fig:new-technologies}
\end{figure}

The objective of using pedometric techniques for soil mapping is to
develop and apply objective and optimal sets of rules to predict the
spatial distribution of soil properties and/or soil classes. Most
typically, rules are developed by fitting statistical relationships
between digital databases representing the spatial distribution of
selected environmental covariates and observed instances of a soil class
or soil property at geo-referenced sample locations. The environmental
covariate databases are selected as predictors of the soil attributes on
the basis of either expert knowledge of known relationships to soil
patterns or through objective assessment of meaningful correlations with
observed soil occurrences. The whole process is amenable to complete
automation and documentation so that it allows for \emph{reproducible
research} (read more in:
\url{http://en.wikipedia.org/wiki/Reproducibility}).

Pedometric soil mapping typically follows six steps as outlined by
\citet{McBratney2003Geoderma}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Select soil variables (or classes) of interest and suitable
  measurement techniques (decide what to map and describe)};
\item
  \emph{Prepare a sampling design (select the spatial locations of
  sampling points and define a sampling intensity)};
\item
  \emph{Collect samples in the field and then estimate values of the
  target soil variables at unknown locations to test and refine
  prediction models};
\item
  \emph{Select and implement the most effective spatial prediction (or
  extrapolation) models and use these to generate soil maps};
\item
  \emph{Select the most representative data model and distribution
  system};
\item
  \emph{Publish and distribute the soil information in the form of maps,
  geographical databases and reports (and provide support to users)};
\end{enumerate}

\begin{rmdnote}
Differences among \emph{conventional soil mapping}, \emph{digital soil
mapping} or \emph{technology-driven or data-driven mapping} relate
primarily to the degree of use of robust statistical methods in
developing prediction models to support the mapping process.
\end{rmdnote}

We here recognize four classes of soil mapping methods (B, C, D and E in
Fig. \ref{fig:pedometric-mapping-vs-dsm}) which all belong to a
continuum of \emph{digital soil mapping} methods
\citep{malone2016using, mcbratney2018pedometrics}. We promote in this
book specifically the Class E soil mapping approach i.e.~which we refer
to as the \emph{predictive} and/or \emph{automated soil mapping}.

\begin{figure}[t]

{\centering \includegraphics[width=0.85\linewidth]{figures/Fig_pedometric_mapping_vs_DSM} 

}

\caption{A classification of approaches to soil mapping: from purely expert driven, to various types of *digital* soil mapping.}\label{fig:pedometric-mapping-vs-dsm}
\end{figure}

Some key advantages of the pedometric (statistical) approach to soil
mapping are that it is: objective, systematic, repeatable, updatable and
represents an optimal expression of statistically validated
understanding of soil-environmental relationships in terms of the
currently available data.

There are, of course, also limitations with pedometric methods that
still require improvement. Firstly, the number of accurately
georeferenced locations of reliable soil observations (particularly with
analytical data) is often not sufficient to completely capture and
describe all significant patterns of soil variation in an area. There
may be too few sampled points and the exact location of available point
data may not be well recorded. In short, data-driven soil mapping is
field-data demanding and collecting field data can require significant
expenditures of time, effort and money.

With legacy soil point data the sampling design, or rationale, used to
decide where to locate soil profile observation or sampling points is
often not clear and may vary from project to project or point to point.
Therefore there is no guarantee that available point data are actually
representative of the dominant patterns and soil forming conditions in
any area. Points may have been selected and sampled to capture
information about unusual conditions or to locate boundaries at points
of transition and maximum confusion about soil properties. Once a soil
becomes recognized as being widely distributed and dominant in the
landscape, many conventional field surveys elect not to record
observations when that soil is encountered, preferring to focus instead
on recording unusual or transition soils. Thus the population of
available legacy soil point observations may not be representative of
the true population of soils, with some soils being either over or
under-represented.

\begin{rmdnote}
We define automated or predictive soil mapping as a data-driven approach
to soil mapping with little or no human interaction, commonly based on
using optimal (where possible) statistical methods that elucidate
relationships between target soil variables (sampled in the field and
geolocated) and covariate layers, primarily coming from remote sensing
data.
\end{rmdnote}

A second key limitation of the automated approach to soil mapping is
that there may be no obvious relationship between observed patterns of
soil variation and the available environmental covariates. This may
occur when a soil property of interest does, indeed, strongly covary
with some mappable environmental covariate (e.g.~soil clay content with
airborne radiometric data) but data for that environmental covariate are
not available for an area. It may also transpire that the pattern of
soil variation is essentially not predictable or related to any known
environmental covariate, available or not. In such cases, only closely
spaced, direct field observation and sampling is capable of detecting
the spatial pattern of variation in soils because there is no, or only a
very weak, correlation with available covariates
\citep{kondolf2003tools}.

\hypertarget{comparison-conventional-pm}{%
\subsection{Comparison of conventional and pedometric soil
mapping}\label{comparison-conventional-pm}}

There has been a tendency to view conventional soil mapping and
automated soil mapping as competing and non-complementary approaches. In
fact, they share more similarities than differences. Indeed, they can be
viewed as end members of a logical continuum. Both rely on applying the
underlying idea that the distribution of soils in the landscape is
largely predictable (the deterministic part) and, where it is not
predictable, it must be revealed through intensive observation, sampling
and interpolation (the stochastic part).

In most cases, the basis of prediction is to relate the distribution of
soils, or soil properties, in the landscape to observable environmental
factors such as topographic position, slope, aspect, underlying parent
material, drainage conditions, patterns of climate, vegetation or land
use and so on. This is done manually and empirically (subjectively) in
conventional soil survey, while in automated soil mapping it is done
objectively and mostly in an automated fashion. At the time it was
developed, conventional soil survey lacked both the digital data sets of
environmental covariates and the statistical tools required to
objectively analyze relationships between observed soil properties and
environmental covariates. So, these relationships were, of necessity,
developed empirically and expressed conceptually as expert knowledge.

More recently, it has become increasingly possible to obtain both
environmental covariate data and field point soil observations in
georegistered and digital format and to analyze and express
relationships objectively and optimally, using statistical methods
\citep{Pebesma2006TiG, McBratney2011HSS}. Where the relationship between
available environmental covariates and observed soil variation is weak,
as in featureless plains or complex flood plains, both methods rely on
similar approaches of using densely spaced point observations to reveal
the spatial patterns. Conventional soil mappers \emph{`dig out'} these
patterns while digital soil mappers interpolate using geostatistical
procedures, but here too the two methods are quite analogous. Hard facts
(point data and covariates) can often be beneficially enhanced using
soft data (expert knowledge).

In summary, we suggest that next generation soil surveyors will
increasingly benefit from having a solid background in statistics and
computer science, especially in Machine Learning and A.I. However,
effective selection and application of appropriate statistical sampling
and analysis techniques can also benefit from consideration of expert
knowledge.

\hypertarget{top-down}{%
\subsection{Top-down versus bottom-up approaches: subdivision versus
agglomeration}\label{top-down}}

There are two fundamentally different ways to approach the production of
soil maps for areas of larger extent, whether by conventional or
pedometric means. For ease of understanding we refer to these two
alternatives here as \emph{``bottom-up''} versus \emph{``top-down''}.
\citet{Rossiter2001} refers to a synthetic approach that he calls the
\emph{``bottom-up''} or \emph{``name and then group''} approach versus
an analytic approach that he calls the \emph{``top-down''} or
\emph{``divide and then name''} approach.

The bottom up approach is agglomerative and synthetic. It is implemented
by first collecting observations and making maps at the finest possible
resolution and with the greatest possible level of detail. Once all
facts are collected and all possible soils and soil properties, and
their respective patterns of spatial distribution, are recorded, these
detailed data are generalized at successively coarser levels of
generalization to detect, analyse and describe broader scale (regional
to continental) patterns and trends. The fine detail synthesized to
extract broader patterns leads to the identification and formulation of
generalizations, theories and concepts about how and why soils organize
themselves spatially. The bottom-up approach makes little, to no, use of
generalizations and theories as tools to aid in the conceptualization
and delineation of mapping entities. Rather, it waits until all the
facts are in before making generalizations. The bottom-up approach tends
to be applied by countries and organizations that have sufficient
resources (people and finances) to make detailed field surveys feasible
to complete for entire areas of jurisdiction. Soil survey activities of
the US national cooperative soil survey (NCSS) primarily adopt this
bottom-up approach. Other smaller countries with significant resources
for field surveys have also adopted this approach (e.g.~Netherlands,
Denmark, Cuba). The bottom-up approach was, for example, used in the
development and elaboration of the US Soil Taxonomy system of
classification and of the US SSURGO (1:20,000) and STATSGO (1:250,000)
soil maps \citep{ZHONG2011491}.

The top-down approach is synoptic, analytic and divisive. It is
implemented by first collecting just enough observations and data to
permit construction of generalizations and theoretical concepts about
how soils arrange themselves in the landscape in response to controlling
environmental variables. Once general theories are developed about how
environmental factors influence how soils arrange themselves spatially,
these concepts and theories are tested by using them to predict what
types of soils are likely to occur under similar conditions at
previously unvisited sites. The theories and concepts are adjusted in
response to initial application and testing until such time as they are
deemed to be reliable enough to use for production mapping. Production
mapping proceeds in a divisive manner by stratifying areas of interest
into successively smaller, and presumably more homogeneous, areas or
regions through application of the concepts and theories to available
environmental data sets. The procedures begin with a synoptic overview
of the environmental conditions that characterize an entire area of
interest. These conditions are then interpreted to impose a hierarchical
subdivision of the whole area into smaller, and more homogeneous
subareas. This hierarchical subdivision approach owes its origins to
early Russian efforts to explain soil patterns in terms of the
geographical distribution of observed soils and vegetation. The top-down
approach tends to be applied preferentially by countries and agencies
that need to produce maps for very large areas but that lack the people
and resources to conduct detailed field programs everywhere (see e.g.
\citet{Henderson2004Geoderma} and \citet{Mansuy201459}). Many of these
divisive hierarchical approaches adopt principals and methods associated
with the ideas of Ecological Land Classification
\citep{rowe1981ecological} (in Canada) or Land Systems Mapping
\citep{gibbons1964study, rowan1990land} (in Australia).

As observed by \citet{Rossiter2001} \emph{``neither approach is usually
applied in its pure form''} and most approaches to soil mapping use both
approaches simultaneously, to varying degrees. Similarly, it can be
argued that PSM provides support for both approaches to soil mapping.
PSM implements two activities that bear similarities to bottom-up
mapping. Firstly, PSM uses \emph{all} available soil profile data
globally as input to initial global predictions at coarser resolutions
(\emph{``top-down''} mapping). Secondly, PSM is set up to ingest finer
resolution maps produced via detailed \emph{``bottom-up''} mapping
methods and to merge these more detailed maps with initial,
coarser-resolution predictions \citep{ramcharan2018soil}.

\hypertarget{sources-of-soil-data-for-soil-mapping}{%
\section{Sources of soil data for soil
mapping}\label{sources-of-soil-data-for-soil-mapping}}

\hypertarget{soil-data-sources-targeted-by-psm}{%
\subsection{Soil data sources targeted by
PSM}\label{soil-data-sources-targeted-by-psm}}

PSM aims at integrating and facilitating exchange of global soil data.
Most (global) soil mapping initiatives currently rely on capture and use
of \emph{legacy soil data}. This raises several questions. What is meant
by legacy soil data? What kinds of legacy soil data exist? What are the
advantages and limitations of the main kinds of legacy soil data?

In its most general sense, a legacy is something of value bequeathed
from one generation to the next. It can be said that global soil legacy
data consists of the sum of soil data and knowledge accumulated since
the first soil investigations 100 or more years ago. More specifically,
the concept of a legacy is usually accompanied by an understanding that
there is an obligation and duty of the recipient generation to not
simply protect the legacy but to make positive and constructive use of
it.

The idea is that a legacy is not a priceless artifact, to be hidden away
somewhere for static preservation and protection, but a living resource
to be invested, improved upon, and grown for the sake of successive
generations. The intention of any PSM framework is therefore not simply
to rescue and protect the existing accumulation of legacy soil data, but
to put it to new and beneficial uses, so that its value is increased and
not just preserved.

\begin{rmdnote}
Four main groups of legacy data of interest for global soil mapping are:
(1) soil field records, (2) soil polygon maps and legends, (3)
soil-landscape diagrams and sketches, (d) soil (profile) photographs.
\end{rmdnote}

In the context of soils, legacy soil data consist of the sum total of
data, information and knowledge about soils accumulated since soils were
first studied as independent natural objects. At its broadest, this
includes information about soil characteristics and classification, soil
use and management, soil fertility, soil bio-chemistry, soil formation,
soil geography and many other subdisciplines.

In the more focused context of PSM, we are primarily interested in four
main kinds of legacy soil data:

\begin{itemize}
\item
  \emph{Soil field observations and measurements} --- Observations and
  analytical data obtained for soils at point locations represent a
  primary type of legacy soil data. These point source data provide
  objective evidence of observed soil characteristics at known locations
  that can be used to develop knowledge and rules about how soils, or
  individual soil properties, vary across the landscape. The quality and
  precision of these data can vary greatly. Some data points might be
  accurately located, or geo-referenced, while others might have very
  coarse geo-referencing (for example coordinates rounded in decimal
  minutes or kilometers). Some point data might only have a rough
  indication of the location obtained from a report (for example
  \emph{`2 km south of village A'}), or might even lack geo-referencing.
  Soil profile descriptions can be obtained from pits (relatively
  accurate) or auger bores (less accurate). Soil attributes can be
  determined in the laboratory (relatively accurate) or by
  hand-estimation in the field (less accurate). Legacy point data is
  characterized by great variation in precision, accuracy, completeness,
  relevance and age. It needs to be used with caution and with
  understanding of how these issues affect its potential use.
\item
  \emph{Soil (polygon) maps and legends} --- Soil maps and legends are
  one of the primary means by which information and knowledge about how
  soils vary spatially have been observed, distilled, recorded and
  presented to users. Soil maps provide lists, or inventories, of soils
  that occur in mapped regions, illustrate the dominant spatial patterns
  displayed by these listed soils and provide information to
  characterize the main properties of these soils. Soil maps can
  themselves be used as sources of evidence to develop knowledge and
  quantitative rules about how soils, or individual soil properties,
  vary across the landscape. On the other hand, similar to soil
  observations, soil maps also can exhibit significant errors with
  respect to measurement, classification, generalization, interpretation
  and spatial interpolation.
\item
  \emph{Tacit expert soil knowledge} --- In the context of soils, tacit
  expert knowledge represents a diffuse domain of information about the
  characteristics and spatial distribution of soils that has not been
  captured and recorded formally or explicitly. It may reside in the
  minds and memories of experts who have conducted field and laboratory
  studies but have been unable to record all their observations in a
  formal way. It may be captured informally and partially in maps,
  legends, conceptual diagrams, block diagrams, generalized decision
  rules and so on. Tacit knowledge represents soft data, in comparison
  to the more hard data of point observations and maps.
\item
  \emph{Photographs} --- Traditional soil survey is heavily based on use
  of aerial photographs. Older aerial photographs (even if not
  stereoscopic) are an important resource for land degradation
  monitoring and vegetation succession studies. Field photographs of
  soil profiles, soil sites and soil processes are another important
  source of information that has been under-used for soil mapping. ISRIC
  for example has an archive of over 30 thousand photographs from
  various continents. Most of these can be geo-coded and distributed via
  image sharing web-services such as WikiMedia, Instagram and/or Flickr.
  In theory, even a single photograph of a soil profile could be used to
  (automatically?) identify soil types, even extract analytical soil
  properties. Although it is very likely that prediction by using
  photographs only would be fairly imprecise, such data could
  potentially help fill large gaps for areas where there are simply no
  soil observations.
\end{itemize}

\hypertarget{field-observations}{%
\subsection{Field observations of soil
properties}\label{field-observations}}

Perhaps the most significant, certainly the most reliable, inputs to
soil mapping are the \emph{field observations} (usually at point
locations) of descriptive and analytical soil properties
\citep{SSDS1993, Schoeneberger1998}. This is the \emph{hard data} or
\emph{ground truth} in soil mapping \citep{Rossiter2001}. Field
observations are also the main input to spatial prediction modelling and
the basis for assessment of mapping accuracy. Other synthetically or
empirically generated estimates of values of target variables in the
field are considered as \emph{soft data}. Soft data are less desirable
as the primary input to model estimation, but sometimes there is no
alternative. It is in any case important to recognize differences
between \emph{hard} and \emph{soft} data and to suggest ways to access
the uncertainty of models that are based on either or both.

The object of observation and description of a soil is almost always a
soil profile or \emph{pedon}. Officially, a soil pedon is defined as a
body of soil having a limited horizontal extent of no more than 1--2 m
in horizontal and a vertical dimension (\(d\)) that typically extends to
only 1--2 m but may occasionally extend to greater depths. In practice,
the vast majority of soil profile data pertain to soil observations and
samples collected over very limited horizontal dimensions (10--50 cm)
and down to maximum depths of 1--2 m.

In geostatistical terms, soil observations are most commonly collected
at point support, meaning that they are representative of a point in
space with very limited horizontal extent. It is relatively rare to
encounter legacy soil profile data collected over larger horizontal
extents and bulked to create a sample representative of a larger volume
of soil that can be treated as providing block support for statistical
purposes. On the other hand, there is an increasing interest in soil
predictions at varying support sizes e.g.~1 ha for which composite
sampling can be used.

In the vertical dimension, soil profiles are usually described and
sampled with respect to \emph{genetic soil horizons}, which are
identifiable layers in the soil that reflect differences in soil
development or depositional environments. Less frequently, soils are
described and sampled in the vertical dimension with respect to
arbitrary depth intervals or layers e.g.~at fixed depths intervals
e.g.~10, 20, 30, 40, \(\ldots\) cm.

\begin{rmdnote}
A soil profile record is a set of field observations of the soil at a
location --- a collection of descriptive and analytical soil properties
attached to a specific location, depth and sampling support size (volume
of soil body).
\end{rmdnote}

Soil profile descriptions in the vertical dimension are usually
accompanied by additional soil site descriptions that describe
attributes of the site in the horizontal dimension for distances of a
few meters to up 10 m to surrounding the location where the vertical
profile was sampled and described. Site attributes described typically
characterize the immediately surrounding landscape, including slope
gradient, aspect, slope position, surface shape, drainage condition,
land use, vegetation cover, stoniness and unusual or site specific
features.

Two main types of information are typically recorded for point soil
profiles. The first consists of field observations and classifications
of observable profile and \emph{site characteristics}. Profile
attributes usually include the location and thickness of observably
different horizons or layers, the color, texture, structure and
consistence of each recognized horizon or layer and other observable
attributes such as stone content, presence, size and abundance of roots,
pores, mottles, cracks and so on. Despite their potential for
subjectivity, these field observations provide much useful information
at a relatively low cost, since there is no need to sample or transport
the soil or analyze it at considerable cost in a distant laboratory.

The second main type of information collected to describe soil profiles
consists of various types of objective measurements and analyses. Some
objective measurements can be taken on-site, in the field. Examples of
field measurements include \emph{in-situ} assessment of bulk density,
infiltration rate, hydraulic conductivity, electrical conductivity,
penetration resistance and, more recently, spectral analysis of soil
reflectance
\citep{kondolf2003tools, GehlRice2005, ShepherdWalsh2007JNIS}. The most
frequently obtained and reported objective measurements are obtained by
off-site \emph{laboratory analysis of soil samples} collected from soil
profiles at sampled locations. A wide variety of chemical and physical
laboratory analyses can be, and have been, carried out on soil samples
included in legacy soil profile data bases.

Within PSM we are mainly interested in a core set of laboratory analyses
for e.g.~pH, organic carbon, sand, silt, clay, coarse fragment content,
bulk density, available water capacity, exchangeable cations and acidity
and electrical conductivity. This core set was selected partly because
it is considered to represent the key soil functional properties of most
interest and use for interpretation and analysis and partly because
these soil properties are the most widely analyzed and reported in the
soil legacy literature
\citep{Sanchez2009Science, Hartemink2010Springer}. The significant
feature of objective measurements is that they are expected to be
consistent, repeatable and comparable across time and space. We will see
in the following chapter that this is not always the case.

\begin{rmdnote}
An advantage of descriptive field observations such as soil color, stone
content, presence, size and abundance of roots, pores, mottles, cracks,
diagnostic horizons etc. is that they provide much useful information at
a relatively low cost, since there is no need to sample or transport the
soil or analyze it at considerable cost in a distant laboratory.
\end{rmdnote}

\hypertarget{legacy-soil-profile-data}{%
\subsection{Legacy soil profile data}\label{legacy-soil-profile-data}}

The principal advantage of legacy soil profile data at point locations
is simply that the observations and measurements are referenced to a
known location in space (and usually also time). Knowledge of the
spatial location of soil profile data provides the opportunity to
analyze relationships between known data values at a location and other
covariate (predictor) data sets. It also becomes possible to simply
analyze spatial patterns i.e.~represent spatial variability using values
at known point locations. In the first instance, knowing the location of
a point at which a soil property has been described or measured permits
that location to be overlaid onto other spatially referenced digital
data layers to produce data sets of related environmental values that
all occur at the same site.

The known point values of soil properties (or classes) can be analyzed
relative to the known values of environmental covariates at
corresponding locations. If a statistically significant relationship can
be established between the value of a soil property at numerous
locations and the corresponding values of a environmental variables at
the same locations, a predictive model can be developed. Development of
predictive models based on such observed environmental correlations is a
fundamental aspect of modern pedometric soil mapping.

A second main advantage of point profile data is that the data values
are, more or less, objective assessments of a soil property or
characteristic at a location. Objective values are more amenable to
exploration using statistical techniques than subjective observations
and classifications. They typically (but not always) exhibit less
measurement error.

As important and useful as soil point data are, they also possess
limitations and problems that must be recognized and addressed. One
common limitation of legacy soil point data is lack of accurate
geo-referencing information. The location information provided for older
soil legacy profile data is often poor. Prior to the widespread adoption
of the Global Positioning Systems (GPS) the locations of most soil
sampling points were obtained and described in terms of estimated
distances and directions from some known local reference point (Fig.
\ref{fig:gps-evolution}). Even the best located of such older (prior to
1990's) sampling points cannot be expected to be located with an
accuracy of better than 50--100 m. Some widely used profile data from
developing countries cannot be reliably located to within 1 km
\citep{Leenaars2012}.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_GPS_evolution} 

}

\caption{Evolution of the Open Access Navigation and positioning technologies (left) and the open access remote sensing monitoring systems (right). API — Aerial photo-interpretation; S.A. --- Selective Availability; L.R.S.P.A. — Land Remote Sensing Policy Act (made Landsat digital data and images available at the lowest possible cost).}\label{fig:gps-evolution}
\end{figure}

This relatively poor positional accuracy has implications when
intersecting legacy point data with covariate data layers to discover
and quantify statistical relationships. It can be difficult to
impossible to develop meaningful relationships between soil properties
at point locations and environmental covariates that vary significantly
over short horizontal distances. Consider, for example, topography, in
which the largest portion of significant variation is often local and is
related to individual hill slopes from ridge line to channel. Many hill
slopes, especially in agricultural landscapes, have total lengths of
from 50--100 m. If the location of a point soil profile is only known
with an accuracy of 100 m, then, when overlaid on topographic data, that
point may fall at almost any point on a typical hill slope from channel
bottom to ridge top.

In such cases, it is unlikely that statistical analysis of the
relationship between soil properties and slope position will reveal
anything meaningful. Even if a strong relationship does exist in
reality, it will not be apparent in the poorly geo-referenced data. The
likelihood of establishing a meaningful relationship becomes even
smaller when the accuracy of the point location is ±1 km. In such cases,
subjective information on the conceptual location of the soil in the
landscape (e.g.~manually observed slope position) may be more useful for
establishing rules and patterns than intersection of the actual point
data with fine resolution covariates.

Another common limitation of legacy soil point data is that the criteria
used to select locations at which to sample soils have not always been
consistent. This can lead to bias in which soils and which parts of the
landscape get sampled in any given area. So, available information on
soil classes or soil properties at known points in the landscape may, or
may not, be representative of the dominant or actual landscape
conditions. Sometimes soils are sampled because they are believed to be
representative of the dominant conditions in a landscape. At other
times, soils are sampled because they are unusual and stand out or
because they occupy a transitional position and the sampler is trying to
identify a boundary. Most statistical techniques for extracting patterns
and relationships from analysis of soil point data assume that the point
data are somewhat representative of the landscape and cover the full
range of both covariate space and physical space. This assumption is
often not met and point samples, in many areas, may not be fully
representative of the full range of conditions in an area.

In analyzing legacy soil profile data to develop rules and
relationships, it is usually also assumed that the values reported for
any soil property for all sites are comparable and consistent.
Differences in methods used to sample and analyze soils lead to
considerable differences in the values reported for any given soil
property depending upon such factors as method of analysis, laboratory
at which the analysis was done, time of analysis (results vary year to
year), person doing the analysis and so on. These differences in values
for what should be the same soil property produce noise that confounds
the ability to discern and quantify statistical relationships between
observed soil property values and values for covariates at the same
locations.

In the case of automated soil mapping, efforts are usually made to try
to harmonize values produced using different laboratory methods to
achieve roughly equivalent values relative to a single standard
reference method. Even where harmonization is applied, some noise and
inconsistency always remains and the ability to establish statistical
relationships is often somewhat compromised.

\begin{rmdnote}
If not collected using probability sampling and with high location
accuracy, soil field records are often only marginally suitable for
building spatial prediction models, especially at fine spatial
resolution. Legacy data can carry significant positional and attribute
error, and is possibly not representative of all soil forming conditions
in an area of interest. All these limitations can seriously degrade the
final map accuracy, so that sometimes better accuracy cannot be achieved
without collecting new field data.
\end{rmdnote}

What needs to be emphasized is that much of the legacy soils profile
data in the world is under used. It tends to be fragmented, non-standard
between countries and often even within countries. Many original field
observations are still not converted into digital format and these data
are in considerable danger of being lost to effective use forever (!) as
government sponsored soil institutions lose support and close and the
current generation of experienced soil specialists retire and are not
replaced. Even where these data are in digital format, it is not easy to
share or exchange data across national, state or even project borders
because of significant differences in standards, methods, definitions,
ownership and legends \citep{Omuto2012GSP}.

\hypertarget{soil-covariates}{%
\subsection{Soil covariates}\label{soil-covariates}}

Following the work of Jenny \citep{white2009principles} and further
\citet{McBratney2011HSS}, we recognize six main groups of soil
covariates of interest for pedometric soil mapping:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Raw spectral and multi-spectral images} of the land surface
  (remote sensing bands),
\item
  \emph{DEM-derived covariates},
\item
  \emph{Climatic images},
\item
  \emph{Vegetation and land-cover} based covariates,
\item
  \emph{Land survey and land use information} --- human-made objects,
  manageemnt, fertilization and tillage practice maps etc,
\item
  \emph{Expert-based covariates} --- soil delineations or delineations
  of soil parent material or geology (manually or semi-automatically
  prepared); empirical maps of soil processes and features ( e.g.~catena
  sequences etc).
\end{enumerate}

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_DEM_evolution} 

}

\caption{Evolution of global DEM data sources: (right) SRTM DEM released in 2002, as compared to (left) WorldDEM released in 2014 [@Baade2014IEE]. Sample data set for city of Quorn in South Australia. As with many digital technologies, the level of detail and accuracy of GIS and remote sensing data is exhibiting exponential growth.}\label{fig:dem-evolution}
\end{figure}

\begin{rmdnote}
The most common environmental covariates typically used in soil mapping
are: (1) Raw spectral and multi-spectral images of the land surface, (2)
DEM-derivatives, (3) Climatic maps, (4) Vegetation and land-cover based
covariates, (5) Land survey and land use information, and (6)
Expert-based covariates e.g.~soil or surficial geology maps.
\end{rmdnote}

Different environmental covariates will be the dominant spatial
predictors of targeted soil properties and this relationship is often
scale dependent. Often, only a few key covariates can explain over 50\%
of the fitted model, but these are unknown until we fit the actual
models. The only way to ensure that the most relevant environmental
covariates are included in the modelling process is to start with the
most extensive list of all possible environmental covariates, then
subset and prioritize.

\hypertarget{soil-delineations}{%
\subsection{Soil delineations}\label{soil-delineations}}

\emph{Soil delineations} are manually drawn entities --- soil mapping
units --- that portray boundaries between soil bodies. Soil polygons are
usually assumed to differ across boundaries and to be relatively
homogeneous within boundaries, but other criteria are sometimes used
\citep{Simonson1968AA, Schelling1970Geoderma}. They are commonly
generated through photo-interpretation i.e.~stereoscopic interpretation
of aerial photographs of the area of interest (Fig.
\ref{fig:from-photointerpretation-to-soilmap}). Soil delineations based
on expert knowledge about an area are the main output of conventional
soil mapping. If available imagery is of high detail (scales
\textgreater{}1:25k), and if the soil surveyor has developed an
extensive knowledge of the soil---land-use---topography relations in an
area, soil delineations can produce useful and relatively accurate maps
of soil bodies and are, in a way, irreplaceable \citep{SSS1983USDA}.
However, in many parts of the world, soil delineations have been
produced using relatively weak source materials and these can be of
variable accuracy.

\begin{figure}[t]

{\centering \includegraphics[width=0.95\linewidth]{figures/Fig_from_photointerpretation_to_soilmap} 

}

\caption{In conventional soil mapping, soil delineations are usually manually drawn polygons representing (assumed) bodies of homogenous soil materials (often geomorphological units). These are first validated in the field before a final area-class map is produced, which can then be generalized and used to extract soil property maps. After @SSS1983USDA.}\label{fig:from-photointerpretation-to-soilmap}
\end{figure}

In soil mapping terms, soil map delineations can be considered to be
expert-based covariates. They can be used as input to spatial prediction
in the same way as DEM-derived predictors or remote sensing indices.
This is assuming that a standardized legend is attached to the soil
polygon map systematically describing types of polygons (
e.g.~soil-geomorphological units). Soil delineations, in combination
with with other auxiliary predictors, can generate soil property maps
that exhibit both abrupt and smooth transitions in values. An analyst
can objectively assess the utility and importance of hybrid covariates
and then try to obtain optimal covariates that can be clearly
demonstrated to be significant predictors. In practice, expert-based
predictors can sometimes perform better than alternatives such as
DEM-derived predictors or remote sensing indices. \emph{``Perform
better''} in this case indicates that the predictors will be more
distinctly correlated with target soil properties. In all applications
of PSM methods, it is advisable to obtain and assess the utility of
available soil polygon maps.

Most legacy polygon soil maps represent a distillation and summary of
expert knowledge about the main spatial patterns of variation in soil
types (classes) within an area. This knowledge has been abstracted and
generalized in order to convey dominant patterns at specific scales.
Thus, it is often not reasonable to expect to be able to go to a
specific point portrayed on a soil map and find a single specific soil
class or soil property value (see Fig. \ref{fig:smu-aggregation}). Most
often, soil maps provide lists or inventories of soil classes that occur
within a given map area and give outlines of areas (polygons) within
which lists of specific soils are predicted to occur with specified
frequencies or possibilities. Soils are conceptualized as objects that
belong to defined soil classes.

\begin{rmdnote}
Soil delineations are manually drawn entities that portray boundaries
between soil bodies assumed to be internally homogeneous. Soil
delineations can be considered to be expert-based soil covariates.
\end{rmdnote}

Each class of soil (often a soil series or taxonomic class) is assumed
to have a limited and describable range of characteristics i.e.~physical
and chemical properties that can be used to characterize it. Within
mapped polygons, the manner in which soils vary horizontally across the
landscape is usually not explicitly portrayed (Fig.
\ref{fig:smu-aggregation}). At best, such internal polygon variation may
be described in conceptual terms relative to how different soils may be
more likely to occupy specific landscape positions or occur on specific
parent materials or under different drainage conditions. For example the
USDA's Soil Survey Manual distinguishes between \emph{consociations}
(relatively homogeneous polypedons), \emph{associations} (heterogeneous
unit with two or more similar polypedons), and \emph{complexes} (mix of
two or more contrasting polypedons), but in most cases none of the
described components is actually mapped separately.

Variation of soil properties in the vertical dimension is usually
described in terms of variation in the type, thickness and arrangement
of various different soil horizons. Soil horizons are themselves a
collection of class objects, with each class also expected to display a
characteristic range of attributes and soil property values. All soils
do not always have the same types or sequences of horizons and so, most
horizons are not laterally continuous and mappable. So, most legacy soil
maps portray abstract representations of how various classes of soils
vary horizontally between soil polygons and vertically by soil horizons.

Interpretation of most maps of soil classes often requires a
considerable amount of knowledge and understanding of both underlying
soil mapping concepts and of local classes of soils and soil horizons.
This restricts effective use of many soils maps to persons with the
necessary background knowledge.

\hypertarget{advantages-and-disadvantages-of-using-soil-delineations}{%
\subsection{Advantages and disadvantages of using soil
delineations}\label{advantages-and-disadvantages-of-using-soil-delineations}}

One of the key advantages of conventional soil polygon map data is its
availability. In many parts of the world, the number of instances of
reliably located soil profile observations is quite low and the spatial
extent of areas for which sufficient point data are available can be
small \citep{Hartemink2008SMD}. However, many areas with only limited
amounts of geo--referenced point data are covered by soil maps of
various types and scales. So, conventional soil polygon maps are often
available for areas that lack sufficient amounts of soil point data.

For most of the last 80--100 years, conventional polygonal (area-class)
soil maps have been seen as the most effective way to convey information
about horizontal and vertical variation in soils and soil properties
across the landscape \citep{Wysocki2005Geoderma}. Conventional soil maps
do manage to achieve some partitioning of the total amount of variation
in soils and soil properties in the horizontal dimension. Soil maps have
always acknowledged that they are unable to capture and explicitly
portray variation that occurs at distances shorter than some minimum
sized area that is feasible to display at any particular scale of
mapping.

Since soil types and soil properties can exhibit a significant amount of
variation over rather short distances, there is always a relatively
large amount of total variation in soils and soil properties that is not
explicitly captured or described by polygonal soil maps. For some highly
variable soil properties, as much as 40--60\% of the total variation in
that soil property within a mapped area can occur over distances of
meters to tens of meters. This means that most soil maps cannot
explicitly display this portion of the variation and can only try to
portray the remaining portion of the variation (40--60\%) that occurs
over longer distances \citep{Heuvelink2001Geoderma}. Much of this longer
range variation is often related to observable and mappable physical or
landscape features such as slope gradient, slope position, landform
elements, definable bodies of different surficial geological materials,
readily apparent differences in moisture or drainage conditions or
observable changes in soil color, accumulation of surface salts or
visible erosion.

Soil surveyors make use of these correlations to manually delineate soil
polygon boundaries that outline areas that display different soil
assemblages in response to observable differences in landscape or
environmental conditions. These manually drawn polygon boundaries can,
and do, provide much useful information about variation in readily
observable soil and landscape attributes. So, soil maps are often one of
the best sources of information on local variation in surficial
geological materials, because soil surveyors have observed, recorded and
mapped this variation in delineating their polygons.

Likewise, soil maps are often able to be quite successful in outlining
areas of significantly different moisture or drainage conditions,
climate or vegetation related conditions, depth to bedrock, slope or
slope position, salinity or calcareousness. Where they exist,
conventional soil polygon maps can act as one of the most effective
sources of covariate information describing medium to long range
variation in key environmental factors such as parent material,
drainage, climate, vegetation and topography.

In terms of automated soil mapping, one of the key advantages of
conventional soil maps is that they provide a useful initial indication
of the main soils that are likely to be encountered within any given
area (map sheet or individual polygon). This listing limits the number
of soils that need to be considered as possible or likely to occur at
any point or within any area to a much smaller and more manageable
number than a full list of all possible soils in a region. Most soil
maps provide a hierarchical stratification of an area into smaller areas
of increasing homogeneity and more limited soil and environmental
conditions.

Many soil maps, or their accompanying reports, also provide some
indication about how named soils within polygons or map units vary
spatially, within the polygon, in response to changes in slope, landform
position, parent material, drainage and so on
\citep{SSDS1993, Wysocki2005Geoderma}. This information on which soils
are most likely to occur within a given geographic area and under what
environmental conditions (slope position, drainage, parent material)
each listed soil is most likely to occur, can provide a foundation for
heuristic (or expert-based) modeling of the more detailed and shorter
range variation in soil types that lies at the heart of DSM methods of
\emph{soil polygon disaggregation}. Disaggregation of conventional soil
polygon maps into more detailed representations of the most likely finer
scale spatial pattern of variation of the named component soils is an
attractive and feasible method of producing more detailed estimates of
the spatial distribution of soils and soil properties for many areas for
which point data are scarce and conventional soil polygon maps are
available (Fig. \ref{fig:smu-aggregation}).

The list of limitations and potential problems with using conventional
soil polygon map data is long and must be acknowledged and dealt with.
Two of the most serious issues are completeness and consistency. It is
extremely rare to have entire regions or countries for which there is
complete coverage with a consistent set of soil polygon maps of
consistent scale, content and vintage. In fact, the normal situation for
most regions and countries is one of incomplete coverage with patches of
maps of different scale, content, design and vintage covering portions
of areas of interest with large gaps of unmapped areas between mapped
areas.

Only a very few countries or regions (e.g.~USA, UK, Japan, western
European countries, Jamaica, Gambia etc) have achieved anywhere near
complete national coverage at scales more detailed than 1:50,000
\citep{Rossiter2004SUM, Hartemink2008SMD}. Most smaller scale (1:1M or
smaller) national or continental soil maps are based on manual
interpolation and extrapolation of scattered and incomplete maps that
provide only partial coverage for these mapped areas. Even where
coverage is complete, or nearly complete, consistency is often a
significant issue.

\begin{rmdnote}
Conventional soil polygon maps (manually-drawn delineations) are often
one of the best sources of information on local variation in soil
polypedons. On the other hand, conventional soil polygon maps often
suffer from incompleteness, inconsistency and low accuracy of thematic
content, as well as from suspect positional accuracy.
\end{rmdnote}

Mapping concepts change across time and vary among different mappers and
agencies. Consequently, the normal situation is that no two maps are
entirely comparable and many collections of maps exhibit very marked and
significant differences in what has been mapped and described, the
concepts and legends used to map and describe, the classification rules
and taxonomies and the scale and level of detail of mapping. Joining
maps of different scales, vintages and legend concepts into consistent
compilations that cover large regions is challenging and not always
entirely successful.

Even in the USA, where a single set of mapping guidelines and
specifications is ostensibly in place for national mapping programs,
there are readily apparent differences in the concepts used to produce
maps in different areas and visible differences in the naming and
description of dominant mapped soils on the same landforms and landform
positions in adjoining map sheets \citep{LathropJr19951, ZHONG2011491}.

For conventional soil polygon maps to be of maximum utility for
automated soil mapping, they really benefit from being compiled and
harmonized into regional maps that have a common legend, common scale,
common list of described landform and soil attributes and consistent
application of terminologies and methods. There have been some successes
in developing and demonstrating methods for compiling harmonized soil
polygon maps at regional to continental scales from scattered and
disparate collections of available soil polygon maps
\citep{Bui2003Geoderma, Grinand2008Geoderma} but these methods have not
yet been formalized or widely adopted for global use. If soil polygon
maps are not harmonized to produce complete and consistent regional to
national coverages, then each map needs to be treated as a separate
entity which complicates use of soil maps to build consistent rules for
predicting soils or soil properties across large areas.

\hypertarget{accuracy-of-conventional-soil-polygon-maps}{%
\subsection{Accuracy of conventional soil polygon
maps}\label{accuracy-of-conventional-soil-polygon-maps}}

The spatial accuracy of conventional soil polygon maps is also a
frequent concern. Most legacy soil maps were prepared before the advent
of ortho-rectified digital base maps and GPS. Many legacy maps exist
only on non-stable media (e.g.~paper), are of unknown or uncertain
projection and datum and were compiled onto uncontrolled base maps,
usually in paper format. Even though the boundaries of soil polygons are
generally subjective and fuzzy, the correct location of many polygon
boundaries on legacy soil maps is compromised by problems related to
unknown or unstable geo-referencing. It is very common to encounter
highly obvious discrepancies between the observed location of soil
polygon boundaries on newly digitized soil polygon maps and the
obviously intended location of those same boundaries. For example,
polygon boundaries, clearly intended to delineate drainage channels are
often displaced relative to the channels or cut back and forth across
the channels.

Similarly, boundaries intended to delineate an obvious break in slope
are often strongly displaced relative to the actual location of the
slope break in correct geographic space. The mismatch between observed
geographic features and soil polygon map boundary locations is often
compounded when boundaries delineated by hand at a coarse resolution are
overlain onto, and compared to, landscape features observable at finer
resolution on newer digital base maps and digital elevation models.

The displacements in boundary locations and level of generalization can
be disturbing and reduce confidence in the accuracy of the polygon soil
map, even when the original polygon boundaries were significant and
reflected legitimate changes in soil properties at locations of likely
change in soils. There are also numerous instances where boundaries on
conventional soil polygons maps do not define locations of significant
real change in soils or soil properties and simply reflect an arbitrary
subdivision of the landscape.

Several soil survey cross-validation studies
\citep{Marsman1986ALTERRA, Hengl2006SSSAJ} have shown that traditional
polygon-based maps can be of limited accuracy and usability. First, they
are created using irreproducible methods and hence difficult to update.
Second, at broader scales, polygon maps produced by different teams are
often incompatible and can not be merged without harmonization. A
non-soil scientist introduced to a continental-scale soil map where soil
boundaries follow country boundaries will potentially lose confidence
and look for another source of information \citep{DAvello1998SSH}.
Consider for example the Harmonized World Soil Database product. On the
HWSD-derived maps one can still notice numerous soil borders that match
country borders (most often an artifact), but also inconsistent
effective scale within continents. All these limitations reduce
confidence in the final product and its usage.

\begin{rmdnote}
For legacy soil maps to be of maximum possible utility for digital soil
mapping they need to be harmonized with respect to thematic content and
accuracy, and they need to be corrected with respect to positional
accuracy.
\end{rmdnote}

So, conventional soil polygon maps suffer from issues related to
completeness, consistency and accuracy of thematic content as well as
from issues related to positional accuracy and relevance of soil polygon
boundaries. If these issues are not dealt with, and corrections are not
implemented, the likelihood of extracting meaningful and consistent
patterns and rules for use in soil mapping is considerably compromised.

\hypertarget{tacit-knowledge}{%
\subsection{Legacy soil expertise (tacit
knowledge)}\label{tacit-knowledge}}

The dominant characteristic of most legacy soil expert knowledge is that
it has often not been formalized or made explicit and systematic.
\citet{Hudson2000SSSAJ} refers to the vast amount of soils knowledge
that exists in tacit form, as \emph{``unstated and unformalized rules
and understanding that exists mainly in the minds and memories of the
individuals who conducted field studies and mapping''}. Soil maps are
one mechanism by which experts try to capture and portray their
understanding of how and why soils vary across the landscape
\citep{Bui2004Geoderma}. Other methods include:

\begin{itemize}
\item
  \emph{2D cross sections},
\item
  \emph{random catenas} \citep{McBratney2006WCSS},
\item
  \emph{3D block diagrams},
\item
  \emph{decision trees or rules},
\item
  \emph{mapping keys and textual descriptions of where, how and why
  soils have been observed to vary in particular areas or under
  particular conditions}.
\end{itemize}

All of these methods are imperfect and all leave some portion of expert
knowledge un-expressed and uncaptured. Modern methods of digital soil
mapping often represent attempts to capture expert knowledge in a
systematic and formal way
\citep{Zhu2001, McBratney2003Geoderma, Bui2004Geoderma, MacMillan2005CJSS}.

Integration of expert pedological knowledge into soil mapping methods
provides the opportunity of potentially improving both the predictions
themselves and understanding of the reasons or rationale for the success
(or failure) of predictions
\citep{Walter2006DSS, Lagacherie1995Geoderma, Lagacherie2001Geoderm}.
There is increasing realization of the benefits of incorporating both
hard and soft knowledge into prediction and decision making procedures
\citep{christakos2001temporal}. Soft knowledge can help to smooth out or
generalize patterns that are incompletely represented by hard data or
that are noisy when assessed using hard data. A definite advantage of
expert tacit knowledge is that a significant amount of it exists.
Conceptual understanding of where, how and why soils and soil properties
vary across landscapes is relatively widespread, if not always well
documented or expressed.

In the absence of any hard data, in the form of point profile
observations or even soil polygon maps, expert knowledge of the main
patterns of variation in soils can represent the only feasible way of
producing a first approximation model of soil spatial variation for an
area. There will be vast tracts of the world for which both soil point
data and soil maps will be lacking (e.g.~remote portions of Russia and
northern Canada) but for which there is considerable expert knowledge of
the main kinds of soils, their properties and the patterns in which they
vary across the landscape, at least at a conceptual level. It may be
possible to capture and apply this expert tacit knowledge in such as way
as to permit creation of initial prediction rules that can subsequently
be modified and improved upon.

As with much legacy soils data, one of the main limitations of legacy
soil tacit knowledge is --- its accessibility. By definition, tacit
knowledge has not been formalized and has often not even been written
down. So, a challenge exists to simply locate legacy soil expert
knowledge. Once located, a second challenge is how to best capture and
formalize it i.e.~how to turn it into rules for a mapping algorithm.

\begin{rmdnote}
The first challenge to using legacy soil expert knowledge is to locate
it. Once located, a second challenge is how to best capture and
formalize it i.e.~how to turn it into rules for a mapping algorithm.
\end{rmdnote}

Common approaches to codifying expert knowledge about soil-landscape
patterns include construction of \emph{decision trees}
\citep{Walter2006DSS, Zhou2004JZUS}, \emph{fuzzy logic rule} bases
\citep{Zhu2001} or Bayesian maximum likelihood equations
\citep{Zhou2004JZUS}. A less sophisticated, but more generalized,
approach is to apply general conceptual understanding of soil-landscape
relationships to existing databases of soils and landform data to
automatically associate named soil classes with conceptual landform
positions \citep{MacMillan2005CJSS}. Expert tacit knowledge is often
inexact and incomplete but it can express and reveal widely recognized
general patterns and can provide a reasonable first approximation of
soil-landscape patterns. In order to be used effectively, for activities
such as PSM, platforms and procudures need to be agreed upon, and put in
place, to support knowledge capture and application. Agreement on such
platforms and procedures is not yet widespread.

To integrate all available tacit knowledge systems into a one, all
encompassing, prediction algorithm is probably beyond human capacities,
but it could well be assisted using e.g.~web crawling applications for
legacy soils data i.e.~by scanning documents, soil survey reports and
books and then extracting rules and procedures using automated methods.
Alternately, different methods, using different types of expert
knowledge, could be implemented regionally to locally and the resulting
maps merged using harmonization procedures.

\hypertarget{pseudo-observations}{%
\subsection{Pseudo-observations}\label{pseudo-observations}}

When applying Statistical or Machine Learning methods to larger (global
to continental) sized areas, one thing that often limits the success of
predictions is the existance of very extensive areas with extreme
climatic conditions and/or very restricted access, that are consequently
significantly under-sampled. This occurs largely in the following five
types of areas \citep{Hengl2017SoilGrids250m}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Semi-arid and arid lands, deserts and sand dunes,
\item
  Mountain tops, steep slopes of mountains and similar inaccessible
  areas,
\item
  Areas covered by ice and/or snow, i.e.~glaciers,
\item
  Inaccessible tropical forest,
\item
  Areas governed by totalitarian and hostile regimes, with military
  conflicts or war.
\end{enumerate}

It might seem obvious to soil surveyors that there is no soil organic
carbon on the top of the active sand dunes in the Sahara, but any model
fitted without observations from the Sahara could result in dubious
extrapolation and questionable predictions. In addition, relationships
across transitional areas --- from semi-arid zones to deserts --- can be
difficult to represent without enough points at both edges of the
feature space. Some sand dunes in the USA have fortunately been sampled
and analyzed in the laboratory. For example, \citet{Lei1998} has shown
that sand dunes in the Mojave desert have an average pH of 8.1. Again,
although it might seem obvious that deserts consist mainly of sand, and
that steep slopes without vegetation are either very shallow or show
bedrock at the surface, prediction models may not be aware of such
expert knowledge and hence such unsampled features need to be
`numerically represented' in the calibration dataset.

Instead of masking out all such areas from soil mapping, one can
alternatively generate a number of pseudo-observations to fill sampling
gaps in the feature space. Pseudo-observations can be generated by
photo-interpretation of high resolution imagery or by using very
detailed land cover, soil or similar maps.
\citet{Hengl2017SoilGrids250m} use the following data sources to
delineate sand dunes, bare rock and glaciers:

\begin{itemize}
\item
  Mean annual long-term surface temperature generated from the MODIS LST
  data product (MOD11A2), long-term MODIS Mid-Infrared (MIR) band
  (MCD43A4) and slope map can be used to delineate --- sand dunes mask.
\item
  The MODIS MIR band (MCD43A4) and a slope map can be used to delineate
  --- bare rock areas. Bare rock or dominantly rocky areas show high MIR
  surface reflectance and are associated with steep slopes.
\item
  Global distribution of glaciers i.e.~the GLIMS Geospatial Glacier
  Database \citep{raup2007glims} can be used to delineate --- glaciers
  and permafrost.
\end{itemize}

For each of these three masks \citet{Hengl2017SoilGrids250m} generated
randomly 100--400 points based on their relative global extent and
assigned soil properties and soil classes accordingly (e.g.~in the case
of WRB's Protic Arenosols for sand dunes, Lithic and Rendzic Leptosols
for bare rock areas, Cryosols for areas adjacent to glaciers; in the
case of USDA's Psamments for sand dunes, Orthents for bare rock areas
and Turbels for glaciers; for sand dunes they also inserted estimated
values of 0 for soil organic carbon, sand and coarse fragments).

When inserting pseudo-observations one should try to follow some basic
rules (to minimize any negative effects):

\begin{itemize}
\item
  keep the relative percentage of pseudo-points small i.e.~try not to
  exceed 1--5\% of the total number of training points,
\item
  only insert pseudo-points for which the actual ground value is known
  with high confidence, e.g.~sand content in sand dune areas,
\item
  if polygon maps are used to insert pseudo-observations, try to use the
  most detailed soil polygon maps and focus on polygons with the very
  highest thematic purity.
\end{itemize}

\hypertarget{soil-databases}{%
\section{Soil databases and soil information
systems}\label{soil-databases}}

\hypertarget{soil-databases-1}{%
\subsection{Soil databases}\label{soil-databases-1}}

To facilitate usage of soil data, soil field records and soil
delineations can be digitized and organized into databases. Soil
profiles are commonly put into a \emph{Soil--Profile (geographical)
Database} (SPDB); soil delineations are digitized and represented as
polygon maps with attributes attached via mapping units and soil classes
\citep{Rossiter2004SUM}. Soil profile databases and soil polygon maps
can be combined to produce attribute maps of soil properties and classes
to answer soil or soil--land use specific questions. Once the data are
in a database, one can generate maps and statistical plots by running
spatial queries \citep{Beaudette2009CG}.

\begin{figure}[t]

{\centering \includegraphics[width=0.8\linewidth]{figures/Fig_SITE_HORIZON_structure} 

}

\caption{An example of a basic soil profile geographical database, which commonly consists of four tables: SITE, HORIZON, DESCRIPTION and NAMES tables (a). To facilitate rapid display and use of soil variables, SITE and HORIZON tables can be combined into a single (*wide*) table structure (b).}\label{fig:site-horizon-structure}
\end{figure}

A common database model used for SPDB is one where soil site, soil
horizon data and metadata are split into separate tables (Fig.
\ref{fig:site-horizon-structure}a; here referred to as the
\emph{horizon-site} or layer-site database model. Note that soil
surveyors typically like to include in the database also meta data that
describe column names and classes for factor type variables, because
these are often area/project specific and need to be attached to the
original soil data. Many variations on this horizon-site database model
exist, so that each new user of SPDB typically requires some initial
training to understand where soil variables of interest are located and
how they can be exported and visualized.

Any horizon-site database model can be converted to a single table where
each soil profile becomes one record (Fig.
\ref{fig:site-horizon-structure}b). The single-table database model
simplifies subsequent efforts to visualize sampled values and to import
them to a platform to run spatial analysis. Note also that conversion
from one data model to the other in software for statistical computing
is relatively easy to accomplish.

\hypertarget{a-soil-information-system}{%
\subsection{A Soil Information System}\label{a-soil-information-system}}

A \emph{Soil Information System} (SIS) consists of a combination of
input soil data (soil profiles, soil polygon maps, soil covariates),
output predictions (soil properties and classes) and software to browse
these data. A SIS is basically a thematic GIS focused on soil resources
and offering the best possible soil information at some given scale(s).
A SIS is often the end product of a soil survey. In the ideal case, it
should meet some common predefined soil survey specifications, for
example:

\begin{itemize}
\item
  \emph{It corresponds to a specified soil survey scale}.
\item
  \emph{It provides spatial information about a list of targeted soil
  variables which can be used directly for spatial planning and
  environmental modelling}.
\item
  \emph{It provides enough meta-information to allow use by a non-soil
  science specialist}.
\item
  \emph{It has been cross-checked and validated by an independent
  assessment}.
\item
  \emph{It follows national and/or international data standards}.
\item
  \emph{It has a defined information usage and access policy}.
\end{itemize}

Many soil data production agencies are often unclear about where the
work of a soil surveyor stops. Is a SPDB and a soil polygon map an
intermediate product or can it be delivered as a soil information
system? Does a SIS need to already hold all predictions or only inputs
to prediction models? In this book we will adhere to a strict definition
of a SIS as a complete and standardized geographical information system
that contains both initial inputs and final outputs of spatial
predictions of soil variables, and which is fully documented and ready
to be used for spatial planning. The PSM tools described in this book,
in that context, have been designed as a step forward to producing more
complete soil information systems.

\begin{rmdnote}
A Soil Information System is an end product of soil mapping --- a
standardized collection of (usually gridded) soil property and class
maps of an area that can be used for spatial planning, environmental
modelling, agricultural engineering, land degradation studies,
biodiversity assessment and similar. A SIS tries to provide the best
possible soil information at some given scale for the spatial domain of
interest.
\end{rmdnote}

Another important point is that a modern SIS needs to be user-oriented.
As \citet{Campbell2008NCST} argues: \emph{``Soil science, soil
classification, mapping and monitoring systems and resources are not
ends in themselves, they are means to an end. The objective is more
sustainable management of soil.''} We envisage that in the near future
soil surveyors will have to completely open soil information systems to
users so that they can also contribute to construction and influence
content. \citet{Goodchild2008Accuracy} calls this \emph{``Web 2.0''}
(read and write) and/or \emph{``Web 3.0''} (read, write and execute)
approaches to content creation. We also envisage that soil information
will increasingly be produced using global vs local models and
increasingly using distributed data and computing (Fig.
\ref{fig:automap-future}).

\begin{figure}[t]

{\centering \includegraphics[width=0.75\linewidth]{figures/Fig_automap_future} 

}

\caption{The future of global mapping and environmental monitoring activities is expected to be increasingly automated and distributed.}\label{fig:automap-future}
\end{figure}

One example of a web-interface, provided to make access to input and
output soil data more efficient, is the California Soil Resource Lab
SoilWeb \citep{OGeen2017soilweb}. Here, a series of web-apps and simple
interfaces to PostGIS and similar databases are used to empower users,
including developers, to access soil data without using a sophisticated
GIS or similar.

There is also increasing interest in the economic aspects of soil
functions in relation to soil mapping and soil information use. For a
soil mapper to justify the importance of producing spatial soil
information there is no better argument that a thorough economic
assessment of its use.

\begin{rmdnote}
There is an increasing need to quantify economic aspects of soil
functions in relation to soil mapping and soil information use: What is
the value of soil information for food production? How much does some
sophisticated geostatistical mapping method reduce costs (while
producing equally accurate information)? How much does soil
(environmental) remediation cost? What is the cost-benefit ratio between
soil mapping and soil exploitation? What is the global value of soil for
fixation of atmospheric gasses or for water filtering or retention?
\end{rmdnote}

\hypertarget{soil-information-users}{%
\subsection{Soil information users}\label{soil-information-users}}

Typical \emph{user groups of soil information} include
\citep{SSDS1993, harpstead2001soil}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{At local/farm level}:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    farmers and ranchers who want to maximize sustainability and/or
    production efficiency;
  \item
    fertilizer dealers and agricultural consulting companies, who want
    to sell competitive products and services;
  \item
    civil engineers who plan roads, airports and similar;
  \item
    land development agencies who must consider the soil foundations,
    streets, lawns and e.g.~locations for septic systems,
  \item
    bankers and financial agencies who give loans, provide insurance or
    buy or sell land;
  \item
    foresters who plan harvesting or reforestation operations and must
    know the relevant conditions and capabilities of the soil;
  \item
    tax assessors who assign potential value for a given piece of
    farmland and/or ranch land;
  \end{enumerate}
\item
  \emph{At national level}:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    agricultural ministries and land use planning agencies (for
    developing and implementing policies and plans);
  \item
    environmental protection agencies, who develop and enforce
    management plans for protected areas or areas of special value;
  \item
    environmental impact assessment companies and agencies, who model
    various management scenarios;
  \item
    agricultural extension agencies;
  \item
    natural hazard (e.g.~flooding or landslide) monitoring agencies;
  \end{enumerate}
\item
  \emph{At continental or global levels}:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    agricultural development organizations such as FAO, CGIAR
    (Consortium of International Agricultural Research Centers) research
    institutes;
  \item
    international environmental protection agencies, such as UNEP;
  \item
    global financial organizations and trading entties, such as the
    World Bank;
  \item
    global biogeochemical cycle modelers;
  \item
    climate change modelers;
  \end{enumerate}
\end{enumerate}

The future for digital soil data may well lie in \emph{task-oriented
Soil Information Systems} (as proposed by Gerard Heuvelink at the DSM
2010 conference in Rome), in which only input data and analytical models
are stored, permitting an infinite number of maps and visualizations to
be generated on-demand by users. This implies that future soil mappers
will eventually evolve from people that draw maps to \emph{process
moderators}, and the maps will evolve from static to \emph{interactive,
on-demand created} maps. Likewise, if the soil mapping tools are exposed
to the public, anyone will be able to evolve from a passive user into an
active soil mapper. In that sense, there is also an increasing potential
in crowd-sourcing soil mapping to a wider possible community.

\hypertarget{usability-of-soil-geographical-database}{%
\subsection{Usability of soil geographical
database}\label{usability-of-soil-geographical-database}}

Through PSM, a soil data production agency aims at delivering products
of known and reported quality. The quality of a soil geographical
database is a product of a number of factors (Fig.
\ref{fig:usability-scheme}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Attribute and thematic accuracy} --- How well do the attribute
  data correspond to reality? How well do map legends correspond to
  reality?
\item
  \emph{Adequacy and consistency} --- How adequate is the produced map
  for its intended use? How consistent is the mapping methodology
  (sampling intensity, thematic coverage, lab analysis techniques)?
\item
  \emph{Geographical coverage and completeness} --- Does the GIS provide
  information for the whole area of interest? How many areas are missing
  and when will they be made available? Are all requested variables
  available?
\item
  \emph{Completeness and accuracy of the metadata} --- How exactly was
  the map produced? What do certain abbreviations mean and where can
  more technical information about data processing steps be found?
\item
  \emph{Data integrity and interoperability} --- How can the data be
  integated within an existing GIS? Are the data optimized for
  distribution and import?
\item
  \emph{Accessibility and data sharing capacity} --- Are the data
  available for download and are they easy to obtain? How many users can
  access the data at the same time? Are the data free and easily
  obtained?
\end{enumerate}

\begin{figure}[t]

{\centering \includegraphics[width=0.95\linewidth]{figures/Fig_usability_scheme} 

}

\caption{Usability of a Soil Information System is basically a function of a number of data usability measures from which the following four (C’s) are essential: completeness, consistency, correctness and currency.}\label{fig:usability-scheme}
\end{figure}

By maximizing each of the usability measures listed above we can be
confident of achieving the maximum quality for output products. In
reality, we can only improve each of the listed factors up to a certain
level. Then, due to practical limits, we reach some best possible
performance given the available funds and methods, beyond which no
further improvement is feasible. For example, the capacity to serve
geodata is determined by the technical capacity of the server system. In
order to improve this performance we either have to invest more money to
get better computers or re-design the data model so that it is more
efficient in fulfilling some operation.

While the objective of PSM (as outlined in this book) is to increase
measures such as adequacy, coverage and completeness, inherent
properties of the legacy data unfortunately can not be as easily
improved. We can at least assess, and report on, the input data
consistency, and evaluate and report the final accuracy of the output
products. Once we have estimated the true mapping accuracy, and under
the assumption that mapping accuracy can be linearly improved by
increasing the sampling intensity, we can estimate the total number of
additional samples necessary to reach a desired level of accuracy
(e.g.~even approaching 100\% accuracy).

For Keith Shepherd (ICRAF; personal communication) the key to
optimization of decision making is to accurately account for uncertainty
--- to make sense out of measurements one needs to:

\begin{itemize}
\item
  \emph{Know the decision you are trying to make},
\item
  \emph{Know the current state of uncertainty (your priors)},
\item
  \emph{Measure where it matters and only enough to make a sound
  decision}.
\end{itemize}

\begin{rmdnote}
The quality of a geospatial database is a function of accuracy,
adequacy, consistency, completeness, interoperability, accessibility and
serving capacity. Each of these usability measures can be optimized up
to a certain level depending on the available resources.
\end{rmdnote}

In practice, soil surveyors rarely have the luxury of returning to the
field to collect additional samples to iteratively improve predictions
and maps, but the concept of iterative modeling of spatial variation is
now increasingly accepted.

\hypertarget{uncertainty-soil-variables}{%
\section{Uncertainty of soil
variables}\label{uncertainty-soil-variables}}

\hypertarget{basic-concepts}{%
\subsection{Basic concepts}\label{basic-concepts}}

An important aspect of more recent soil mapping projects, such as the
\emph{GlobalSoilmap} project, is a commitment to estimating and
reporting the uncertainty associated with all predictions. This is a
recent improvement to soil data, as uncertainty in traditional soil maps
has often been reported (if given at all) only using global estimates.
Maps of uncertainty (confidence limits or prediction error) of soil
properties is a new soil data product and there is an increasing demand
for such maps. But what is \emph{`uncertainty'} and how do we measure
and describe it, particularly for specific point locations?

\citet{Walker2003IA} define uncertainty as \emph{``any deviation from
the unachievable ideal of completely deterministic knowledge of the
relevant system''}. The purpose of measurement is to reduce decision
uncertainty; the purpose of planning soil sampling campaigns is to find
an optimum between project budget and targeted accuracy. A general
framework for assessing and representing uncertainties in general
environmental data is reviewed by \citet{Refsgaard2007UEM}. In this
framework, a distinction is made regarding how uncertainty can be
described, i.e.~whether this can be done by means of:

\begin{itemize}
\item
  \emph{probability distributions} or upper and lower bounds,
\item
  some \emph{qualitative indication of uncertainty},
\item
  or \emph{scenarios}, in which a partial (not exhaustive) set of
  possible outcomes is simulated.
\end{itemize}

Further, the \emph{methodological quality} of an uncertain variable can
be assessed by expert judgement, e.g.~whether or not instruments or
methods used are reliable and to what degree, or whether or not an
experiment for measuring an uncertain variable was properly conducted.
Finally, the \emph{``longevity''}, or presistence, of uncertain
information can be evaluated, i.e.~to what extent does the information
on the uncertainty of a variable change over time.

\begin{rmdnote}
Estimates of uncertainty of soil property and soil class predictions are
an increasingly important extension to soil mapping outputs. Maps of
spatial variation in uncertainty can be submitted as maps of upper and
lower confidence limits, probability distributions or density functions,
prediction error maps and/or equiprobable simulations.
\end{rmdnote}

\citet{Heuvelink2006Elsevier} observed that soil data are rarely certain
or \emph{`error free'}, and these errors may be difficult to quantify in
practice. Indeed, the quantification of error, defined here as a
\emph{`departure from reality'}, implies that the \emph{`true'} state of
the environment is known, which is often not possible.

\hypertarget{sources-uncertainty}{%
\subsection{Sources of uncertainty}\label{sources-uncertainty}}

There are several sources of uncertainty in soil data. For soil profile
data the sources of error are for example:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{sampling (human) bias or omission of important areas};
\item
  \emph{positioning error (location accuracy)};
\item
  \emph{sampling error (at horizon level i.e.~in a pit)};
\item
  \emph{measurement error (in the laboratory)};
\item
  \emph{temporal sampling error (changes in property value with time are
  ignored)};
\item
  \emph{data input error (or typing error)};
\item
  \emph{data interpretation error};
\end{enumerate}

\begin{figure}[t]

{\centering \includegraphics[width=0.5\linewidth]{figures/Fig_lines_Legros1997} 

}

\caption{20 photo-interpretations done independently using the same aerial photograph overlaid on top of each other. Image credit: @Legros1997ESG.}\label{fig:lines-legros1997}
\end{figure}

For soil delineations, the common sources of error (as illustrated in
Fig. \ref{fig:lines-legros1997}) are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{human bias (under or over representation) / omission of
  important areas};
\item
  \emph{artifacts and inaccuracies in the aerial photographs and other
  covariate data sources};
\item
  \emph{weak or non-obvious relationships between environmental
  conditions and observed spatial distributions of soils};
\item
  \emph{use of inconsistent mapping methods};
\item
  \emph{digitizing error};
\item
  \emph{polygonization (mapping unit assignment) error};
\end{enumerate}

Another important source of uncertainty is the diversity of laboratory
methods (see further chapter \ref{statistical-theory}). Many columns in
the soil profile databases in pan-continental projects where produced by
merging data produced using a diversity of methods for data collection
and analysis (see e.g. \citet{Panagos2013439}). So even if all these are
quite precise, if we ignore harmonization of this data we introduce
intrinsic uncertainty which is practically invisible but possibly
significant.

\citet{kuhn2013applied} lists the four most common reasons why a
predictive model fails:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  inadequate pre-processing of the input data,
\item
  inadequate model validation,
\item
  unjustified extrapolation (application of the model to data that
  reside in a space unknown to the model),,
\item
  over-fitting of the model to the existing data,
\end{enumerate}

Each of these is addressed in further chapters and can often be tracked
back with repeated modeling and testing.

\hypertarget{quantifying-the-uncertainty-in-soil-data-products}{%
\subsection{Quantifying the uncertainty in soil data
products}\label{quantifying-the-uncertainty-in-soil-data-products}}

To quantify the uncertainty we must derive probability distributions.
There are three main approaches to achieve this
\citep{Brus2011EJSS, Heuvelink2014GSM}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Direct uncertainty quantification through geostatistical modelling of
  soil properties.
\item
  Geostatistical modelling of the error in existing soil property maps.
\item
  Expert judgement/heuristic approaches.
\end{enumerate}

In the first case uncertainty is directly reported by a geostatistical
model. However, any model is a simplified representation of reality, and
so is the geostatistical model, so that if our assumptions are incorrect
then also the estimate of the uncertainty will also be poor. A
model-free assessment of uncertainty can be produced by collecting
independent samples, preferably by using some pre-defined probability
sampling \citep{Brus2011EJSS}. This procedure basically works the same
way as for geostatistical modelling of the soil property itself. The
problem with model-free assessment of uncertainty is that this is often
the most expensive approach to quantification of uncertainty as new soil
samples need to be collected. Also, there is a difference between global
assessment of uncertainty and producing maps that depict spatial
patterns of uncertainty. To assess mean error over an entire study area
we might need only 50--100 points, but to accurately map the spatial
pattern of actual errors we might need an order of magnitude more
points.

\begin{rmdnote}
Uncertainty in soil data products can be quantified either via the
geostatistical model, or by using a model-free assessment of uncertainty
(independent validation), or by relying on expert judgement.
\end{rmdnote}

\hypertarget{common-uncertainty-levels-in-soil-maps}{%
\subsection{Common uncertainty levels in soil
maps}\label{common-uncertainty-levels-in-soil-maps}}

Even small errors can compound and propagate to much larger errors, so
that predictions can exceed realistic limits. In some cases, even though
we spend significant amounts of money to collect field data, we can
still produce statistically insignificant predictions. For example,
imagine if the location accuracy for soil profiles is ±5 km or poorer.
Even if all other data collection techniques are highly accurate, the
end result of mapping will be relatively poor because we are simply not
able to match the environmental conditions with the actual soil
measurements.

Already at that site level, soil survey can result in significant
uncertainty. \citet{Pleijsier1986ISRIC} sent the same soil samples to a
large number of soil labs in the world and then compared results they
got independently. This measure of uncertainty is referred to as the
\emph{``inter-laboratory variation''}. Soil lab analysis studies by
\citet{Pleijsier1986ISRIC} and \citeauthor{vanReeuwijk1982}
\citetext{\citeyear{vanReeuwijk1982}; \citealp{vanReeuwijk1984ISRIC}}
have shown that inter-laboratory variation in analytical results is much
greater than previously suspected.

As mentioned previously, if all other sources of error in the soil
mapping framework have been reduced, the only remaining strategy to
reduce uncertainty in soil maps is to increase sampling intensity (Fig.
\ref{fig:lagacherie1992}). This is again possible only up to a certain
degree --- even if we would sample the whole study area with an infinite
number of points, we would still not be able to explain some significant
portion of uncertainty. A map can never be 100\% valid
\citep{Oreskes04021994}.

\begin{figure}[t]

{\centering \includegraphics[width=0.85\linewidth]{figures/Fig_Lagacherie1992} 

}

\caption{Reduction of prediction error as a function of sampling intensity (for three control areas). Based on @Lagacherie1992PhD.}\label{fig:lagacherie1992}
\end{figure}

Soil mapping is not a trivial task. Validation results for soil maps can
often be discouraging. \citet{Kempen2011Geoderma} for example use the
highest quality soil (17 complete profiles per square-km) and auxiliary
data (high quantity of 25 m resolution maps) to map the distribution of
soil organic matter in a province of the Netherlands. The validation
results showed that, even with such high quality and density of input
data and extensive modeling, they were able to explain only an average
of 50\% of the variability in soil organic carbon (at 3D prediction
locations). This means that commonly, at the site level, we might
encounter a significant short-range variability, which is unmappable at
a feasible resolution resolution, that we will not be able to model even
with the most sophisticated methods.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_sigma_RMSE_relationship} 

}

\caption{Relationship between the numeric resolution (visualized using a histogram plot on the left) and amount of variation explained by the model for soil pH.}\label{fig:sigma-rmse-relationship}
\end{figure}

As a rule of thumb, the amount of variation explained by a model, when
assessed using validation, can be used to determine the numeric
resolution of the map. For example, if the sampling (or global) variance
of soil pH is 1.85 units (i.e.~s.d. = 1.36), then to be able to provide
an effective numeric resolution of 0.5 units, we need a model that can
explain at least 47\% of the original variance (Fig.
\ref{fig:sigma-rmse-relationship}). However, to be able to provide an
effective numeric resolution of 0.2 units, we would need a model that
explains 91\% of variability, which would be fairly difficult to
achieve.

\hypertarget{summary-and-conclusions}{%
\section{Summary and conclusions}\label{summary-and-conclusions}}

In this chapter we have presented and described conventional soil
resource inventories and soil data products and discussed how these are
related to new and emerging methods for automated soil mapping. We have
identified, reviewed and discussed the scientific theory and methods
that underlie both conventional and pedometric soil mapping and
discussed how each is related to the other within a framework of the
universal model of soil variation. We have provided an in-depth review
of the major sources of legacy soils data as collected by conventional
soil survey activities (point profile data, maps and expert knowledge)
and discussed the strengths and limitations of each source for
supporting current efforts to produce new soils information (within PSM)
using state-of-the-art Statistical and Machine Learning methods. We have
also outlined a vision of what a Soil Information System is and how such
systems can be configured and used to support production and
distribution of global maps of soil properties and soil classes using
PSM.

The main point of this chapter is to provide full documentation of, and
justification for, the choices that have been made in designing and
implementing the PSM framework. At present, PSM is designed to produce
local to global maps of soil properties and soil classes using legacy
soil data (point profile data, maps and expert knowledge), along with
available global covariate data, as inputs to multi-scale, hierarchical,
quantitative, global prediction models. At some future date, it is
hoped, and expected, that PSM will be able to make increasing use of
newly collected (likely crowd-sourced) field observations and laboratory
analysis data that are accurately geo-referenced, consistent, widespread
and of sufficient density to support production of accurate predictions
at finer spatial resolutions (e.g.~10's to 100's of m). In the meantime,
in order to produce interim products immediately, it is necessary, and
desirable, to make use of existing legacy soil data and existing
covariates. It is important to acknowledge and understand the
capabilities and limitations of the existing legacy data sources at our
disposal presently and of the methods that we currently possess to
process and use these data.

Each cycle of production in PSM is also a learning cycle that should
lead to improved methods, improved products and lower costs. PSM is not
a static process but, rather, it is a dynamic endeavour meant to grow,
evolve and improve through time. Initial products, produced using
existing legacy soil information sources, will increasingly evolve into
new products produced using a combination of existing legacy data and
newly collected data.

\hypertarget{software}{%
\chapter{Software installation and first steps}\label{software}}

\emph{Edited by: T. Hengl}

This section contains instructions on how to install and use software to
run predictive soil mapping and export results to GIS or web
applications. It has been written (as has most of the book) for Linux
users, but should not be too much of a problem to adoat to Microsoft
Windows OS and/or Mac OS.

\hypertarget{list-of-software-in-use}{%
\section{List of software in use}\label{list-of-software-in-use}}

\begin{figure}[t]

{\centering \includegraphics[width=0.6\linewidth]{figures/software_triangle} 

}

\caption{Software combination used in this book.}\label{fig:software-triangle}
\end{figure}

For processing the covariates we used a combination of Open Source GIS
software, primarily SAGA GIS \citep{gmd-8-1991-2015}, packages raster
\citep{raster}, sp \citep{pebesma2005classes}, and GDAL
\citep{mitchell2014geospatial} for reprojecting, mosaicking and merging
tiles. GDAL and parallel packages in R are highly suitable for
processing large data.

Software (required):

\begin{itemize}
\item
  \href{http://cran.r-project.org/bin/windows/base/}{R} or
  \href{https://mran.microsoft.com/download/}{MRO};
\item
  \href{http://www.rstudio.com/products/RStudio/}{RStudio};
\item
  R packages: GSIF, plotKML, aqp, ranger, caret, xgboost, plyr, raster,
  gstat, randomForest, ggplot2, e1071 (see:
  \href{http://www.r-bloggers.com/installing-r-packages/}{how to install
  R package})
\item
  \href{http://sourceforge.net/projects/saga-gis/}{SAGA GIS} (on Windows
  machines run windows installer);
\item
  Google Earth or Google Earth Pro;
\item
  \href{https://trac.osgeo.org/gdal/wiki/DownloadingGdalBinaries}{GDAL
  v2.x} for Windows machines use e.g.
  \href{http://download.gisinternals.com/sdk/downloads/release-1800-x64-gdal-2-1-3-mapserver-7-0-4/gdal-201-1800-x64-core.msi}{"gdal-*-1800-x64-core.msi"};
\end{itemize}

R script used in this tutorial can be downloaded from the
\textbf{\href{https://github.com/envirometrix/PredictiveSoilMapping}{github}}.
As a gentle introduction to the R programming language and to soil
classes in R we recommend the chapter on importing and using soil data.
Some more examples of SAGA GIS + R usage can be found in the soil
covariates chapter. To visualize spatial predictions in a web-browser or
Google Earth you could also consider following the soil web-maps
tutorial. As a gentle introduction to the R programming language and
spatial classes in R we recommend following
\href{https://geocompr.robinlovelace.net/}{the Geocomputation with R
book}. Obtaining also the
\href{https://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf}{R
reference card} is highly recommended.

\hypertarget{installing-software-on-ubuntu-os}{%
\section{Installing software on Ubuntu
OS}\label{installing-software-on-ubuntu-os}}

On Ubuntu (often the recommended standard for the GIS community) the
main required software can be installed within 10--20 minutes. We start
with installing GDAL, proj4 and some packages that you might need later
on:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sudo}\NormalTok{ apt-get install libgdal-dev libproj-dev libjasper-dev}
\FunctionTok{sudo}\NormalTok{ apt-get install gdal-bin python-gdal}
\end{Highlighting}
\end{Shaded}

Next, we can install R and RStudio. For R studio you can use the CRAN
distribution or the optimized distribution provided by (the former
REvolution company; now Microsoft):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wget}\NormalTok{ https://mran.blob.core.windows.net/install/mro/3.4.3/microsoft-r-open-3.4.3.tar.gz}
\FunctionTok{tar}\NormalTok{ -xf microsoft-r-open-3.4.3.tar.gz}
\BuiltInTok{cd}\NormalTok{ microsoft-r-open/}
\FunctionTok{sudo}\NormalTok{ ./install.sh}
\end{Highlighting}
\end{Shaded}

Note that R versions are constantly being updated so you will need to
replace the URL above based on information provided on the home page
(\url{http://mran.microsoft.com}). Once you run \texttt{install.sh} you
will have to accept the license terms two times before the installation
can be completed. If everything completes successfully, you can get the
session info by:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sessionInfo}\NormalTok{()}
\CommentTok{#> R version 3.5.1 (2018-07-02)}
\CommentTok{#> Platform: x86_64-redhat-linux-gnu (64-bit)}
\CommentTok{#> Running under: Fedora 28 (Workstation Edition)}
\CommentTok{#> }
\CommentTok{#> Matrix products: default}
\CommentTok{#> BLAS/LAPACK: /usr/lib64/R/lib/libRblas.so}
\CommentTok{#> }
\CommentTok{#> locale:}
\CommentTok{#>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              }
\CommentTok{#>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    }
\CommentTok{#>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   }
\CommentTok{#>  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 }
\CommentTok{#>  [9] LC_ADDRESS=C               LC_TELEPHONE=C            }
\CommentTok{#> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       }
\CommentTok{#> }
\CommentTok{#> attached base packages:}
\CommentTok{#> [1] stats     graphics  grDevices utils     datasets  methods   base     }
\CommentTok{#> }
\CommentTok{#> other attached packages:}
\CommentTok{#> [1] microbenchmark_1.4-4}
\CommentTok{#> }
\CommentTok{#> loaded via a namespace (and not attached):}
\CommentTok{#>  [1] Rcpp_0.12.19     bookdown_0.7     codetools_0.2-15 digest_0.6.18   }
\CommentTok{#>  [5] rprojroot_1.3-2  backports_1.1.2  magrittr_1.5     evaluate_0.11   }
\CommentTok{#>  [9] stringi_1.2.4    rmarkdown_1.10   tools_3.5.1      stringr_1.3.1   }
\CommentTok{#> [13] xfun_0.3         yaml_2.2.0       compiler_3.5.1   htmltools_0.3.6 }
\CommentTok{#> [17] knitr_1.20}
\KeywordTok{system}\NormalTok{(}\StringTok{"gdalinfo --version"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This shows, for example, that the this installation of R is based on the
Ubuntu 16.* LTS and the version of GDAL is up to date. Using an
optimized distribution of R (read more about
\href{https://mran.microsoft.com/documents/rro/multithread}{``The
Benefits of Multithreaded Performance with Microsoft R Open''}) is
especially important if you plan to use R for production purposes
i.e.~to optimize computing and generation of soil maps for large numbers
of pixels.

To install RStudio we can run:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sudo}\NormalTok{ apt-get install gdebi-core}
\FunctionTok{wget}\NormalTok{ https://download1.rstudio.org/rstudio-1.1.447-amd64.deb }
\FunctionTok{sudo}\NormalTok{ gdebi rstudio-1.1.447-amd64.deb}
\FunctionTok{sudo}\NormalTok{ rm rstudio-1.1.447-amd64.deb}
\end{Highlighting}
\end{Shaded}

Again, RStudio is constantly updated so you might have to adjust the
rstudio version and distribution.

Predictive soil mapping is about making maps, and working with maps
requires use of GIS software to open, view overlay and analyze the data
sptially. GIS software recommended for soil mapping in this book
consists of SAGA GIS, QGIS, GRASS GIS and Google Earth. To, install SAGA
GIS on Ubuntu we can use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sudo}\NormalTok{ add-apt-repository ppa:ubuntugis/ubuntugis-unstable}
\FunctionTok{sudo}\NormalTok{ apt-get update}
\FunctionTok{sudo}\NormalTok{ apt-get install saga}
\end{Highlighting}
\end{Shaded}

If installation was successful, you should be able to access SAGA
command line also from R by using:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system}\NormalTok{(}\StringTok{"saga_cmd --version"}\NormalTok{)}
\CommentTok{#> Warning in system("saga_cmd --version"): error in running command}
\end{Highlighting}
\end{Shaded}

To install QGIS (\url{https://download.qgis.org/}) you might first have
to add the location of the debian libraries:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sudo}\NormalTok{ sh -c }\StringTok{'echo "deb http://qgis.org/debian xenial main" >> /etc/apt/sources.list'}  
\FunctionTok{sudo}\NormalTok{ sh -c }\StringTok{'echo "deb-src http://qgis.org/debian xenial main " >> /etc/apt/sources.list'}  
\FunctionTok{sudo}\NormalTok{ apt-get update }
\FunctionTok{sudo}\NormalTok{ apt-get install qgis python-qgis qgis-plugin-grass}
\end{Highlighting}
\end{Shaded}

Other utility software that you might need include \texttt{htop} that
allows you to track processing progress:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sudo}\NormalTok{ apt-get install htop iotop}
\end{Highlighting}
\end{Shaded}

and some additional libraries use \texttt{devtools}, \texttt{geoR} and
similar, which can be installed via:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sudo}\NormalTok{ apt-get install build-essential automake}\KeywordTok{;} 
        \ExtensionTok{libcurl4-openssl-dev}\NormalTok{ pkg-config libxml2-dev}\KeywordTok{;}
        \ExtensionTok{libfuse-dev}\NormalTok{ mtools libpng-dev libudunits2-dev}
\end{Highlighting}
\end{Shaded}

You might also need the \texttt{7z} software for easier compression and
\texttt{pigz} for parallelized compression:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sudo}\NormalTok{ apt-get install pigz zip unzip p7zip-full }
\end{Highlighting}
\end{Shaded}

\hypertarget{Whitebox}{%
\section{WhiteboxTools}\label{Whitebox}}

WhiteboxTools (\url{http://www.uoguelph.ca/~hydrogeo/WhiteboxTools/}),
contributed by John Lindsay, is an extensive suite of functions and
tools for DEM analysis which is especially useful for extending the
hydrological and morphometric analysis tools available in SAGA GIS and
GRASS GIS \citep{lindsay2016whitebox}. Probably the easiest way to use
WhiteboxTools is to install a QGIS plugin (kindly maintained by
Alexander Bruy: \url{https://plugins.bruy.me/}) and then learn and
extend the WhiteboxTools scripting language by testing things out in
QGIS (see below).

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/whiteboxtools-preview} 

}

\caption{Calling WhiteboxTools from QGIS via the WhiteboxTools plugin.}\label{fig:whiteboxtools-preview}
\end{figure}

The function \texttt{FlowAccumulationFullWorkflow} is, for example, a
wrapper function to filter out all spurious sinks and to derive a
hydrological flow accumulation map in the same step. To run it from
command line we can use:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{'"/home/tomislav/software/WBT/whitebox_tools" '}\NormalTok{,}
  \StringTok{'--run=FlowAccumulationFullWorkflow --dem="./extdata/DEMTOPx.tif" '}\NormalTok{,}
  \StringTok{'--out_type="Specific Contributing Area" --log="False" --clip="False" --esri_pntr="False" '}\NormalTok{,}
  \StringTok{'--out_dem="./extdata/DEMTOPx_out.tif" '}\NormalTok{,}
  \StringTok{'--out_pntr="./extdata/DEMTOPx_pntr.tif" '}\NormalTok{,}
  \StringTok{'--out_accum="./extdata/DEMTOPx_accum.tif" -v'}\NormalTok{))}
\CommentTok{#> Warning in system(paste0("\textbackslash{}"/home/tomislav/software/WBT/whitebox_tools\textbackslash{}"}
\CommentTok{#> ", : error in running command}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/eberg_hydroflow_preview_3d} 

}

\caption{Hydrological flow accummulation map based on the Ebergotzen DEM derived using WhiteboxTools.}\label{fig:eberg-hydroflow-preview-3d}
\end{figure}

This produces a number of maps, from which the hydrological flow
accumulation map is usually the most useful. It is highly recommended
that, before running analysis on large DEM's using WhiteboxTools and/or
SAGA GIS, you test functionality using smaller data sets i.e.~either a
subset of the original data or using a DEM at very coarse resolutions
(so that width and height of a DEM are only few hundred pixels). Also
note that WhiteboxTools do not presently work with GeoTIFs that use the
\texttt{COMPRESS=DEFLATE} creation options.

\hypertarget{Rstudio}{%
\section{RStudio}\label{Rstudio}}

RStudio is, in principle, the main R scripting environment and can be
used to control all other software used in this tutorial. A more
detailed RStudio tutorial is available at:
\href{http://www.rstudio.com/resources/training/online-learning/}{RStudio
--- Online Learning}. Consider also following some spatial data
tutorials e.g.~by James Cheshire (\url{http://spatial.ly/r/}). Below is
an example of RStudio session with R editor on right and R console on
left.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/rstudio_example} 

}

\caption{RStudio is a commonly used R editor written in C++.}\label{fig:rstudio-example}
\end{figure}

To install all required R packages used in this tutorial at once, you
can use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ls <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"rgdal"}\NormalTok{, }\StringTok{"raster"}\NormalTok{, }\StringTok{"GSIF"}\NormalTok{, }\StringTok{"plotKML"}\NormalTok{, }
        \StringTok{"nnet"}\NormalTok{, }\StringTok{"plyr"}\NormalTok{, }\StringTok{"ROCR"}\NormalTok{, }\StringTok{"randomForest"}\NormalTok{, }
        \StringTok{"psych"}\NormalTok{, }\StringTok{"mda"}\NormalTok{, }\StringTok{"h2o"}\NormalTok{, }\StringTok{"dismo"}\NormalTok{, }\StringTok{"grDevices"}\NormalTok{, }
        \StringTok{"snowfall"}\NormalTok{, }\StringTok{"hexbin"}\NormalTok{, }\StringTok{"lattice"}\NormalTok{, }\StringTok{"ranger"}\NormalTok{, }
        \StringTok{"soiltexture"}\NormalTok{, }\StringTok{"aqp"}\NormalTok{, }\StringTok{"colorspace"}\NormalTok{, }\StringTok{"Cubist"}\NormalTok{,}
        \StringTok{"randomForestSRC"}\NormalTok{, }\StringTok{"ggRandomForests"}\NormalTok{, }\StringTok{"scales"}\NormalTok{,}
        \StringTok{"xgboost"}\NormalTok{, }\StringTok{"parallel"}\NormalTok{, }\StringTok{"doParallel"}\NormalTok{, }\StringTok{"caret"}\NormalTok{, }
        \StringTok{"gam"}\NormalTok{, }\StringTok{"glmnet"}\NormalTok{, }\StringTok{"matrixStats"}\NormalTok{, }\StringTok{"SuperLearner"}\NormalTok{,}
        \StringTok{"quantregForest"}\NormalTok{, }\StringTok{"LITAP"}\NormalTok{, }\StringTok{"intamap"}\NormalTok{)}
\NormalTok{new.packages <-}\StringTok{ }\NormalTok{ls[}\OperatorTok{!}\NormalTok{(ls }\OperatorTok{%in%}\StringTok{ }\KeywordTok{installed.packages}\NormalTok{()[,}\StringTok{"Package"}\NormalTok{])]}
\ControlFlowTok{if}\NormalTok{(}\KeywordTok{length}\NormalTok{(new.packages)) }\KeywordTok{install.packages}\NormalTok{(new.packages)}
\end{Highlighting}
\end{Shaded}

This will basically check if any package is installed already, then
install it only if it is missing. You can put this line at the top of
each R script that you share so that anybody using that script will
automatically get all required packages.

The h2o package requires Java libraries, so you should first install
Java by using e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sudo}\NormalTok{ add-apt-repository ppa:webupd8team/java}
\FunctionTok{sudo}\NormalTok{ apt-get update}
\FunctionTok{sudo}\NormalTok{ apt-get install oracle-java8-installer}
\ExtensionTok{java}\NormalTok{ -version}
\end{Highlighting}
\end{Shaded}

\hypertarget{plotkml-and-gsif-packages}{%
\section{plotKML and GSIF packages}\label{plotkml-and-gsif-packages}}

Many examples in this course rely on the top 5 most commonly used
packages for spatial data: (1)
\href{https://cran.r-project.org/web/views/Spatial.html}{sp and rgdal},
(2) \href{https://cran.r-project.org/web/packages/raster/}{raster}, (3)
\href{http://plotkml.r-forge.r-project.org/}{plotKML} and (4)
\href{http://gsif.r-forge.r-project.org/}{GSIF}. To install the most
up-to-date version of plotKML/GSIF, you can also use the R-Forge
versions of the package:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(GSIF))\{}
  \KeywordTok{install.packages}\NormalTok{(}\StringTok{"GSIF"}\NormalTok{, }\DataTypeTok{repos=}\KeywordTok{c}\NormalTok{(}\StringTok{"http://R-Forge.R-project.org"}\NormalTok{), }
                 \DataTypeTok{type =} \StringTok{"source"}\NormalTok{, }\DataTypeTok{dependencies =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{\}}
\CommentTok{#> Loading required package: GSIF}
\CommentTok{#> GSIF version 0.5-4 (2017-04-25)}
\CommentTok{#> URL: http://gsif.r-forge.r-project.org/}
\end{Highlighting}
\end{Shaded}

A copy of the most-up-to-date and stable versions of plotKML and GSIF is
also available on \href{https://github.com/cran/GSIF}{github}. To run
only some specific function from the GSIF package you could do for
example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{source_https <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(url, ...) \{}
   \CommentTok{# load package}
   \KeywordTok{require}\NormalTok{(RCurl)}
   \CommentTok{# download:}
   \KeywordTok{cat}\NormalTok{(}\KeywordTok{getURL}\NormalTok{(url, }\DataTypeTok{followlocation =} \OtherTok{TRUE}\NormalTok{, }
       \DataTypeTok{cainfo =} \KeywordTok{system.file}\NormalTok{(}\StringTok{"CurlSSL"}\NormalTok{, }\StringTok{"cacert.pem"}\NormalTok{, }\DataTypeTok{package =} \StringTok{"RCurl"}\NormalTok{)), }
       \DataTypeTok{file =} \KeywordTok{basename}\NormalTok{(url))}
   \KeywordTok{source}\NormalTok{(}\KeywordTok{basename}\NormalTok{(url))}
\NormalTok{\}}
\KeywordTok{source_https}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/cran/GSIF/master/R/OCSKGM.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To test if these packages work properly, create soil maps and visualize
them in Google Earth by running the following lines of code (see also
function:
\href{http://gsif.r-forge.r-project.org/fit.gstatModel.html}{fit.gstatModel}):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(GSIF)}
\KeywordTok{library}\NormalTok{(sp)}
\KeywordTok{library}\NormalTok{(boot)}
\KeywordTok{library}\NormalTok{(aqp)}
\CommentTok{#> This is aqp 1.16-3}
\KeywordTok{library}\NormalTok{(plyr)}
\KeywordTok{library}\NormalTok{(rpart)}
\KeywordTok{library}\NormalTok{(splines)}
\KeywordTok{library}\NormalTok{(gstat)}
\KeywordTok{library}\NormalTok{(quantregForest)}
\CommentTok{#> Loading required package: randomForest}
\CommentTok{#> randomForest 4.6-14}
\CommentTok{#> Type rfNews() to see new features/changes/bug fixes.}
\CommentTok{#> Loading required package: RColorBrewer}
\KeywordTok{library}\NormalTok{(plotKML)}
\CommentTok{#> plotKML version 0.5-8 (2017-05-12)}
\CommentTok{#> URL: http://plotkml.r-forge.r-project.org/}
\KeywordTok{demo}\NormalTok{(meuse, }\DataTypeTok{echo=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{omm <-}\StringTok{ }\KeywordTok{fit.gstatModel}\NormalTok{(meuse, om}\OperatorTok{~}\NormalTok{dist}\OperatorTok{+}\NormalTok{ffreq, meuse.grid, }\DataTypeTok{method=}\StringTok{"quantregForest"}\NormalTok{)}
\CommentTok{#> Fitting a Quantile Regression Forest model...}
\CommentTok{#> Fitting a 2D variogram...}
\CommentTok{#> Saving an object of class 'gstatModel'...}
\NormalTok{om.rk <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(omm, meuse.grid)}
\CommentTok{#> Subsetting observations to fit the prediction domain in 2D...}
\CommentTok{#> Prediction error for 'randomForest' model estimated using the 'quantreg' package.}
\CommentTok{#> Generating predictions using the trend model (RK method)...}
\CommentTok{#> [using ordinary kriging]}
\CommentTok{#> }
\DecValTok{100}\NormalTok{% done}
\CommentTok{#> Running 5-fold cross validation using 'krige.cv'...}
\CommentTok{#> Creating an object of class "SpatialPredictions"}
\NormalTok{om.rk}
\CommentTok{#>   Variable           : om }
\CommentTok{#>   Minium value       : 1 }
\CommentTok{#>   Maximum value      : 17 }
\CommentTok{#>   Size               : 153 }
\CommentTok{#>   Total area         : 4964800 }
\CommentTok{#>   Total area (units) : square-m }
\CommentTok{#>   Resolution (x)     : 40 }
\CommentTok{#>   Resolution (y)     : 40 }
\CommentTok{#>   Resolution (units) : m }
\CommentTok{#>   Vgm model          : Exp }
\CommentTok{#>   Nugget (residual)  : 2.32 }
\CommentTok{#>   Sill (residual)    : 4.76 }
\CommentTok{#>   Range (residual)   : 2930 }
\CommentTok{#>   RMSE (validation)  : 1.75 }
\CommentTok{#>   Var explained      : 73.8% }
\CommentTok{#>   Effective bytes    : 1203 }
\CommentTok{#>   Compression method : gzip}
\CommentTok{#plotKML(om.rk)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=0.9\linewidth]{figures/ge_preview} 

}

\caption{Example of plotKML output.}\label{fig:ge-preview}
\end{figure}

\hypertarget{connecting-r-and-saga-gis}{%
\section{Connecting R and SAGA GIS}\label{connecting-r-and-saga-gis}}

SAGA GIS is an extensive GIS geoprocessing software with over
\href{http://www.saga-gis.org/saga_tool_doc/index.html}{600 functions}.
SAGA GIS can not be installed from RStudio (it is not a package for R).
Instead, you need to install SAGA GIS using the installation
instructions from the
\href{https://sourceforge.net/projects/saga-gis/}{software homepage}.
After you have installed SAGA GIS, you can send processes from R to SAGA
GIS by using the \texttt{saga\_cmd} command line interface:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{Sys.info}\NormalTok{()[}\StringTok{'sysname'}\NormalTok{]}\OperatorTok{==}\StringTok{"Linux"}\NormalTok{)\{}
\NormalTok{  saga_cmd =}\StringTok{ "C:/Progra~1/SAGA-GIS/saga_cmd.exe"}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{  saga_cmd =}\StringTok{ "saga_cmd"}
\NormalTok{\}}
\KeywordTok{system}\NormalTok{(}\KeywordTok{paste}\NormalTok{(saga_cmd, }\StringTok{"-v"}\NormalTok{))}
\CommentTok{#> Warning in system(paste(saga_cmd, "-v")): error in running command}
\end{Highlighting}
\end{Shaded}

To use some SAGA GIS function you need to carefully follow the
\href{http://www.saga-gis.org/saga_tool_doc/index.html}{SAGA GIS command
line arguments}. For example,

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(plotKML)}
\KeywordTok{library}\NormalTok{(rgdal)}
\CommentTok{#> rgdal: version: 1.3-4, (SVN revision 766)}
\CommentTok{#>  Geospatial Data Abstraction Library extensions to R successfully loaded}
\CommentTok{#>  Loaded GDAL runtime: GDAL 2.2.4, released 2018/03/19}
\CommentTok{#>  Path to GDAL shared files: /usr/share/gdal}
\CommentTok{#>  GDAL binary built with GEOS: TRUE }
\CommentTok{#>  Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493]}
\CommentTok{#>  Path to PROJ.4 shared files: (autodetected)}
\CommentTok{#>  Linking to sp version: 1.3-1}
\KeywordTok{library}\NormalTok{(raster)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'raster'}
\CommentTok{#> The following objects are masked from 'package:aqp':}
\CommentTok{#> }
\CommentTok{#>     metadata, metadata<-}
\KeywordTok{data}\NormalTok{(}\StringTok{"eberg_grid"}\NormalTok{)}
\KeywordTok{gridded}\NormalTok{(eberg_grid) <-}\StringTok{ }\ErrorTok{~}\NormalTok{x}\OperatorTok{+}\NormalTok{y}
\KeywordTok{proj4string}\NormalTok{(eberg_grid) <-}\StringTok{ }\KeywordTok{CRS}\NormalTok{(}\StringTok{"+init=epsg:31467"}\NormalTok{)}
\KeywordTok{writeGDAL}\NormalTok{(eberg_grid[}\StringTok{"DEMSRT6"}\NormalTok{], }\StringTok{"./extdata/DEMSRT6.sdat"}\NormalTok{, }\StringTok{"SAGA"}\NormalTok{)}
\KeywordTok{system}\NormalTok{(}\KeywordTok{paste}\NormalTok{(saga_cmd, }\StringTok{'ta_lighting 0 -ELEVATION "./extdata/DEMSRT6.sgrd" }
\StringTok{             -SHADE "./extdata/hillshade.sgrd" -EXAGGERATION 2'}\NormalTok{))}
\CommentTok{#> Warning in system(paste(saga_cmd, "ta_lighting 0 -ELEVATION \textbackslash{}"./extdata/}
\CommentTok{#> DEMSRT6.sgrd\textbackslash{}" \textbackslash{}n -SHADE \textbackslash{}"./extdata/hillshade.sgrd\textbackslash{}" -EXAGGERATION 2")):}
\CommentTok{#> error in running command}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/rstudio_saga_gis} 

}

\caption{Deriving hillshading using SAGA GIS and then visualizing the result in R.}\label{fig:rstudio-saga-gis}
\end{figure}

\hypertarget{connecting-r-and-gdal}{%
\section{Connecting R and GDAL}\label{connecting-r-and-gdal}}

Another very important software for handling spatial data (and
especially for exchanging / converting spatial data) is GDAL. GDAL also
needs to be installed separately (for Windows machines use e.g.
\href{http://download.gisinternals.com/sdk/downloads/release-1800-x64-gdal-2-1-3-mapserver-7-0-4/gdal-201-1800-x64-core.msi}{``gdal-201-1800-x64-core.msi''})
and then can be called from command line:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(.Platform}\OperatorTok{$}\NormalTok{OS.type }\OperatorTok{==}\StringTok{ "windows"}\NormalTok{)\{}
\NormalTok{  gdal.dir <-}\StringTok{ }\KeywordTok{shortPathName}\NormalTok{(}\StringTok{"C:/Program files/GDAL"}\NormalTok{)}
\NormalTok{  gdal_translate <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(gdal.dir, }\StringTok{"/gdal_translate.exe"}\NormalTok{)}
\NormalTok{  gdalwarp <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(gdal.dir, }\StringTok{"/gdalwarp.exe"}\NormalTok{) }
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{  gdal_translate =}\StringTok{ "gdal_translate"}
\NormalTok{  gdalwarp =}\StringTok{ "gdalwarp"}
\NormalTok{\}}
\KeywordTok{system}\NormalTok{(}\KeywordTok{paste}\NormalTok{(gdalwarp, }\StringTok{"--help"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We can use GDAL to reproject the grid from the previous example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system}\NormalTok{(}\StringTok{'gdalwarp ./extdata/DEMSRT6.sdat ./extdata/DEMSRT6_ll.tif -t_srs }\CharTok{\textbackslash{}"}\StringTok{+proj=longlat +datum=WGS84}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{)}
\KeywordTok{library}\NormalTok{(raster)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{raster}\NormalTok{(}\StringTok{"./extdata/DEMSRT6_ll.tif"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{Software_files/figure-latex/plot-eberg-ll-1} 

}

\caption{Ebergotzen DEM reprojected in geographical coordinates.}\label{fig:plot-eberg-ll}
\end{figure}

\hypertarget{soil-variables-chapter}{%
\chapter{Soil observations and variables}\label{soil-variables-chapter}}

\emph{Edited by: Hengl T., MacMillan R.A. and Leenaars J.G.B.}

This chapter identifies, and provides comprehensive definitions and
descriptions for, a standardized set of soil properties (and classes)
that are commonly predicted using PSM. We first discuss the complexity
of measuring and standardizing (or harmonizing) soil attributes, then
focus on the key soil properties and classes of interest for global soil
mapping. The purpose of this chapter is to serve as a reference, and
background, for other chapters where the focus is on generating soil
maps, interpreting accuracy results and similar.

The R tutorial at the end of the chaper reviews soil data classes and
functions for R. It illustrates how to organize and reformat soil data
in R for spatial analysis, how to import soil data to R and how to
export data and plot it in Google Earth. To learn more about the Global
Soil Information Facilities (GSIF) package, visit the main
\href{http://gsif.r-forge.r-project.org/00Index.html}{documentation
page}.

\hypertarget{basic-concepts-1}{%
\section{Basic concepts}\label{basic-concepts-1}}

\hypertarget{types-of-soil-observations}{%
\subsection{Types of soil
observations}\label{types-of-soil-observations}}

As mentioned in the previous chapter, values for soil properties or
attributes are obtained through observation and/or measurement of a soil
feature, using a specified method. We refer to observations and
measurements of the characteristics of soil properties and/or feature
attributes as \emph{soil observations} (see also the
\href{http://www.opengeospatial.org/standards/om}{Observation and
Measurements OGC standard}; ISO/DIS 19156). From the perspective of the
technology used, soil observations can be grouped as follows (see also
Fig. \ref{fig:soil-vars}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Direct measurements obtained using analytical instruments and
  procedures in a laboratory or in the field} --- the results of
  measurements are analytical values considered representative for a
  given soil property.
\item
  \emph{Indirect soil measurements obtained using mechanical devices,
  analytical instruments and procedures} --- measurement of soil
  properties that can be used to infer information about a different
  target soil property. These can be based on soil spectroscopy and
  similar close-range or remote sensing systems
  \citep{ShepherdWalsh2007JNIS, ViscarraRossel2010DSS}.
\item
  \emph{Direct observations of soil properties and interpretations} ---
  subjectively assessed values based on protocols for soil description
  as presented in manuals i.e.~abundance of mottles, soil drainage
  class, soil colour.
\item
  \emph{Indirect or derived interpretations} --- subjectively assessed
  values or conditions based mainly on an expert's knowledge and
  interpretation of observations e.g.~soil classification, soil
  fertility class.
\end{enumerate}

\begin{figure}[t]

{\centering \includegraphics[width=0.65\linewidth]{figures/Fig_types_observations} 

}

\caption{Types of soil observations in relation to data usage and production costs. Descriptive soil observations (e.g. manual texture or diagnostic soil horizons) are often not directly useable by end users, who are often more interested in specific secondary soil properties (e.g. water holding capacity, erosion index, soil fertility) as inputs to their modeling. However, descriptive field observations are often some orders of magnitude more affordable to obtain than laboratory analysis.}\label{fig:soil-vars}
\end{figure}

\begin{rmdnote}
Soil can be assessed quantitatively based on direct or indirect
measurements using analytical techniques (in a laboratory or in the
field) and qualitatively or descriptively based on observations in the
field that adhere to some soil description guidelines. Examples of
subjective observations are: diagnostic soil materials and horizons,
soil classes, Munsell color classes, manual texture assessment
(texture-by-hand), structure, compaction, root abundance and similar.
\end{rmdnote}

Field campaigns are usuallly the most costly part of soil surveys. Large
numbers of soil observations are made in the field to assess the spatial
distribution of readily observable soil properties to provide empirical
evidence for soil mapping. Because a soil analytical measurement in the
laboratory is generally much more costly than a soil observation in the
field, only a smaller subset of soil samples is taken from the larger
number of field soil observations and brought to the laboratory for
subsequent analysis. Ideally, every soil observation would be
accompanied by corresponding soil analytical measurements to produce the
most accurate and comprehensive soil information possible.

It is important to emphasize that soil properties, and the methods used
to assess soil properties, are two distinctly different concepts. The
two can be defined together (functional definition) or can be defined
separately, as given by numerous national and international manuals and
guidelines for analytical procedures and soil description: e.g.~in
\citeauthor{Burt2004SSIR}
\citetext{\citeyear{Burt2004SSIR}; \citealp{carter2007soil}; \citealp{food2006guidelines}},
and/or \citet{VanReeuwijk2002}. Also in this chapter we make a
distinction between the \emph{`target variable'} (i.e.~target soil
properties) and \emph{`paths'} ( i.e.~determination methods).

Soil analytical data obtained in a laboratory are typically an order of
magnitude more expensive to produce than descriptive field observations
\citep{Burrough1971, GehlRice2005, Kempen2011PhDthesis}. To reduce these
high costs, surveyors collect descriptive soil observations (Fig.
\ref{fig:soil-vars}), which can subsequently be interpreted and linked
to soil types and soil classes, which are then assumed to be
characterised by a limited and definable range of soil properties
\citep{bouma1998exploring}. It is also possible to convert observed
values for certain soil properties to values comparable to those
measured by analytical methods (albeit with unknown precision) by using
various calibration models or \emph{conversion functions}. For example,
\emph{manual texturing} analysis \citep{FAO1990, SSDS1993} can be used
as a basis for estimating soil texture fractions with a precision of ±5
\% at fraction of the cost of laboratory analysis.

Soils are usually sampled per depth interval or layer, generally using a
genetic A-B-C-R system i.e.~corresponding to a \emph{soil horizon} --- a
relatively homogeneous layer of soil (with upper and lower depth) that
is \emph{``distinctly different from other layers and informative for
the soil's nature''} \citep{harpstead2001soil}. Actual soil samples are
either taken from the centre of a soil horizon or are mixed samples of
the material from the whole horizon (Fig. \ref{fig:soi-var-depth}).
Decades of soil survey have shown that soil horizons can be fuzzy
objects. They may be difficult for different surveyors to distinguish
and delineate consistently
\citep{Burrough1989JSS, DeGruijter1997Geoderma}. Soil correlation
exercises try (not always successfully) to help different surveyors
consistently recognize similar soil horizons and assign similar codes
with comparable upper and lower boundaries so as to produce similar
descriptions and classifications for any observed soil.

\begin{figure}[t]

{\centering \includegraphics[width=0.7\linewidth]{figures/Fig_soi_var_depth} 

}

\caption{Soil observations can refer to genetic horizons (left), fixed depths i.e. point support (center) and/or can be aggregate values for the complete profile (right).}\label{fig:soi-var-depth}
\end{figure}

An emerging approach to soil characterization is to scan the complete
soil profile in different parts of the spectra, and then decide on
vertical stratification \emph{a posteriori}
\citep{ViscarraRossel2010DSS}. Nevertheless, much of the analytical data
available in existing legacy soil profile databases is sampled per soil
layer and described by soil horizon.

Soil observations are taken at a geographic position and at a specific
depth (or depth interval), which is either 3D or refers to the whole
solum. The 3D (longitude, latitude, depth) position implies that the
property varies not only in geographic space, but also with depth. Soil
properties that describe an entire site are by implication 2D, as are
soil properties that summarise or refer to the soil profile as a whole
(2D). For example, soil type does not change with depth. Also rock
outcrops, depth to bedrock and depth to ground water table are single
attributes that apply to an entire profile.

\hypertarget{soil-properties-of-interest-for-global-soil-mapping}{%
\subsection{Soil properties of interest for global soil
mapping}\label{soil-properties-of-interest-for-global-soil-mapping}}

There are many soil properties, possibly hundreds, used in the
international domain of soil science including pedology, soil survey,
soil fertility, soil hydrology, soil biology, etc. Not all of these can
be mapped globally, nor are all of explicit interest for global
applications or use.

Soil data have been, and are, collected and compiled into maps at
various scales for various purposes and soil inventory projects
typically begin by first carefully identifying the specific list of soil
properties that are of interest for the anticipated uses of the planned
survey. Different soil data are required for different purposes, such as
applying different models with different data requirements.

In the past, soil surveys typically elected to focus on observing and
measuring soil attributes and properties that were considered to be
relatively stable, or static, in time. For example the particle size
distribution of a soil, or its depth to bedrock, were considered to be
relatively stable and not subject to large changes over relatively short
time periods (e.g.~decades). Even attributes that were known to change
with management and time, such as topsoil thickness, organic carbon or
pH, were treated as relatively stable properties for the purposes of
mapping.

This choice to emphasize relatively stable soil properties and
attributes was a logical consequence of the fact that it could take
years to produce a single soil map and decades to complete mapping for
an entire area of interest. Consequently, for maps to be relevant, and
to remain relevant and useful for their anticipated lifetime of use,
they had to restrict themselves to trying to describe the variation in
space only (not time) of properties that could be considered stable and
static.

The idea that soil properties could be assumed to remain relatively
stable through time was partially based on an assumption that most soils
had achieved a relatively stable condition that was in equilibruim with
their current environment. If a soil is in equilibruim with its
enviromment, it can be assumed that it will retain its present
attributes, since there are no strong drivers for change. This may well
apply to undisturbed soils in their natural environment, but it is not
valid for disturbed or managed soils. It is well established that human
management practices can, and do, significantly alter some key soil
properties, such as pH, organic matter and topsoil thickness. Most
conventional soil maps recognized, and reported on, differences in soil
properties, such as pH or organic matter, between natural soils and
managed soils. However, it was never a common practice to name, map and
characterize managed soils seperately from natural soils.

Local or national soil survey projects are of direct relevance to global
soil mapping initiatives if the range of data collected encompasses the
minimum data set as specified for global initiatives. For example,
completion of an update to the SOTER database for the World requires an
extensive range of soil property data as specified in the procedures
manual \citep{VanEngelen2012}. An update of the Harmonised World Soil
Database \citep{FAO2012HWSD} requires a smaller range of attributes. The
\emph{GlobalSoilMap} project \citep{Arrouays201493} selected a list of
only \emph{twelve soil properties} considered relevant for global
analyses and feasible to map globally. This list includes seven basic
attributes, assessed through primary observation or measurement, and
three derived attributes which are calculated from the primary soil
properties (Tbl. \ref{tab:globalsoilmap}). These attributes are being
mapped (as and where possible) at a fine resolution of six depth
intervals in the vertical and, 3--arcseconds in the horizontal dimension
(ca. 100 m) (Fig. \ref{fig:scheme-solum}).

\begin{figure}[t]

{\centering \includegraphics[width=0.75\linewidth]{figures/Fig_scheme_solum} 

}

\caption{Standard soil horizons, solum thickness and depth to bedrock (left), six standard depths used in the *GlobalSoilMap* project (right).}\label{fig:scheme-solum}
\end{figure}

\begin{table}

\caption{\label{tab:globalsoilmap}The *GlobalSoilMap* project has selected seven primary (depth to bedrock, organic carbon content, pH, soil texture fractions, coarse fragments), three derived (effective soil depth, bulk density and available water capacity) and two optional (effective cation exchange capacity and electrical conductivity) target soil properties of interest for global soil mapping and modelling.}
\centering
\begin{tabular}[t]{lll}
\toprule
Variable.name & Units & Reference\\
\midrule
Total profile depth (depth to bedrock) & cm & (SSDS, [-@SSDS1993], p.5)\\
Plant exploitable (effective depth) & cm & (SSDS, [-@SSDS1993], p.60)\\
Soil organic carbon (dry combustion) & permille & ISO 10694\\
pH index (the 1:5 H\$\_2\$O solution) & – & ISO 10390\\
Sand content (gravimetric) & \% & (NRCS, [-@Burt2004SSIR], p.347)\\
\addlinespace
Silt content (gravimetric) & \% & (NRCS, [-@Burt2004SSIR], p.347)\\
Clay content (gravimetric) & \% & (NRCS, [-@Burt2004SSIR], p.347)\\
Coarse fragments (volumetric) & \% & (NRCS, [-@Burt2004SSIR], p.36)\\
Effective Cation Exchange Capacity & cmol & ISO 11260\\
Bulk density of the whole soil & kg/m\$\textasciicircum{}3\$ & ISO 11272\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{reference-methods}{%
\subsection{Reference methods}\label{reference-methods}}

A pragmatic solution to ensuring efficient exchange, sharing and
interpretation of global soil data is to establish reference methods for
soil measurement and description. The \emph{GlobalSoilMap} project
agreed that their target soil properties would be assessed and reported
relative to specific, designated \emph{reference methods}. For example,
soil organic carbon content of the fine earth fraction is to be assessed
and reported according to ISO10694 dry combustion method
\citep{Sleutel2007CSSPA}. Values for pH are to be be reported for a 1:5
suspension of soil in water or using the CaCl\(_2\) solution, with a
precision of 1 decimal place. It has also been recommended that ISO TC
190 --- soil quality standards --- should be used to assess and report
all data measured from air-dried soil samples.

Soil properties designated as optional for the \emph{GlobalSoilMap}
consortium include Effective Cation Exchange Capacity assessed and
reported according to ISO11260 Barium Chloride (cmol+/kg = centi-mole+
per kilogram) and Electrical conductivity in 1:1 soil--water solution
(dS/m = deci-siemens per metre). The list of soil properties identified
for global soil mapping and modelling is likely to grow in the years to
come. Initially, GSIF has elected to simply accept and adopt the list of
soil properties specified for the \emph{GlobalSoilMap} project and to
extend this list through time in consultation with this and other global
soil entities.

The International Organisation for Standardisation (ISO) provides
international standard definitions of soil properties, and of associated
methods to assess those soil properties, through \texttt{ISO\ TC-190}
and \texttt{ISO\ TC-345}. Such unambiguously defined international
standards are required for purposes as such as multi-partner global soil
mapping.

In the following sections we focus our discussion on the soil properties
that have been mapped for the \url{www.soilgrids.org} project: depth to
bedrock, occurrence of the \texttt{R} horizon, organic carbon content of
the fine earth fraction, pH of the fine earth fraction, particle size
class contents (sand, silt, clay) of the fine earth fraction, gravel
content of the whole soil, bulk density of the whole soil (and
subsequently of the fine earth fraction) and Cation Exchange Capacity of
the fine earth fraction. We define those attributes as completely and
unambiguously as possible, including the associated reference method.
For each soil property the following will be discussed:

\begin{itemize}
\item
  \emph{Brief introduction to the soil property (what is it, what does
  it reflect, why is it of interest, considerations; in general terms)};
\item
  \emph{Definition of the soil feature related to the soil property and
  it's spatial domain (2D, 3D)};
\item
  \emph{Definition of the reference method to assess the soil property
  value};
\item
  \emph{Definition of the convention used to express the soil property
  value (units, precision, range)};
\item
  \emph{Review of the variation in soil property definitions and in
  methods to assess the attribute, including listings of several of the
  most widely used conversion functions cited from literature and with
  emphasis on harmonisation or conversion to the reference method}.
\end{itemize}

We also identify, and review, a number of other widely used measurement
methods, in addition to our selected standard methods. We describe if
and how these other methods relate to the selected reference methods and
discuss issues related to harmonization and standardization for
attributes of current interest for global mapping.

\hypertarget{standard-soil-variables-of-interest-for-soil-mapping}{%
\subsection{Standard soil variables of interest for soil
mapping}\label{standard-soil-variables-of-interest-for-soil-mapping}}

Some standard soil legends for listed soil properties are embedded
within the GSIF package and can be loaded by:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(GSIF)}
\CommentTok{#> GSIF version 0.5-4 (2017-04-25)}
\CommentTok{#> URL: http://gsif.r-forge.r-project.org/}
\KeywordTok{data}\NormalTok{(soil.legends)}
\KeywordTok{str}\NormalTok{(soil.legends)}
\CommentTok{#> List of 12}
\CommentTok{#>  $ ORCDRC  :'data.frame':    40 obs. of  4 variables:}
\CommentTok{#>   ..$ MIN  : num [1:40] 0 0.2 0.4 0.6 0.8 1.1 1.5 1.9 2.4 3 ...}
\CommentTok{#>   ..$ MAX  : num [1:40] 0.2 0.4 0.6 0.8 1.1 1.5 1.9 2.4 3 3.6 ...}
\CommentTok{#>   ..$ CPROB: num [1:40] 0.0161 0.0301 0.0518 0.0717 0.113 0.159 0.203 0.264 0.328 0.373 ...}
\CommentTok{#>   ..$ COLOR: chr [1:40] "#000180" "#000393" "#0006A6" "#000FB7" ...}
\CommentTok{#>  $ PHIHOX  :'data.frame':    40 obs. of  4 variables:}
\CommentTok{#>   ..$ MIN  : num [1:40] 20 42 45 46 48 49 50 51 52 53 ...}
\CommentTok{#>   ..$ MAX  : num [1:40] 42 45 46 48 49 50 51 52 53 54 ...}
\CommentTok{#>   ..$ CPROB: num [1:40] 0.0125 0.0375 0.0625 0.0875 0.1125 ...}
\CommentTok{#>   ..$ COLOR: chr [1:40] "#FF0000" "#FF1C00" "#FF3900" "#FF5500" ...}
\CommentTok{#>  $ PHIKCL  :'data.frame':    40 obs. of  4 variables:}
\CommentTok{#>   ..$ MIN  : num [1:40] 20 33 35 36 37 38 38.5 39 40 40.5 ...}
\CommentTok{#>   ..$ MAX  : num [1:40] 33 35 36 37 38 38.5 39 40 40.5 41 ...}
\CommentTok{#>   ..$ CPROB: num [1:40] 0.0125 0.0375 0.0625 0.0875 0.1125 ...}
\CommentTok{#>   ..$ COLOR: chr [1:40] "#FF0000" "#FF1C00" "#FF3900" "#FF5500" ...}
\CommentTok{#>  $ BLDFIE  :'data.frame':    40 obs. of  4 variables:}
\CommentTok{#>   ..$ MIN  : num [1:40] 200 850 1000 1100 1150 1200 1220 1260 1300 1310 ...}
\CommentTok{#>   ..$ MAX  : num [1:40] 850 1000 1100 1150 1200 1220 1260 1300 1310 1340 ...}
\CommentTok{#>   ..$ CPROB: num [1:40] 0.0125 0.0375 0.0625 0.0875 0.1125 ...}
\CommentTok{#>   ..$ COLOR: chr [1:40] "#3D3FFF" "#3A42FF" "#3745FF" "#304CFF" ...}
\CommentTok{#>  $ CECSOL  :'data.frame':    40 obs. of  4 variables:}
\CommentTok{#>   ..$ MIN  : num [1:40] 0 5 5.2 5.3 5.5 5.8 6 6.3 6.7 7.1 ...}
\CommentTok{#>   ..$ MAX  : num [1:40] 5 5.2 5.3 5.5 5.8 6 6.3 6.7 7.1 7.5 ...}
\CommentTok{#>   ..$ CPROB: num [1:40] 0.23 0.241 0.247 0.259 0.277 0.292 0.308 0.328 0.351 0.37 ...}
\CommentTok{#>   ..$ COLOR: chr [1:40] "#001998" "#0025A4" "#0031B1" "#003EBD" ...}
\CommentTok{#>  $ SNDPPT  :'data.frame':    40 obs. of  4 variables:}
\CommentTok{#>   ..$ MIN  : num [1:40] 0 1 3 4 6 8 10 12 14 16 ...}
\CommentTok{#>   ..$ MAX  : num [1:40] 1 3 4 6 8 10 12 14 16 19 ...}
\CommentTok{#>   ..$ CPROB: num [1:40] 0.0125 0.0375 0.0625 0.0875 0.1125 ...}
\CommentTok{#>   ..$ COLOR: chr [1:40] "#FFFF00" "#F8F806" "#F1F10C" "#EBEB13" ...}
\CommentTok{#>  $ SLTPPT  :'data.frame':    40 obs. of  4 variables:}
\CommentTok{#>   ..$ MIN  : num [1:40] 0 2 3 4 5 6.7 8 9 10 12 ...}
\CommentTok{#>   ..$ MAX  : num [1:40] 2 3 4 5 6.7 8 9 10 12 13 ...}
\CommentTok{#>   ..$ CPROB: num [1:40] 0.0125 0.0375 0.0625 0.0875 0.1125 ...}
\CommentTok{#>   ..$ COLOR: chr [1:40] "#FFFF00" "#F8F806" "#F1F10C" "#EBEB13" ...}
\CommentTok{#>  $ CLYPPT  :'data.frame':    40 obs. of  4 variables:}
\CommentTok{#>   ..$ MIN  : num [1:40] 0 2 3 4 5 6 7 8 9.3 10 ...}
\CommentTok{#>   ..$ MAX  : num [1:40] 2 3 4 5 6 7 8 9.3 10 12 ...}
\CommentTok{#>   ..$ CPROB: num [1:40] 0.0125 0.0375 0.0625 0.0875 0.1125 ...}
\CommentTok{#>   ..$ COLOR: chr [1:40] "#FFFF00" "#F8F806" "#F1F10C" "#EBEB13" ...}
\CommentTok{#>  $ CRFVOL  :'data.frame':    40 obs. of  4 variables:}
\CommentTok{#>   ..$ MIN  : num [1:40] 0 0.1 0.3 0.4 0.6 0.8 1 1.2 1.5 1.8 ...}
\CommentTok{#>   ..$ MAX  : num [1:40] 0.1 0.3 0.4 0.6 0.8 1 1.2 1.5 1.8 2.2 ...}
\CommentTok{#>   ..$ CPROB: num [1:40] 0.408 0.41 0.411 0.416 0.418 0.504 0.506 0.513 0.514 0.558 ...}
\CommentTok{#>   ..$ COLOR: chr [1:40] "#FFFF00" "#FDF800" "#FBF100" "#F9EB00" ...}
\CommentTok{#>  $ TAXOUSDA:'data.frame':    74 obs. of  4 variables:}
\CommentTok{#>   ..$ Number : int [1:74] 0 1 2 3 5 6 7 10 11 12 ...}
\CommentTok{#>   ..$ Group  : Factor w/ 75 levels "","Albolls","Anthrepts",..: 39 50 47 38 35 54 41 28 26 34 ...}
\CommentTok{#>   ..$ Generic: Factor w/ 17 levels "","Alfisols",..: 11 14 13 8 6 6 6 7 7 7 ...}
\CommentTok{#>   ..$ COLOR  : chr [1:74] "#1414FF" "#D2D2D2" "#FFB9B9" "#F5F5F5" ...}
\CommentTok{#>  $ TAXGWRB :'data.frame':    32 obs. of  4 variables:}
\CommentTok{#>   ..$ Number: int [1:32] 1 2 3 4 5 6 7 8 9 10 ...}
\CommentTok{#>   ..$ Code  : Factor w/ 32 levels "AB","AC","AL",..: 2 1 3 4 6 5 8 9 7 10 ...}
\CommentTok{#>   ..$ Group : Factor w/ 32 levels "Acrisols","Albeluvisols",..: 1 2 3 4 5 6 7 8 9 10 ...}
\CommentTok{#>   ..$ COLOR : chr [1:32] "#FDA463" "#FFEBBE" "#FFFFCC" "#FC6B5D" ...}
\CommentTok{#>  $ TAXNWRB :'data.frame':    118 obs. of  5 variables:}
\CommentTok{#>   ..$ Number        : int [1:118] 1 2 3 4 5 6 7 8 9 10 ...}
\CommentTok{#>   ..$ Group         : Factor w/ 118 levels "Acric Ferralsols",..: 28 29 30 31 104 116 32 84 111 18 ...}
\CommentTok{#>   ..$ Shortened_name: Factor w/ 118 levels "Acric.Ferralsols",..: 28 29 30 31 104 116 32 84 111 18 ...}
\CommentTok{#>   ..$ Generic       : Factor w/ 30 levels "Acrisols","Albeluvisols",..: 1 1 1 1 1 1 2 2 2 3 ...}
\CommentTok{#>   ..$ COLOR         : chr [1:118] "#FE813E" "#FD9F39" "#FDAE6B" "#FD8D3C" ...}
\end{Highlighting}
\end{Shaded}

which illustrates the referent cumulative probabilities (\texttt{CPROB})
and appropriate color legend (\texttt{COLOR}; coded as a six-digit,
three-byte hexadecimal number) for the values of the target soil
variables. The cumulative probabilities were derived using the
collection of records in the World Soil Profiles repository and can be
considered as an estimate of global prior probabilities for soil pH (see
further for example Fig. ).

A general intention is to maintain a \emph{Global Soil Data Registry} so
that a short variable name (in further text \emph{``GSIF code''}) can be
linked to a unique set of metadata which should include:

\begin{itemize}
\item
  Full description;
\item
  Variable type (numeric, quantity, binary, factor etc)
\item
  Measurement unit;
\item
  Biblio reference (URL or DOI);
\item
  ISO code (if available);
\item
  Physical limits (lower / upper);
\item
  Detection limit i.e.~numeric resolution;
\item
  Priority level (required, suggested or optional);
\end{itemize}

Note that MySQL has some restrictions considering column names: special
characters, those outside the set of alphanumeric characters from the
current character set, can not be used in the column names. Proposed
abbreviations for standard method names are \(\mathtt{VOL}\) --- volume
fraction, \(\mathtt{ABU}\) --- abundance or relative area cover,
\(\mathtt{PCT}\) --- mass percentage, \(\mathtt{ICM}\) --- thickness in
cm, \(\mathtt{MHT}\) --- texture by-hand or manual hand texture and
\(\mathtt{MNS}\) --- Munsell color codes, horizon sequence is coded with
the capital ASCII letters --- \(\mathtt{A}\), \(\mathtt{B}\),
\(\mathtt{C}\),\(\ldots\) \(\mathtt{Z}\). Another option is to simply
use the US Goverment National Cooperative Soil Characterization Database
column names (\url{http://ncsslabdatamart.sc.egov.usda.gov/}).

Also note that the metadata can be easily separated from the code so
that the short GSIF code (variable name) could be used as a shorthand
(replacement) for the long description of the complete metadata. Using
short GSIF codes is also important for programming because unique code
names are used consistently in all scripts / functions.

\hypertarget{descriptive-soil-profile-observations}{%
\section{Descriptive soil profile
observations}\label{descriptive-soil-profile-observations}}

\hypertarget{depth-to-bedrock}{%
\subsection{Depth to bedrock}\label{depth-to-bedrock}}

Soil depth (specifically depth to bedrock) is predicted because it is an
important consideration for a wide variety of engineering, hydrological
and agronomic interpretations. Shallow and lithic soils are of
particular interest as they impose restrictions for foundations and
structures in engineering, limit infiltration and storage of moisture
and produce more rapid runoff and erosion and limit growth of many crops
by restricting rooting depth and limiting available moisture storage.
Most soil legacy profile data do not provide any information about the
soil below depths of 1 m \citep{Richter1995}. This characteristic of
legacy soil data limits its usefulness for predicting soil depths
greater than 2 m.

Soil depth is measured from the soil surface downwards and expressed in
positive values increasing with depth. Google Earth and the KML data
standard (via the \texttt{altitudeMode} tag) allow one to specify if the
vertical dimension refers to actual altitude (vertical distance from the
land surface) or to distance from the sea level (\texttt{absolute}). In
this case soil depths can be represented using \texttt{clampToGround}
and negative values. For example, depth of 30 cm can be expressed as
\citep{OGCKML2008}:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{<}\ExtensionTok{Placemark}\OperatorTok{>} \OperatorTok{<}\NormalTok{Point}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{altitudeMode}\OperatorTok{>}\NormalTok{clampToGround}\OperatorTok{<}\NormalTok{/altitudeMode}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{coordinates}\OperatorTok{>}\NormalTok{17.2057,45.8851,-0.}\OperatorTok{3<}\NormalTok{/coordinates}\OperatorTok{>}
\OperatorTok{<}\NormalTok{/}\ExtensionTok{Point}\OperatorTok{>} \OperatorTok{<}\NormalTok{/Placemark}\OperatorTok{>}
\end{Highlighting}
\end{Shaded}

Soil surface (depth = 0 cm) is the top of the mineral soil; or, for
soils with a litter layer (\texttt{O} horizon), the soil surface is the
top of the part of the \texttt{O} horizon that is at least slightly
decomposed \citep{FAO2006}. Fresh leaf or needle fall that has not
undergone observable decomposition is not considered to be part of the
soil and may be described separately. For organic soils, the top of any
surface horizon identified as an \texttt{O} horizon is considered the
soil surface.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_depth_2_bedrock} 

}

\caption{Depth to bedrock for censured and uncensured observations. Image source: @shangguan2016.}\label{fig:scheme-depth-to-bedrock}
\end{figure}

The \emph{depth to bedrock} i.e.~depth to the \texttt{R} horizon is
measured from the soil surface downwards and is expressed in cm with a
precision of ±1 cm. Depth to bedrock deeper than e.g.~2--3 m is most
often not recorded. Bedrock is consolidated hard rock, with only a few
cracks, underlying the soil. It is not necessarily parent material. We
imagine it often as something distinct and easy to recognize in the
field. In practice, depth to bedrock can be difficult to determine, and
is often confused with stoniness or depth to parent material (which can
be unconsolidated material). Another issue is that, for most of the
soils in the world, hard bedrock is \textgreater{}2 m deep so that we
actually don't know the correct depth to enter, other than
\textgreater{}2 m. Rootability is physically restricted by the bedrock,
whether hard or soft (see Fig. \ref{fig:scheme-depth-to-bedrock}).

\begin{rmdnote}
Depth to bedrock is the mean distance to \texttt{R} horizon which is the
layer impenetrable by roots or agricultural machinery. Depth to bedrock
deeper than 2 m is most often not recorded in field survey descriptions.
\end{rmdnote}

In traditional soil characterisation, the total depth of the \texttt{O},
\texttt{A},\texttt{E}, and \texttt{B} horizons is referred to as the
\emph{solum} \citep{harpstead2001soil}, while the underlaying layer is
referred to as parent material or substratum \citep{SSDS1993}. Parent
material can be coarse or fine unconsolidated deposits of e.g.~alluvial,
colluvial or windblown origin (\texttt{C} horizon) or consolidated
residual hard bedrock (\texttt{R} horizon).

\hypertarget{effective-soil-depth-and-rooting-depth}{%
\subsection{Effective soil depth and rooting
depth}\label{effective-soil-depth-and-rooting-depth}}

\emph{Effective soil depth} is of interest for soil mapping because it
is a key indicator of the capability of the soil to store moisture,
support crop growth and sustain beneficial land uses. It is often an
essential indicator of soil health. The effective soil depth is the
depth to which micro-organisms are active in the soil, where roots can
develop and where soil moisture can be stored \citep{FAO2006}.

\begin{table}

\caption{\label{tab:rootingdepths}Summary of maximum rooting depth by biome (after @Canadell1996Oecologia). MMRD = Mean maximum rooting depth in m; HVRD = Highest value for rooting depth in m.}
\centering
\begin{tabular}[t]{lrlr}
\toprule
Biome & N & MMRD & HVRD\\
\midrule
Boreal Forest & 6 & 2.0 ± 0.3 & 3.3\\
Cropland & 17 & 2.1 ± 0.2 & 3.7\\
Desert & 22 & 9.5 ± 2.4 & 53.0\\
Sclerophyllous shrubland and forest & 57 & 5.2 ± 0.8 & 40.0\\
Temperate coniferous forest & 17 & 3.9 ± 0.4 & 7.5\\
\addlinespace
Temperate deciduous forest & 19 & 2.9 ± 0.2 & 4.4\\
Temperate grassland & 82 & 2.6 ± 0.2 & 6.3\\
Tropical deciduous forest & 5 & 3.7 ± 0.5 & 4.7\\
Tropical evergreen forest & 5 & 7.3 ± 2.8 & 18.0\\
Tropical savanna & 15 & 15.0 ± 5.4 & 68.0\\
\bottomrule
\end{tabular}
\end{table}

There are many thoughts on how to define effective soil depth. Effective
soil depth is closely related to, but not necessarily equivalent to, the
\emph{rooting depth}. Rooting depth is measured and reported relative to
a specific prevailing land cover and land use category, while effective
soil depth is supposedly the maximum possible depth of soil that can be
used by any growing plant (see Tbl. \ref{tab:rootingdepths}).

In some cases soil ends with an abrupt change of material which is
either solid, compacted or distinctly impenetrable for plants and
organisms living in soil. The root restricting i.e.~plant accessible
depth, is the depth at which root penetration is strongly inhibited
because of physical (including soil temperature), chemical or
hydrological characteristics \citep[ p.60]{SSDS1993}. Restriction means
the inability to support more than a very few fine or few very fine
roots if depth from the soil surface and water state, other than the
occurrence of frozen water, are not limiting. For some crops like cotton
plants or soybeans, and possibly other crops with less abundant roots
than the grasses, the very few class is used instead of the few class.
The restriction may be below where plant roots normally occur because of
limitations in water state, temperatures, or depth from the surface.
This evaluation can be based on the specific plants that are important
to the use of the soil, as indicated in Tbl. \ref{tab:rootingdepths};
see also \citet[p.60]{SSDS1993}.

Root restriction can be also influenced by certain pedogenic horizons,
such as \emph{fragipans}. A change in particle size distribution alone,
as for example loamy sand over gravel, is not always a basis for
physical root restriction. A common indication of physical root
restriction is a combination of structure and consistence which together
suggest that the resistance of the soil fabric to root entry is high and
that vertical cracks and planes of weakness for root entry are absent or
widely spaced. Root restriction is inferred for a continuously cemented
zone of any thickness; or a zone \textgreater{}10 cm thick that when
very moist or wet is massive, platy, or has weak structure of any type
for a vertical repeat distance of \textgreater{}10 cm and while very
moist or wet is very firm (firm, if sandy), extremely firm, or has a
large penetration resistance. Chemical restrictions, such as high
extractable aluminum, manganese and/or low extractable calcium, can also
be considered but are plant-specific. Root-depth observations preferably
should be used to make the generalization. If these are not available
then inferences may be made from morphology.

As a general recommendation, it is advisable to focus first on mapping
soil properties that limit rooting, including content of coarse
fragments and the depth to bedrock, and then define effective soil depth
\emph{a posteriori} using distinct analytical rules. A similar approach
has also been promoted by \citet{rijsberman1985effect} and
\citet{driessen1992land} who refer to it as the \emph{Soil-productivity
Index} --- a product of soil-water sufficiency, soil pH sufficiency and
soil bulk density sufficiency. Here we consider somewhat wider range of
soil properties that can affect rooting depth, such as:

\begin{itemize}
\item
  coarse fragments,
\item
  compaction / porosity (possibly derived from structure and
  consistence),
\item
  drainage i.e.~soil oxygen availability,
\item
  toxicity e.g.~Al content,
\item
  acidity, salinity and similar.
\end{itemize}

In-field expert interpretation explicitly summarising observations into
a single expression for rooting depth is likely the most effective and
reliable source of information. The genetically determined maximum
rooting depth of vegetation isn't always a reliable indicator of actual
observed effective rooting depth of a given soil at a given site (Fig.
\ref{fig:lri-scheme}). Possibly a more robust way to determine the
effective rooting depth is to map all limiting soil properties with high
accuracy, and then derive rooting index per layer.

\begin{figure}[t]

{\centering \includegraphics[width=0.9\linewidth]{figures/Fig_LRI_scheme} 

}

\caption{Derivation of the Limiting Rooting Index: (left) soil pH values and corresponding LRI, (right) coarse fragments and corresponding LRI [@Leenaars2018].}\label{fig:lri-scheme}
\end{figure}

By using the GSIF package, one can determine Limiting Rooting Index,
which can be a good indicator of the effective rooting depth. Consider
the following soil profile from Nigeria \citep{Leenaars2012}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## sample profile from Nigeria (ISRIC:NG0017):}
\NormalTok{UHDICM =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{65}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{127}\NormalTok{)}
\NormalTok{LHDICM =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{18}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{65}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{127}\NormalTok{, }\DecValTok{181}\NormalTok{)}
\NormalTok{SNDPPT =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{66}\NormalTok{, }\DecValTok{70}\NormalTok{, }\DecValTok{54}\NormalTok{, }\DecValTok{43}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{47}\NormalTok{)}
\NormalTok{SLTPPT =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{13}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{23}\NormalTok{)}
\NormalTok{CLYPPT =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{21}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{43}\NormalTok{, }\DecValTok{47}\NormalTok{, }\DecValTok{30}\NormalTok{)}
\NormalTok{CRFVOL =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{17}\NormalTok{, }\DecValTok{72}\NormalTok{, }\DecValTok{73}\NormalTok{, }\DecValTok{54}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{17}\NormalTok{)}
\NormalTok{BLD =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{1.57}\NormalTok{, }\FloatTok{1.60}\NormalTok{, }\FloatTok{1.52}\NormalTok{, }\FloatTok{1.50}\NormalTok{, }\FloatTok{1.40}\NormalTok{, }\FloatTok{1.42}\NormalTok{)}\OperatorTok{*}\DecValTok{1000}
\NormalTok{PHIHOX =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{6.5}\NormalTok{, }\FloatTok{6.9}\NormalTok{, }\FloatTok{6.5}\NormalTok{, }\FloatTok{6.2}\NormalTok{, }\FloatTok{6.2}\NormalTok{, }\FloatTok{6.0}\NormalTok{)}
\NormalTok{CEC =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{9.3}\NormalTok{, }\FloatTok{4.5}\NormalTok{, }\FloatTok{6.0}\NormalTok{, }\FloatTok{8.0}\NormalTok{, }\FloatTok{9.4}\NormalTok{, }\FloatTok{10.9}\NormalTok{)}
\NormalTok{ENA =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{)}
\NormalTok{EACKCL =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
\NormalTok{EXB =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{8.9}\NormalTok{, }\FloatTok{4.0}\NormalTok{, }\FloatTok{5.7}\NormalTok{, }\FloatTok{7.4}\NormalTok{, }\FloatTok{8.9}\NormalTok{, }\FloatTok{10.4}\NormalTok{)}
\NormalTok{ORCDRC =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{18.4}\NormalTok{, }\FloatTok{4.4}\NormalTok{, }\FloatTok{3.6}\NormalTok{, }\FloatTok{3.6}\NormalTok{, }\FloatTok{3.2}\NormalTok{, }\FloatTok{1.2}\NormalTok{)}
\NormalTok{x <-}\StringTok{ }\KeywordTok{LRI}\NormalTok{(}\DataTypeTok{UHDICM=}\NormalTok{UHDICM, }\DataTypeTok{LHDICM=}\NormalTok{LHDICM, }\DataTypeTok{SNDPPT=}\NormalTok{SNDPPT, }
   \DataTypeTok{SLTPPT=}\NormalTok{SLTPPT, }\DataTypeTok{CLYPPT=}\NormalTok{CLYPPT, }\DataTypeTok{CRFVOL=}\NormalTok{CRFVOL, }
   \DataTypeTok{BLD=}\NormalTok{BLD, }\DataTypeTok{ORCDRC=}\NormalTok{ORCDRC, }\DataTypeTok{CEC=}\NormalTok{CEC, }\DataTypeTok{ENA=}\NormalTok{ENA, }\DataTypeTok{EACKCL=}\NormalTok{EACKCL, }
   \DataTypeTok{EXB=}\NormalTok{EXB, }\DataTypeTok{PHIHOX=}\NormalTok{PHIHOX, }\DataTypeTok{print.thresholds=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{x}
\CommentTok{#> [1] TRUE TRUE TRUE TRUE TRUE TRUE}
\CommentTok{#> attr(,"minimum.LRI")}
\CommentTok{#> [1] 35.0 29.5 47.0 54.5 73.0 61.5}
\CommentTok{#> attr(,"most.limiting.factor")}
\CommentTok{#> [1] "tetaS" "tetaS" "tetaS" "tetaS" "tetaS" "tetaS"}
\CommentTok{#> attr(,"thresholds")}
\CommentTok{#> attr(,"thresholds")$ERscore1}
\CommentTok{#>  [1] 100.0  80.0  50.0   0.0  95.0  40.0  40.0   5.5   7.8   1.5  10.0}
\CommentTok{#> [12]   1.0  35.0   2.5 150.0 150.0}
\CommentTok{#> }
\CommentTok{#> attr(,"thresholds")$ERscore2}
\CommentTok{#>  [1]   0.00  90.00  30.00   0.35 100.00  60.00  60.00   3.62   9.05   6.75}
\CommentTok{#> [11]  25.00   5.00  85.00   6.50 750.00 750.00}
\CommentTok{#> }
\CommentTok{#> attr(,"thresholds")$Trend}
\CommentTok{#>  [1]  0 -1  1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1}
\CommentTok{#> }
\CommentTok{#> attr(,"thresholds")$Score}
\CommentTok{#>  [1] 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20}
\CommentTok{#> }
\CommentTok{#> attr(,"thresholds.names")}
\CommentTok{#> attr(,"thresholds.names")$variable}
\CommentTok{#>  [1] "range"    "CRFVOL"   "tetaS"    "BLD.f"    "SNDPPT"   "CLY.d"   }
\CommentTok{#>  [7] "SND.d"    "PHIHOX.L" "PHIHOX.H" "ECN"      "ENA.f"    "ENA"     }
\CommentTok{#> [13] "EACKCL.f" "EACKCL"   "CRB"      "GYP"}
\NormalTok{## Most limiting: BLD.f and CRFVOL, but nothing < 20}
\end{Highlighting}
\end{Shaded}

where \texttt{UHDICM} and \texttt{LHDICM} are the upper and lower
horizon depth in cm, \texttt{SNDPPT}, \texttt{SLTPPT} and
\texttt{CLYPPT} are the sand, silt and clay content in percent,
\texttt{CRFVOL} is the volume percentage of coarse fragments
(\textgreater{}2 mm), \texttt{BLD} is the bulk density in t/m\(^3\),
\texttt{ORCDRC} is the soil organic carbon concentration in permille or
g/kg, \texttt{ECN} is the electrical conductivity in dS/m, \texttt{CEC}
is the Cation Exchange Capacity in cmol/kg (centi-mol per kilogram),
\texttt{ENA} is the exchangable Na in cmol/kg, \texttt{EACKCL} is the
exchangeable acidity in cmol/kg, \texttt{EXB} is the exchangeable bases
in cmol/kg, \texttt{PHIHOX} is the soil pH in water suspension,
\texttt{CRB} is the CaCO\(_3\) (carbonates) in g/kg, \texttt{GYP} is the
CaSO\(_4\) (gypsum) in and \texttt{tetaS} is the volumetric percentage
of water.

For this specific profile, the most limiting soil property is
\texttt{tetaS}, but because none of the soil properties got
\textless{}20 points, we can conclude that the maximum rooting depth is
(MISSING VALUE HERE!). Note that the threshold values in the
\texttt{LRI} function used to derive Limiting Rootability scores are set
based on common soil agricultural productivity tresholds (e.g.~for
maize; see also Fig. \ref{fig:lri-scheme}), and can be adjusted via the
\texttt{thresholds} argument. The computation is done per list of soil
layers (minimum three) to account for textural changes i.e.~sudden
changes in sand and clay content and for the limiting layers such as
layer saturated with water. To determine futher the effective rooting
depth we can run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sel <-}\StringTok{ }\NormalTok{x}\OperatorTok{==}\OtherTok{FALSE}
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{all}\NormalTok{(sel}\OperatorTok{==}\OtherTok{FALSE}\NormalTok{))\{ }
\NormalTok{  UHDICM[}\KeywordTok{which}\NormalTok{(sel}\OperatorTok{==}\OtherTok{TRUE}\NormalTok{)[}\DecValTok{1}\NormalTok{]] }
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
  \KeywordTok{max}\NormalTok{(LHDICM)}
\NormalTok{\}}
\CommentTok{#> [1] 181}

\NormalTok{xI <-}\StringTok{ }\KeywordTok{attr}\NormalTok{(x, }\StringTok{"minimum.LRI"}\NormalTok{)}
\NormalTok{## derive Effective rooting depth:}
\KeywordTok{ERDICM}\NormalTok{(}\DataTypeTok{UHDICM=}\NormalTok{UHDICM, }\DataTypeTok{LHDICM=}\NormalTok{LHDICM, }\DataTypeTok{minimum.LRI=}\NormalTok{xI, }\DataTypeTok{DRAINFAO=}\StringTok{"M"}\NormalTok{)}
\CommentTok{#> [1] 100}
\end{Highlighting}
\end{Shaded}

\hypertarget{chemical-soil-properties}{%
\section{Chemical soil properties}\label{chemical-soil-properties}}

\hypertarget{soil-organic-carbon}{%
\subsection{Soil organic carbon}\label{soil-organic-carbon}}

Organic carbon is a soil property of great current global interest
\citep{Smith2004SUM, Smith2010CUP, Panagos2013439}. It is commonly
recognized and used as a key indicator of soil health. The amount of
carbon present in the soil, and particularly in topsoil horizons, is
grossly indicative of potential productivity for crops. Amounts of
organic carbon throughout the profile influence soil structure,
permeability, porosity, bulk density, water holding capacity, nutrient
retention and availability and, consequently, overall soil health. The
ability of soils to sequester significant quantities of atmospheric
carbon is of considerable interest as a potential mechanism for
mitigating the adverse effects of increases in green house gasses in the
atmosphere \citep{Smith2004SUM, Conant2010, Scharlemann2014CM}.
Consequently, soil organic carbon is probably the soil property of
greatest current interest and utility from the point of view of global
mapping, and interpretation, of soil properties.

\begin{rmdnote}
Soil Organic Carbon is one the key measures of soil health. The standard
reference method for assessing and reporting soil organic carbon content
of the fine earth fraction is by dry combustion to at least (ISO 10694).
Values of organic carbon content are typically reported in (permilles)
with integer precision over a range of 0--1000.
\end{rmdnote}

The \emph{dry combustion method} (Leco at 1000°C) is based on thermal
oxidation of both mineral carbon (IC) and organic carbon by means of a
furnace. It is a reliable method for the determination of the soil
organic carbon when IC is removed through combustion at low temperature
prior to combustion at high temperature. Dry combustion is considered to
ensure oxidation of all ORC and is considered an accurate method which
has been used in many studies as a reference method against which to
calibrate other methods
\citep{Grewal1991JSS, Meersmans2009SUM, Bisutti2004TAC}. A global
estimate of the distribution of soil organic carbon is shown in Fig.
\ref{fig:sprofs-soil-carbon}.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_sprofs_ORCDRC} 

}

\caption{Histogram and soil-depth density distribution for a global compilation of measurements of soil organic carbon content in permilles. Based on the records from WOSIS [@Batjes2017ESSD]. The log-transformation is used to ensure close-to-normal distribution in the histogram.}\label{fig:sprofs-soil-carbon}
\end{figure}

\begin{rmdnote}
Soil organic carbon content is most commonly expressed in weight
percentage and for GSIF in grams per kilogram fine earth fraction or
permilles. The standard method of determining the soil organic carbon
content is by dry combustion method (Leco at 1000°C).
\end{rmdnote}

In the dry combustion method, all carbon present in the soil is oxidized
to carbon dioxide (CO\(_2\)) by heating the soil to at least (WHAT TEMP
HERE? 1000 c?) in a flow of oxygen-containing gas that is free from
carbon dioxide. The amount of carbon dioxide released is then measured
by titrimetry, gravimetry, conductometry, gas chromatography or using an
infrared detection method, depending on the apparatus used. When the
soil is heated to a temperature of at least (WHAT TEMP HERE? 1000 c?),
in addition to organic carbon any inorganic carbon present as carbonate
is also completely decomposed. Total organic carbon can be determined
directly or indirectly. Direct determination consists of previous
removal of any carbonates present by treating the soil with hydrochloric
acid. Indirect determination consists of applying an empirical
correction to the total carbon content to account for for the inorganic
carbonates present.

Examples of studies that have used dry combustion for calibrating other
methods of analyzing organic carbon include
\citeauthor{Kalembasa1973JSFA}
\citetext{\citeyear{Kalembasa1973JSFA}; \citealp{Grewal1991JSS}; \citealp{Soon1991CSSPA}; \citealp{Wang1996AJSR}; \citealp{Konen2002SSSAJ}; \citealp{Brye2003CSSPA}; \citealp{Mikhailova2003CSSPA}; \citealp{Bisutti2004TAC}; \citealp{Jankauskas2006CSSPA}; \citealp{DeVos2007SUM}}
and \citet{Meersmans2009SUM}. It is possible to produce regression
equations to permit conversion of results for organic carbon produced by
one method into equivalent values in a specified reference method
(generally dry combustion). However, local calibration equations that
reflect differences in soils on a regional basis are usually needed. It
is not possible to provide a single universal equation suitable for use
everywhere to convert organic carbon values produced using other methods
of analysis to equivalent values in the reference method of dry
combustion.

\hypertarget{soil-ph}{%
\subsection{Soil pH}\label{soil-ph}}

PH is of interest for global soil mapping because it is one of the more
widely available and easily interpreted chemical measures of the health
and productivity of the soil. pH provides an indication of base status
of the soil which influences nutrient availability, mobility of both
beneficial and detrimental ions and the ecology of micro-organisms
within the soil. For most crops and agricultural uses, a pH in the range
of 5.5 to 7.5 is optimum (considering the agricultural productivity of
soil). Low pH is associated with acidic conditions and with increased
mobility of toxic ions such as aluminum iron and even acid sulphates.
High pH is associated with reduced availability of phosphorus and at
higher levels with alkaline conditions that impede water uptake by
plants. A global estimate of the distribution of the soil pH is shown in
Figs. \ref{fig:sprops-phiho5} and \ref{fig:sprops-phikcl}.

PH index approximates concentration of dissolved hydrogen ions
(H\(_3\)O\(^+\)) in a soil suspension. It is estimated as the negative
decimal logarithm of the hydrogen ion activity in a soil suspension. As
a single measurement, pH describes more than relative acidity or
alkalinity. It also provides information on nutrient availability, metal
dissolution chemistry, and the activity of microorganisms
\citep{Miller2010SSSAJ}.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_sprofs_PHIHO5} 

}

\caption{Histogram and soil-depth density distribution for a global compilation of measurements of soil pH (suspension of soil in H$_2$O). Based on the records from WOSIS [@Batjes2017ESSD].}\label{fig:sprops-phiho5}
\end{figure}

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_sprofs_PHIKCL} 

}

\caption{Histogram and soil-depth density distribution for a global compilation of measurements of soil pH (suspension of soil in KCl). Based on the records from WOSIS [@Batjes2017ESSD].}\label{fig:sprops-phikcl}
\end{figure}

The standard reference method for reporting pH is ISO 10390:2005. This
standard specifies an instrumental method for the routine determination
of pH using a glass electrode in a 1:5 (volume fraction) suspension of
soil in water (pH in H\(_2\)O), in potassium chloride solution (pH in
KCl) or in calcium chloride solution (pH in CaCl\(_2\)).

The most common method for analyzing pH in North America is a 1:1
soil/water suspension \citep{Miller2010SSSAJ}. Adopting ISO 10390:2005
as a standard with its specification of pH measured in a 1:5 suspension
of soil in water requires US values to be converted from 1:1 soil/water
to 1:5 soil/water equivalent values.

\begin{figure}[t]

{\centering \includegraphics[width=0.85\linewidth]{figures/Fig_color_legend_PHI} 

}

\caption{Histogram for soil pH and connected color legend available via the GSIF package. Color breaks in the legend have been selected using histogram equalization (i.e. by using constant quantiles) to ensure maximum contrast in the output maps.}\label{fig:color-legend-phi}
\end{figure}

The ratio of soil to water in a suspension has a net effect of
increasing the pH with a decrease in the soil/water ratio.
\citet{Davis1943SS} has shown that decreasing the soil/water ratio from
10:1 to 1:10 resulted in an increase of 0.40 pH units. Values for pH
computed using methods with a lower ratio of soil to water (e.g.~1:1 or
1:2.5) will generally be lower than equivalent values for pH in 1:5
water and will need to be adjusted higher. Several authors have
demonstrated that fitting quadratic or curvilinear functions to soil pH
data produces regression equations with higher coefficients of
determination that those obtained from a linear fit
\citep{Aitken1991AJSR, Miller2010SSSAJ}. For example,
\citet{Brennan1998} have estimated that (at least in Southwestern
Australia) pH in CaCl\(_2\) can be estimated from the pH 1:5 water by
using a simple conversion:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ph_h2o =}\StringTok{ }\FloatTok{7.2}
\FloatTok{0.918}\OperatorTok{*}\NormalTok{ph_h2o}\FloatTok{-0.3556}
\CommentTok{#> [1] 6.25}
\end{Highlighting}
\end{Shaded}

This model fitted explains 94\% of variation in the values of pH
CaCl\(_2\) (R-square=0.9401).

\begin{rmdnote}
Soil pH is negative decimal logarithm of the hydrogen ion activity in a
soil suspension. Soil pH values are usually in the range 3--11 and are
recorded with a precision of ±0.1 pH in the range of 5.5 to 7.5 is
optimal for growing crops.
\end{rmdnote}

Soil pH varies with season and soil moisture content, with higher pH
values associated with wetter soils and winter conditions and lower pH
values with drier soils and summer conditions \citep{Miller2010SSSAJ}.
The effects of both temporal variation in pH and variation due to
different analytical methods means that differences in pH of less than
some specified range (e.g. ±0.3 units) may not be meaningful in the
context of predictions made using noisy legacy soils data analyzed using
a variety of different analytical methods. Consequently, it is not
necessary or beneficial to report pH with a precision greater than ±0.1
unit. Natural variation of pH in soils is over a range of 2 to 11 with a
standard deviation of 1.4. Note also that pH follows a close-to-normal
distribution, although it is often argued that, locally, it can show
bimodal or even trimodal peaks (Fig. \ref{fig:color-legend-phi}).

\hypertarget{soil-nutrients}{%
\subsection{Soil nutrients}\label{soil-nutrients}}

Nutrients are chemical elements or substances essential for the growth
of plants. The most essential elements important for the growth of
plants are, in fact, carbon, hydrogen and oxygen. Other essential
elements can be separated into macro-nutrients (\textgreater{}100
\(\mu\)g or \textgreater{}100 ppm) and micro-nutrients (\textless{}100
ppm), although there is no strict border between the two
\citep{harpstead2001soil, hengl2017soil}. Some macro-nutrients of global
importance for soil management and protection are
(\url{http://en.wikipedia.org/wiki/Plant_nutrition}):

\begin{itemize}
\item
  \emph{Nitrogen} (N) --- It is often considered synonymous with soil
  fertility. Controls leafy growth. Occurs in soil as nitrates
  (e.g.~NO\(_3\), NO\(_2\)).
\item
  \emph{Phosphorus} (P) --- High phosphorus deficiency may result in the
  leaves becoming denatured and showing signs of necrosis. Occurs in the
  form of phosphates.
\item
  \emph{Potassium} (K) --- Potassium deficiency may result in higher
  risk of pathogens, wilting, chlorosis, brown spotting, and higher
  chances of damage from frost and heat.
\item
  \emph{Sulfur} (S) --- Symptoms of deficiency include yellowing of
  leaves and stunted growth. Occurs in soil in the form of sulfate salts
  (SO\(_4\)).
\item
  \emph{Calcium} (Ca) --- Calcium is involved in photosynthesis and
  plant structure. Calcium deficiency results in stunting. Occurs in the
  form of calcium carbonates (CaCO\(_3\)).
\item
  \emph{Magnesium} (Mg) --- Magnesium is also an important part of
  chlorophyll. Magnesium deficiency can result in interveinal chlorosis.
\end{itemize}

\begin{rmdnote}
Nitrogen, Phosphorus and Potassium are the three relatively mobile and
dynamic nutrients in soil that are most often lacking and hence have
been identified of primary interest for the fertilizer industry. Other
micro-nutrients of interest for global soil mapping would be: Iron (Fe),
Zinc (Zn), Manganese (Mn), Copper (Cu), Boron (B), Chlorine (Cl),
Molybdenum (Mo), Nickel (Ni) and Sodium (Na).
\end{rmdnote}

Apart from macro- and micro-nutrients important for plant growth, there
is an increasing interest in the distribution of heavy metals in soils,
especially ones that are considered toxic or dangerous for human health.
Some common heavy metals of interest for soil management and soil
protection in developed industrial and / or developing countries are
Lead (Pb), Arsenic (As), Zinc (Zn), Cadmium (Cd), Nickel (Ni), Copper
(Cu), and Aluminium (Al)
\citep{Markus2001399, reimann2011statistical, Morel2005202, Rodriguez-Lado23082013, hengl2017soil}.

Macro- and micro-nutrients and heavy metals are measured and mapped in
parts per million or \(\mu\)g per kg of soil. The AfSIS project,
provides a good example of mapping macro- and micro-nutrients over a
large area \citep{hengl2017soil}. The problem with mapping such chemical
soil properties, however, is that they are highly dynamic. For example,
nitrogen, phosphorus, and potassium are highly mobile nutrients. Their
concentration changes from month to month, even from day to day so that
space-time models (2D-T or 3D-T) need to be developed and the amount of
analysis / storage needed can easily escalate.

\hypertarget{physical-and-hydrological-soil-properties}{%
\section{Physical and hydrological soil
properties}\label{physical-and-hydrological-soil-properties}}

\hypertarget{coarse-fragments}{%
\subsection{Coarse fragments}\label{coarse-fragments}}

Soil texture is connected with soil granulometry or the composition of
the particle sizes, typically measured as volume percentages. The most
common subdivision of soil granulometry is \citep{Shirazi2001SSSAJ}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Fine earth (\textless{}2 m)

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    sand (coarser particles in the fine earth),
  \item
    silt (medium size particles),
  \item
    clay (fine particles \textless{}2 \(\mu\)m),
  \end{enumerate}
\item
  Coarse fragments (\textgreater{}2 mm)

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    gravel (2 mm to 8 cm)
  \item
    stones or boulders (\textgreater{}8 cm)
  \end{enumerate}
\end{enumerate}

Coarse fragments occupy volume in the soil matrix, reducing water and
nutrient availability as well as influencing rooting depth and
workability. We elect to produce maps of coarse fragment content because
many assessments, such as total stocks of carbon or available water, are
volumetric based and require knowledge of the volume of non-soil
materials throughout the profile. This information is required to
support calculations of the total volume of the fine earth fraction that
is available to hold water or retain organic carbon. Without some
estimate of the volume of the soil occupied by solid particles larger
than 2 mm, it would not be possible to compute volumetric estimates of
stocks of soil carbon or available moisture for fine earth soil.

Coarse fragments include stones as well as gravel (hard and soft
nodules) and the attribute is defined as consisting of all mineral
particles with a size exceeding 2 mm. Coarse fragment content is most
commonly expressed in volume fraction (volume percentage) of the
horizon, layer or sample considered. Laboratory analyses tend to be
applied to the fine earth fraction of the soil only and commonly omit
consideration of the coarse fragment content. Data for coarse fragment
content are generally derived from descriptive field observations on
soil layer morphology. Those descriptions generally express the content
of coarse fragments by class values or categories as for example
\emph{`frequent stones'} indicating an estimated volumetric content of
15--40\% according to the FAO guidelines of 1977 (similar to \emph{`many
stones'} according to SOTER conventions and the FAO guidelines of 2006).
Because coarse fragment content is most frequently based on generalized
visual field estimates, and is often lacking in legacy soil
descriptions, it is not reasonable to predict or present estimates of
coarse fragment content with a precision greater than 1--5\%.

Note that the uncertainty associated with coarse fragment content,
propagated from the field observed class values, has significant impact
on estimations of the volumetric totals of attributes assessed and
mapped for the fine earth fraction (see also section \ref{SOC-chapter}).
Whilst a 1 meter deep soil, with a bulk density of 1.5 tonne per
cubic-metre and an organic carbon content of 10 g per kg, contains 150
tonnes organic carbon. A similar soil with bulk density adjusted for the
presence of \emph{`frequent stones'} contains 90--127.5 tonnes organic
carbon. Despite the inaccuracy of the data for field observed coarse
fragments content, it is strongly recommended to collect and compile
these data as completely as possible because of their relevance for
estimating whole soil bulk density, total volume and volume of the fine
earth fraction alone.

The possible nature (and size) of coarse fragments is highly variable
(quartz, carbonate, iron, basalt) with consequent variable manageability
and variable characteristics such as breakability, solubility, bulk
density, etc. Where the coarse fragment content is dominant
(\textgreater{}80\%), approaching 100\%, rootability is near nil which
is a determinant for the rooting or effective soil depth and generally
also for depth to bedrock. An estimated global distribution of coarse
fragments and soil textures is given in Fig. \ref{fig:sprofs-crfvol}.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_sprofs_CRFVOL} 

}

\caption{Histogram and soil-depth density distribution for a global compilation of measurements of coarse fragments in percent. Based on the records from WOSIS [@Batjes2017ESSD]. This variable in principle follows a zero inflated distribution.}\label{fig:sprofs-crfvol}
\end{figure}

\hypertarget{particle-size-class-distribution-sand-silt-and-clay}{%
\subsection{Particle size class distribution: sand, silt and
clay}\label{particle-size-class-distribution-sand-silt-and-clay}}

The majority of global soil mapping initiatives elect to predict the
spatial distribution of particle size classes (soil texture) because
texture controls or influences many mechanical, hydrological and
engineering aspects of use of the soil. Soil texture affects how a soil
responds to engineering uses such as construction of roads, buildings,
dams and other structures, how water infiltrates into the soil and is
stored or transmitted through it, how nutrients, chemicals and dissolved
substances adhere to surfaces and are retained or transformed and how
energy and matter enter into the soil and are stored or transmitted
through it. Texture is the fundamental physical and mechanical property
of soils and, as such, it is one of the most widely analysed and widely
reported soil properties.

The size of particles in the soil varies greatly from less than a 2
\(\mu\)m to several cm's and occasionally even meters (boulders). This
represents a range from to 1 \(\mu\)m to 1 million \(\mu\)m. Generally,
particle size distribution has been simplified through aggregation or
classification. The fine earth fraction (\textless{}2 mm) is the soil
considered for laboratory analyses. This fine earth is further
subdivided into particle size classes including, depending on the
guidelines or laboratory concerned, fine and coarse clay, fine and
coarse silt and very fine, fine, medium, coarse and very coarse sand.
The three major particle size classes of the fine earth fraction though
are sand, silt and clay. They are generally reported in units of percent
by weight with a precision of ±1\%.

Soil texture represents the relative composition of sand, silt, and clay
in soil. The \emph{particle-size class distribution} is usually
represented in a texture diagram, relating the percentages of sand,
silt, and clay (mass percentage of fine earth) to a \emph{texture class}
\citep{Minasny2001AJSR}. Particle size distribution has been defined
using a number of systems. One of the most widely used systems is the
USDA Soil Survey Laboratory Methods Manual \citep{Burt2004SSIR}. The
USDA definition of particle size classes has also been recommended by
FAO for use in the Soil Map of the World (Fig.
\ref{fig:texture-limits}). The standard reference method adopted by GSIF
for reporting particle size classes of sand, silt and clay, is as per
the USDA Soil Survey Laboratory Methods Manual \citep[
p.347]{Burt2004SSIR}. An estimated global distribution of sand, silt,
and clay is given in Figs. \ref{fig:sprofs-snd}, \ref{fig:sprofs-slt}
and \ref{fig:sprofs-cly}.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_texture_limits_Minasny2001} 

}

\caption{Particle size limits used in European countries, Australia and America. Image source: @Minasny2001AJSR.}\label{fig:texture-limits}
\end{figure}

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_sprofs_SNDPPT} 

}

\caption{Histogram and soil-depth density distribution for a global compilation of measurements of sand content in percent. Based on the records from WOSIS [@Batjes2017ESSD].}\label{fig:sprofs-snd}
\end{figure}

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_sprofs_SLTPPT} 

}

\caption{Histogram and soil-depth density distribution for a global compilation of measurements of silt content in percent. Based on the records from WOSIS [@Batjes2017ESSD].}\label{fig:sprofs-slt}
\end{figure}

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_sprofs_CLYPPT} 

}

\caption{Histogram and soil-depth density distribution for a global compilation of measurements of clay content in percent. Based on the records from WOSIS [@Batjes2017ESSD].}\label{fig:sprofs-cly}
\end{figure}

The current standard for particle size classes adopted by FAO for use in
the Harmonized World Soil Database is ISO 10390:2005. This standard
differs from the USDA definition in defining the size range for silt as
2--63 \(\mu\)m instead of 2--50 \(\mu\)m and sand as 63--2000 \(\mu\)m
instead of 50--2000 \(\mu\)m. This is a relatively new standard for FAO
which previously adopted the USDA definitions for the digital soil map
of the world \citep{Nachtergaele2010press}. These differences in
attribute definition cause differences in values reported for soil
particle size classes. Differences in values can also arise because of
differences in method of analysis (e.g.~hygrometer, pipette, laser
diffraction, dispersion etc). Most literature on harmonization of soil
texture data deals with harmonizing differences in attribute definitions
or the reported particle size classes (Fig. \ref{fig:texture-limits}).

\begin{rmdnote}
The most commonly used standard for designation of fine earth texture
fractions, used by the \emph{GlobalSoilMap} project, is the USDA system
(sand: 50--2000 \(\mu\)m, silt: 2--50 \(\mu\)m, clay: \textless{}2
\(\mu\)m).
\end{rmdnote}

\citet{Minasny2001AJSR} identified two major textural classifications in
the world as the International and USDA/FAO systems (Tbl.
\ref{tab:usdafaotexture}). The significant difference between these two
was the choice of a threshold value for differentiating silt from sand
of 20 \(\mu\)m for the International and 50 \(\mu\)m for the USDA/FAO
systems. The new ISO/FAO standard adds an additional difference by
changing the threshold value between silt and sand from 50 \(\mu\)m to
63 \(\mu\)m. Another very important difference in attribute definition
concerns the Russian system which defines the clay fraction as
\textless{}1 \(\mu\)m and the fine earth fraction, or the upper limit of
the sand fraction, at 1 cm instead of 2 cm.

\begin{table}

\caption{\label{tab:usdafaotexture}Differences between the International, USDA and ISO/FAO particle size classifications.}
\centering
\begin{tabular}[t]{llll}
\toprule
Size.Fraction & International & USDA & ISO.FAO\\
\midrule
clay & \$<\$ 2 \$\textbackslash{}mu\$m & \$<\$ 2 \$\textbackslash{}mu\$m & \$<\$ 2 \$\textbackslash{}mu\$m\\
silt & 2–20 \$\textbackslash{}mu\$m & 2–50 \$\textbackslash{}mu\$m & 2–63 \$\textbackslash{}mu\$m\\
sand & 20–2000 \$\textbackslash{}mu\$m & 50–2000 \$\textbackslash{}mu\$m & 63–2000 \$\textbackslash{}mu\$m\\
\bottomrule
\end{tabular}
\end{table}

Both \citet{Nemes1999G} and \citet{Minasny2001AJSR} investigated options
for harmonizing values for sand, silt and clay reported using different
systems for classifying particle size fractions. Using a compilation of
four large databases consisting of a total of 1620 samples,
\citet{Minasny2001AJSR} developed a single multiple linear regression
model for converting between silt fraction based on the international
standard of 2--20 \(\mu\)m (\(P_{\mathtt{2-20}}\)) to the 2--50 \(\mu\)m
range of the USDA standard (\(P_{\mathtt{2-50}}\)) and vice versa:

\begin{equation}
\begin{cases}
\begin{matrix} \hat P_{\mathtt{2-50}} = & -18.3914 + 2.0971 \cdot P_{\mathtt{2-20}} + 0.6726 \cdot P_{\mathtt{20-2000}}   \\
   & - 0.0142 \cdot P_{\mathtt{2-20}}^2  - 0.0049 \cdot P_{\mathtt{20-2000}}^2
\end{matrix}   & \text{ if } \hat P_{\mathtt{2-50}} > 0 \\ \begin{matrix} \hat P_{\mathtt{2-50}} = & 0.8289 \cdot P_{\mathtt{2-20}} + 0.0198 \cdot P_{\mathtt{20-2000}} \end{matrix} & \text{ if } \hat P_{\mathtt{2-50}} < 0
\end{cases}
(\#P2_50)
\end{equation}

where \(P_{\mathtt{20-2000}}\) is the international sand fraction. This
conversion is fairly accurate since the model explains most of the
observed variability in the original values (\(R^2\)=0.823). Together
with the conversion of the silt fraction is the conversion of the sand
fraction.

\citet{Minasny2001AJSR} argued that most countries should consider
adopting the particle size limits and texture classes of the USDA
system. They noted that the 2--50 \(\mu\)m particle size range is
usually more useful than the 2--20 \(\mu\)m range for estimating water
retention in pedo-transfer functions and observed that translations from
one system into another were relatively easy, given improved computing
power and algorithms.

\citeauthor{Nemes1999}
\citetext{\citeyear{Nemes1999}; \citealp{Nemes1999G}} evaluated four
different interpolation methods (log-linear interpolation, fitting a
Gompertz curve, spline interpolation, and similarity method) in order to
achieve compatibility of particle-size distributions within the
\emph{European soil hydraulic database HYPRES}
(\url{http://www.macaulay.ac.uk/hypres/}). They introduced a new
similarity procedure, which uses an external reference data set that
contains a wide variety of reference soil materials, each with 7 or 8
measured particle-size fractions. The procedure involves searching for
soil samples in the external reference data set that match the
particle-size distribution of the soil to be interpolated. From each
search. 10 similar soils are selected that have fractions at the
particle size limits similar to the soil under investigation. The
arithmetic mean of the fractions of these 10 soils at the specified
particle size limit is calculated and assigned as the estimate of the
fraction for the soil under investigation.

The HYPRES reference database and the similarity procedures applied to
it are appropriate for harmonizing a wide range of soils from a variety
of countries and could be used as one of the main methods in a global
Soil Reference Library. The generic nature of this conversion approach,
and the fact that it does not rely on multiple, locally developed,
regression equations, makes it an attractive option for use in
harmonization of global particle size data.

\hypertarget{bulk-density}{%
\subsection{Bulk density}\label{bulk-density}}

Measurement of soil Bulk Density (BLD) is often time consuming and
relatively costly. For this reason, it is not analysed and reported for
legacy soil profiles as frequently or consistently as many other, more
common, soil properties. Consequently, predicting bulk density globally
using digital soil mapping methods is fraught with difficulties and
uncertainties. However, it is critical to at least attempt to make some
kind of estimate of how bulk density varies spatially because we need to
know the bulk density of the soil in order to make any estimates of
volumetric concentrations of materials such as organic carbon, water or
nutrients.

In practice, we need to be able to make estimates of two different types
of bulk density, namely the bulk density of the whole soil and the
\emph{bulk density of the fine earth fraction} (particles \textless{}2
mm) only. Calculations such as those for total stocks of carbon are
first applied using the bulk density of the fine earth fraction only but
this value is then reduced in accordance with the volume proportion of
the soil that is occupied by coarse fragments greater than 2 mm in size.
Bulk density is also of interest for global soil mapping applications
because it influences infiltration and movement of water in the soil,
penetration of the soil by plant roots and mechanical workability of the
soil using farm implements.

Bulk density is the over-dry mass of soil material divided by the total
volume. The standard reference method for reporting bulk density for
GSIF is the core method (ISO 11272). The dry bulk density (BD) is the
ratio between the mass of oven dry soil material and the volume of the
undisturbed fresh sample. The ISO standard defines dry bulk density as
the ratio of the oven-dry mass of the solids to the volume (the bulk
volume includes the volume of the solids and of the pore space) of the
soil. The recommended ISO method (core method) uses steel cylinders of
known volume (100 cubic cm, 400 cubic cm) that are driven into the soil
vertically or horizontally by percussion. Sampling large volumes results
in smaller relative errors but requires heavy equipment. The method
cannot be used if stones or large roots are present or when the soil is
too dry or too hard.

For soils with a high stone or root content or when the soil is too dry
or too hard, methods based on the excavation technique are used as an
alternative to the core method. In the excavation method a hole on a
horizontal surface is dug and then filled with a material with a known
density (e.g.~sand which packs to a calibrated volume or water separated
from the soil material by an elastic membrane) to assess the volume of
the hole or the sample taken. The soil obtained from the hole, is
oven-dried to remove the water and the oven-dry mass of the total sample
is weighed. The volumetric percentage of the coarse fragments needs to
be determined and the weight of the coarse fragments assessed, in order
to be able to calculate the oven-dry bulk density of the fine earth
separately.

The USDA handbook for analytical procedures describes various methods
for assessing various types of bulk density. USDA soil data report
values for bulk density of the fine earth as well as of the whole earth
(including gravel), with the weight assessed oven-dry as well as at
field capacity e.g.~including water. The latter method relates the
weight of moist soil to the volume of moist or oven-dry soil. Experience
has shown that organic carbon and texture or clay content predominately
influence soil bulk density, even though the nature of the clay
(mineralogy) is as important as the percentage content of the clay.
Organic carbon and texture information is often available in soil survey
reports, while bulk density is often not as frequently reported.

Many attempts have therefore been made to estimate soil bulk densities
through pedo-transfer functions (PTFs) based on soil organic carbon and
texture data
\citep{Curtis1964SSSAP, Adams1973JSS, Alexander1980SSSAJ, Federer1993CJFR, Rawls1983SS, Manrique1991SSSAJ, Bernoux1998SSSAJ}.
\citet{Heuscher2005SSSAJ} applied a stepwise multiple regression
procedure to predict oven-dried bulk density from soil properties using
the NRCS National Soil Survey Characterization Data. The database
included both subsoil and topsoil samples. An overall regression
equation for predicting oven-dried bulk density from soil properties
(\(R^2=0.45\), \(P<0.001\)) was developed using almost 47,000 soil
samples. Partitioning the database by soil suborders improved regression
relationships (\(R^2=0.62\), \(P<0.001\)). Of the soil properties
considered, the stepwise multiple regression indicated that organic C
content was the strongest contributor to bulk density prediction. Other
significant variables included clay content, water content and to a
lesser extent, silt content, and depth.

\begin{rmdnote}
Bulk density is the oven-dry mass of soil material divided by the total
volume and typically ranges from 0.7 to 1.8 t/kg\(^3\). The average bulk
density of fine earth fraction of soil is about 1.3 t/kg\(^3\); soils
with a bulk density higher than tend to restrict root growth. Different
values for bulk density typically apply for different soils with
different soil genesis as reflected by different materials and
mineralogy, e.g.~Histosols (organic), Arenosols (sandy), Andosols
(allophanic clay), Acrisols (low activity clays) and Vertisols (high
activity clays).
\end{rmdnote}

Bulk density tends to be measured and reported less frequently in legacy
data bases and reports than most other commonly measured soil analytical
properties. Such values as are reported are most often based on field
measurements of in-situ bulk density using the core method. Bulk density
of the fine earth fraction alone is measured and reported even less
frequently than bulk density for the whole soil (Fig.
\ref{fig:sprofs-bld}).

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_sprofs_BLD} 

}

\caption{Histogram and soil-depth density distribution for a global compilation of measurements of bulk density (tonnes per cubic metre). Based on the records from WOSIS [@Batjes2017ESSD].}\label{fig:sprofs-bld}
\end{figure}

Given that there are more values reported for the bulk density of the
whole soil than for the fine earth fraction, we propose to first
estimate the bulk density of the whole soil (using appropriate
pedo-transfer functions) and then apply corrections to estimate the bulk
density of the fine earth fraction, correcting for the effect of course
fragments. Correction involves subtracting the volume of coarse
fragments from the total volume of soil sampled for assessing bulk
density in-situ in the field and then also subtracting the (estimated)
weight of coarse fragments from the measured oven-dry weight of the
sampled soil.

The revised weight of the fine-earth fraction alone (minus the weight of
the coarse fragments) is divided by the adjusted volume of the sample
(reduced by the volume of coarse fragments) to obtain an estimate of
bulk density for the fine earth fraction alone. This value of density of
the fine-earth fraction alone is the one needed to compute estimates of
volumetric soil properties, such as total carbon stocks. It is therefore
the critical measure of bulk density for reporting concentrations of
soil chemical properties. Conversely, bulk density of the whole soil, in
situ, is generally of greater use and interest for assessing
hydrological behaviours and properties, such as hydraulic conductivity
and moisture holding capacity.

\citet{Tranter2007SUM} proposed a conceptual model that incorporated a
priori knowledge for predicting soil bulk density from other, more
regularly measured, properties. The model considers soil bulk density to
be a function of soil mineral packing structures (\(\rho_m\)) and soil
structure (\(\Delta \rho\)). Bulk densities were also observed to
increase with depth, suggesting the influence of over-burden pressure.
Residuals from the \(\rho_m\) model, referred to as \(\Delta \rho\),
correlated with organic carbon.

\citet{Torri1994C} developed a nomogram for transforming rock fragment
content from a by-mass to a by-volume basis and vice versa based on bulk
density data. This nomogram facilitates conversion of data on rock
fragment content expressed in different units. Most PTFs for predicting
bulk density, except those developed by \citet{Rawls1983SS} and
\citet{Bernoux1998SSSAJ}, are a function only of organic matter
i.e.~organic carbon content. Although studies conducted by
\citet{Saini1966N} and \citet{Jeffrey1970JE} have shown that organic
matter has a dominating effect on soil bulk density and that it can be
used alone as a good predictor of soil bulk density, it has been
observed (e.g. \citet{Alexander1980SSSAJ} and \citet{Manrique1991SSSAJ})
that, where organic matter is a minor component, soil texture plays a
major role in controlling bulk density .

\hypertarget{soil-organic-carbon-stock}{%
\subsection{Soil organic carbon stock}\label{soil-organic-carbon-stock}}

Primary soil properties such as organic carbon content, bulk density and
coarse fragments can be further used as inputs for estimation of
secondary soil properties which are typically not measured directly in
the field and need to be derived from primary soil properties. For
instance, consider estimation of the global carbon stock (in permille).
This secondary soil property can be derived from a number of primary
soil properties \citep{Nelson1982, sanderman2018soil} (see Fig.
\ref{fig:ocs-calculus-scheme}):

\begin{equation}
    \mathtt{OCS} \; [\mathrm{kg \; m^{-2}}] = \frac{{\mathtt{ORC}}}{{1000}} \; [\mathrm{kg \; kg^{-1}}] \cdot \frac{{\mathtt{HOT}}}{{100}} \; [\mathrm{m}] \cdot \mathtt{BLD} \; [\mathrm{kg \; m^{-3}}] \cdot \frac{{100-\mathtt{CRF} \; [\mathrm{\%}]}}{{100}}
\label{eq:ocs-calc}
\end{equation}

where \texttt{OCS} is soil organic carbon stock, \texttt{ORC} is soil
organic carbon mass fraction in permilles, \texttt{HOT} is horizon
thickness in , \texttt{BLD} is soil bulk density in and \texttt{CRF} is
volumetric fraction of coarse fragments (\(>\) 2 mm) in percent.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_OCS_calculus_scheme} 

}

\caption{Soil organic carbon stock calculus scheme. Example of how total soil organic carbon stock (OCS), and its propagated error, can be estimated for a given volume of soil using organic carbon content (ORC), bulk density (BLD), thickness of horizon (HOT), and percentage of coarse fragments (CRF). Image source: @Hengl2014SoilGrids1km. OCSKGM function also available via the [GSIF package](http://gsif.r-forge.r-project.org/OCSKGM.html).}\label{fig:ocs-calculus-scheme}
\end{figure}

The propagated error of the soil organic carbon stock
(Eq.\eqref{eq:ocs-calc}) can be estimated using the Taylor series method
\citep{Heuvelink1998a} i.e.~by using the standard deviations of the
predicted soil organic carbon content, bulk density and coarse
fragments, respectively (Fig. \ref{fig:ocs-calculus-scheme}).
\texttt{OCS} values can be derived for all depths / horizons, then
aggregated to estimate the total stock for the whole profile (e.g.~0--2
m).

The formulas to derive soil organic carbon stock and the propagated
uncertainty are implemented in the GSIF package e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Area <-}\StringTok{ }\FloatTok{1E4}\NormalTok{  ## 1 ha}
\NormalTok{HSIZE <-}\StringTok{ }\DecValTok{30}\NormalTok{ ## 0--30 cm}
\NormalTok{ORCDRC <-}\StringTok{ }\DecValTok{50}\NormalTok{  ## 5%}
\NormalTok{ORCDRC.sd <-}\StringTok{ }\DecValTok{10}\NormalTok{  ## +/-1%}
\NormalTok{BLD <-}\StringTok{ }\DecValTok{1500}\NormalTok{  ## 1.5 tonnes per cubic meter}
\NormalTok{BLD.sd <-}\StringTok{ }\DecValTok{100}\NormalTok{  ## +/-0.1 tonnes per cubic meter}
\NormalTok{CRFVOL <-}\StringTok{ }\DecValTok{10}\NormalTok{  ## 10%}
\NormalTok{CRFVOL.sd <-}\StringTok{ }\DecValTok{5}\NormalTok{  ## +/-5%         }
\NormalTok{x <-}\StringTok{ }\KeywordTok{OCSKGM}\NormalTok{(ORCDRC, BLD, CRFVOL, HSIZE, ORCDRC.sd, BLD.sd, CRFVOL.sd)}
\NormalTok{x  ## 20.25 +/-4.41 kg/m^2}
\CommentTok{#> [1] 20.2}
\CommentTok{#> attr(,"measurementError")}
\CommentTok{#> [1] 4.41}
\CommentTok{#> attr(,"units")}
\CommentTok{#> [1] "kilograms per square-meter"}
\NormalTok{x[[}\DecValTok{1}\NormalTok{]] }\OperatorTok{*}\StringTok{ }\NormalTok{Area }\OperatorTok{/}\StringTok{ }\DecValTok{1000}\NormalTok{ ## in tonnes per ha:}
\CommentTok{#> [1] 202}
\end{Highlighting}
\end{Shaded}

A more robust way to estimate the propagated uncertainty of deriving
\texttt{OCS} would be to use geostatistical simulations e.g.~derive
standard error from a large number of realizations (e.g.
\textgreater{}100) that incorporate spatial and vertical correlations.
Because, in the case of soil mapping, we are often dealing with massive
data sets, running geostatistical simulations for millions of pixels is
current not yet a feasible option.

\hypertarget{available-water-capacity}{%
\subsection{Available Water Capacity}\label{available-water-capacity}}

The available water holding capacity (AWC) is a complex soil property.
It is basically a soil or land quality \citep{food1977framework}, that
provides valuable information about the capacity of the soil to hold
water, particularly water that is potentially available for root uptake
by plants and vegetative transpiration. In practice, AWC is land cover
specific. The water available for root uptake depends on the soil
properties that determine rootability or rooting depth as genetically
required by the vegetative land cover.

The water available for root uptake also depends on the pressure head
that the vegetative land cover can generate or bridge between the
pressure in the atmosphere and the pressure in the soil matrix. E.g.
cotton can still extract water at -2500 kPa (pF 4.4) while pepper wilts
at -350 kPa (pF 3.5). The ability of a soil to accept and store water
has implications beyond simply the capacity to support plant growth. It
also affects how a soil responds to hydrological events such as
rainfall, snowmelt and runoff. Soils that can rapidly absorb and retain
significant amounts of rainfall act as a buffer reducing rapid runoff
and flooding. Soils that have a limited ability to accept and store
rainfall contribute to rapid runoff with increased chances of erosion
and flooding. Models of crop growth, runoff, erosion and flooding all
have requirements for location-specific information about available
water capacity.

The AWC is expressed in mm (which equals mm water/cm soil depth, or
water/ soil). This volume of water depends on the volume of soil
(influenced by depth interval and by volumetric gravel content) and the
volumetric fraction of water that is contained by the soil between field
capacity and wilting point. GSIF reports AWC with a precision of 1 mm
and a maximum range of 0--2000 mm.

Values for AWC are preferably assessed for the fine earth fraction per
depth interval and expressed as volumetric fraction. This value can be
corrected for the gravel content of the depth interval and summed up
over the interval. Preferably, the values for volumetric AWC of the fine
earth fraction per depth interval are derived from values for water
content at specific water tensions (e.g.~at pF 0.1, 2, 2.5, 3, 4.2,
4.5). For pragmatic reasons though the permanent wilting point is set at
-1500 kPa (or 15,000cm, 15 bar, 15 atmosphere or pF 4.2).

The standard reference method adopted by GSIF for reporting available
water capacity is as per the USDA Soil Survey Laboratory Methods Manual
\citep[ p.137]{Burt2004SSIR}. Calculation of the \emph{Water Retention
Difference} (WRD) is considered the initial step in the approximation of
the available water capacity (AWC). WRD is a calculated value that
denotes the volume fraction for water in the whole soil that is retained
between -1500 kPa suction and an upper limit of usually -33 or -10 kPa
suction (pF 2.5 or pF 2) \citep[ p.137]{Burt2004SSIR}. The upper limit
(lower suction) is selected so that the volume of water retained
approximates the volume of water held at field capacity. The -33 and
-1500 kPa gravimetric water contents are then converted to a whole soil
volume basis by multiplying by the oven dry bulk density of the fine
earth fraction (\texttt{Db33}) and adjusting downward for the volume
fraction of rock fragments, if present, in the soil.

\begin{rmdnote}
Available water capacity (expressed in mm of water for the effective
soil depth) can be estimated based on the Water Retention Difference
(WRD) which denotes the volume fraction for water in the whole soil,
including gravel, that is retained between -1500 kPa suction and an
upper limit of 33 kPa suction.
\end{rmdnote}

\emph{``The development of hydraulic PTFs has become a boom industry,
mostly in the US and Europe''} \citep{Minasny2007JITL}. Results of such
research have been reported widely, including in the USA
\citep{Rawls1991AA}, UK, the Netherlands \citep{Wosten1995G}, and
Germany. Research has attempted to correlate particle size distribution,
bulk density and organic matter content with water content at field
capacity (FC, \(\theta\) at -33 kPa), permanent wilting point (PWP,
\(\theta\) at -1500 kPa), and available water content (AWC = FC - PWP)
\citep{Minasny2007JITL}. \citet{Gijsman2007CEA} reported that
\emph{``many PTFs for estimating soil hydraulic properties have been
published already''} (see overviews by \citet{Rawls1991AA},
\citet{Timlin1996AS} and \citet{Wosten2001JH}). \citet{Timlin1996AS}
reported 49 methods and estimated that this covers only about 30\% of
the total. \citet{Gijsman2007CEA} compared eight methods for all the
soil classes that make up the texture triangle. They went through the
triangle in steps of sand, silt and clay and determined the estimated
values of wilting point or lower limit of \emph{plant extractable water}
(LL), \emph{field capacity} or the drained upper limit (DUL), and soil
saturation (SAT). They finally concluded that none of the methods were
universally good. The best method in the comparison of
\citet{Gijsman2007CEA} was \citet{Saxton1986SSSAJ}, closely followed by
\citet{Rawls1982JIDDASCE}.

Alterra institute in collaboration with ISRIC validated the PTF
developed by \citet{hodnett2002marked} on the basis of the data present
in the Africa Soil Profiles database \citep{Leenaars2012} to predict
tension specific volumetric water content \citep{wosten2013soil} to
assess WRD. \citet{Jagtap2004TASAE} developed an approach that does not
fit a mathematical equation through the data, but rather compares the
soil layer for which the key soil water contents of lower limit (LL),
drained upper limit (DUL), and soil saturation (SAT), have to be
estimated with all layers in a database of field-measured
soil-water-retention data. The layer that is most similar in texture and
organic carbon concentration is considered to be the \emph{`nearest
neighbor'} among all the layers in the database and its
soil-water-retention values are assumed to be similar to those that need
to be estimated. To avoid making estimated soil-water-retention values
dependent on only one soil in the database, the six \emph{`nearest
neighbors'} are used and weighted according to their degree of
similarity \citep{Jagtap2004TASAE}. This is a non-parametric procedure,
in the sense that it does not assume a fixed mathematical relationship
between the physical properties and the water holding properties of
soils. The similarity method to convert soil particle size fraction data
proposed by \citeauthor{Nemes1999G}
\citetext{\citeyear{Nemes1999G}; \citealp{Nemes1999}} is a direct
analogue of this similarity method of \citet{Jagtap2004TASAE} for soil
hydraulic properties.

\citet{Zacharias2007SSSAJ} identified three different approaches for
deriving the WRD from more easily available parameters as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Point-based estimation methods}: estimating the water content of
  selected matric potentials from predictors such as the percentage of
  sand, silt, or clay, the amount of organic matter, or the bulk density
  (e.g. \citet{Rawls1982JIDDASCE}).
\item
  \emph{Semi-physical approach}: deriving the WRD from information on
  the cumulative particle size distribution \citep{Arya1981SSSAJ};
  theoretically, this approach is based on the similarity between
  cumulative particle size distribution and water retention curves. The
  water contents are derived from the soil's predicted pore volume and
  the hydraulic potentials are derived from capillarity relationships.
\item
  \emph{Parameter estimation methods}: using multiple regression to
  derive the parameters of an analytical closed-form equation for
  describing the WRD, using predictors such as the percentage of sand,
  silt, and clay, the amount of organic matter, or the bulk density
  (e.g. \citeauthor{van1980closed}
  \citetext{\citeyear{van1980closed}; \citealp{Wosten1999G}; \citealp{wosten2013soil}}).
\end{enumerate}

\citet{Zacharias2007SSSAJ} concluded that approach (1) has the
disadvantage that it uses a large number of regression parameters
depending on the number of WRD sampling points, which makes its use in
the mathematical modeling more difficult; while for approach 2 very
detailed information about the particle size distribution is required.
They therefore preferred use of (3) the parameter estimation methods.

\citet{Zacharias2007SSSAJ} also observed that pedo-transfer functions
that do not consider soil organic matter are rare and gave the following
examples. \citet{Hall1977} developed point-based regression equations
using soil texture and bulk density (only for subsoils) for British
soils. \citet{Oosterveld1980CAE} developed an exponential regression
equation for Canadian soils for fitting the relationship between clay
and sand content, depth of soil, and moisture content. Equations to
estimate the WRC from mean particle diameter and bulk density have been
proposed by \citet{Campbell1989}. \citet{Williams1992} analyzed
Australian data sets and developed regression equations for the
relationship between soil moisture and soil texture, structure
information, and bulk density including variants for both the case where
there is available information on soil organic matter and where the soil
organic matter is unknown. \citet{Rawls1989} reported regression
equations to estimate soil water retention as a function of soil texture
and bulk density. \citet{Canarache1993ST} developed point based
regression equations using clay content and bulk density for Romanian
soils. More recently, \citet{Nemes2003SSSAJ} developed different PTFs
derived from different scales of soil data (Hungary, Europe, and
international data) using artificial neural network modeling including a
PTF that uses soil texture and bulk density only.

\citet{Zacharias2007SSSAJ} developed two different regression equations
depending upon the percentage of sand in a soil as follows:

\begin{equation}
\begin{cases}
 \begin{matrix}
 \theta_r  = 0 \\
 \theta_s  = 0.788 + 0.001 \cdot {\mathtt{clay}} -0.263 \cdot D_b \\
 \ln(\alpha) = -0.648 + 0.023 \cdot {\mathtt{sand}} + 0.044 \cdot {\mathtt{clay}} -3.168 \cdot D_b \\
 n = 1.392- 0.418\cdot {\mathtt{sand}}^{-0.024} + 1.212\cdot {\mathtt{clay}}^{-0.704}
\end{matrix} & \text{ if } {\mathtt{sand}} < 66.5\% \\ \hline \begin{matrix}
 \theta_r  = 0 \\
 \theta_s  = 0.890 + 0.001 \cdot {\mathtt{clay}} -0.332 \cdot D_b \\
 \ln(\alpha) = -4.197 + 0.013 \cdot {\mathtt{sand}} + 0.076 \cdot {\mathtt{clay}} -0.276 \cdot D_b \\
 n = 2.562 - 7 \cdot 10^{-9} \cdot {\mathtt{sand}} + 3.750\cdot {\mathtt{clay}}^{-0.016}
\end{matrix} & \text{ if } {\mathtt{sand}} > 66.5\%
\end{cases}
\label{eq:thetas}
\end{equation}

The regression coefficients from these models were almost identical to
those reported by \citet{Vereecken1989SS} (i.e.
\(\theta_s = 0.81 + 0.001 \cdot {\mathtt{clay}} - 0.283 \cdot D_b\)) for
a different data set, adding further credibility to their general
applicability. \citet{Zacharias2007SSSAJ} recommended using the PTFs of
\citet{Vereecken1989SS} if data on soil organic matter were available
but felt that their proposed equations were suitable for use where soil
organic matter data were not available.

Empirical equations developed by \citet{Williams1992} for the prediction
of the constants \(A\) and \(B\) in the Campbell function have been
widely used in Australia and elsewhere. These regression equations
require particle size distribution, field texture and bulk density
inputs as follows:

\begin{equation}
\begin{split}   
A =& 1.996 + 0.136 \cdot \ln({\mathtt{clay}}) - 0.00007 \cdot {\mathtt{fsand}} + \\
  & + 0.145\cdot \ln({\mathtt{silt}}) + 0.382 \cdot \ln({\mathtt{TEXMHT}})
\end{split}
\end{equation}

\begin{equation}
B = -0.192 + 0.0946\cdot \ln({\mathtt{TEXMHT}}) - 0.00151\cdot \mathtt{fsand}
\end{equation}

where \(\mathtt{clay}\) (\textless{}0.002 mm), \(\mathtt{fsilt}\)
(0.02--0.20 mm), and \(\mathtt{sand}\) (0.002--0.02 mm) are expressed in
\%; \(\mathtt{TEXMHT}\) is texture group from 1--6 as defined by
Northcote in \citet{peverill1999soil}.

\citet{Cresswell2006SUM} demonstrated applicability of the
\citet{Williams1992} method for French soils and confirmed that the
approach of assuming a Campbell SWC model and empirically predicting the
slope and air entry potential has merit. They concluded that the
empirical regression equations of Campbell appeared transferable to
different data sets from very different geographical locations. They
provided regression equations for all samples and stratified by horizon
type that had R-square values ranging from 0.81 to 0.91.

\citet{Cresswell2006SUM} further suggested a strategy for achieving
adequate coverage of soil hydraulic property data for France that
included an efficient sampling strategy based on the use of functional
horizons \citep{Bouma1989S}, and a series of reference sites where soil
hydraulic properties could be measured comprehensively. They argued that
the functional horizon method recognizes the soil texture class of the
horizon rather than the profile as the individual or building block for
prediction \citep{Wosten1985SSSAJ, Wosten1992}. A significant feature of
this approach is the capacity to create a complex range of different
hydrologic soil classes from simple combinations of horizon type,
sequence, and thickness.

Pedo-transfer functions for available water capacity typically have a
general form of:

\begin{equation}
{\mathtt{AWAIMM}} = f ({\rm organic} \; {\rm carbon}, {\rm sand}, {\rm silt}, {\rm clay},  {\rm bulk} \; {\rm density})
\end{equation}

where the total profile available water (\(\mathtt{AWAIMM}\)) can be
summed over the effective depth.

By using the GSIF package, one can estimate \(\mathtt{AWAIMM}\) using
the pedo-transfer function described by \citet{hodnett2002marked} and
\citet{wosten2013soil}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SNDPPT =}\StringTok{ }\DecValTok{30} 
\NormalTok{SLTPPT =}\StringTok{ }\DecValTok{25} 
\NormalTok{CLYPPT =}\StringTok{ }\DecValTok{48} 
\NormalTok{ORCDRC =}\StringTok{ }\DecValTok{23} 
\NormalTok{BLD =}\StringTok{ }\DecValTok{1200} 
\NormalTok{CEC =}\StringTok{ }\DecValTok{12} 
\NormalTok{PHIHOX =}\StringTok{ }\FloatTok{6.4}
\NormalTok{x <-}\StringTok{ }\KeywordTok{AWCPTF}\NormalTok{(SNDPPT, SLTPPT, CLYPPT, ORCDRC, BLD, CEC, PHIHOX)}
\KeywordTok{str}\NormalTok{(x)}
\CommentTok{#> 'data.frame':    1 obs. of  5 variables:}
\CommentTok{#>  $ AWCh1: num 0.16}
\CommentTok{#>  $ AWCh2: num 0.122}
\CommentTok{#>  $ AWCh3: num 0.0999}
\CommentTok{#>  $ WWP  : num 0.259}
\CommentTok{#>  $ tetaS: num 0.511}
\CommentTok{#>  - attr(*, "coef")=List of 4}
\CommentTok{#>   ..$ lnAlfa: num  -2.29 0 -3.53 0 2.44 ...}
\CommentTok{#>   ..$ lnN   : num  62.986 0 0 -0.833 -0.529 ...}
\CommentTok{#>   ..$ tetaS : num  81.799 0 0 0.099 0 ...}
\CommentTok{#>   ..$ tetaR : num  22.733 -0.164 0 0 0 ...}
\CommentTok{#>  - attr(*, "PTF.names")=List of 1}
\CommentTok{#>   ..$ variable: chr  "ai1" "sand" "silt" "clay" ...}
\KeywordTok{attr}\NormalTok{(x, }\StringTok{"coef"}\NormalTok{)}
\CommentTok{#> $lnAlfa}
\CommentTok{#>  [1]  -2.294   0.000  -3.526   0.000   2.440   0.000  -0.076 -11.331}
\CommentTok{#>  [9]   0.019   0.000   0.000   0.000}
\CommentTok{#> }
\CommentTok{#> $lnN}
\CommentTok{#>  [1] 62.986  0.000  0.000 -0.833 -0.529  0.000  0.000  0.593  0.000  0.007}
\CommentTok{#> [11] -0.014  0.000}
\CommentTok{#> }
\CommentTok{#> $tetaS}
\CommentTok{#>  [1]  81.7990   0.0000   0.0000   0.0990   0.0000 -31.4200   0.0180}
\CommentTok{#>  [8]   0.4510   0.0000   0.0000   0.0000  -0.0005}
\CommentTok{#> }
\CommentTok{#> $tetaR}
\CommentTok{#>  [1] 22.7330 -0.1640  0.0000  0.0000  0.0000  0.0000  0.2350 -0.8310}
\CommentTok{#>  [9]  0.0000  0.0018  0.0000  0.0026}
\end{Highlighting}
\end{Shaded}

where \texttt{SNDPPT}, \texttt{SLTPPT} and \texttt{CLYPPT} are the
measured sand, silt and clay content in percent, \texttt{ORCDRC} is the
soil organic carbon concentration in permille or , \texttt{BLD} is the
bulk density in , \texttt{CEC} is the Cation Exchange Capacity in , and
\texttt{PHIHOX} is the soil pH in water suspension. The output
\texttt{AWCh1}, \texttt{AWCh2}, \texttt{AWCh3} are the available soil
water capacity (volumetric fraction) for pF 2.0, 2.3 and 2.5,
\texttt{WWP} is the soil water capacity (volumetric fraction) until
wilting point, and \texttt{tetaS} is the saturated water content,
respectively.

\hypertarget{harmonisation-of-soil-data-and-pedo-transfer-functions}{%
\section{Harmonisation of soil data and pedo-transfer
functions}\label{harmonisation-of-soil-data-and-pedo-transfer-functions}}

\hypertarget{basic-concepts-of-harmonisation-of-soil-property-values}{%
\subsection{Basic concepts of harmonisation of soil property
values}\label{basic-concepts-of-harmonisation-of-soil-property-values}}

A well known issue with legacy soils data is the use of different
methods for analyzing soils in the laboratory or describing them in the
field. These different methods yield different values that are not
exactly equivalent or comparable. This creates a need to assess the
significance of the differences in values arising from different methods
or method-groups, and possibly the need to harmonize values produced
using different methods in order to make them roughly equivalent and
comparable. The process of conversion of values measured according to an
original method to values roughly equivalent to those measured according
to an agreed-upon standard reference method is referred to as \emph{data
harmonization}.

Note that differences in methods are not necessarily reflected in
different values for a given attribute. The value reported is
fundamentally related to the particular method used for analysis, which
we correctly or incorrectly label similarly regardless of the analytical
method used.

When using legacy soils data for global soil mapping and analysis
projects, it is important to first decide whether it is necessary and
important to convert measurements made using various different
laboratory methods into equivalent values in the specified standard
reference method. This assessment can be made for each soil property
individually. Decisions as to whether harmonization is necessary may be
influenced by the resolution of the mapping and the desired precision
and accuracy of the output predictions.

The process of conversion of values measured by an original method to
values roughly equivalent to those measured by an agreed-upon standard
reference method is referred to as data harmonization. Examples of
harmonisation would be converting values assessed by e.g.~pH in 1:2
water to values as if assessed by pH in 1:5 water, or organic carbon by
\emph{Walkley-Black} into organic carbon by \emph{dry combustion}.

\hypertarget{example-of-harmonization-using-texture-by-hand-classes}{%
\subsection{Example of harmonization using texture-by-hand
classes}\label{example-of-harmonization-using-texture-by-hand-classes}}

Harmonization of values reported for sand, silt and clay computed using
methods of textural analysis that use definitions for particle size
fractions different from the reference method will also have to be
converted to the standard particle size definitions adopted for some
international specifications. For example, classes in the texture
triangle represent fractions for sand, silt and clay which can be
assessed using the gravity point for the class (Tbl.
\ref{tab:usdatexturec}; see also further Fig.
\ref{fig:plot-tt-triangle}).

\begin{table}

\caption{\label{tab:usdatexturec}Simple conversion of the USDA texture-by-hand classes to texture fractions (sd indicates estimated standard deviation).}
\centering
\begin{tabular}[t]{rlrrrrrr}
\toprule
Number & Texture.class & Sand & Silt & Clay & Sand\_sd & Silt\_sd & Clay\_sd\\
\midrule
1 & clay (C) & 22 & 22 & 56 & 11.8 & 9.8 & 11.1\\
2 & clay loam (CL) & 32 & 34 & 33 & 7.0 & 7.7 & 3.5\\
3 & loam (L) & 41 & 39 & 20 & 6.8 & 6.0 & 5.1\\
4 & loamy sand (LS) & 83 & 11 & 7 & 3.8 & 5.8 & 3.2\\
5 & sand (S) & 92 & 4 & 3 & 3.0 & 3.0 & 2.2\\
\addlinespace
6 & sandy clay (SC) & 51 & 9 & 40 & 4.3 & 4.4 & 3.9\\
7 & sandy clay loam (SCL) & 60 & 14 & 26 & 7.9 & 7.3 & 4.2\\
8 & silt (Si) & 7 & 85 & 9 & 3.9 & 3.2 & 3.1\\
9 & silty clay (SiC) & 7 & 47 & 46 & 4.5 & 4.7 & 4.4\\
10 & silty clay loam (SiCL) & 9 & 58 & 33 & 5.7 & 6.8 & 3.5\\
\addlinespace
11 & silty loam (SiL) & 18 & 64 & 18 & 10.9 & 8.8 & 6.5\\
12 & sandy loam (SL) & 67 & 22 & 12 & 8.5 & 10.2 & 4.7\\
\bottomrule
\end{tabular}
\end{table}

Neither the \emph{GlobalSoilMap} project nor GSIF has yet identified and
selected specific functions to use to harmonize data produced using
different analytical methods for any of the soil properties that are to
be predicted and mapped. It is possible that a single
globally-applicable default harmonisation function could potentially be
identified for each of the methods of analysis for each of the soil
properties selected for global analysis. However, this would require the
current multitude of method definitions to be unambiguously defined and
uniquely identified (IDx), and possibly grouped into aggregate classes,
for subsequent conversion from IDx to IDy.

\begin{rmdnote}
Soil observations, such as observation of texture by hand class, are
often inexpensive, but rely on good expert knowledge skills. Statistical
frameworks are needed that can use both highly precise and
quick-and-inaccurate observations to generate better soil maps.
\end{rmdnote}

We have previously noted that locally-specific harmonisation functions
have consistently proven to be more effective than global ones and there
is widespread agreement that there is generally no universal equation
for converting from one method to another in all instances
\citep{Konen2002SSSAJ, Meersmans2009SUM, Jankauskas2006CSSPA, Jolivet1998CSSPA, DeVos2007SUM}.
Consequently, there will likely be a need to develop locally relevant
harmonisation functions at the continental or regional level that apply
to restricted soil-landscape domains.

\citet{McBratney2002Geoderma} proposed the concept of a \emph{soil
inference system} (SINFERS) that incorporated both expert soil knowledge
and statistical prediction equations. The proposed system was intended
to implement two major functions, namely:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Predict all soil properties using all possible (known) combinations of
  inputs and harmonisation functions.
\item
  Select the combination that leads to a prediction with the minimum
  variance.
\end{enumerate}

\hypertarget{soil-class-data}{%
\section{Soil class data}\label{soil-class-data}}

\hypertarget{soil-types}{%
\subsection{Soil types}\label{soil-types}}

Soil types or soil classes are categories of soil bodies with similar
soil properties and/or genesis and functions. There are three main
approaches to soil classification \citep{eswaran2010soil, buol2011soil}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Classification of soils for the purpose of engineering} --- Here
  the focus is put on predicting soil engineering properties and
  behaviors i.e.~on physical and hydrological soil properties.
\item
  \emph{Descriptive classification of soil for the purpose of explaining
  the soil genesis} --- Here the focus is put on soil morphology and
  pedogenesis i.e.~functioning of the soil as part of an ecosystem. The
  representative soil types derived through morphological classification
  are often visualized as soil profiles or by using soil-depth
  functions.
\item
  \emph{Numerical or statistical classification of soils} --- This is
  purely data-driven soil classification which can result in significant
  groupings of soil properties, but that then do not have any cognitive
  name and are difficult to visualize.
\end{enumerate}

Soil classification or soil taxonomy supports the transfer of soil
information from one place, or individual, to another. Classifying soils
can also often be very cost effective --- if we identify the soil class
correctly it is highly likely that we will be able to predict multiple
additional soil properties that co-vary by soil type that would
otherwise require significant resources to measure in the lab.

There are two major international soil taxonomic systems of primary
interest for globla soil mapping: The USDA's \emph{Soil Taxonomy}
\citep{agriculture2010keys}, and the FAO's \emph{World Reference Base}
\citep{FAO2006WRB}. Both KST and WRB are hierarchial, key-based
morphological classification systems, but with increasingly more
analytical data required to reach a specific, more refined, class.
Mapping soil types, using WRB or KST or both, has been of interest for
global soil mapping projects since the first development of the global
classification systems. As a matter of interest, the term \emph{``World
soil map''} has been used exclusively for cartographic presentation of
the global distribution of KST soil orders (12) and/or FAO WRB soil
groups (32).

\begin{rmdnote}
USDA's Soil Taxonomy is probably the most developed soil classification
system in the world. Using it for producing soil data is highly
recommended also because all documents, databases and guidelines are
publicly available.
\end{rmdnote}

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_worldmap_suborders} 

}

\caption{The USDA-NRCS map of the Keys to Soil Taxonomy soil suborders of the world at 20 km. The map shows the distribution of 12 soil orders. The original map contains assumed distributions also for suborders e.g. Histels, Udolls, Calcids, and similar. Projected in the Robinson projection commonly used to display world maps.}\label{fig:worldmap-suborders}
\end{figure}




\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_barplot_suborders} 

}

\caption{Distribution of the USDA suborders shown in Fig.
\ref{fig:worldmap-suborders}.}\label{fig:barplot-suborders}
\end{figure}

Soil types can be mapped from field observations using statistically
robust methods such as multinomial logistic regression as implemented in
the nnet package for R \citep{Venables2002Springer}. Theoretically,
given a sufficient number and an adequate spatial distribution of field
observed classes, multinomial logistic regression could even be used to
map soil taxa at lower taxonomic levels with hundreds of unique
taxonomic entities.

\begin{figure}[t]

{\centering \includegraphics[width=0.6\linewidth]{figures/Fig_USDA_categories} 

}

\caption{USDA classification system and approximate minimum required number of observations to fit a global multinomial regression model.}\label{fig:usda-categories}
\end{figure}

The map in Fig. \ref{fig:worldmap-suborders} shows the global
distribution of Soil Taxonomy soil suborders according to
\href{https://www.nrcs.usda.gov/wps/portal/nrcs/detail/soils/use/worldsoils/?cid=nrcs142p2_054010}{USDA-NRCS
World Soil Index}. Assuming a rule of thumb that we need at least 5 and,
if possible, 10 observations of a specific soil taxonomic entity per
unique combination of predictor variables and observations,
\citep{harrell2001regression}, it is possible to estimate that the
optimum number of field observations required to e.g.~predict the global
distribution of USDA soil series would be in the order of few millions
of soil profiles (Fig. \ref{fig:usda-categories}).

\hypertarget{other-factor-type-variables}{%
\subsection{Other factor-type
variables}\label{other-factor-type-variables}}

Pedometric / geostatistical methods can be used not only to predict the
spatial distribution of soil types but also any other categorical soil
variables. There are many soil categorical variables for which maps
would be extremely useful for soil management and modelling. We list
here some of the most well known / most widely used soil categorical
variables:

\begin{itemize}
\item
  \emph{Diagnostic soil horizons} --- Diagnostic soil horizons
  (e.g.~Mollic or Gypsic horizon in the WRB system) are soil layers with
  specific soil properties commonly developed as a result of soil
  formation processes. They are much easier to detect in the field than
  soil types but are rarely mapped over entire areas. Diagnostic soil
  horizons can theoretically be mapped as 3D soil polygons or
  probabilities (rasters) attached to specific depths.
\item
  \emph{Soil material classes} --- Soil horizons or whole profiles can
  be dominated by minerals or their combinations e.g.~organic material
  in the soil, calcaric material, tephric material etc.
\item
  \emph{Munsell colour classes} --- Soil in dry and/or wet condition can
  be described using some 1--2 thousand Munsell colour classes. Each
  Munsell colour class carries a lot of information about the soil
  \citep{fernandez1988color}, so that a map of Munsell soil colour
  classes could be very useful for soil management.
\item
  \emph{Soil management zones} --- Each unique combination of soil
  properties or types and management zones can be further expanded into
  a mixed classification system.
\item
  \emph{Land degradation classes} --- Land degradation classes contain
  information about soil, but also about land cover and land use.
\end{itemize}

As with any map, categorical, factor-type soil variables can be mapped
globally (together with the uncertainty) as long as there is sufficient
training field data to properly support application of the prediction
algorithm. The other technical problem is the amount of storage required
to save and share all the produced predictions. Each category of a soil
categorical variable can be mapped separately, which can lead to
hundreds of grids. The global land cover map for example contains only
some 35 categories, so that it is relatively easy to distribute and use
that GIS layer.

\hypertarget{importing-and-formatting-soil-data-in-r}{%
\section{Importing and formatting soil data in
R}\label{importing-and-formatting-soil-data-in-r}}

\hypertarget{converting-texture-by-hand-classes-to-fractions}{%
\subsection{Converting texture-by-hand classes to
fractions}\label{converting-texture-by-hand-classes-to-fractions}}

In the following example we look at how to convert texture-by-hand
estimated classes to texture fractions i.e.~sand, silt and clay content
in \%. We focus on the USDA texture-by-hand classes. There classes are
embedded in the
\href{http://cran.r-project.org/web/packages/soiltexture/}{soiltexture
package} kindly contributed by Julien Moeys. The USDA texture triangle
can be accessed by:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(soiltexture)}
\KeywordTok{TT.plot}\NormalTok{(}\DataTypeTok{class.sys =} \StringTok{"USDA.TT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{Soil_variables_files/figure-latex/plot-tt-triangle-1} 

}

\caption{Soil texture triangle based on the USDA system. Generated using the [soiltexture package](http://cran.r-project.org/web/packages/soiltexture/) in R.}\label{fig:plot-tt-triangle}
\end{figure}

We can also print out a table with all class names and vertices numbers
that defines each class:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{TT.classes.tbl}\NormalTok{(}\DataTypeTok{class.sys=}\StringTok{"USDA.TT"}\NormalTok{, }\DataTypeTok{collapse=}\StringTok{", "}\NormalTok{)}
\CommentTok{#>       abbr     name              points                          }
\CommentTok{#>  [1,] "Cl"     "clay"            "24, 1, 5, 6, 2"                }
\CommentTok{#>  [2,] "SiCl"   "silty clay"      "2, 6, 7"                       }
\CommentTok{#>  [3,] "SaCl"   "sandy clay"      "1, 3, 4, 5"                    }
\CommentTok{#>  [4,] "ClLo"   "clay loam"       "5, 4, 10, 11, 12, 6"           }
\CommentTok{#>  [5,] "SiClLo" "silty clay loam" "6, 12, 13, 7"                  }
\CommentTok{#>  [6,] "SaClLo" "sandy clay loam" "3, 8, 9, 10, 4"                }
\CommentTok{#>  [7,] "Lo"     "loam"            "10, 9, 16, 17, 11"             }
\CommentTok{#>  [8,] "SiLo"   "silty loam"      "11, 17, 22, 23, 18, 19, 13, 12"}
\CommentTok{#>  [9,] "SaLo"   "sandy loam"      "8, 14, 21, 22, 17, 16, 9"      }
\CommentTok{#> [10,] "Si"     "silt"            "18, 23, 26, 19"                }
\CommentTok{#> [11,] "LoSa"   "loamy sand"      "14, 15, 20, 21"                }
\CommentTok{#> [12,] "Sa"     "sand"            "15, 25, 20"}
\end{Highlighting}
\end{Shaded}

So knowing that the soil texture classes are defined geometrically, a
logical estimate of the texture fractions from a class is to take the
geometric centre of each polygon in the texture triangle. To estimate
where the geometric centre is, we can for example use the functionality
in the sp package. We start by creating a `'SpatialPolygons'' object,
for which we have to calculate coordinates in the xy space and bind
polygons one by one:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vert <-}\StringTok{ }\KeywordTok{TT.vertices.tbl}\NormalTok{(}\DataTypeTok{class.sys =} \StringTok{"USDA.TT"}\NormalTok{)}
\NormalTok{vert}\OperatorTok{$}\NormalTok{x <-}\StringTok{ }\DecValTok{1}\OperatorTok{-}\NormalTok{vert}\OperatorTok{$}\NormalTok{SAND}\OperatorTok{+}\NormalTok{(vert}\OperatorTok{$}\NormalTok{SAND}\OperatorTok{-}\NormalTok{(}\DecValTok{1}\OperatorTok{-}\NormalTok{vert}\OperatorTok{$}\NormalTok{SILT))}\OperatorTok{*}\FloatTok{0.5}
\NormalTok{vert}\OperatorTok{$}\NormalTok{y <-}\StringTok{ }\NormalTok{vert}\OperatorTok{$}\NormalTok{CLAY}\OperatorTok{*}\KeywordTok{sin}\NormalTok{(pi}\OperatorTok{/}\DecValTok{3}\NormalTok{)}
\NormalTok{USDA.TT <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{TT.classes.tbl}\NormalTok{(}\DataTypeTok{class.sys =} \StringTok{"USDA.TT"}\NormalTok{, }\DataTypeTok{collapse =} \StringTok{", "}\NormalTok{))}
\NormalTok{TT.pnt <-}\StringTok{ }\KeywordTok{as.list}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{length}\NormalTok{(USDA.TT}\OperatorTok{$}\NormalTok{name)))}
\NormalTok{poly.lst <-}\StringTok{ }\KeywordTok{as.list}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{length}\NormalTok{(USDA.TT}\OperatorTok{$}\NormalTok{name)))}
\end{Highlighting}
\end{Shaded}

next we strip the vertices and create a list of polygons:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sp)}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(USDA.TT}\OperatorTok{$}\NormalTok{name))\{}
\NormalTok{  TT.pnt[[i]] <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{strsplit}\NormalTok{(}\KeywordTok{unclass}\NormalTok{(}\KeywordTok{paste}\NormalTok{(USDA.TT[i, }\StringTok{"points"}\NormalTok{])), }\StringTok{", "}\NormalTok{)[[}\DecValTok{1}\NormalTok{]])}
\NormalTok{  poly.lst[[i]] <-}\StringTok{ }\NormalTok{vert[TT.pnt[[i]],}\KeywordTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{,}\StringTok{"y"}\NormalTok{)]}
\NormalTok{  ## add extra point:}
\NormalTok{  pp <-}\StringTok{ }\KeywordTok{Polygon}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(poly.lst[[i]], poly.lst[[i]][}\DecValTok{1}\NormalTok{,]))}
\NormalTok{  poly.lst[[i]] <-}\StringTok{ }\NormalTok{sp}\OperatorTok{::}\KeywordTok{Polygons}\NormalTok{(}\KeywordTok{list}\NormalTok{(pp), }\DataTypeTok{ID=}\NormalTok{i)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

and convert texture triangle to a spatial object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poly.sp <-}\StringTok{ }\KeywordTok{SpatialPolygons}\NormalTok{(poly.lst, }\DataTypeTok{proj4string=}\KeywordTok{CRS}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(}\OtherTok{NA}\NormalTok{)))}
\NormalTok{poly.USDA.TT <-}\StringTok{ }\KeywordTok{SpatialPolygonsDataFrame}\NormalTok{(poly.sp, }
                      \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{ID=}\NormalTok{USDA.TT}\OperatorTok{$}\NormalTok{name), }\DataTypeTok{match.ID=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The resulting object now contains also slots of type `'labpt'' which is
exactly the geometric gravity point of the first polygon automatically
derived by the `'SpatialPolygons'' function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{slot}\NormalTok{(}\KeywordTok{slot}\NormalTok{(poly.USDA.TT, }\StringTok{"polygons"}\NormalTok{)[[}\DecValTok{1}\NormalTok{]], }\StringTok{"labpt"}\NormalTok{)}
\CommentTok{#> [1] 0.490 0.545}
\end{Highlighting}
\end{Shaded}

Next we need to create a function that converts the xy coordinates
(columns) in a texture triangle to texture fraction values. Let's call
this function \texttt{get.TF.from.XY}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get.TF.from.XY <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(df, xcoord, ycoord) \{}
\NormalTok{  df}\OperatorTok{$}\NormalTok{CLAY <-}\StringTok{ }\NormalTok{df[,ycoord]}\OperatorTok{/}\KeywordTok{sin}\NormalTok{(pi}\OperatorTok{/}\DecValTok{3}\NormalTok{)}
\NormalTok{  df}\OperatorTok{$}\NormalTok{SAND <-}\StringTok{ }\NormalTok{(}\DecValTok{2} \OperatorTok{-}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{CLAY }\OperatorTok{-}\StringTok{ }\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{df[,xcoord]) }\OperatorTok{*}\StringTok{ }\FloatTok{0.5}
\NormalTok{  df}\OperatorTok{$}\NormalTok{SILT <-}\StringTok{ }\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{(df}\OperatorTok{$}\NormalTok{SAND }\OperatorTok{+}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{CLAY)}
  \KeywordTok{return}\NormalTok{(df)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

and now everything is ready to estimate the soil fractions based on a
system of classes. For the case of the USDA classifications system we
get:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{USDA.TT.cnt <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{t}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(}\KeywordTok{slot}\NormalTok{(poly.USDA.TT, }\StringTok{"polygons"}\NormalTok{), slot, }\StringTok{"labpt"}\NormalTok{)))}
\NormalTok{USDA.TT.cnt}\OperatorTok{$}\NormalTok{name <-}\StringTok{ }\NormalTok{poly.USDA.TT}\OperatorTok{$}\NormalTok{ID}
\NormalTok{USDA.TT.cnt <-}\StringTok{ }\KeywordTok{get.TF.from.XY}\NormalTok{(USDA.TT.cnt, }\StringTok{"X1"}\NormalTok{, }\StringTok{"X2"}\NormalTok{)}
\NormalTok{USDA.TT.cnt[,}\KeywordTok{c}\NormalTok{(}\StringTok{"SAND"}\NormalTok{,}\StringTok{"SILT"}\NormalTok{,}\StringTok{"CLAY"}\NormalTok{)] <-}\StringTok{ }\KeywordTok{signif}\NormalTok{(USDA.TT.cnt[,}\KeywordTok{c}\NormalTok{(}\StringTok{"SAND"}\NormalTok{,}\StringTok{"SILT"}\NormalTok{,}\StringTok{"CLAY"}\NormalTok{)], }\DecValTok{2}\NormalTok{)}
\NormalTok{USDA.TT.cnt[,}\KeywordTok{c}\NormalTok{(}\StringTok{"name"}\NormalTok{,}\StringTok{"SAND"}\NormalTok{,}\StringTok{"SILT"}\NormalTok{,}\StringTok{"CLAY"}\NormalTok{)]}
\CommentTok{#>               name  SAND  SILT  CLAY}
\CommentTok{#> 1             clay 0.200 0.180 0.630}
\CommentTok{#> 2       silty clay 0.067 0.470 0.470}
\CommentTok{#> 3       sandy clay 0.520 0.067 0.420}
\CommentTok{#> 4        clay loam 0.320 0.340 0.340}
\CommentTok{#> 5  silty clay loam 0.100 0.560 0.340}
\CommentTok{#> 6  sandy clay loam 0.600 0.130 0.270}
\CommentTok{#> 7             loam 0.410 0.400 0.190}
\CommentTok{#> 8       silty loam 0.210 0.650 0.130}
\CommentTok{#> 9       sandy loam 0.650 0.250 0.100}
\CommentTok{#> 10            silt 0.073 0.870 0.053}
\CommentTok{#> 11      loamy sand 0.820 0.120 0.058}
\CommentTok{#> 12            sand 0.920 0.050 0.033}
\end{Highlighting}
\end{Shaded}

Now that we have created a function that converts values in the texture
triangle to texture fractions, we can go further and even estimate the
uncertainty of estimating each texture fraction based on the class. For
this we can use simulations i.e.~randomly sample 100 points within some
texture class and then derive standard deviations for each texture
fraction. Note that, although this sounds like a complicated operation,
we can run this in two lines of code. For example to estimate
uncertainty of converting the class `'Cl'' (clay) to texture fractions
we can simulate 100 random points the class polygon using the
`'spsample'' function from the sp package \citep{Bivand2013Springer}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.Cl <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{spsample}\NormalTok{(poly.USDA.TT[poly.USDA.TT}\OperatorTok{$}\NormalTok{ID}\OperatorTok{==}\StringTok{"clay"}\NormalTok{,], }
                              \DataTypeTok{type=}\StringTok{"random"}\NormalTok{, }\DataTypeTok{n=}\DecValTok{100}\NormalTok{))}
\NormalTok{sim.Cl <-}\StringTok{ }\KeywordTok{get.TF.from.XY}\NormalTok{(sim.Cl, }\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{)}
\KeywordTok{sd}\NormalTok{(sim.Cl}\OperatorTok{$}\NormalTok{SAND); }\KeywordTok{sd}\NormalTok{(sim.Cl}\OperatorTok{$}\NormalTok{SILT); }\KeywordTok{sd}\NormalTok{(sim.Cl}\OperatorTok{$}\NormalTok{CLAY)}
\CommentTok{#> [1] 0.123}
\CommentTok{#> [1] 0.113}
\CommentTok{#> [1] 0.142}
\end{Highlighting}
\end{Shaded}

which means that we should not expect better precision of estimating the
clay content based on class \texttt{Cl} than ±15\%.

For some real soil profile data set we could also plot all texture
fractions in the texture triangle to see how frequently one should
expect some soil classes to appear:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(GSIF)}
\KeywordTok{data}\NormalTok{(afsp)}
\NormalTok{tdf <-}\StringTok{ }\NormalTok{afsp}\OperatorTok{$}\NormalTok{horizons[,}\KeywordTok{c}\NormalTok{(}\StringTok{"CLYPPT"}\NormalTok{, }\StringTok{"SLTPPT"}\NormalTok{, }\StringTok{"SNDPPT"}\NormalTok{)]}
\NormalTok{tdf <-}\StringTok{ }\NormalTok{tdf[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(tdf}\OperatorTok{$}\NormalTok{SNDPPT)}\OperatorTok{&!}\KeywordTok{is.na}\NormalTok{(tdf}\OperatorTok{$}\NormalTok{SLTPPT)}\OperatorTok{&!}\KeywordTok{is.na}\NormalTok{(tdf}\OperatorTok{$}\NormalTok{CLYPPT),]}
\NormalTok{tdf <-}\StringTok{ }\NormalTok{tdf[}\KeywordTok{runif}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(tdf))}\OperatorTok{<}\NormalTok{.}\DecValTok{15}\NormalTok{,]}
\NormalTok{tdf}\OperatorTok{$}\NormalTok{Sum <-}\StringTok{ }\KeywordTok{rowSums}\NormalTok{(tdf)}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \KeywordTok{c}\NormalTok{(}\StringTok{"CLYPPT"}\NormalTok{, }\StringTok{"SLTPPT"}\NormalTok{, }\StringTok{"SNDPPT"}\NormalTok{)) \{ tdf[,i] <-}\StringTok{ }\NormalTok{tdf[,i]}\OperatorTok{/}\NormalTok{tdf}\OperatorTok{$}\NormalTok{Sum }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{ \}}
\KeywordTok{names}\NormalTok{(tdf)[}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{] <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"CLAY"}\NormalTok{, }\StringTok{"SILT"}\NormalTok{, }\StringTok{"SAND"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{TT.plot}\NormalTok{(}\DataTypeTok{class.sys =} \StringTok{"USDA.TT"}\NormalTok{, }\DataTypeTok{tri.data =}\NormalTok{ tdf, }
        \DataTypeTok{grid.show =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{pch=}\StringTok{"+"}\NormalTok{, }\DataTypeTok{cex=}\NormalTok{.}\DecValTok{4}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{Soil_variables_files/figure-latex/plot-tt-afsis-1} 

}

\caption{Distribution of observed soil textures for the [Africa Soil Profiles](http://gsif.r-forge.r-project.org/afsp.html).}\label{fig:plot-tt-afsis}
\end{figure}

This shows that not all positions in the triangle have the same prior
probability. So probably a more sensitive way to estimate uncertainty of
converting soil texture classes to fractions would be to run simulations
using a density image showing the actual distribution of classes and
then, by using the \texttt{rpoint} function in the
\href{http://spatstat.org}{spatstat package}, we could also derive even
more realistic conversions from texture-by-hand classes to texture
fractions.

\hypertarget{converting-munsell-color-codes-to-other-color-systems}{%
\section{Converting Munsell color codes to other color
systems}\label{converting-munsell-color-codes-to-other-color-systems}}

In the next example we look at the Munsell color codes and conversion
algorithms from a code to RGB and other color spaces. Munsell color
codes can be matched with RGB values via the
\href{http://www.cis.rit.edu/mcsl/online/munsell.php}{Munsell color
codes conversion table}. You can load a table with 2350 entries from the
GSIF dokuwiki:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{load}\NormalTok{(}\StringTok{"extdata/munsell_rgb.rdata"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(colorspace)}
\NormalTok{munsell.rgb[}\KeywordTok{round}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\DecValTok{2350}\NormalTok{, }\DecValTok{0}\NormalTok{),]}
\CommentTok{#>       Munsell  R  G  B}
\CommentTok{#> 2254 7.5Y_1_2 37 34 17}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as}\NormalTok{(colorspace}\OperatorTok{::}\KeywordTok{RGB}\NormalTok{(}\DataTypeTok{R=}\NormalTok{munsell.rgb[}\DecValTok{1007}\NormalTok{,}\StringTok{"R"}\NormalTok{]}\OperatorTok{/}\DecValTok{255}\NormalTok{, }
                   \DataTypeTok{G=}\NormalTok{munsell.rgb[}\DecValTok{1007}\NormalTok{,}\StringTok{"G"}\NormalTok{]}\OperatorTok{/}\DecValTok{255}\NormalTok{, }
                   \DataTypeTok{B=}\NormalTok{munsell.rgb[}\DecValTok{1007}\NormalTok{,}\StringTok{"B"}\NormalTok{]}\OperatorTok{/}\DecValTok{255}\NormalTok{), }\StringTok{"HSV"}\NormalTok{)}
\CommentTok{#>         H      S     V}
\CommentTok{#> [1,] 3.53 0.0798 0.835}
\end{Highlighting}
\end{Shaded}

This shows that, for any given Munsell color code, it is relatively easy
to convert them to any other color system available in R.

Within the R
\href{http://casoilresource.lawr.ucdavis.edu/drupal/node/201}{package
aqp} one can directly transform Munsell color codes to some color
classes in R \citep{Beaudette2013CompGeo}. For example, to convert the
Munsell color code to RGB values from the example above we would run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aqp}\OperatorTok{::}\KeywordTok{munsell2rgb}\NormalTok{(}\DataTypeTok{the_hue =} \StringTok{"10B"}\NormalTok{, }\DataTypeTok{the_value =} \DecValTok{2}\NormalTok{, }\DataTypeTok{the_chroma =} \DecValTok{12}\NormalTok{)}
\CommentTok{#> [1] "#003A7CFF"}
\end{Highlighting}
\end{Shaded}

Now the colors are coded in the
\href{http://en.wikipedia.org/wiki/Web_colors\#Hex_triplet}{hexadecimal
format}, which is quite abstract but can be easily browsed via some web
color table. To get the actual RGB values we would run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grDevices}\OperatorTok{::}\KeywordTok{col2rgb}\NormalTok{(}\StringTok{"#003A7CFF"}\NormalTok{)}
\CommentTok{#>       [,1]}
\CommentTok{#> red      0}
\CommentTok{#> green   58}
\CommentTok{#> blue   124}
\end{Highlighting}
\end{Shaded}

The hex triplet format is also very similar to the color format used in
the
\href{https://developers.google.com/kml/documentation/kmlreference}{KML
reference}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plotKML}\OperatorTok{::}\KeywordTok{col2kml}\NormalTok{(}\StringTok{"#003A7CFF"}\NormalTok{)}
\CommentTok{#> [1] "#ff7c3a00"}
\end{Highlighting}
\end{Shaded}

To plot the actual colors based on an actual soil profile data base we
often need to prepare the color codes before we can run the conversion
\citep{VISCARRAROSSEL2006320}. In the case of the
\href{http://gsif.r-forge.r-project.org/afsp.html}{Africa Soil Profile
Database}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(afsp)}
\KeywordTok{head}\NormalTok{(afsp}\OperatorTok{$}\NormalTok{horizons[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(afsp}\OperatorTok{$}\NormalTok{horizons}\OperatorTok{$}\NormalTok{MCOMNS),}\StringTok{"MCOMNS"}\NormalTok{])}
\CommentTok{#> [1] 10YR3/3 10YR3/3 10YR3/3 10YR3/3 10YR3/3 10YR3/3}
\CommentTok{#> 289 Levels: 10BG4/1 10R2.5/1 10R2/1 10R2/2 10R3/2 10R3/3 10R3/4 ... N7/0}
\end{Highlighting}
\end{Shaded}

the Munsell color codes have been prepared as text. Hence we need to
spend some effort to separate hue from saturation and intensity before
we can derive and plot actual colors. We start by merging the tables of
interest so both coordinates and Munsell color codes are available in
the same table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcol <-}\StringTok{ }\NormalTok{plyr}\OperatorTok{::}\KeywordTok{join}\NormalTok{(afsp}\OperatorTok{$}\NormalTok{horizons[,}\KeywordTok{c}\NormalTok{(}\StringTok{"SOURCEID"}\NormalTok{,}\StringTok{"MCOMNS"}\NormalTok{,}\StringTok{"UHDICM"}\NormalTok{,}\StringTok{"LHDICM"}\NormalTok{)],}
\NormalTok{                   afsp}\OperatorTok{$}\NormalTok{sites[,}\KeywordTok{c}\NormalTok{(}\StringTok{"SOURCEID"}\NormalTok{,}\StringTok{"LONWGS84"}\NormalTok{,}\StringTok{"LATWGS84"}\NormalTok{)])}
\CommentTok{#> Joining by: SOURCEID}
\NormalTok{mcol <-}\StringTok{ }\NormalTok{mcol[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(mcol}\OperatorTok{$}\NormalTok{MCOMNS),]}
\KeywordTok{str}\NormalTok{(mcol)}
\CommentTok{#> 'data.frame':    31502 obs. of  6 variables:}
\CommentTok{#>  $ SOURCEID: Factor w/ 26270 levels "100902","100903",..: 974 974 974 974 974 974 975 975 975 975 ...}
\CommentTok{#>  $ MCOMNS  : Factor w/ 289 levels "10BG4/1","10R2.5/1",..: 40 40 40 40 40 40 23 23 23 23 ...}
\CommentTok{#>  $ UHDICM  : num  0 8 25 50 81 133 0 8 19 30 ...}
\CommentTok{#>  $ LHDICM  : num  8 25 50 81 133 160 8 19 30 50 ...}
\CommentTok{#>  $ LONWGS84: num  17.6 17.6 17.6 17.6 17.6 ...}
\CommentTok{#>  $ LATWGS84: num  -11 -11 -11 -11 -11 ...}
\end{Highlighting}
\end{Shaded}

Next we need to format all Munsell color codes to
`'Hue\_Saturation\_Intensity'' format. We can incrementally replace the
existing codes until all codes can be matched with the RGB table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcol}\OperatorTok{$}\NormalTok{Munsell <-}\StringTok{ }\KeywordTok{sub}\NormalTok{(}\StringTok{" "}\NormalTok{, }\StringTok{""}\NormalTok{, }\KeywordTok{sub}\NormalTok{(}\StringTok{"/"}\NormalTok{, }\StringTok{"_"}\NormalTok{, mcol}\OperatorTok{$}\NormalTok{MCOMNS))}
\NormalTok{hue.lst <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"2.5"}\NormalTok{, }\StringTok{"5"}\NormalTok{, }\StringTok{"7.5"}\NormalTok{, }\StringTok{"10"}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\StringTok{"YR"}\NormalTok{,}\StringTok{"GY"}\NormalTok{,}\StringTok{"BG"}\NormalTok{,}\StringTok{"YE"}\NormalTok{,}\StringTok{"YN"}\NormalTok{,}\StringTok{"YY"}\NormalTok{,}\StringTok{"R"}\NormalTok{,}\StringTok{"Y"}\NormalTok{,}\StringTok{"B"}\NormalTok{,}\StringTok{"G"}\NormalTok{))}
\NormalTok{hue.lst}\OperatorTok{$}\NormalTok{mhue <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(hue.lst}\OperatorTok{$}\NormalTok{Var1, hue.lst}\OperatorTok{$}\NormalTok{Var2, }\DataTypeTok{sep=}\StringTok{""}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in}\NormalTok{ hue.lst}\OperatorTok{$}\NormalTok{mhue[}\DecValTok{1}\OperatorTok{:}\DecValTok{28}\NormalTok{])\{}
\NormalTok{  mcol}\OperatorTok{$}\NormalTok{Munsell <-}\StringTok{ }\KeywordTok{sub}\NormalTok{(j, }\KeywordTok{paste}\NormalTok{(j, }\StringTok{"_"}\NormalTok{, }\DataTypeTok{sep=}\StringTok{""}\NormalTok{), mcol}\OperatorTok{$}\NormalTok{Munsell, }\DataTypeTok{fixed=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{\}}
\NormalTok{mcol}\OperatorTok{$}\NormalTok{depth <-}\StringTok{ }\NormalTok{mcol}\OperatorTok{$}\NormalTok{UHDICM }\OperatorTok{+}\StringTok{ }\NormalTok{(mcol}\OperatorTok{$}\NormalTok{LHDICM}\OperatorTok{-}\NormalTok{mcol}\OperatorTok{$}\NormalTok{UHDICM)}\OperatorTok{/}\DecValTok{2}
\NormalTok{mcol.RGB <-}\StringTok{ }\KeywordTok{merge}\NormalTok{(mcol, munsell.rgb, }\DataTypeTok{by=}\StringTok{"Munsell"}\NormalTok{)}
\KeywordTok{str}\NormalTok{(mcol.RGB)}
\CommentTok{#> 'data.frame':    11806 obs. of  11 variables:}
\CommentTok{#>  $ Munsell : chr  "10R_2_2" "10R_2_2" "10R_2_2" "10R_2_2" ...}
\CommentTok{#>  $ SOURCEID: Factor w/ 26270 levels "100902","100903",..: 18724 18724 20331 18724 20331 20331 18724 9089 4859 23688 ...}
\CommentTok{#>  $ MCOMNS  : Factor w/ 289 levels "10BG4/1","10R2.5/1",..: 4 4 4 4 4 4 4 5 5 5 ...}
\CommentTok{#>  $ UHDICM  : num  90 35 30 10 53 0 0 18 0 0 ...}
\CommentTok{#>  $ LHDICM  : num  135 90 53 35 98 30 10 24 15 5 ...}
\CommentTok{#>  $ LONWGS84: num  32.23 32.23 4.76 32.23 4.76 ...}
\CommentTok{#>  $ LATWGS84: num  -26.15 -26.15 8.79 -26.15 8.79 ...}
\CommentTok{#>  $ depth   : num  112.5 62.5 41.5 22.5 75.5 ...}
\CommentTok{#>  $ R       : int  67 67 67 67 67 67 67 91 91 91 ...}
\CommentTok{#>  $ G       : int  48 48 48 48 48 48 48 68 68 68 ...}
\CommentTok{#>  $ B       : int  45 45 45 45 45 45 45 63 63 63 ...}
\end{Highlighting}
\end{Shaded}

Which allows us to plot the actual observed colors of the top soil
(0--30 cm) for the whole of Africa:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcol.RGB <-}\StringTok{ }\NormalTok{mcol.RGB[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(mcol.RGB}\OperatorTok{$}\NormalTok{R),]}
\NormalTok{mcol.RGB}\OperatorTok{$}\NormalTok{Rc <-}\StringTok{ }\KeywordTok{round}\NormalTok{(mcol.RGB}\OperatorTok{$}\NormalTok{R}\OperatorTok{/}\DecValTok{255}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{mcol.RGB}\OperatorTok{$}\NormalTok{Gc <-}\StringTok{ }\KeywordTok{round}\NormalTok{(mcol.RGB}\OperatorTok{$}\NormalTok{G}\OperatorTok{/}\DecValTok{255}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{mcol.RGB}\OperatorTok{$}\NormalTok{Bc <-}\StringTok{ }\KeywordTok{round}\NormalTok{(mcol.RGB}\OperatorTok{$}\NormalTok{B}\OperatorTok{/}\DecValTok{255}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{mcol.RGB}\OperatorTok{$}\NormalTok{col <-}\StringTok{ }\KeywordTok{rgb}\NormalTok{(mcol.RGB}\OperatorTok{$}\NormalTok{Rc, mcol.RGB}\OperatorTok{$}\NormalTok{Gc, mcol.RGB}\OperatorTok{$}\NormalTok{Bc)}
\NormalTok{mcol.RGB <-}\StringTok{ }\NormalTok{mcol.RGB[mcol.RGB}\OperatorTok{$}\NormalTok{depth}\OperatorTok{>}\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{mcol.RGB}\OperatorTok{$}\NormalTok{depth}\OperatorTok{<}\DecValTok{30} \OperatorTok{&}\StringTok{ }\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(mcol.RGB}\OperatorTok{$}\NormalTok{col),]}
\KeywordTok{coordinates}\NormalTok{(mcol.RGB) <-}\StringTok{ }\ErrorTok{~}\StringTok{ }\NormalTok{LONWGS84}\OperatorTok{+}\NormalTok{LATWGS84}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{load}\NormalTok{(}\StringTok{"extdata/admin.af.rda"}\NormalTok{)}
\KeywordTok{proj4string}\NormalTok{(admin.af) <-}\StringTok{ "+proj=longlat +datum=WGS84"}
\CommentTok{#> Warning in ReplProj4string(obj, CRS(value)): A new CRS was assigned to an object with an existing CRS:}
\CommentTok{#> +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0}
\CommentTok{#> without reprojecting.}
\CommentTok{#> For reprojection, use function spTransform}
\NormalTok{country <-}\StringTok{ }\KeywordTok{as}\NormalTok{(admin.af, }\StringTok{"SpatialLines"}\NormalTok{)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar=}\KeywordTok{c}\NormalTok{(.}\DecValTok{0}\NormalTok{,.}\DecValTok{0}\NormalTok{,.}\DecValTok{0}\NormalTok{,.}\DecValTok{0}\NormalTok{), }\DataTypeTok{mai=}\KeywordTok{c}\NormalTok{(.}\DecValTok{0}\NormalTok{,.}\DecValTok{0}\NormalTok{,.}\DecValTok{0}\NormalTok{,.}\DecValTok{0}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(country, }\DataTypeTok{col=}\StringTok{"darkgrey"}\NormalTok{, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{)}
\KeywordTok{points}\NormalTok{(mcol.RGB, }\DataTypeTok{pch=}\DecValTok{21}\NormalTok{, }\DataTypeTok{bg=}\NormalTok{mcol.RGB}\OperatorTok{$}\NormalTok{col, }\DataTypeTok{col=}\StringTok{"black"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{Soil_variables_files/figure-latex/plot-af-soil-cols-1} 

}

\caption{Actual observed soil colors (moist) for the top soil based on the [Africa Soil Profiles Database](http://gsif.r-forge.r-project.org/afsp.html).}\label{fig:plot-af-soil-cols}
\end{figure}

Finally, via the plotKML package you can also plot the actual colors of
horizons by converting tables to SoilProfileCollection class in the
\href{http://cran.r-project.org/web/packages/aqp/}{aqp package}.
Consider this soil profile from Nigeria:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(plyr)}
\KeywordTok{library}\NormalTok{(aqp)}
\CommentTok{#> This is aqp 1.16-3}
\NormalTok{lon =}\StringTok{ }\FloatTok{3.90}\NormalTok{; lat =}\StringTok{ }\FloatTok{7.50}\NormalTok{; id =}\StringTok{ "ISRIC:NG0017"}\NormalTok{; FAO1988 =}\StringTok{ "LXp"}
\NormalTok{top =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{65}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{127}\NormalTok{)}
\NormalTok{bottom =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{18}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{65}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{127}\NormalTok{, }\DecValTok{181}\NormalTok{)}
\NormalTok{ORCDRC =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{18.4}\NormalTok{, }\FloatTok{4.4}\NormalTok{, }\FloatTok{3.6}\NormalTok{, }\FloatTok{3.6}\NormalTok{, }\FloatTok{3.2}\NormalTok{, }\FloatTok{1.2}\NormalTok{)}
\NormalTok{hue =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"7.5YR"}\NormalTok{, }\StringTok{"7.5YR"}\NormalTok{, }\StringTok{"2.5YR"}\NormalTok{, }\StringTok{"5YR"}\NormalTok{, }\StringTok{"5YR"}\NormalTok{, }\StringTok{"10YR"}\NormalTok{)}
\NormalTok{value =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{); chroma =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{## prepare a SoilProfileCollection:}
\NormalTok{prof1 <-}\StringTok{ }\NormalTok{plyr}\OperatorTok{::}\KeywordTok{join}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(id, top, bottom, ORCDRC, hue, value, chroma),  }
              \KeywordTok{data.frame}\NormalTok{(id, lon, lat, FAO1988), }\DataTypeTok{type=}\StringTok{'inner'}\NormalTok{)}
\CommentTok{#> Joining by: id}
\NormalTok{prof1}\OperatorTok{$}\NormalTok{soil_color <-}\StringTok{ }\KeywordTok{with}\NormalTok{(prof1, aqp}\OperatorTok{::}\KeywordTok{munsell2rgb}\NormalTok{(hue, value, chroma))}
\CommentTok{#> Notice: converting hue to character}
\KeywordTok{depths}\NormalTok{(prof1) <-}\StringTok{ }\NormalTok{id }\OperatorTok{~}\StringTok{ }\NormalTok{top }\OperatorTok{+}\StringTok{ }\NormalTok{bottom}
\CommentTok{#> Warning: converting IDs from factor to character}
\KeywordTok{site}\NormalTok{(prof1) <-}\StringTok{ }\ErrorTok{~}\StringTok{ }\NormalTok{lon }\OperatorTok{+}\StringTok{ }\NormalTok{lat }\OperatorTok{+}\StringTok{ }\NormalTok{FAO1988}
\KeywordTok{coordinates}\NormalTok{(prof1) <-}\StringTok{ }\ErrorTok{~}\StringTok{ }\NormalTok{lon }\OperatorTok{+}\StringTok{ }\NormalTok{lat}
\KeywordTok{proj4string}\NormalTok{(prof1) <-}\StringTok{ }\KeywordTok{CRS}\NormalTok{(}\StringTok{"+proj=longlat +datum=WGS84"}\NormalTok{)}
\NormalTok{prof1}
\CommentTok{#> Object of class SoilProfileCollection}
\CommentTok{#> Number of profiles: 1}
\CommentTok{#> }
\CommentTok{#> Horizon attributes:}
\CommentTok{#>             id top bottom ORCDRC   hue value chroma soil_color}
\CommentTok{#> 1 ISRIC:NG0017   0     18   18.4 7.5YR     3      2  #584537FF}
\CommentTok{#> 2 ISRIC:NG0017  18     36    4.4 7.5YR     4      4  #7E5A3BFF}
\CommentTok{#> 3 ISRIC:NG0017  36     65    3.6 2.5YR     5      6  #A96C4FFF}
\CommentTok{#> 4 ISRIC:NG0017  65     87    3.6   5YR     5      8  #B06A32FF}
\CommentTok{#> 5 ISRIC:NG0017  87    127    3.2   5YR     5      4  #9A7359FF}
\CommentTok{#> 6 ISRIC:NG0017 127    181    1.2  10YR     7      3  #C4AC8CFF}
\CommentTok{#> }
\CommentTok{#> Sampling site attributes:}
\CommentTok{#>             id FAO1988}
\CommentTok{#> 1 ISRIC:NG0017     LXp}
\CommentTok{#> }
\CommentTok{#> Spatial Data:}
\CommentTok{#>     min max}
\CommentTok{#> lon 3.9 3.9}
\CommentTok{#> lat 7.5 7.5}
\CommentTok{#> [1] "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"}
\end{Highlighting}
\end{Shaded}

Once an object is in the format of `'SoilProfileCollection'' it can be
directly plotted in Google Earth via the generic plotKML command:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plotKML}\NormalTok{(prof1, }\DataTypeTok{var.name=}\StringTok{"ORCDRC"}\NormalTok{, }\DataTypeTok{color.name=}\StringTok{"soil_color"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=0.7\linewidth]{figures/soil_profile_plot} 

}

\caption{Soil profile from Nigeria plotted in Google Earth with actual observed colors.}\label{fig:soil-profile-plot}
\end{figure}

\hypertarget{mla-ptfs}{%
\section{Using Machine Learning to build
Pedo-Transfer-Functions}\label{mla-ptfs}}

\hypertarget{ptf-for-bulk-density}{%
\subsection{PTF for Bulk Density}\label{ptf-for-bulk-density}}

In the following examples we look at possibilities of using
\href{wiki/soilmapping_using_mla}{Machine Learning} to predict soil
properties and classes from other soil properties and classes. In the
first example, we try to build a Pedo-Transfer-Function (PTF) to predict
bulk density using soil properties such as organic carbon content, soil
texture and coarse fragments. Bulk density is often available only for a
part of soil profiles, so if we could use a PTF to fill in all gaps in
bulk density, then most likely we would not need to omit BD from further
analysis. For testing PTFs to predict bulk density from other soil
properties we will use a subset of the ISRIC WISE soil profile data set
\citep{Batjes2009SUM}, which can be loaded from:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(randomForestSRC)}
\CommentTok{#> }
\CommentTok{#>  randomForestSRC 2.7.0 }
\CommentTok{#>  }
\CommentTok{#>  Type rfsrc.news() to see new features, changes, and bug fixes. }
\CommentTok{#> }
\KeywordTok{library}\NormalTok{(ggRandomForests)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'ggRandomForests'}
\CommentTok{#> The following object is masked from 'package:randomForestSRC':}
\CommentTok{#> }
\CommentTok{#>     partial.rfsrc}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(scales)}
\KeywordTok{load}\NormalTok{(}\StringTok{"extdata/sprops.wise.rda"}\NormalTok{)}
\KeywordTok{str}\NormalTok{(SPROPS.WISE)}
\CommentTok{#> 'data.frame':    47833 obs. of  17 variables:}
\CommentTok{#>  $ SOURCEID: Factor w/ 10253 levels "AF0001","AF0002",..: 1 1 1 2 2 2 2 3 3 3 ...}
\CommentTok{#>  $ SAMPLEID: chr  "AF0001_1" "AF0001_2" "AF0001_3" "AF0002_1" ...}
\CommentTok{#>  $ UHDICM  : int  0 15 60 0 20 60 110 0 20 50 ...}
\CommentTok{#>  $ LHDICM  : int  15 60 150 20 60 110 170 20 50 110 ...}
\CommentTok{#>  $ DEPTH   : num  7.5 37.5 105 10 40 85 140 10 35 80 ...}
\CommentTok{#>  $ CRFVOL  : int  20 NA NA NA NA NA NA NA NA NA ...}
\CommentTok{#>  $ CECSUM  : num  NA NA NA NA NA NA NA NA NA NA ...}
\CommentTok{#>  $ SNDPPT  : int  40 10 10 40 15 10 40 40 65 60 ...}
\CommentTok{#>  $ CLYPPT  : int  20 35 35 20 20 35 20 20 10 25 ...}
\CommentTok{#>  $ BLD     : num  NA NA NA NA NA NA NA NA NA NA ...}
\CommentTok{#>  $ SLTPPT  : int  40 55 55 40 65 55 40 40 25 15 ...}
\CommentTok{#>  $ PHIHOX  : num  7.9 7.9 7.9 8.5 8.6 8.5 8.8 8.8 9.2 8.9 ...}
\CommentTok{#>  $ PHIKCL  : num  NA NA NA NA NA NA NA NA NA NA ...}
\CommentTok{#>  $ ORCDRC  : num  7.6 2.3 0.9 12.8 6 3.9 2.7 5.9 2.4 NA ...}
\CommentTok{#>  $ LONWGS84: num  69.2 69.2 69.2 69.2 69.2 ...}
\CommentTok{#>  $ LATWGS84: num  34.5 34.5 34.5 34.5 34.5 34.5 34.5 34.5 34.5 34.5 ...}
\CommentTok{#>  $ SOURCEDB: chr  "WISE" "WISE" "WISE" "WISE" ...}
\end{Highlighting}
\end{Shaded}

For model fitting we will use the
\href{https://cran.r-project.org/package=randomForestSRC}{randomForestSRC}
package, which is a robust implementation of random forest algorithm
with options for parallelization and visualization of model outputs:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bd.fm =}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(}\StringTok{"BLD ~ ORCDRC + PHIHOX + SNDPPT + CLYPPT + CRFVOL + DEPTH"}\NormalTok{)}
\NormalTok{rfsrc_BD <-}\StringTok{ }\KeywordTok{rfsrc}\NormalTok{(bd.fm, }\DataTypeTok{data=}\NormalTok{SPROPS.WISE)}
\NormalTok{rfsrc_BD}
\CommentTok{#>                          Sample size: 3330}
\CommentTok{#>                      Number of trees: 1000}
\CommentTok{#>            Forest terminal node size: 5}
\CommentTok{#>        Average no. of terminal nodes: 685}
\CommentTok{#> No. of variables tried at each split: 2}
\CommentTok{#>               Total no. of variables: 6}
\CommentTok{#>        Resampling used to grow trees: swr}
\CommentTok{#>     Resample size used to grow trees: 3330}
\CommentTok{#>                             Analysis: RF-R}
\CommentTok{#>                               Family: regr}
\CommentTok{#>                       Splitting rule: mse *random*}
\CommentTok{#>        Number of random split points: 10}
\CommentTok{#>                 % variance explained: 39.6}
\CommentTok{#>                           Error rate: 46370}
\end{Highlighting}
\end{Shaded}

which shows that the model explains about 40\% with an RMSE of ±200
kg/m\(^3\). Although the MSE is relatively high, it least can be used to
fill in the missing values for BD which can be significant. We can plot
the partial plots between the target variable and all covariates by
using:

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/bulk_density_ptf_plots} 

}

\caption{Bulk density as a function of organic carbon, pH, sand and clay content coarse fragments and depth.}\label{fig:bulk-density-ptf}
\end{figure}

Obviously the key to explaining bulk density is soil organic carbon,
while depth and pH are the 2nd and 3rd most important covariates. Using
this MLA-based model we can predict bulk density for various
combinations of soil properties:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(rfsrc_BD, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{ORCDRC=}\FloatTok{1.2}\NormalTok{, }\DataTypeTok{PHIHOX=}\FloatTok{7.6}\NormalTok{, }
                  \DataTypeTok{SNDPPT=}\DecValTok{45}\NormalTok{, }\DataTypeTok{CLYPPT=}\DecValTok{12}\NormalTok{, }\DataTypeTok{CRFVOL=}\DecValTok{0}\NormalTok{, }\DataTypeTok{DEPTH=}\DecValTok{20}\NormalTok{))}\OperatorTok{$}\NormalTok{predicted}
\CommentTok{#> [1] 1548}
\end{Highlighting}
\end{Shaded}

and a soil with higher organic carbon content:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(rfsrc_BD, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{ORCDRC=}\DecValTok{150}\NormalTok{, }\DataTypeTok{PHIHOX=}\FloatTok{4.6}\NormalTok{, }
                  \DataTypeTok{SNDPPT=}\DecValTok{25}\NormalTok{, }\DataTypeTok{CLYPPT=}\DecValTok{35}\NormalTok{, }\DataTypeTok{CRFVOL=}\DecValTok{0}\NormalTok{, }\DataTypeTok{DEPTH=}\DecValTok{20}\NormalTok{))}\OperatorTok{$}\NormalTok{predicted}
\CommentTok{#> [1] 906}
\end{Highlighting}
\end{Shaded}

\hypertarget{ptf-for-correlating-classification-systems}{%
\subsection{PTF for correlating classification
systems}\label{ptf-for-correlating-classification-systems}}

In the second example we use ISRIC WISE to to build a correlation
function to translate soil classes from one classification system to the
other. The training data can be loaded from:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{load}\NormalTok{(}\StringTok{"extdata/wise_tax.rda"}\NormalTok{)}
\KeywordTok{str}\NormalTok{(WISE_tax)}
\CommentTok{#> 'data.frame':    8189 obs. of  7 variables:}
\CommentTok{#>  $ SOURCEID: Factor w/ 8189 levels "AF0001","AF0002",..: 1 2 3 4 5 6 7 8 9 10 ...}
\CommentTok{#>  $ LATWGS84: num  34.5 34.5 34.5 34.3 32.4 ...}
\CommentTok{#>  $ LONWGS84: num  69.2 69.2 69.2 61.4 62.1 ...}
\CommentTok{#>  $ TAXNWRB : Factor w/ 146 levels "#N/A","Albic Arenosol",..: 104 9 9 72 17 16 122 49 8 9 ...}
\CommentTok{#>  $ TAXOUSDA: Factor w/ 1728 levels ""," Calciorthid",..: 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ LFORM   : chr  "LV" "LV" "LV" "LV" ...}
\CommentTok{#>  $ LANDUS  : chr  "AA4" "AA6" "AA6" "AA4" ...}
\end{Highlighting}
\end{Shaded}

we also need to get the \emph{cleaned} legend for USDA classification:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{leg <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"extdata/taxousda_greatgroups.csv"}\NormalTok{)}
\KeywordTok{str}\NormalTok{(leg)}
\CommentTok{#> 'data.frame':    434 obs. of  4 variables:}
\CommentTok{#>  $ Great_Group: Factor w/ 434 levels "Acraquox","Acrohumox",..: 9 57 77 112 121 145 170 259 286 301 ...}
\CommentTok{#>  $ Suborder   : Factor w/ 79 levels "Albolls","Andepts",..: 4 4 4 4 4 4 4 4 4 4 ...}
\CommentTok{#>  $ Order      : Factor w/ 12 levels "Alfisols","Andisols",..: 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ TAX        : Factor w/ 434 levels "Alfisols_Aqualfs_Albaqualfs",..: 1 2 3 4 5 6 7 8 9 10 ...}
\end{Highlighting}
\end{Shaded}

Our objective is to develop a function to translate WRB classes into
USDA classes with help of some soil properties. We can try to add soil
pH and clay content to increase the accuracy of the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x.PHIHOX <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(SPROPS.WISE}\OperatorTok{$}\NormalTok{PHIHOX, }
                      \DataTypeTok{by=}\KeywordTok{list}\NormalTok{(SPROPS.WISE}\OperatorTok{$}\NormalTok{SOURCEID), }
                      \DataTypeTok{FUN=}\NormalTok{mean, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{); }\KeywordTok{names}\NormalTok{(x.PHIHOX)[}\DecValTok{1}\NormalTok{] =}\StringTok{ "SOURCEID"}
\NormalTok{x.CLYPPT <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(SPROPS.WISE}\OperatorTok{$}\NormalTok{CLYPPT, }
                      \DataTypeTok{by=}\KeywordTok{list}\NormalTok{(SPROPS.WISE}\OperatorTok{$}\NormalTok{SOURCEID), }
                      \DataTypeTok{FUN=}\NormalTok{mean, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{); }\KeywordTok{names}\NormalTok{(x.CLYPPT)[}\DecValTok{1}\NormalTok{] =}\StringTok{ "SOURCEID"}
\NormalTok{WISE_tax}\OperatorTok{$}\NormalTok{PHIHOX <-}\StringTok{ }\NormalTok{plyr}\OperatorTok{::}\KeywordTok{join}\NormalTok{(WISE_tax, x.PHIHOX, }\DataTypeTok{type=}\StringTok{"left"}\NormalTok{)}\OperatorTok{$}\NormalTok{x}
\CommentTok{#> Joining by: SOURCEID}
\NormalTok{WISE_tax}\OperatorTok{$}\NormalTok{CLYPPT <-}\StringTok{ }\NormalTok{plyr}\OperatorTok{::}\KeywordTok{join}\NormalTok{(WISE_tax, x.CLYPPT, }\DataTypeTok{type=}\StringTok{"left"}\NormalTok{)}\OperatorTok{$}\NormalTok{x}
\CommentTok{#> Joining by: SOURCEID}
\end{Highlighting}
\end{Shaded}

After that we need to cleanup the classes so that we can focus on USDA
suborders only:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sel.tax =}\StringTok{ }\KeywordTok{complete.cases}\NormalTok{(WISE_tax[,}\KeywordTok{c}\NormalTok{(}\StringTok{"TAXNWRB"}\NormalTok{,}\StringTok{"PHIHOX"}\NormalTok{,}\StringTok{"CLYPPT"}\NormalTok{,}\StringTok{"TAXOUSDA"}\NormalTok{)])}
\NormalTok{WISE_tax.sites <-}\StringTok{ }\NormalTok{WISE_tax[sel.tax,]}
\NormalTok{WISE_tax.sites}\OperatorTok{$}\NormalTok{TAXOUSDA.f <-}\StringTok{ }\OtherTok{NA}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in}\NormalTok{ leg}\OperatorTok{$}\NormalTok{Suborder)\{}
\NormalTok{  sel <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(j, WISE_tax.sites}\OperatorTok{$}\NormalTok{TAXOUSDA, }\DataTypeTok{ignore.case=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{  WISE_tax.sites}\OperatorTok{$}\NormalTok{TAXOUSDA.f[sel] =}\StringTok{ }\NormalTok{j}
\NormalTok{\}}
\NormalTok{WISE_tax.sites}\OperatorTok{$}\NormalTok{TAXOUSDA.f <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(WISE_tax.sites}\OperatorTok{$}\NormalTok{TAXOUSDA.f)}
\NormalTok{WISE_tax.sites}\OperatorTok{$}\NormalTok{TAXNWRB <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(}\KeywordTok{paste}\NormalTok{(WISE_tax.sites}\OperatorTok{$}\NormalTok{TAXNWRB))}
\end{Highlighting}
\end{Shaded}

and finally we can fit a model to translate WRB profiles to USDA
suborders:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TAXNUSDA.rf <-}\StringTok{ }\KeywordTok{rfsrc}\NormalTok{(TAXOUSDA.f }\OperatorTok{~}\StringTok{ }\NormalTok{TAXNWRB }\OperatorTok{+}\StringTok{ }\NormalTok{PHIHOX }\OperatorTok{+}\StringTok{ }\NormalTok{CLYPPT, }\DataTypeTok{data=}\NormalTok{WISE_tax.sites)}
\CommentTok{#TAXNUSDA.rf}
\end{Highlighting}
\end{Shaded}

which shows that the average accuracy is about 45\%. We can test
converting some classes with the help of additional soil properties:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newdata =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{TAXNWRB=}\KeywordTok{factor}\NormalTok{(}\StringTok{"Calcaric Cambisol"}\NormalTok{, }
                  \DataTypeTok{levels=}\KeywordTok{levels}\NormalTok{(WISE_tax.sites}\OperatorTok{$}\NormalTok{TAXNWRB)), }
                  \DataTypeTok{PHIHOX=}\FloatTok{7.8}\NormalTok{, }\DataTypeTok{CLYPPT=}\DecValTok{12}\NormalTok{)}
\NormalTok{x <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{predict}\NormalTok{(TAXNUSDA.rf, newdata, }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{)}\OperatorTok{$}\NormalTok{predicted)}
\NormalTok{x[,}\KeywordTok{order}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{x)[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]]}
\CommentTok{#>   Ochrepts Orthids}
\CommentTok{#> 1    0.288   0.154}
\end{Highlighting}
\end{Shaded}

so for example, the two most likely classes to equate to Calcaric
Cambisols seem to be Ochrepts and Orthids, which is not that much
different from correlation classes reported in
\citet{Krasilnikov2009handbook} in fact.

\hypertarget{summary-points}{%
\section{Summary points}\label{summary-points}}

In this chapter, we have endeavoured to provide precise and explicit
descriptions of the soil properties and soil classes of interest to
current PSM activities. For each soil property (or class) we have
provided an explanation for why that property (or class) is of interest
to users and why it has been selected to be mapped. In many cases, the
most obvious reason is that the soil property is widely recorded and
reported in legacy soil profile data bases and is therefore available.
But these soil properties are widely reported for good reasons, mainly
because they have been found to be important to consider when managing
land or making decisions about land capability or use. We have defined
the spatial attributes of the soil properties mapped at various scales,
defined a standard reference (analysis) method for each soil property,
provided information on the units, precision and range of values used to
describe each mapped soil property and reviewed problems and
opportunities related to harmonization of soil property values contained
in legacy soil profile databases that have been analysed using different
methods of analysis.

It should be noted that, in this chapter, we have emphasized the use of
existing, or legacy, soil profile data to provide the evidence used to
construct predicton models for PSM. This emphasis reflects the reality
that, for most of the world, legacy soil profile data is all that we
have to work with, at the present time, and all that we can reasonably
expect to obtain for the foreseeable future. Many of the issues and
challenges related to use and harmonization of legacy soil profile data
discussed in this chapter will hopefully be of less importance as newer,
contemporary data are collected in the field and analysed in the
laboratory using more robust, statistically valid and reproduceable
methods (e.g.~spectroscopy). In the meantime, standardization and
harmonization of legacy soil profile data will continue to present a
challenge for global to regional PSM.

One attractive option for harmonizing soil analytical data following the
SINFER concept would be to create and maintain a Global Soil Reference
Library (GSRL). This concept is further discussed in the final chapter.
Such a library would need to include data for a significant number of
soils from each continent or region. Each soil would be analysed for all
properties of interest using all commonly used methods of analysis.
Values for a soil property for any soil analysed by a given method could
be converted into equivalent values in any other analytical method (as
long as data analysed by both methods were included in the GSRL) by
developing pedo-transfer (or conversion) functions using the fully
analysed samples in the conversion library. In particular, some
variation of the similarity approach described by
\citet{Jagtap2004TASAE} for avaialble water capacity and
\citet{Nemes1999G} for harmonization of particle size data could be
implemented to harmonize all types of soil property values anywhere in
the world. The value of the soil property in the desired reference
method could be estimated by finding the soil or soils in the reference
library that were most similar to the soil for which harmonization was
required and then using the value of the soil (or soils) in the desired
reference method as the predicted harmonized value. If several similar
soils were identified, as is done by \citet{Nemes1999G}, then the
predicted harmonized value would be computed as the weighted mean, in
the appropriate reference method, of all similar soils; with weights
selected according to the similarity of the soils in the conversion
library to the soil being harmonized.

What a GSRL would do, in effect, is to provide a single, centralized
framework for harmonization and conversion of soil property values. It
would do this using a database of reference soils analysed for all soil
properties of interest by all major analytical methods. These fully
analysed reference soils would provide a framework for computing
individualized, locally relevant conversion or pedo-transfer functions
in a consistent and standardized manner. Consequently, global soil
mapping would benefit from having the best of both worlds, namely
locally-specific harmonization functions (which are known to be most
effective) and also ones that were computed everywhere in a single
standardized manner using data in a single comprehensive reference
database (which is desirable in terms of simplifying harmonization and
maintaining a record of how any value was harmonized).

Over time, we expect to see progress made in developing, applying and
documenting harmonization methods so that the values for any given soil
property used to create predictive models for global soil property
mapping are fully harmonized and roughly equivalent for all input data
sets. In the shorter term, it is likely that the accuracy of global
predictions will be reduced because of weak, inconsistent or completely
absent efforts to harmonize soil property values produced using
different analytical methods. In the longer term, we hope, and expect,
that data collected in the future, as we move forward, will benefit from
adoption of methods of data collection and analysis that are more
systematic, more reproduceable, more accurate and more interchangeable.
These improvements should reduce the need for harmonization and
standardization and should make use of soil observation and measurement
data easier and more consistent.

\hypertarget{soil-covs-chapter}{%
\chapter{Preparation of soil covariates for soil
mapping}\label{soil-covs-chapter}}

\emph{Edited by: T. Hengl}

\hypertarget{soil-covariate-data-sources}{%
\section{Soil covariate data
sources}\label{soil-covariate-data-sources}}

\hypertarget{soil-covs-30m}{%
\subsection{Soil covariate data sources (30--100 m
resolution)}\label{soil-covs-30m}}

Adding relevant covariates that can help explain the distribution of
soil properties increases the accuracy of spatial predictions. Hence,
prior to generating predictions of soil properties, it is a good idea to
invest in preparing a list of Remote Sensing (RS),
geomorphological/lithologic and DEM-based covariates that can
potentially help explain the spatial distribution of soil properties and
classes. There are now many finer resolution (30--250 m) covariates with
global coverage that are publicly available without restrictions. The
spatial detail, accessibility and accuracy of RS-based products has been
growing exponentially and there is no evidence that this trend is going
to slow down in the coming decades \citep{Herold2016}.

The most relevant (global) publicly available remote sensing-based
covariates that can be downloaded and used to improve predictive soil
mapping at high spatial resolutions are, for example:

\begin{itemize}
\item
  \href{https://lta.cr.usgs.gov/SRTM1Arc}{SRTM} and/or
  \href{http://www.eorc.jaxa.jp/ALOS/en/aw3d/index_e.htm}{ALOS W3D}
  Digital Elevation Model (DEM) at 30 m and
  \href{http://hydro.iis.u-tokyo.ac.jp/~yamadai/MERIT_DEM/}{MERIT DEM}
  at 100 m (these can be used to derive at least 8--12 DEM derivatives
  of which some generally prove to be beneficial for mapping of soil
  chemical and hydrological properties);
\item
  Landsat 7, 8 satellite images, either available from USGS's
  \href{http://glovis.usgs.gov/}{GloVis} /
  \href{http://earthexplorer.usgs.gov/}{EarthExplorer}, or from the
  \href{https://earthenginepartners.appspot.com/science-2013-global-forest/download_v1.2.html}{GlobalForestChange
  project} repository \citep{hansen2013high};
\item
  \href{https://global-surface-water.appspot.com/download}{Landsat-based
  Global Surface Water (GSW) dynamics images} at 30 m resolution for the
  period 1985--2016 \citep{pekel2016high};
\item
  Global Land Cover (GLC) maps based on the
  \href{http://www.globallandcover.com}{GLC30 project} at 30 m
  resolution for 2000 and 2010 \citep{Chen2014} and similar land cover
  projects \citep{Herold2016};
\item
  USGS's \href{https://landcover.usgs.gov/glc/}{global bare surface
  images} at 30 m resolution;
\item
  \href{http://www.eorc.jaxa.jp/ALOS/en/dataset/dataset_index.htm}{JAXA's
  ALOS} (PALSAR/PALSAR-2) radar images at 20 m resolution
  \citep{shimada2014new}; radar images, bands HH: -27.7 (5.3) dB and HV:
  -35.8 (3.0) dB, from the JAXA's ALOS project are especially
  interesting for mapping rock outcrops and exposed bedrock but are also
  to distinguish between bare soil and dense vegation;
\end{itemize}

Note that the download time for 30 m global RS data can be significant
if the data are needed for a larger area (hence you might consider using
some RS data processing hub such as
\href{http://www.sentinel-hub.com/}{Sentinel hub},
\href{https://earthengine.google.com/}{Google Earth Engine} and/or
\href{https://aws.amazon.com/public-datasets/}{Amazon Web Services}
instead of trying to download large mosaics yourself).

\hypertarget{soil-covs-250m}{%
\subsection{Soil covariate data sources (250 m resolution or
coarser)}\label{soil-covs-250m}}

\citet{Hengl2017SoilGrids250m} used a large stack of coarser resolution
covariate layers for producing SoilGrids250m predictions, most of which
were based on remote sensing data:

\begin{itemize}
\item
  DEM-derived surfaces --- slope, profile curvature, Multiresolution
  Index of Valley Bottom Flatness (VBF), deviation from Mean Value,
  valley depth, negative and positive Topographic Openness and SAGA
  Wetness Index --- all based on a global merge of SRTMGL3 DEM and
  GMTED2010 \citep{Danielson2011GMTED}. All DEM derivatives were
  computed using SAGA GIS \citep{gmd-8-1991-2015},
\item
  Long-term averaged monthly mean and standard deviation of the MODIS
  Enhanced Vegetation Index (EVI). Derived using a stack of MOD13Q1 EVI
  images \citep{Savtchenko2004ASR},
\item
  Long-term averaged mean monthly surface reflectances for MODIS bands 4
  (NIR) and 7 (MIR). Derived using a stack of MCD43A4 images
  \citep{mira2015modis},
\item
  Long-term averaged monthly mean and standard deviation of the MODIS
  land surface temperature (daytime and nighttime). Derived using a
  stack of MOD11A2 LST images \citep{wan2006modis},
\item
  Long-term averaged mean monthly hours under snow cover based on a
  stack of MOD10A2 8-day snow occurrence images
  \citep{hall2007accuracy},
\item
  Land cover classes (cultivated land, forests, grasslands, shrublands,
  wetlands, tundra, artificial surfaces and bareland cover) for the year
  2010 based on the GlobCover30 product by the National Geomatics Center
  of China \citep{Chen2014}. Upscaled to 250 m resolution and expressed
  as percent of pixel coverage,
\item
  Monthly precipitation images derived as the weighted average between
  the WorldClim monthly precipitation \citep{Hijmans2005IJC} and GPCP
  Version 2.2 \citep{huffman2009gpcp},
\item
  Long-term averaged mean monthly hours under snow cover. Derived using
  a stack of MOD10A2 8-day snow occurrence images,
\item
  Lithologic units (acid plutonics, acid volcanic, basic plutonics,
  basic volcanics, carbonate sedimentary rocks, evaporite, ice and
  glaciers, intermediate plutonics, intermediate volcanics,
  metamorphics, mixed sedimentary rocks, pyroclastics, siliciclastic
  sedimentary rocks, unconsolidated sediment) based on a Global
  Lithological Map GLiM \citep{GGGE:GGGE2352},
\item
  Landform classes (breaks/foothills, flat plains, high mountains/deep
  canyons, hills, low hills, low mountains, smooth plains) based on the
  USGS's Map of Global Ecological Land Units \citep{sayre2014new}.
\item
  Global Water Table Depth in meters \citep{fan2013global},
\item
  Landsat-based estimated distribution of Mangroves
  \citep{giri2011status},
\item
  Average soil and sedimentary-deposit thickness in meters
  \citep{Pelletier2016}.
\end{itemize}

These covariates were selected to represent factors of soil formation
according to \citet{jenny1994factors}: climate, relief, living
organisms, water dynamics and parent material. Out of the five main
factors, water dynamics and living organisms (especially vegetation
dynamics) are not trivial to represent as these operate over long
periods of time and often exhibit chaotic behaviour. Using reflectance
bands such as the mid-infrared MODIS bands from a single day, would be
of little use for soil mapping for areas with dynamic vegetation,
i.e.~with strong seasonal changes in vegetation cover. To account for
seasonal fluctuation and for inter-annual variations in surface
reflectance, long-term temporal signatures of the soil surface derived
as monthly averages from long-term MODIS imagery (15 years of data) can
be used \citep{Hengl2017SoilGrids250m}. Long-term average seasonal
signatures of surface reflectance or vegetation index provide a better
indication of soil characteristics than only a single snapshot of
surface reflectance. Computing temporal signatures of the land surface
requires a considerable investment of time (comparable to the generation
of climatic images vs temporary weather maps), but it is possibly the
only way to represent the cumulative influence of living organisms on
soil formation.

\citet{Behrens2018128} recently reported that, for example, DEM
derivatives derived at coarser resolutions correlated better with some
targeted soil properties than the derivatives derived at finer
resolutions. In this case, resolution (or scale) was represented through
various DEM aggregation levels and filter sizes. Some physical and
chemical processes of soil formation or vegetation distribution might
not be effecive or obvious at finer aggregation levels, but these can
become very visible at coarser aggregation levels. In fact, it seems
that spatial dependencies and interactions of the covariates can often
be explained better simply by a aggregating DEM and its derivatives.

\hypertarget{preparing-soil-covariate-layers}{%
\section{Preparing soil covariate
layers}\label{preparing-soil-covariate-layers}}

Before we are able to fit spatial prediction models and generate soil
maps, a significant amount of effort is also spent on preparing
covariate ``layers'' that can be used as independent variables (i.e.
``predictor variables'') in the statistical modelling. Typical
operations used to generate soil covariate layers include:

\begin{itemize}
\item
  Converting polygon maps to rasters,
\item
  Downscaling or upscaling (aggregating) rasters to match the target
  resolution (i.e.~preparing a \emph{stack}),
\item
  Filtering out missing pixels / reducing noise and multicolinearity
  (data overlap) problems,
\item
  Overlaying and subsetting raster stacks and points,
\end{itemize}

The following examples should provide some ideas about how to program
these steps using the shortest possible syntax running the fastest and
most robust algorithms. Raster data can often be very large
(e.g.~millions of pixels) so processing large stacks of remote sensing
scenes in R needs to be planned carefully. The complete R tutorial you
can download from the
\textbf{\href{https://github.com/envirometrix/PredictiveSoilMapping}{github
repository}}. Instructions on how to install and set-up all software
used in this example can be found in the software installation chapter
\ref{software}.

\hypertarget{converting-polygon-maps-to-rasters}{%
\subsection{Converting polygon maps to
rasters}\label{converting-polygon-maps-to-rasters}}

Before we can attach a polygon map to other stacks of covariates, it
needs to be rasterized i.e.~converted to a raster layer defined with its
bounding box (extent) and spatial resolution. Consider for example the
\href{http://plotkml.r-forge.r-project.org/eberg.html}{Ebergötzen data
set} polygon map from the plotKML package (Fig.
\ref{fig:eberg-zones-spplot}):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rgdal)}
\CommentTok{#> Loading required package: sp}
\CommentTok{#> rgdal: version: 1.3-4, (SVN revision 766)}
\CommentTok{#>  Geospatial Data Abstraction Library extensions to R successfully loaded}
\CommentTok{#>  Loaded GDAL runtime: GDAL 2.2.4, released 2018/03/19}
\CommentTok{#>  Path to GDAL shared files: /usr/share/gdal}
\CommentTok{#>  GDAL binary built with GEOS: TRUE }
\CommentTok{#>  Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493]}
\CommentTok{#>  Path to PROJ.4 shared files: (autodetected)}
\CommentTok{#>  Linking to sp version: 1.3-1}
\KeywordTok{library}\NormalTok{(raster)}
\KeywordTok{library}\NormalTok{(plotKML)}
\CommentTok{#> plotKML version 0.5-8 (2017-05-12)}
\CommentTok{#> URL: http://plotkml.r-forge.r-project.org/}
\KeywordTok{data}\NormalTok{(eberg_zones)}
\KeywordTok{spplot}\NormalTok{(eberg_zones[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=0.7\linewidth]{Soil_covariates_files/figure-latex/eberg-zones-spplot-1} 

}

\caption{Ebergotzen parent material polygon map with legend.}\label{fig:eberg-zones-spplot}
\end{figure}

We can convert this object to a raster by using the
\href{https://cran.r-project.org/web/packages/raster/}{raster package}.
Note that before we can run the operation, we need to know the target
grid system i.e.~the extent of the grid and its spatial resolution. We
can use this from an existing layer:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(plotKML)}
\KeywordTok{data}\NormalTok{(}\StringTok{"eberg_grid25"}\NormalTok{)}
\KeywordTok{gridded}\NormalTok{(eberg_grid25) <-}\StringTok{ }\ErrorTok{~}\NormalTok{x}\OperatorTok{+}\NormalTok{y}
\KeywordTok{proj4string}\NormalTok{(eberg_grid25) <-}\StringTok{ }\KeywordTok{CRS}\NormalTok{(}\StringTok{"+init=epsg:31467"}\NormalTok{)}
\NormalTok{r <-}\StringTok{ }\KeywordTok{raster}\NormalTok{(eberg_grid25)}
\NormalTok{r}
\CommentTok{#> class       : RasterLayer }
\CommentTok{#> dimensions  : 400, 400, 160000  (nrow, ncol, ncell)}
\CommentTok{#> resolution  : 25, 25  (x, y)}
\CommentTok{#> extent      : 3570000, 3580000, 5708000, 5718000  (xmin, xmax, ymin, ymax)}
\CommentTok{#> coord. ref. : +init=epsg:31467 +proj=tmerc +lat_0=0 +lon_0=9 +k=1 +x_0=3500000 +y_0=0 +datum=potsdam +units=m +no_defs +ellps=bessel +towgs84=598.1,73.7,418.2,0.202,0.045,-2.455,6.7 }
\CommentTok{#> data source : in memory}
\CommentTok{#> names       : DEMTOPx }
\CommentTok{#> values      : 159, 428  (min, max)}
\end{Highlighting}
\end{Shaded}

The \texttt{eberg\_grids25} object is a \texttt{SpatialPixelsDataFrame},
which is a spatial gridded data structure of the
\href{https://cran.r-project.org/web/packages/sp/}{sp package} package.
The raster package also offers data structures for spatial (gridded)
data, and stores such data as \texttt{RasterLayer} class. Gridded data
can be converted from class \texttt{SpatialPixelsDataFrame} to Raster
layer with the
\href{http://www.rdocumentation.org/packages/raster/functions/raster}{\texttt{raster}}
command. The
\href{http://www.inside-r.org/packages/cran/sp/docs/CRS}{\texttt{CRS}}
command of the sp package can be used to set a spatial projection.
\href{http://spatialreference.org/ref/epsg/31467/}{EPSG projection
31467} is the German coordinate system (each coordinate system has an
associated EPSG number that can be obtained from
\url{http://spatialreference.org/}.

Conversion from polygon to raster is now possible via the
\href{http://www.rdocumentation.org/packages/raster/functions/rasterize}{\texttt{rasterize}}
command:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(eberg_zones)}
\CommentTok{#> [1] "ZONES"}
\NormalTok{eberg_zones_r <-}\StringTok{ }\KeywordTok{rasterize}\NormalTok{(eberg_zones, r, }\DataTypeTok{field=}\StringTok{"ZONES"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(eberg_zones_r)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=0.7\linewidth]{Soil_covariates_files/figure-latex/eberg-zones-grid-1} 

}

\caption{Ebergotzen parent material polygon map rasterized.}\label{fig:eberg-zones-grid}
\end{figure}

Converting large polygons in R using the raster package can be very
time-consuming. To speed up the rasterization of polygons we highly
recommend using instead the \texttt{fasterize} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sf)}
\CommentTok{#> Linking to GEOS 3.6.1, GDAL 2.2.4, PROJ 4.9.3}
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(fasterize))\{ devtools}\OperatorTok{::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"ecohealthalliance/fasterize"}\NormalTok{) \}}
\CommentTok{#> Loading required package: fasterize}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'fasterize'}
\CommentTok{#> The following object is masked from 'package:graphics':}
\CommentTok{#> }
\CommentTok{#>     plot}
\NormalTok{eberg_zones_sf <-}\StringTok{ }\KeywordTok{as}\NormalTok{(eberg_zones, }\StringTok{"sf"}\NormalTok{)}
\NormalTok{eberg_zones_r <-}\StringTok{ }\KeywordTok{fasterize}\NormalTok{(eberg_zones_sf, r, }\DataTypeTok{field=}\StringTok{"ZONES"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{fasterize} function is an order of magnitude faster and hence
more suitable for operational work. Note also that it only works with
Simple Feature (sf) objects.

Another efficient approach to rasterize polygons is to use SAGA GIS as
this can handle large data and is easy to run in parallel. First, you
need to export the polygon map to shapefile format which can be done
with commands of the
\href{https://cran.r-project.org/web/packages/rgdal/}{rgdal package}
package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eberg_zones}\OperatorTok{$}\NormalTok{ZONES_int <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(eberg_zones}\OperatorTok{$}\NormalTok{ZONES)}
\KeywordTok{writeOGR}\NormalTok{(eberg_zones[}\StringTok{"ZONES_int"}\NormalTok{], }\StringTok{"extdata/eberg_zones.shp"}\NormalTok{, }\StringTok{"."}\NormalTok{, }\StringTok{"ESRI Shapefile"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{writeOGR()} command writes a SpatialPolygonsDataFrame (the
data structure for polygon data in R) to an ESRI shapefile. Here we only
write the attribute \texttt{"ZONES\_int"} to the shapefile. It is,
however, also possible to write all attributes of the
SpatialPolygonsDataFrame to a shapefile.

Next, you can locate the (previously installed) SAGA GIS command line
program (on Microsoft Windows OS or Linux system):

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(.Platform}\OperatorTok{$}\NormalTok{OS.type}\OperatorTok{==}\StringTok{"unix"}\NormalTok{)\{}
\NormalTok{  saga_cmd =}\StringTok{ "saga_cmd"}
\NormalTok{\}}
\ControlFlowTok{if}\NormalTok{(.Platform}\OperatorTok{$}\NormalTok{OS.type}\OperatorTok{==}\StringTok{"windows"}\NormalTok{)\{}
\NormalTok{  saga_cmd =}\StringTok{ "C:/Progra~1/SAGA-GIS/saga_cmd.exe"}
\NormalTok{\}}
\NormalTok{saga_cmd}
\CommentTok{#> [1] "saga_cmd"}
\end{Highlighting}
\end{Shaded}

and finally use the module
\href{http://saga-gis.org/saga_module_doc/2.2.7/grid_gridding_0.html}{`'grid\_gridding''}
to convert the shapefile to a grid:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pix =}\StringTok{ }\DecValTok{25}
\KeywordTok{system}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(saga_cmd, }\StringTok{' grid_gridding 0 -INPUT }\CharTok{\textbackslash{}"}\StringTok{extdata/eberg_zones.shp}\CharTok{\textbackslash{}"}\StringTok{ '}\NormalTok{,}
      \StringTok{'-FIELD }\CharTok{\textbackslash{}"}\StringTok{ZONES_int}\CharTok{\textbackslash{}"}\StringTok{ -GRID }\CharTok{\textbackslash{}"}\StringTok{extdata/eberg_zones.sgrd}\CharTok{\textbackslash{}"}\StringTok{ -GRID_TYPE 0 '}\NormalTok{,}
      \StringTok{'-TARGET_DEFINITION 0 -TARGET_USER_SIZE '}\NormalTok{, pix, }\StringTok{' -TARGET_USER_XMIN '}\NormalTok{, }
      \KeywordTok{extent}\NormalTok{(r)[}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\NormalTok{pix}\OperatorTok{/}\DecValTok{2}\NormalTok{,}\StringTok{' -TARGET_USER_XMAX '}\NormalTok{, }\KeywordTok{extent}\NormalTok{(r)[}\DecValTok{2}\NormalTok{]}\OperatorTok{-}\NormalTok{pix}\OperatorTok{/}\DecValTok{2}\NormalTok{, }
      \StringTok{' -TARGET_USER_YMIN '}\NormalTok{, }\KeywordTok{extent}\NormalTok{(r)[}\DecValTok{3}\NormalTok{]}\OperatorTok{+}\NormalTok{pix}\OperatorTok{/}\DecValTok{2}\NormalTok{,}\StringTok{' -TARGET_USER_YMAX '}\NormalTok{, }
      \KeywordTok{extent}\NormalTok{(r)[}\DecValTok{4}\NormalTok{]}\OperatorTok{-}\NormalTok{pix}\OperatorTok{/}\DecValTok{2}\NormalTok{))}
\CommentTok{#> Warning in system(paste0(saga_cmd, " grid_gridding 0 -INPUT \textbackslash{}"extdata/}
\CommentTok{#> eberg_zones.shp\textbackslash{}" ", : error in running command}
\NormalTok{eberg_zones_r2 <-}\StringTok{ }\KeywordTok{readGDAL}\NormalTok{(}\StringTok{"extdata/eberg_zones.sdat"}\NormalTok{)}
\CommentTok{#> extdata/eberg_zones.sdat has GDAL driver SAGA }
\CommentTok{#> and has 400 rows and 400 columns}
\end{Highlighting}
\end{Shaded}

With the \texttt{system()} command we can invoke an operating system
(OS) command, here we use it to run the \texttt{saga\_cmd.exe} file from
R. The paste0 function is used to paste together a string that is passed
to the \texttt{system()} command. The string starts with the OS command
we would like to invoke (here \texttt{saga\_cmd.exe}) followed by input
required for the running the OS command.

Note that the bounding box (in SAGA GIS) needs to be defined using the
center of the corner pixel and not the corners, hence we take half of
the pixel size for extent coordinates from raster package. Also note
that the class names have been lost during rasterization (we work with
integers in SAGA GIS), but we can attach them back by using e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{levels}\NormalTok{(eberg_zones}\OperatorTok{$}\NormalTok{ZONES)}
\CommentTok{#> [1] "Clay_and_loess"  "Clayey_derivats" "Sandy_material"  "Silt_and_sand"}
\NormalTok{eberg_zones_r2}\OperatorTok{$}\NormalTok{ZONES <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(eberg_zones_r2}\OperatorTok{$}\NormalTok{band1)}
\KeywordTok{levels}\NormalTok{(eberg_zones_r2}\OperatorTok{$}\NormalTok{ZONES) <-}\StringTok{ }\KeywordTok{levels}\NormalTok{(eberg_zones}\OperatorTok{$}\NormalTok{ZONES)}
\KeywordTok{summary}\NormalTok{(eberg_zones_r2}\OperatorTok{$}\NormalTok{ZONES)}
\CommentTok{#>  Clay_and_loess Clayey_derivats  Sandy_material   Silt_and_sand }
\CommentTok{#>           28667           35992           21971           73370}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=0.7\linewidth]{figures/eberg_zones_rasterized} 

}

\caption{Eberg zones rasterized to 25 m resolution.}\label{fig:eberg-zones-rasterized}
\end{figure}

\hypertarget{downscaling-upscaling}{%
\subsection{Downscaling or upscaling (aggregating)
rasters}\label{downscaling-upscaling}}

In order for all covariates to perfectly \emph{stack}, we also need to
adjust the resolution of some covariates that have either too coarse or
too fine a resolution compared to the target resolution. The process of
bringing raster layers to a target grid resolution is also known as
\textbf{resampling}. Consider the following example from the Ebergotzen
case study:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(eberg_grid)}
\KeywordTok{gridded}\NormalTok{(eberg_grid) <-}\StringTok{ }\ErrorTok{~}\NormalTok{x}\OperatorTok{+}\NormalTok{y}
\KeywordTok{proj4string}\NormalTok{(eberg_grid) <-}\StringTok{ }\KeywordTok{CRS}\NormalTok{(}\StringTok{"+init=epsg:31467"}\NormalTok{)}
\KeywordTok{names}\NormalTok{(eberg_grid)}
\CommentTok{#> [1] "PRMGEO6" "DEMSRT6" "TWISRT6" "TIRAST6" "LNCCOR6"}
\end{Highlighting}
\end{Shaded}

In this case we have a few layers that we would like to use for spatial
prediction in combination with the maps produced in the previous
sections, but their resolution is 100 m i.e.~about 16 times coarser.
Probably the most robust way to resample rasters is to use the
\href{http://www.gdal.org/gdalwarp.html}{\texttt{gdalwarp}} function
from the GDAL software. Assuming that you have already installed GDAL,
you only need to locate the program on your system, and then you can
again run \href{http://www.gdal.org/gdalwarp.html}{\texttt{gdalwarp}}
via the system command:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{writeGDAL}\NormalTok{(eberg_grid[}\StringTok{"TWISRT6"}\NormalTok{], }\StringTok{"extdata/eberg_grid_TWISRT6.tif"}\NormalTok{)}
\KeywordTok{system}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{'gdalwarp extdata/eberg_grid_TWISRT6.tif'}\NormalTok{,}
              \StringTok{' extdata/eberg_grid_TWISRT6_25m.tif -r }\CharTok{\textbackslash{}"}\StringTok{cubicspline}\CharTok{\textbackslash{}"}\StringTok{ -te '}\NormalTok{, }
              \KeywordTok{paste}\NormalTok{(}\KeywordTok{as.vector}\NormalTok{(}\KeywordTok{extent}\NormalTok{(r))[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{)], }\DataTypeTok{collapse=}\StringTok{" "}\NormalTok{),}
              \StringTok{' -tr '}\NormalTok{, pix, }\StringTok{' '}\NormalTok{, pix, }\StringTok{' -overwrite'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The writeGDAL command writes the TWISRT6 grid, that is stored in the
eberg\_grid grid stack, to a TIFF file. This TIFF is subsequently read
by the gdalwarp function and resampled to a 25m TIFF file using
\texttt{cubicspline}, which will fill in values between original grid
nodes using smooth surfaces. Note that the paste0 function in the
\texttt{system()} command pastes together the following string:

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{"C:/Progra~1/GDAL/gdalwarp.exe eberg_grid_TWISRT6.tif }
\StringTok{eberg_grid_TWISRT6_25m.tif -r }\DataTypeTok{\textbackslash{}"}\StringTok{cubicspline}\DataTypeTok{\textbackslash{}"}\StringTok{ }
\StringTok{-te 3570000 5708000 3580000 5718000 -tr 25 25 -overwrite"}
\end{Highlighting}
\end{Shaded}

We can compare the two maps (the original and the downscaled) next to
each other by using:

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/eberg_original_vs_downscaled} 

}

\caption{Original TWI vs downscaled map from 100 m to 25 m.}\label{fig:eberg-original-vs-downscaled}
\end{figure}

The map on the right looks much smoother of course (assuming that this
variable varies continuously in space, this could very well be an
accurate picture), but it is important to realize that downscaling can
only be implemented up to certain target resolution i.e.~only for
certain features. For example, downscaling TWI from 100 to 25 m is not
much of problem, but to go beyond 10 m would probably result in large
differences from a TWI calculated at 10 m resolution (in other words: be
careful with downscaling because it is often not trivial).

The opposite process to downscaling is upscaling or aggregation.
Although this one can also potentially be tricky, it is a much more
straightforward process than downscaling. We recommend using the
\texttt{average} method in GDAL for aggregating values e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{'gdalwarp extdata/eberg_grid_TWISRT6.tif'}\NormalTok{,}
              \StringTok{' extdata/eberg_grid_TWISRT6_250m.tif -r }\CharTok{\textbackslash{}"}\StringTok{average}\CharTok{\textbackslash{}"}\StringTok{ -te '}\NormalTok{, }
              \KeywordTok{paste}\NormalTok{(}\KeywordTok{as.vector}\NormalTok{(}\KeywordTok{extent}\NormalTok{(r))[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{)], }\DataTypeTok{collapse=}\StringTok{" "}\NormalTok{),}
              \StringTok{' -tr 250 250 -overwrite'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/eberg_original_vs_aggregated} 

}

\caption{Original TWI vs aggregated map from 100 m to 250 m.}\label{fig:eberg-original-vs-aggregated}
\end{figure}

\hypertarget{deriving-dem-parameters-using-saga-gis}{%
\subsection{Deriving DEM parameters using SAGA
GIS}\label{deriving-dem-parameters-using-saga-gis}}

Now that we have established a connection between R and SAGA GIS, we can
also use SAGA GIS to derive some standard DEM parameters of interest to
soil mapping. To automate further processing, we make the following
function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{saga_DEM_derivatives <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(INPUT, }\DataTypeTok{MASK=}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{sel=}\KeywordTok{c}\NormalTok{(}\StringTok{"SLP"}\NormalTok{,}\StringTok{"TWI"}\NormalTok{,}\StringTok{"CRV"}\NormalTok{,}\StringTok{"VBF"}\NormalTok{,}\StringTok{"VDP"}\NormalTok{,}\StringTok{"OPN"}\NormalTok{,}\StringTok{"DVM"}\NormalTok{))\{}
  \ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.null}\NormalTok{(MASK))\{}
\NormalTok{    ## Fill in missing DEM pixels:}
    \KeywordTok{suppressWarnings}\NormalTok{( }\KeywordTok{system}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(saga_cmd, }
                                    \StringTok{' grid_tools 25 -GRID=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, INPUT, }
                                    \StringTok{'}\CharTok{\textbackslash{}"}\StringTok{ -MASK=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, MASK, }\StringTok{'}\CharTok{\textbackslash{}"}\StringTok{ -CLOSED=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
\NormalTok{                                    INPUT, }\StringTok{'}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{)) )}
\NormalTok{  \}}
\NormalTok{  ## Slope:}
  \ControlFlowTok{if}\NormalTok{(}\KeywordTok{any}\NormalTok{(sel }\OperatorTok{%in%}\StringTok{ "SLP"}\NormalTok{))\{}
    \KeywordTok{try}\NormalTok{( }\KeywordTok{suppressWarnings}\NormalTok{( }\KeywordTok{system}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(saga_cmd, }
                                         \StringTok{' ta_morphometry 0 -ELEVATION=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
\NormalTok{                                         INPUT, }\StringTok{'}\CharTok{\textbackslash{}"}\StringTok{ -SLOPE=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
                                         \KeywordTok{gsub}\NormalTok{(}\StringTok{".sgrd"}\NormalTok{, }\StringTok{"_slope.sgrd"}\NormalTok{, INPUT), }
                                         \StringTok{'}\CharTok{\textbackslash{}"}\StringTok{ -C_PROF=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
                                         \KeywordTok{gsub}\NormalTok{(}\StringTok{".sgrd"}\NormalTok{, }\StringTok{"_cprof.sgrd"}\NormalTok{, INPUT), }\StringTok{'}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{) ) ) )}
\NormalTok{  \}}
\NormalTok{  ## TWI:}
  \ControlFlowTok{if}\NormalTok{(}\KeywordTok{any}\NormalTok{(sel }\OperatorTok{%in%}\StringTok{ "TWI"}\NormalTok{))\{}
    \KeywordTok{try}\NormalTok{( }\KeywordTok{suppressWarnings}\NormalTok{( }\KeywordTok{system}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(saga_cmd, }
                                         \StringTok{' ta_hydrology 15 -DEM=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
\NormalTok{                                         INPUT, }\StringTok{'}\CharTok{\textbackslash{}"}\StringTok{ -TWI=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
                                         \KeywordTok{gsub}\NormalTok{(}\StringTok{".sgrd"}\NormalTok{, }\StringTok{"_twi.sgrd"}\NormalTok{, INPUT), }\StringTok{'}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{) ) ) )}
\NormalTok{  \}}
\NormalTok{  ## MrVBF:}
  \ControlFlowTok{if}\NormalTok{(}\KeywordTok{any}\NormalTok{(sel }\OperatorTok{%in%}\StringTok{ "VBF"}\NormalTok{))\{}
    \KeywordTok{try}\NormalTok{( }\KeywordTok{suppressWarnings}\NormalTok{( }\KeywordTok{system}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(saga_cmd, }
                                         \StringTok{' ta_morphometry 8 -DEM=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
\NormalTok{                                         INPUT, }\StringTok{'}\CharTok{\textbackslash{}"}\StringTok{ -MRVBF=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{,}
                                         \KeywordTok{gsub}\NormalTok{(}\StringTok{".sgrd"}\NormalTok{, }\StringTok{"_vbf.sgrd"}\NormalTok{, INPUT),}
                                         \StringTok{'}\CharTok{\textbackslash{}"}\StringTok{ -T_SLOPE=10 -P_SLOPE=3'}\NormalTok{) ) ) )}
\NormalTok{  \}}
\NormalTok{  ## Valley depth:}
  \ControlFlowTok{if}\NormalTok{(}\KeywordTok{any}\NormalTok{(sel }\OperatorTok{%in%}\StringTok{ "VDP"}\NormalTok{))\{}
    \KeywordTok{try}\NormalTok{( }\KeywordTok{suppressWarnings}\NormalTok{( }\KeywordTok{system}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(saga_cmd, }
                                         \StringTok{' ta_channels 7 -ELEVATION=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
\NormalTok{                                         INPUT, }\StringTok{'}\CharTok{\textbackslash{}"}\StringTok{ -VALLEY_DEPTH=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
                                         \KeywordTok{gsub}\NormalTok{(}\StringTok{".sgrd"}\NormalTok{, }\StringTok{"_vdepth.sgrd"}\NormalTok{, }
\NormalTok{                                              INPUT), }\StringTok{'}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{) ) ) )}
\NormalTok{  \}}
\NormalTok{  ## Openess:}
  \ControlFlowTok{if}\NormalTok{(}\KeywordTok{any}\NormalTok{(sel }\OperatorTok{%in%}\StringTok{ "OPN"}\NormalTok{))\{}
    \KeywordTok{try}\NormalTok{( }\KeywordTok{suppressWarnings}\NormalTok{( }\KeywordTok{system}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(saga_cmd, }
                                         \StringTok{' ta_lighting 5 -DEM=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
\NormalTok{                                         INPUT, }\StringTok{'}\CharTok{\textbackslash{}"}\StringTok{ -POS=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
                                         \KeywordTok{gsub}\NormalTok{(}\StringTok{".sgrd"}\NormalTok{, }\StringTok{"_openp.sgrd"}\NormalTok{, INPUT), }
                                         \StringTok{'}\CharTok{\textbackslash{}"}\StringTok{ -NEG=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
                                         \KeywordTok{gsub}\NormalTok{(}\StringTok{".sgrd"}\NormalTok{, }\StringTok{"_openn.sgrd"}\NormalTok{, INPUT), }
                                         \StringTok{'}\CharTok{\textbackslash{}"}\StringTok{ -METHOD=0'}\NormalTok{ ) ) ) )}
\NormalTok{  \}}
\NormalTok{  ## Deviation from Mean Value:}
  \ControlFlowTok{if}\NormalTok{(}\KeywordTok{any}\NormalTok{(sel }\OperatorTok{%in%}\StringTok{ "DVM"}\NormalTok{))\{}
    \KeywordTok{suppressWarnings}\NormalTok{( }\KeywordTok{system}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(saga_cmd, }
                                    \StringTok{' statistics_grid 1 -GRID=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
\NormalTok{                                    INPUT, }\StringTok{'}\CharTok{\textbackslash{}"}\StringTok{ -DEVMEAN=}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
                                    \KeywordTok{gsub}\NormalTok{(}\StringTok{".sgrd"}\NormalTok{, }\StringTok{"_devmean.sgrd"}\NormalTok{, INPUT), }
                                    \StringTok{'}\CharTok{\textbackslash{}"}\StringTok{ -RADIUS=11'}\NormalTok{ ) ) )}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

To run this function we only need DEM as input:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{writeGDAL}\NormalTok{(eberg_grid[}\StringTok{"DEMSRT6"}\NormalTok{], }\StringTok{"extdata/DEMSRT6.sdat"}\NormalTok{, }\StringTok{"SAGA"}\NormalTok{)}
\KeywordTok{saga_DEM_derivatives}\NormalTok{(}\StringTok{"DEMSRT6.sgrd"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

which derives all DEM derivatives in a single operation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dem.lst <-}\StringTok{ }\KeywordTok{list.files}\NormalTok{(}\DataTypeTok{pattern=}\KeywordTok{glob2rx}\NormalTok{(}\StringTok{"^DEMSRT6_*.sdat"}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{stack}\NormalTok{(dem.lst), }\DataTypeTok{col=}\NormalTok{SAGA_pal[[}\DecValTok{1}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=0.9\linewidth]{figures/dem_derivatives_plot} 

}

\caption{Some standard DEM derivatives calculated using SAGA GIS.}\label{fig:dem-derivatives-plot}
\end{figure}

This function can now be used with any DEM to derive a standard set of
7-8 DEM parameters consisting of slope and curvature, TWI and MrVBF,
positive and negative openess, valley depth and deviation from mean
value. You could easily add more parameters to this function and then
test if some of the other DEM derivatives can help improve mapping soil
properties and classes. Note that SAGA GIS will by default optimize
computing of DEM derivatives by using most of the available cores to
compute (parallelization is turned on automatically).

\hypertarget{filtering-out-missing-pixels-and-artifacts}{%
\subsection{Filtering out missing pixels and
artifacts}\label{filtering-out-missing-pixels-and-artifacts}}

After bringing all covariates to the same grid definition, remaining
problems for using covariates in spatial modelling may include:

\begin{itemize}
\item
  Missing pixels,
\item
  Artifacts and noise,
\item
  Multicolinearity (i.e.~data overlap),
\end{itemize}

In a stack with tens of rasters, the \emph{weakest layer} (i.e.~the
layer with greatest number of missing pixels or largest number of
artifacts) could cause serious problems for producing soil maps as the
missing pixels and artifacts would propagate to predictions: if only one
layer in the raster stack misses values then predictive models might
drop whole rows in the predictions even though data is available for
95\% of rows. Missing pixels occur for various reasons: in the case of
remote sensing, missing pixels can be due to clouds or similar; noise is
often due to atmospheric conditions. Missing pixels (as long as we are
dealing with only a few patches of missing pixels) can be efficiently
filtered by using for example the
\href{http://saga-gis.org/saga_module_doc/2.2.7/grid_tools_7.html}{gap
filling functionality} available in the SAGA GIS e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{image}\NormalTok{(}\KeywordTok{raster}\NormalTok{(eberg_grid[}\StringTok{"test"}\NormalTok{]), }\DataTypeTok{col=}\NormalTok{SAGA_pal[[}\DecValTok{1}\NormalTok{]], }\DataTypeTok{zlim=}\NormalTok{zlim, }\DataTypeTok{main=}\StringTok{"Original"}\NormalTok{, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{)}
\KeywordTok{image}\NormalTok{(}\KeywordTok{raster}\NormalTok{(}\StringTok{"test.sdat"}\NormalTok{), }\DataTypeTok{col=}\NormalTok{SAGA_pal[[}\DecValTok{1}\NormalTok{]], }\DataTypeTok{zlim=}\NormalTok{zlim, }\DataTypeTok{main=}\StringTok{"Filtered"}\NormalTok{, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this example we use the same input and output file for filling in
gaps. There are several other gap filling possibilities in SAGA GIS
including Close Gaps with Spline, Close Gaps with Stepwise Resampling
and Close One Cell Gaps. Note all of these are equally applicable to all
missing pixel problems, but having \textless{}10\% of missing pixels is
often not much of a problem for soil mapping.

Another elegant way to filter the missing pixels, to reduce noise and to
reduce data overlap is to use
\href{http://www.rdocumentation.org/packages/stats/functions/prcomp}{Principal
Components} transformation of original data. This is available also via
the GSIF function
\href{http://www.rdocumentation.org/packages/GSIF/functions/spc}{`'spc''}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(eberg_grid)}
\KeywordTok{gridded}\NormalTok{(eberg_grid) <-}\StringTok{ }\ErrorTok{~}\NormalTok{x}\OperatorTok{+}\NormalTok{y}
\KeywordTok{proj4string}\NormalTok{(eberg_grid) <-}\StringTok{ }\KeywordTok{CRS}\NormalTok{(}\StringTok{"+init=epsg:31467"}\NormalTok{)}
\NormalTok{formulaString <-}\StringTok{ }\ErrorTok{~}\StringTok{ }\NormalTok{PRMGEO6}\OperatorTok{+}\NormalTok{DEMSRT6}\OperatorTok{+}\NormalTok{TWISRT6}\OperatorTok{+}\NormalTok{TIRAST6}
\NormalTok{eberg_spc <-}\StringTok{ }\NormalTok{GSIF}\OperatorTok{::}\KeywordTok{spc}\NormalTok{(eberg_grid, formulaString)}
\KeywordTok{names}\NormalTok{(eberg_spc}\OperatorTok{@}\NormalTok{predicted) }\CommentTok{# 11 components on the end;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/eberg_spc_11_plot} 

}

\caption{11 PCs derived using eberg covariates.}\label{fig:eberg-spc-11-plot}
\end{figure}

The advantages of using the
\href{http://www.rdocumentation.org/packages/GSIF/functions/spc}{`'spc''}
function are:

\begin{itemize}
\item
  All output soil covariates are numeric (and not a mixture of factors
  and numeric),
\item
  The last 1-2 PCs often contain signal noise and could be excluded from
  modelling,
\item
  In subsequent analysis it becomes easier to remove covariates that do
  not help in modelling (e.g.~by using step-wise selection and similar),
\end{itemize}

A disadvantage of using SPCs (spatial predictive components) is that
these components are often abstract so that interpretation of
correlations can become difficult. Also, if one of the layers contains
many factor levels, then the number of output covariates might explode,
which becomes impractical as we should then have at least 10
observations per covariate to avoid overfitting.

\hypertarget{overlaying-and-subsetting-raster-stacks-and-points}{%
\subsection{Overlaying and subsetting raster stacks and
points}\label{overlaying-and-subsetting-raster-stacks-and-points}}

Now that we have prepared all covariates (resampled them to the same
grid and filtered out all problems), we can proceed with running
overlays and fitting statistical models. Assuming that we deal with a
large number of files, an elegant way to read all those into R is by
using the raster package, especially the
\href{http://www.rdocumentation.org/packages/raster/functions/stack}{`'stack''}
and
\href{http://www.rdocumentation.org/packages/raster/functions/stack}{`'raster''}
commands. In the following example we can list all files of interest,
and then read them all at once:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(raster)}
\NormalTok{grd.lst <-}\StringTok{ }\KeywordTok{list.files}\NormalTok{(}\DataTypeTok{pattern=}\StringTok{"25m"}\NormalTok{)}
\NormalTok{grd.lst}
\NormalTok{grid25m <-}\StringTok{ }\KeywordTok{stack}\NormalTok{(grd.lst)}
\NormalTok{grid25m <-}\StringTok{ }\KeywordTok{as}\NormalTok{(grid25m, }\StringTok{"SpatialGridDataFrame"}\NormalTok{)}
\KeywordTok{str}\NormalTok{(grid25m)}
\end{Highlighting}
\end{Shaded}

One could now save all the prepared covariates stored in
SpatialGridDataFrame as an RDS data object for future use.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{saveRDS}\NormalTok{(grid25m, }\DataTypeTok{file =} \StringTok{"extdata/covariates25m.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To overlay rasters and points and prepare a regression matrix, we can
either use the
\href{http://www.rdocumentation.org/packages/sp/functions/over}{\texttt{over}}
function from the sp package, or
\href{http://www.rdocumentation.org/packages/raster/functions/extract}{\texttt{extract}}
function from the raster package. By using the raster package, one can
run overlay even without reading the rasters into memory:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sp)}
\KeywordTok{data}\NormalTok{(eberg)}
\KeywordTok{coordinates}\NormalTok{(eberg) <-}\StringTok{ }\ErrorTok{~}\NormalTok{X}\OperatorTok{+}\NormalTok{Y}
\KeywordTok{proj4string}\NormalTok{(eberg) <-}\StringTok{ }\KeywordTok{CRS}\NormalTok{(}\StringTok{"+init=epsg:31467"}\NormalTok{)}
\NormalTok{ov =}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{extract}\NormalTok{(}\KeywordTok{stack}\NormalTok{(grd.lst), eberg))}
\KeywordTok{str}\NormalTok{(ov[}\KeywordTok{complete.cases}\NormalTok{(ov),])}
\end{Highlighting}
\end{Shaded}

If the raster layers can not be stacked and if each layer is available
in a different projection system, you can also create a function that
reprojects points to the target raster layer projection system:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{overlay.fun =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(i, y)\{}
\NormalTok{  raster}\OperatorTok{::}\KeywordTok{extract}\NormalTok{(}\KeywordTok{raster}\NormalTok{(i), }\DataTypeTok{na.rm=}\OtherTok{FALSE}\NormalTok{, }
      \KeywordTok{spTransform}\NormalTok{(y, }\KeywordTok{proj4string}\NormalTok{(}\KeywordTok{raster}\NormalTok{(i))))\}}
\end{Highlighting}
\end{Shaded}

which can also be run in parallel for example by using the parallel
package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ov =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{mclapply}\NormalTok{(grd.lst, }\DataTypeTok{FUN=}\NormalTok{overlay.fun, }\DataTypeTok{y=}\NormalTok{eberg))}
\KeywordTok{names}\NormalTok{(ov) =}\StringTok{ }\KeywordTok{basename}\NormalTok{(grd.lst)}
\end{Highlighting}
\end{Shaded}

In a similar way, one could also make wrapper functions that
downscale/upscale grids, then filter missing values and stack all data
together so that it becomes available in the working memory (sp grid or
pixels object). Overlay and model fitting is also implemented directly
in the GSIF package, so any attempt to fit models will automatically
perform overlay.

\hypertarget{working-with-larger-rasters}{%
\subsection{Working with large(r)
rasters}\label{working-with-larger-rasters}}

As R is often inefficient in handling large objects in memory (such as
large raster images), a good strategy to run raster processing in R is
to consider using for example the \texttt{clusterR} function from the
\href{https://cran.r-project.org/package=raster}{raster} package, which
automatically parallelizes use of raster functions. To have full control
over parallelization, you can alternatively tile large rasters using the
\texttt{getSpatialTiles} function from the GSIF package and process them
as separate objects in parallel. The following examples shows how to run
a simple function in parallel on tiles and then mosaic these tiles after
all processing has been completed. Consider for example the GeoTiff from
the rgdal package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fn =}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{"pictures/SP27GTIF.TIF"}\NormalTok{, }\DataTypeTok{package =} \StringTok{"rgdal"}\NormalTok{)}
\NormalTok{obj <-}\StringTok{ }\NormalTok{rgdal}\OperatorTok{::}\KeywordTok{GDALinfo}\NormalTok{(fn)}
\CommentTok{#> Warning in rgdal::GDALinfo(fn): statistics not supported by this driver}
\end{Highlighting}
\end{Shaded}

We can split that object in 35 tiles, each of 5 x 5 km in size by
running:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tiles <-}\StringTok{ }\NormalTok{GSIF}\OperatorTok{::}\KeywordTok{getSpatialTiles}\NormalTok{(obj, }\DataTypeTok{block.x=}\DecValTok{5000}\NormalTok{, }\DataTypeTok{return.SpatialPolygons =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{tiles.pol <-}\StringTok{ }\NormalTok{GSIF}\OperatorTok{::}\KeywordTok{getSpatialTiles}\NormalTok{(obj, }\DataTypeTok{block.x=}\DecValTok{5000}\NormalTok{, }\DataTypeTok{return.SpatialPolygons =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{tile.pol  <-}\StringTok{ }\KeywordTok{SpatialPolygonsDataFrame}\NormalTok{(tiles.pol, tiles)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{raster}\NormalTok{(fn), }\DataTypeTok{col=}\KeywordTok{bpy.colors}\NormalTok{(}\DecValTok{20}\NormalTok{))}
\KeywordTok{lines}\NormalTok{(tile.pol, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=0.8\linewidth]{figures/rplot_large_raster_tiles} 

}

\caption{Example of a tiling system derived using the `GSIF::getSpatialTiles` function.}\label{fig:rplot-large-raster-tiles}
\end{figure}

rgdal further allows us to read only a single tile of the GeoTiff by
using the \texttt{offset} and \texttt{region.dim} arguments:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x =}\StringTok{ }\KeywordTok{readGDAL}\NormalTok{(fn, }\DataTypeTok{offset=}\KeywordTok{unlist}\NormalTok{(tiles[}\DecValTok{1}\NormalTok{,}\KeywordTok{c}\NormalTok{(}\StringTok{"offset.y"}\NormalTok{,}\StringTok{"offset.x"}\NormalTok{)]),}
             \DataTypeTok{region.dim=}\KeywordTok{unlist}\NormalTok{(tiles[}\DecValTok{1}\NormalTok{,}\KeywordTok{c}\NormalTok{(}\StringTok{"region.dim.y"}\NormalTok{,}\StringTok{"region.dim.x"}\NormalTok{)]),}
             \DataTypeTok{output.dim=}\KeywordTok{unlist}\NormalTok{(tiles[}\DecValTok{1}\NormalTok{,}\KeywordTok{c}\NormalTok{(}\StringTok{"region.dim.y"}\NormalTok{,}\StringTok{"region.dim.x"}\NormalTok{)]), }\DataTypeTok{silent =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{spplot}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=0.6\linewidth]{figures/sp27gtif_tile} 

}

\caption{A tile produced for a satellite image in the example above.}\label{fig:sp27gtif-tile}
\end{figure}

We would like to run a function on this raster in parallel, for example
a simple function that converts values to 0/1 values based on a
threshold:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun_mask <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(i, tiles, }\DataTypeTok{dir=}\StringTok{"./tiled/"}\NormalTok{, }\DataTypeTok{threshold=}\DecValTok{190}\NormalTok{)\{}
\NormalTok{  out.tif =}\StringTok{ }\KeywordTok{paste0}\NormalTok{(dir, }\StringTok{"T"}\NormalTok{, i, }\StringTok{".tif"}\NormalTok{)}
  \ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{file.exists}\NormalTok{(out.tif))\{}
\NormalTok{    x =}\StringTok{ }\KeywordTok{readGDAL}\NormalTok{(fn, }\DataTypeTok{offset=}\KeywordTok{unlist}\NormalTok{(tiles[i,}\KeywordTok{c}\NormalTok{(}\StringTok{"offset.y"}\NormalTok{,}\StringTok{"offset.x"}\NormalTok{)]), }\DataTypeTok{region.dim=}\KeywordTok{unlist}\NormalTok{(tiles[i,}\KeywordTok{c}\NormalTok{(}\StringTok{"region.dim.y"}\NormalTok{,}\StringTok{"region.dim.x"}\NormalTok{)]), }\DataTypeTok{output.dim=}\KeywordTok{unlist}\NormalTok{(tiles[i,}\KeywordTok{c}\NormalTok{(}\StringTok{"region.dim.y"}\NormalTok{,}\StringTok{"region.dim.x"}\NormalTok{)]), }\DataTypeTok{silent =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{    x}\OperatorTok{$}\NormalTok{mask =}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(x}\OperatorTok{$}\NormalTok{band1}\OperatorTok{>}\NormalTok{threshold, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
    \KeywordTok{writeGDAL}\NormalTok{(x[}\StringTok{"mask"}\NormalTok{], }\DataTypeTok{type=}\StringTok{"Byte"}\NormalTok{, }\DataTypeTok{mvFlag =} \DecValTok{255}\NormalTok{, out.tif, }\DataTypeTok{options=}\KeywordTok{c}\NormalTok{(}\StringTok{"COMPRESS=DEFLATE"}\NormalTok{))}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This can now be run through \texttt{mclapply} function from the parallel
package (which automatically employs all available cores):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x0 <-}\StringTok{ }\KeywordTok{mclapply}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(tiles), }\DataTypeTok{FUN=}\NormalTok{fun_mask, }\DataTypeTok{tiles=}\NormalTok{tiles)}
\end{Highlighting}
\end{Shaded}

We can look in the the tiles folder, and this should show 35 produced
GeoTiffs. These can be further used to construct a virtual mosaic by
using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t.lst <-}\StringTok{ }\KeywordTok{list.files}\NormalTok{(}\DataTypeTok{path=}\StringTok{"extdata/tiled"}\NormalTok{, }\DataTypeTok{pattern=}\KeywordTok{glob2rx}\NormalTok{(}\StringTok{"^T*.tif$"}\NormalTok{), }\DataTypeTok{full.names=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{recursive=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{cat}\NormalTok{(t.lst, }\DataTypeTok{sep=}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\DataTypeTok{file=}\StringTok{"SP27GTIF_tiles.txt"}\NormalTok{)}
\KeywordTok{system}\NormalTok{(}\StringTok{'gdalbuildvrt -input_file_list SP27GTIF_tiles.txt SP27GTIF.vrt'}\NormalTok{)}
\KeywordTok{system}\NormalTok{(}\StringTok{'gdalwarp SP27GTIF.vrt SP27GTIF_mask.tif -ot }\CharTok{\textbackslash{}"}\StringTok{Byte}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{, }
  \StringTok{' -dstnodata 255 -co }\CharTok{\textbackslash{}"}\StringTok{BIGTIFF=YES}\CharTok{\textbackslash{}"}\StringTok{ -r }\CharTok{\textbackslash{}"}\StringTok{near}\CharTok{\textbackslash{}"}\StringTok{ -overwrite -co }\CharTok{\textbackslash{}"}\StringTok{COMPRESS=DEFLATE}\CharTok{\textbackslash{}"}\StringTok{'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note we use a few important settings here for GDAL e.g.
\texttt{-overwrite\ -co\ "COMPRESS=DEFLATE"} to overwrite the GeoTiff
and internally compress it to save space and \texttt{-r\ "near"}
basically no resampling just binding tiles together. Also, if the output
GeoTiff is HUGE, you will most likely have to turn on
\texttt{-co\ "BIGTIFF=YES"} otherwise gdalwarp would not run through.
The output mosaic looks like this:

\begin{figure}[t]

{\centering \includegraphics[width=0.8\linewidth]{figures/sp27gtif_mask} 

}

\caption{Final processed output.}\label{fig:sp27gtif-mask}
\end{figure}

This demonstrates that R can be used to compute with large rasters
provided that these operations can be parallelized. Suggested best
practice for this is to: (1) design a tiling system that optimizes use
of RAM and read/write spead of a disk, (2) prepare and test a function
that can be then run in parallel, and (3) stitch back all tiles to a
large raster using \texttt{gdalwarp}. Note that Tiling and and stitching
can not be applied universally to all problems e.g.~functions that
require global geographical search or all data in the raster, in such
cases tiling should be applied with overlap (to minimize boundary
effects) or to irregular tiling systems (e.g.~per watershed). Once an
optimal tiling system and function is prepared, R is no longer limited
to running efficient computing, but only dependant on how much RAM and
cores you have available i.e.~it becomes more a hardware than a software
problem.

\hypertarget{summary-points-1}{%
\section{Summary points}\label{summary-points-1}}

Soil covariate layers are one of the key inputs to predictive soil
mapping. Before any spatial layer can be used for modeling, it typically
needs to be preprocessed to remove artifacts, resample to a standard
resolution, fill in the missing values etc. All these operations can be
succesfully run by combining R and Open Source GIS software and by
careful programming and optimization.

Preparing soil covariates can often be time and resources consuming so
careful preparation and prioritisation of processing is highly
recommended. \citet{Hengl2017SoilGrids250m} show that, for soil types
and soil textures, DEM-parameters, i.e.~soil forming factors of relief,
especially flow-based DEM-indices, emerge as the second-most dominant
covariates. These results largely correspond with conventional soil
survey knowledge (surveyors have been using relief as a key guideline to
delineate soil bodies for decades).

Although lithology is not in the list of the top 15 most important
predictors, spatial patterns of lithologic classes can often be
distinctly recognized in the output predictions. This is especially true
for soil texture fractions and coarse fragments. In general, for
predicting soil chemical properties, climatic variables (especially
precipitation) and surface reflectance seem to be the most important,
while for soil classes and soil physical properties it is a combination
of relief, vegetation dynamics and parent material. Investing extra time
in preparing a better map of soil parent material is often a good idea.

Other potentially useful covariates for predicting soil properties and
classes could be maps of paleo i.e.~pre-historic climatic conditions of
soil formation, e.g.~glacial landscapes and processes, past climate
conditions and similar. These could likely become significant predictors
of many current soil characteristics. Information on pre-historic
climatic conditions and land use is unfortunately often not available,
especially not at detailed cartographic scales, although there are now
several global products that represent, for example, dynamics of land
use / changes of land cover (see e.g.~HYDE data set
\citep{klein2011hyde}) through the past 1500+ years. As the spatial
detail and completeness of such pre-historic maps increases, they will
become potentially interesting covariates for global soil modeling.

USA's NASA and USGS, with its MODIS, Landsat and similar
civil-applications missions will likely remain the main source of
spatial covariate data to support global and local soil mapping
initiatives.

\hypertarget{statistical-theory}{%
\chapter{Statistical theory for predictive soil
mapping}\label{statistical-theory}}

\emph{Edited by: Hengl T., Heuvelink G.B.M and MacMillan R. A.}

\hypertarget{aspects-variability}{%
\section{Aspects of spatial variability of soil
variables}\label{aspects-variability}}

In this chapter we review the statistical theory for soil mapping. We
focus on models considered most suitable for practical implementation
and use with soil profile data and gridded covariates, and we provide
the mathematical-statistical details of the selected models. We start by
revisiting some basic statistical aspects of soil mapping, and conclude
by illustrating a proposed framework for reproducible, semi-automated
mapping of soil variables using simple, real-world examples.

The code and examples are provided only for illustration. More complex
predictive modeling is described in section \ref{soilmapping-using-mla}.
To install and optimize all packages used in this chapter please refer
to section \ref{Rstudio}.

\hypertarget{modelling-soil-variability}{%
\subsection{Modelling soil
variability}\label{modelling-soil-variability}}

Soils vary spatially in a way that is often only partially understood.
The main (deterministic) causes of soil spatial variation are the
well-known causal factors --- climate, organisms, relief, parent
material and time --- but how these factors jointly shape the soil over
time is a very complex process that is (still) extremely difficult to
model mechanistically. Moreover, mechanistic modelling approaches
require large sets of input data that are realistically not available in
practice. Some initial steps have been made, notably for mechanistic
modelling of vertical soil variation (see e.g. \citet{Finke2008462},
\citet{Sommer2008480}, \citet{Minasny2008140}, and
\citet{vanwalleghem2010spatial}), but existing approaches are still
rudimentary and cannot be used for operational soil mapping. Mainstream
soil mapping therefore takes an empirical approach in which the
relationship between the soil variable of interest and causal factors
(or their proxies) is modelled statistically, using various types of
regression models. The explanatory variables used in regression are also
known as \emph{covariates} (a list of common covariates used in soil
mapping is provided in chapter \ref{soil-covs-chapter}).

Regression models explain only part of the variation (i.e.~variance) of
the soil variable of interest, because:

\begin{itemize}
\item
  \emph{The structure of the regression model does not represent the
  true mechanistic relationship between the soil and its causal
  factors}.
\item
  \emph{The regression model includes only a few of the many causal
  factors that formed the soil}.
\item
  \emph{The covariates used in regression are often only incomplete
  proxies of the true soil forming factors}.
\item
  \emph{The covariates often contain measurement errors and/or are
  measured at a much coarser scale (i.e.~support) than that of the soil
  that needs to be mapped}.
\end{itemize}

As a result, soil spatial regression models will often display a
substantial amount of residual variance, which may well be larger than
the amount of variance explained by the regression itself. The residual
variation can subsequently be analysed on spatial structure through a
variogram analysis. If there is spatial structure, then kriging the
residual and incorporating the result of this in mapping can improve the
accuracy of soil predictions \citep{hengl2007regression}.

\hypertarget{umsv}{%
\subsection{Universal model of soil variation}\label{umsv}}

From a statistical point of view, it is convenient to distinguish
between three major components of soil variation: (1) deterministic
component (trend), (2) spatially correlated component and (3) pure
noise. This is the basis of the \emph{universal model of soil variation}
{[}\citet{Burrough1998OUP};Webster2001Wiley p.133{]}:

\begin{equation}
Z({\bf{s}}) = m({\bf{s}}) + \varepsilon '({\bf{s}}) + \varepsilon ''({\bf{s}})
\label{eq:ukm}
\end{equation}

where \(\bf{s}\) is two-dimensional location, \(m({\bf{s}})\) is the
deterministic component, \(\varepsilon '({\bf{s}})\) is the spatially
correlated stochastic component and \(\varepsilon ''({\bf{s}})\) is the
pure noise (micro-scale variation and measurement error). This model was
probably first introduced by \citet{Matheron1969PhD}, and has been used
as a general framework for spatial prediction of quantities in a variety
of environmental research disciplines.

\begin{rmdnote}
The \emph{universal model of soil variation} assumes that there are
three major components of soil variation: (1) the deterministic
component (function of covariates), (2) spatially correlated component
(treated as stochastic) and (3) pure noise.
\end{rmdnote}

The universal model of soil variation model (Eq.\eqref{eq:ukm}) can be
further generalised to three-dimensional space and the spatio-temporal
domain (3D+T) by letting the variables also depend on depth and time:

\begin{equation}
Z({\bf{s}}, d, t) = m({\bf{s}}, d, t) + \varepsilon '({\bf{s}}, d, t) + \varepsilon ''({\bf{s}}, d, t)
\label{eq:ukm3DT}
\end{equation}

where \(d\) is depth expressed in meters downward from the land surface
and \(t\) is time. The deterministic component \(m\) may be further
decomposed into parts that are purely spatial, purely temporal, purely
depth-related or mixtures of all three. Space-time statistical soil
models are discussed by \citet{Grunwald2005CRCPress}, but this area of
soil mapping is still rather experimental.

In this chapter, we mainly focus on purely 2D models but also present
some theory for 3D models, while 2D+T and 3D+T models of soil variation
are significantly more complex (Fig. \ref{fig:scheme-2D-3D-maps}).

\begin{figure}[t]

{\centering \includegraphics[width=0.6\linewidth]{figures/Fig_2D_3DT_maps} 

}

\caption{Number of variogram parameters assuming an exponential model, minimum number of samples and corresponding increase in number of prediction locations for 2D, 3D, 2D+T and 3D+T models of soil variation. Here *“altitude”* refers to vertical distance from the land surface, which is in case of soil mapping often expressed as negative vertical distance from the land surface.}\label{fig:scheme-2D-3D-maps}
\end{figure}

One of the reasons why 2D+T and 3D+T models of soil variations are rare
is because there are very few point data sets that satisfy the
requirements for analysis. One national soil data set that could be
analyzed using space-time geostatistics is, for example, the Swiss
soil-monitoring network (NABO) data set \citep{JPLN:JPLN200900269}, but
even this data set does not contain complete profile descriptions
following international standards. At regional and global scales it
would be even more difficult to find enough data to fit space-time
models (and to fit 3D+T variogram models could be even more difficult).
For catchments and plots, space-time datasets of soil moisture have been
recorded and used in space-time geostatistical modelling (see e.g.
\citet{snepvangers2003soil} and \citet{jost2005analysing}).

Statistical modelling of the spatial distribution of soils requires
field observations because most statistical methods are data-driven. The
minimum recommended number of points required to fit 2D geostatistical
models, for example, is in the range 50--100 points, but this number
increases with any increase in spatial or temporal dimension (Fig.
\ref{fig:scheme-2D-3D-maps}). The Cookfarm data set for example contains
hundreds of thousands of observations, although the study area is
relatively small and there are only ca. 50 station locations
\citep{Gasch2015SPASTA}.

The deterministic and stochastic components of soil spatial variation
are separately described in more detail in subsequent sections, but
before we do this, we first address soil vertical variability and how it
can be modelled statistically.

\hypertarget{soil-depth-models}{%
\subsection{Modelling the variation of soil with
depth}\label{soil-depth-models}}

Soil properties vary with depth, in some cases much more than in the
horizontal direction. There is an increasing awareness that the vertical
dimension is important and needs to be incorporated in soil mapping. For
example, many spatial prediction models are built using ambiguous
vertical reference frames such as predicted soil property for
\emph{``top-soil''} or \emph{``A-horizon''}. Top-soil can refer to
different depths / thicknesses and so can the A-horizon range from a few
centimeters to over one meter. Hence before fitting a 2D spatial model
to soil profile data, it is a good idea to standardize values to
standard depths, otherwise soil observation depth becomes an additional
source of uncertainty. For example soil organic carbon content is
strongly controlled by soil depth, so combining values from two A
horizons one thick and the other thin, would increase the complexity of
2D soil mapping because a fraction of the variance is controlled by the
depth, which is ignored.

The concept of perfectly homogeneous soil horizons is often too
restrictive and can be better replaced with continuous representations
of soil vertical variation i.e. \emph{soil-depth functions} or curves.
Variation of soil properties with depth is typically modelled using one
of two approaches (Fig. \ref{fig:soil-depth-examples}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Continuous vertical variation} --- This assumes that soil
  variables change continuously with depth. The soil-depth relationship
  is modelled using either:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    \emph{Parametric model} --- The relationship is modelled using
    mathematical functions such as logarithmic or exponential decay
    functions.
  \item
    \emph{Non-parametric model} --- The soil property changes
    continuously but without obvious regularity with depth. Changes in
    values are modelled using locally fitted functions such as piecewise
    linear functions or splines.
  \end{enumerate}
\item
  \emph{Abrupt or stratified vertical variation} --- This assumes that
  soil horizons are distinct and homogeneous bodies of soil material and
  that soil properties are constant within horizons and change abruptly
  at boundaries between horizons.
\end{enumerate}

Combinations of the two approaches are also possible, such as the use of
exponential decay functions per soil horizon \citep{Kempen2011Geoderma}.

Parametric continuous models are chosen to reflect pedological knowledge
e.g.~knowledge of soil forming processes. For example, organic carbon
usually originates from plant production i.e.~litter or roots.
Generally, the upper layers of the soil tend to have greater organic
carbon content, which decreases continuously with depth, so that the
soil-depth relationship can be modelled with a negative-exponential
function:

\begin{equation}
{\texttt{ORC}} (d) = {\texttt{ORC}} (d_0) \cdot \exp(-\tau \cdot d)
\label{eq:SOMdepth}
\end{equation}

where \(\texttt{ORC}(d)\) is the soil organic carbon content at depth
(\(d\)), \({\texttt{ORC}} (d_0)\) is the organic carbon content at the
soil surface and \(\tau\) is the rate of decrease with depth. This model
has only two parameters that must be chosen such that model averages
over sampling horizons match those of the observations as closely as
possible. Once the model parameters have been estimated, we can easily
predict concentrations for any depth interval.

Consider for example this sample profile from Nigeria:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lon =}\StringTok{ }\FloatTok{3.90}\NormalTok{; lat =}\StringTok{ }\FloatTok{7.50}\NormalTok{; id =}\StringTok{ "ISRIC:NG0017"}\NormalTok{; FAO1988 =}\StringTok{ "LXp"} 
\NormalTok{top =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{65}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{127}\NormalTok{) }
\NormalTok{bottom =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{18}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{65}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{127}\NormalTok{, }\DecValTok{181}\NormalTok{)}
\NormalTok{ORCDRC =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{18.4}\NormalTok{, }\FloatTok{4.4}\NormalTok{, }\FloatTok{3.6}\NormalTok{, }\FloatTok{3.6}\NormalTok{, }\FloatTok{3.2}\NormalTok{, }\FloatTok{1.2}\NormalTok{)}
\NormalTok{munsell =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"7.5YR3/2"}\NormalTok{, }\StringTok{"7.5YR4/4"}\NormalTok{, }\StringTok{"2.5YR5/6"}\NormalTok{, }\StringTok{"5YR5/8"}\NormalTok{, }\StringTok{"5YR5/4"}\NormalTok{, }\StringTok{"10YR7/3"}\NormalTok{)}
\NormalTok{## prepare a SoilProfileCollection:}
\NormalTok{prof1 <-}\StringTok{ }\NormalTok{plyr}\OperatorTok{::}\KeywordTok{join}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(id, top, bottom, ORCDRC, munsell), }
         \KeywordTok{data.frame}\NormalTok{(id, lon, lat, FAO1988), }\DataTypeTok{type=}\StringTok{'inner'}\NormalTok{) }
\CommentTok{#> Joining by: id}
\NormalTok{prof1}\OperatorTok{$}\NormalTok{mdepth <-}\StringTok{ }\NormalTok{prof1}\OperatorTok{$}\NormalTok{top}\OperatorTok{+}\NormalTok{(prof1}\OperatorTok{$}\NormalTok{bottom}\OperatorTok{-}\NormalTok{prof1}\OperatorTok{$}\NormalTok{top)}\OperatorTok{/}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

we can fit a log-log model by using e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d.lm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(ORCDRC }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(mdepth), }\DataTypeTok{data=}\NormalTok{prof1, }\DataTypeTok{family=}\KeywordTok{gaussian}\NormalTok{(log))}
\KeywordTok{options}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{scipen=}\DecValTok{3}\NormalTok{, }\DataTypeTok{digits=}\DecValTok{2}\NormalTok{))}
\NormalTok{d.lm}\OperatorTok{$}\NormalTok{fitted.values}
\CommentTok{#>    1    2    3    4    5    6 }
\CommentTok{#> 18.1  6.3  3.5  2.4  1.7  1.2}
\end{Highlighting}
\end{Shaded}

which shows that the log-log fit comes relatively close to the actual
values. Another possibility would be to fit a power-law model:

\begin{equation}
{\texttt{ORC}} (d) = a \cdot d^b
\label{eq:loglog}
\end{equation}

A disadvantage of a single parametric soil property-depth model along
the entire soil profile is that these completely ignore stratigraphy and
abrupt changes at the boundaries between soil horizons. For example,
\citet{Kempen2011Geoderma} show that there are many cases where highly
contrasting layers of peat can be found buried below the surface due to
cultivation practices or holocene drift sand. The model given by
Eq.\eqref{eq:loglog} illustrated in Fig. \ref{fig:soil-depth-examples}
(left) will not be able to represent such abrupt changes.

\begin{rmdnote}
Before fitting a 2D spatial prediction model to soil profile data, it is
important to standardize values to standard depths, otherwise soil
observation depth can be an additional source of uncertainty.
\end{rmdnote}

Non-parametric soil-depth functions are more flexible and can represent
observations of soil property averages for sampling layers or horizons
more accurately. One such technique that is particularly interesting is
\emph{equal-area or mass-preserving splines}
\citep{Bishop1999Geoderma, Malone2009Geoderma} because it ensures that,
for each sampling layer (usually a soil horizon), the average of the
spline function equals the measured value for the horizon. Disadvantages
of the spline model are that it may not fit well if there are few
observations along the soil profile and that it may create unrealistic
values (through overshoots or extrapolation) in some instances, for
example near the surface. Also, mass-preserving splines cannot
accommodate discontinuities unless, of course, separate spline functions
are fitted above and below the discontinuity.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_soil_depth_examples} 

}

\caption{Vertical variation in soil carbon modelled using a logarithmic function (left) and a mass-preserving spline (right) with abrupt changes by horizon ilustrated with solid lines.}\label{fig:soil-depth-examples}
\end{figure}

To fit mass preserving splines we can use:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(aqp)}
\CommentTok{#> This is aqp 1.16-3}
\KeywordTok{library}\NormalTok{(rgdal)}
\CommentTok{#> Loading required package: sp}
\CommentTok{#> rgdal: version: 1.3-4, (SVN revision 766)}
\CommentTok{#>  Geospatial Data Abstraction Library extensions to R successfully loaded}
\CommentTok{#>  Loaded GDAL runtime: GDAL 2.2.4, released 2018/03/19}
\CommentTok{#>  Path to GDAL shared files: /usr/share/gdal}
\CommentTok{#>  GDAL binary built with GEOS: TRUE }
\CommentTok{#>  Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493]}
\CommentTok{#>  Path to PROJ.4 shared files: (autodetected)}
\CommentTok{#>  Linking to sp version: 1.3-1}
\KeywordTok{library}\NormalTok{(GSIF)}
\CommentTok{#> GSIF version 0.5-4 (2017-04-25)}
\CommentTok{#> URL: http://gsif.r-forge.r-project.org/}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'GSIF'}
\CommentTok{#> The following object is masked _by_ '.GlobalEnv':}
\CommentTok{#> }
\CommentTok{#>     munsell}
\NormalTok{prof1.spc <-}\StringTok{ }\NormalTok{prof1}
\KeywordTok{depths}\NormalTok{(prof1.spc) <-}\StringTok{ }\NormalTok{id }\OperatorTok{~}\StringTok{ }\NormalTok{top }\OperatorTok{+}\StringTok{ }\NormalTok{bottom}
\CommentTok{#> Warning: converting IDs from factor to character}
\KeywordTok{site}\NormalTok{(prof1.spc) <-}\StringTok{ }\ErrorTok{~}\StringTok{ }\NormalTok{lon }\OperatorTok{+}\StringTok{ }\NormalTok{lat }\OperatorTok{+}\StringTok{ }\NormalTok{FAO1988 }
\KeywordTok{coordinates}\NormalTok{(prof1.spc) <-}\StringTok{ }\ErrorTok{~}\StringTok{ }\NormalTok{lon }\OperatorTok{+}\StringTok{ }\NormalTok{lat}
\KeywordTok{proj4string}\NormalTok{(prof1.spc) <-}\StringTok{ }\KeywordTok{CRS}\NormalTok{(}\StringTok{"+proj=longlat +datum=WGS84"}\NormalTok{)}
\NormalTok{## fit a spline:}
\NormalTok{ORCDRC.s <-}\StringTok{ }\KeywordTok{mpspline}\NormalTok{(prof1.spc, }\DataTypeTok{var.name=}\StringTok{"ORCDRC"}\NormalTok{)}
\CommentTok{#> Fitting mass preserving splines per profile...}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\NormalTok{ORCDRC.s}\OperatorTok{$}\NormalTok{var.std}
\CommentTok{#>   0-5 cm 5-15 cm 15-30 cm 30-60 cm 60-100 cm 100-200 cm soil depth}
\CommentTok{#> 1     21      17      7.3      3.3       3.6        1.8        181}
\end{Highlighting}
\end{Shaded}

where \texttt{var.std} shows average fitted values for standard depth
intervals (i.e.~those given in the \emph{GlobalSoilMap} specifications),
and \texttt{var.1cm} are the values fitted at 1--cm increments (Fig.
\ref{fig:soil-depth-examples}).

A disadvantage of using mathematical functions to convert soil
observations at specific depth intervals to continuous values along the
whole profile is that these values are only estimates with associated
estimation errors. If estimates are treated as if these were
observations then an important source of error is ignored, which may
jeopardize the quality of the final soil predictions and in particular
the associated uncertainty (see further Section
\ref{accuracy-assessment}). This problem can be avoided by taking, for
example, a 3D modelling approach
\citep{poggio2014national, Hengl2015AfSoilGrids250m}, in which model
calibration and spatial interpolation are based on the original soil
observations directly (although proper use of this requires that the
differences in vertical support between measurements are taken into
account also). We will address this also in later sections of this
chapter, among others in Section \ref{prediction-3D}.

\begin{rmdnote}
Soil property-depth relationships are commonly modelled using various
types of mathematical functions. Mass-preserving splines, which ensure
that the average of the spline function equals the measured value for
each sampling layer or horizon, can be used to convert measurements per
layer to point values along the profile. Because soils can show both
abrupt and continuous transitions within the same profile, no simple
spline model is universally valid and case-dependent adjustments often
need to be made.
\end{rmdnote}

\hypertarget{vertical-aggregation}{%
\subsection{Vertical aggregation of soil
properties}\label{vertical-aggregation}}

As mentioned previously, soil variables refer to aggregate values over
specific depth intervals (see Fig. \ref{fig:soil-depth-examples}). For
example, the organic carbon content is typically observed per soil
horizon with values in e.g.~g/kg or permilles
\citep{Conant2010, Rainer2010, Panagos2013439}. The \emph{Soil Organic
Carbon Storage} (or \emph{Soil Organic Carbon Stock}) in the whole
profile can be calculated by using Eq \eqref{eq:ocs}. Once we have
determined soil organic carbon storage (\(\mathtt{OCS}\)) per horizon,
we can derive the total organic carbon in the soil by summing over all
(\(H\)) horizons:

\begin{equation}
\mathtt{OCS} = \sum\limits_{h = 1}^H { \mathtt{OCS}_h }
\label{eq:ORGCsum}
\end{equation}

Obviously, the horizon-specific soil organic carbon content
(\(\mathtt{ORC}_h\)) and total soil organic carbon content
(\(\mathtt{OCS}\)) are NOT the same variables and need to be analysed
and mapped separately.

In the case of pH (\(\mathtt{PHI}\)) we usually do not aim at estimating
the actual mass or quantity of hydrogen ions. To represent a soil
profile with a single number, we may take a weighted mean of the
measured pH values per horizon:

\begin{equation}
\mathtt{PHI} = \sum\limits_{h = 1}^H { w_h \cdot \mathtt{PHI}_h }; \qquad \sum\limits_{h = 1}^H{w_h} = 1
\label{eq:pHmean}
\end{equation}

where the weights can be chosen proportional to the horizon thickness:

\begin{equation}
w _h  = \frac{{\mathtt{HSIZE}_h}}{\sum\limits_{h = 1}^H {{\mathtt{HSIZE}}_h}}
\end{equation}

Thus, it is important to be aware that all soil variables: (A) can be
expressed as relative (percentages) or absolute (mass / quantities)
values, and (B) refer to specific horizons or depth intervals or to the
whole soil profile.

Similar \emph{support}-effects show up in the horizontal, because soil
observations at \emph{point} locations are not the same as average or
\emph{bulk soil samples} taken by averaging a large number of point
observations on a site or plot \citep{Webster2001Wiley}.

\begin{rmdnote}
Soil variables can refer to a specific depth interval or to the whole
profile. The differences in spatial patterns between variables
representing fundamentally the same feature (e.g.~soil organic carbon in
of a specific soil horizon or soil layer and total organic carbon stock
in of the whole profile), but at different spatial and vertical support,
can be significant.
\end{rmdnote}

In order to avoid misinterpretation of the results of mapping, we
recommend that any delivered map of soil properties should specify the
support size in the vertical and lateral directions, the analysis method
(detection limit) and measurement units. Such information can be
included in the metadata and/or in any key visualization or plot.
Likewise, any end-user of soil data should specify whether estimates of
the relative or total organic carbon, aggregated or at 2D/3D point
support are required.

\hypertarget{spatial-prediction-of-soil-variables}{%
\section{Spatial prediction of soil
variables}\label{spatial-prediction-of-soil-variables}}

\hypertarget{main-principles}{%
\subsection{Main principles}\label{main-principles}}

\emph{``Pragmatically, the goal of a model is to predict, and at the
same time scientists want to incorporate their understanding of how the
world works into their models''} \citep{cressie2011statistics}. In
general terms, spatial prediction consists of the following seven steps
(Fig. \ref{fig:general-sp-process}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Select the target variable, scale (spatial resolution) and
  associated geographical region of interest};
\item
  \emph{Define a model of spatial variation for the target variable};
\item
  \emph{Prepare a sampling plan and collect samples and relevant
  explanatory variables};
\item
  \emph{Estimate the model parameters using the collected data};
\item
  \emph{Derive and apply the spatial prediction method associated with
  the selected model};
\item
  \emph{Evaluate the spatial prediction outputs and collect new data /
  run alternative models if necessary};
\item
  \emph{Use the outputs of the spatial prediction process for decision
  making and scenario testing}.
\end{enumerate}

\begin{figure}[t]

{\centering \includegraphics[width=0.85\linewidth]{figures/Fig_general_SP_process} 

}

\caption{From data to knowledge and back: the general spatial prediction scheme applicable to many environmental sciences.}\label{fig:general-sp-process}
\end{figure}

The spatial prediction process is repeated at all nodes of a grid
covering \(D\) (or a space-time domain in case of spatiotemporal
prediction) and produces three main outputs:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimates of the model parameters (e.g., regression coefficients and
  variogram parameters), i.e.~the \textbf{model};
\item
  Predictions at new locations, i.e.~a \textbf{prediction map};
\item
  Estimate of uncertainty associated with the predictions, i.e.~a
  \textbf{prediction error variance map}.
\end{enumerate}

It is clear from Fig. \ref{fig:general-sp-process} that the key steps in
the mapping procedure are: (a) \emph{choice of the sampling scheme}
(e.g. \citet{Ng2018}), (b) \emph{choice of the model of spatial
variation} (e.g. \citet{Diggle2007Springer}), and (c) \emph{choice of
the parameter estimation technique} (e.g. \citet{lark2006spatial}). When
the sampling scheme is given and cannot be changed, the focus of
optimization of the spatial prediction process is then on selecting and
fine-tuning the best performing spatial prediction method.

In a geostatistical framework, spatial prediction is estimation of
values of some target variable \(Z\) at a new location (\({\bf{s}}_0\))
given the input data:

\begin{equation}
\hat Z({\bf{s}}_0) = E\left\{ Z({\bf{s}}_0)|z({\bf{s}}_i), \; {\bf{X}}({\bf{s}}_0), \; i=1,...,n \right\}
\label{eq:sp}
\end{equation}

where the \(z({\bf{s}}_i)\) are the input set of observations of the
target variable, \({\bf{s}}_i\) is a geographical location, \(n\) is the
number of observations and \({\bf{X}}({\bf{s}}_0)\) is a list of
\emph{covariates} or explanatory variables, available at all prediction
locations within the study area of interest
(\({\bf{s}} \in \mathbb{A}\)). To emphasise that the model parameters
also influence the outcome of the prediction process, this can be made
explicit by writing \citep{cressie2011statistics}:

\begin{equation}
[Z|Y,{\bf{\theta}} ]
\label{eq:datamodel}
\end{equation}

where \(Z\) is the data, \(Y\) is the (hidden) process that we are
predicting, and \({\bf{\theta}}\) is a list of model parameters (
e.g.~trend coefficients and variogram parameters).

There are many spatial prediction methods for generating spatial
predictions from soil samples and covariate information. All differ in
the underlying statistical model of spatial variation, although this
model is not always made explicit and different methods may use the same
statistical model. A review of currently used digital soil mapping
methods is given, for example, in \citet{McBratney2011HSS}, while the
most extensive review can be found in \citet{McBratney2003Geoderma} and
\citet{mcbratney2018pedometrics}. \citet{LiHeap2010EI} list 40+ spatial
prediction / spatial interpolation techniques. Many spatial prediction
methods are often just different names for essentially the same thing.\\
What is often known under a single name, in the statistical, or
mathematical literature, can be implemented through different
computational frameworks, and lead to different outputs (mainly because
many models are not written out in the finest detail and leave
flexibility for actual implementation).

\hypertarget{soil-sampling}{%
\subsection{Soil sampling}\label{soil-sampling}}

A \emph{soil sample} is a collection of field observations, usually
represented as points. Statistical aspects of sampling methods and
approaches are discussed in detail by
\citet{schabenberger2005statistical} and \citet{deGruijter2006sampling},
while some more practical suggestions for soil sampling can be found in
\citet{pansu2001soil} \citet{Webster2001Wiley}, \citet{tan2005soil}, and
\citet{Legros2006SP}. Some general recommendations for soil sampling
are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Points need to cover the entire geographical area of interest
  and not overrepresent specific subareas that have much different
  characteristics than the main area.}
\item
  \emph{Soil observations at point locations should be made using
  consistent measurement methods. Replicates should ideally be taken to
  quantify the measurement error.}
\item
  \emph{Bulk sampling is recommended when short-distance spatial
  variation is expected to be large and not of interest to the map
  user.}
\item
  \emph{If a variogram is to be estimated then the sample size should be
  \textgreater{}50 and there should be sufficient point pairs with small
  separation distances.}
\item
  \emph{If trend coefficients are to be estimated then the covariates at
  sampling points should cover the entire feature space of each
  covariate.}
\end{enumerate}

The sampling design or rationale used to decide where to locate soil
profile observations, or sampling points, is often not clear and may
vary from case to case. Therefore, there is no guarantee that available
legacy point data used as input to geostatistical modelling will satisfy
the recommendations listed above. Many of the legacy profile data
locations in the world were selected using convenience sampling. In
fact, many points in traditional soil surveys may have been selected and
sampled to capture information about unusual conditions or to locate
boundaries at points of transition and maximum confusion about soil
properties \citep{Legros2006SP}. Once a soil becomes recognized as being
widely distributed and dominant in the landscape, field surveyors often
choose not to record observations when that soil is encountered,
preferring to focus instead on recording unusual sites or areas where
soil transition occurs. Thus the population of available soil point
observations may not be representative of the true population of soils,
with some soils being either over or under-represented.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_eberg_sampling_locs} 

}

\caption{Occurrence probabilities derived for the actual sampling locations (left), and for a purely random sample design with exactly the same number of points (right). Probabilities derived using the `spsample.prob` function from the package. The shaded area on the left indicates which areas (in the environmental space) have been systematically represented, while the white colour indicates areas which have been systematically omitted (and which is not by chance).}\label{fig:eberg-sampling-locs}
\end{figure}

Fig. \ref{fig:eberg-sampling-locs} (the Ebergötzen study area)
illustrates a problem of dealing with clustered samples and omission of
environmental features. Using the actual samples shown in the plot on
the left of Fig. \ref{fig:eberg-sampling-locs} we would like to map the
whole area inside the rectangle. This is technically possible, but the
user should be aware that the actual Ebergötzen points systematically
miss sampling some environmental features: in this case natural forests
/ rolling hills that were not of interest to the survey project. This
does not mean that the Ebergötzen point data are not applicable for
geostatistical analyses. It simply means that the sampling bias and
under-representation of specific environmental conditions will lead to
spatial predictions that may be biased and highly uncertain under these
conditions \citep{Brus2007Geoderma}.

\hypertarget{sec:expertsystems}{%
\subsection{Knowledge-driven soil mapping}\label{sec:expertsystems}}

As mentioned previously in section \ref{tacit-knowledge},
knowledge-driven mapping is often based on unstated and unformalized
rules and understanding that exists mainly in the minds and memories of
the individual soil surveyors who conducted field studies and mapping.
Expert, or knowledge-based, information can be converted to mapping
algorithms by applying conceptual rules to decision trees and/or
statistical models \citep{MacMillan2005CJSS, Walter2006DSS, Liu2009}.
For example, a surveyor can define the classification rules
subjectively, i.e.~based on his/her knowledge of the area, then
iteratively adjust the model until the output maps fit his/her
expectation of the distribution of soils.

In areas where few, or no, field observations of soil properties are
available, the most common way to produce estimates is to rely on expert
knowledge, or to base estimates on data from other, similar areas. This
is a kind of \emph{`knowledge transfer'} system. The best example of a
knowledge transfer system is the concept of \emph{soil series} in the
USA \citep{Simonson1968AA}. Soil series (+phases) are the lowest (most
detailed) level classes of soil types typically mapped. Each soil series
should consist of pedons having soil horizons that are similar in
colour, texture, structure, pH, consistence, mineral and chemical
composition, and arrangement in the soil profile.

If one finds the same type of soil series repeatedly at similar
locations, then there is little need to sample the soil again at
additional, similar, locations and, consequently, soil survey field
costs can be reduced. This sounds like an attractive approach because
one can minimize the survey costs by focusing on delineating the
distribution of soil series only. The problem is that there are
\textgreater{}15,000 soil series in the USA \citep{Smith1986SMSS}, which
obviously means that it is not easy to recognize the same soil series
just by doing rapid field observations. In addition, the accuracy with
which one can consistently recognize a soil series may well fail on
standard kappa statistics tests, indicating that there may be
substantial confusion between soil series (e.g.~large measurement
error).

Large parts of the world basically contain very few (sparce) field
records and hence one will need to \emph{improvise} to be able to
produce soil predictions. One idea to map such areas is to build
attribute tables for representative soil types, then map the
distribution of these soil types in areas without using local field
samples. \citet{Mallavan2010PSS} refer to soil classes that can be
predicted far away from the actual sampling locations as
\emph{homosoils}. The homosoils concept is based on the assumption that
locations that share similar environments (e.g.~soil-forming factors)
are likely to exhibit similar soils and soil properties also.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_cross_section_catena} 

}

\caption{Landform positions and location of a prediction point for the Maungawhau data set.}\label{fig:cross-section-catena}
\end{figure}

Expert-based systems also rely on using standard mapping paradigms such
as the concept of relating soil series occurrance to landscape position
along a toposequence, or catena . Fig. \ref{fig:cross-section-catena},
for example, shows a cross-section derived using the elevation data in
Fig. \ref{fig:catena-maungawhau-3d}. An experienced soil surveyor would
visit the area and attempt to produce a diagram showing a sequence of
soil types positioned along this cross-section. This expert knowledge
can be subsequently utilized as manual mapping rules, provided that it
is representative of the area, that it can be formalized through
repeatable procedures and that it can be tested using real observations.

\begin{figure}[t]

{\centering \includegraphics[width=0.8\linewidth]{figures/Fig_catena_Maungawhau_A} 

}

\caption{A cross-section for the Maungawhau volcano dataset commonly used in R to illustrate DEM and image analysis techniques.}\label{fig:catena-maungawhau-3d}
\end{figure}

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_catena_Maungawhau_B} 

}

\caption{Associated values of DEM-based covariates: TWI — Topographic Wetness Index and Valley depth for the cross-section from the previous figure.}\label{fig:catena-maungawhau}
\end{figure}

If relevant auxiliary information, such as a Digital Elevation Model
(DEM), is available for the study area, one can derive a number of DEM
parameters that can help to quantify landforms and geomorphological
processes. Landforms can also automatically be classified by computing
various DEM parameters per pixel, or by using knowledge from, Fig.
\ref{fig:catena-maungawhau} (a sample of the study area) to objectively
extract landforms and associated soils in an area. Such auxiliary
landform information can be informative about the spatial distribution
of the soil, which is the key principle of, for example, the SOTER
methodology \citep{VanEngelen2012}.

The mapping process of knowledge-driven soil mapping can be summarized
as follows \citep{MacMillan2005CJSS, MacMillan2010DSM}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Sample the study area using transects oriented along topographic
  cross-sections};
\item
  \emph{Assign soil types to each landform position and at each sample
  location};
\item
  \emph{Derive DEM parameters and other auxiliary data sets};
\item
  \emph{Develop (fuzzy) rules relating the distribution of soil classes
  to the auxiliary (mainly topographic) variables};
\item
  \emph{Implement (fuzzy) rules to allocate soil classes (or compute
  class probabi;ities) for each grid location};
\item
  \emph{Generate soil property values for each soil class using
  representative observations (class centers)};
\item
  \emph{Estimate values of the target soil variable at each grid
  location using a weighted average of allocated soil class or
  membership values and central soil property values for each soil
  class};
\end{enumerate}

In mathematical terms, soil property prediction based on fuzzy soil
classification values using the SOLIM approach \citeauthor{Zhu2001}
\citetext{\citeyear{Zhu2001}; \citealp{Zhu2010Geoderma}} works as
follows:

\begin{equation}
\begin{aligned}
 \hat z({\bf{s}}_0) = \sum\limits_{c_j = 1}^{c_p} {\nu _{c_j} ({\bf{s}}_0) \cdot z_{c_j} }; & \hspace{.6cm}
 \sum\limits_{c_j = 1}^{c_p} {\nu _j ({\bf{s}}_0)}  = 1\end{aligned}
\label{eq:solim}
\end{equation}

where \(\hat z({\bf{s}}_0)\) is the predicted soil attribute at
\({\bf{s}}_0\), \(\nu _{c_j} ({\bf{s}}_0)\) is the membership value of
class \(c_j\) at location \({\bf{s}}_0\), and \(z_{c_j}\) is the modal
(or best representative) value of the inferred soil attribute of the
\(c_j\)-th category. The predicted soil attribute is mapped directly
from membership maps using a linear additive weighing function. Consider
the example of six soil classes \texttt{A}, \texttt{B}, \texttt{C},
\texttt{D}, \texttt{E} and \texttt{F}. The attribute table indicates
that soil type \texttt{A} has 10\%, \texttt{B} 10\%, \texttt{C} 30\%,
\texttt{D} 40\%, \texttt{E} 25\%, and \texttt{F} 35\% of clay. If the
membership values at a grid position are 0.6, 0.2, 0.1, 0.05, 0.00 and
0.00, then Eq.\eqref{eq:solim} predicts the clay content as 13.5\%.

It is obvious from this work flow that the critical aspects that
determine the accuracy of the final predictions are the selection of
where we locate the cross-sections and the \emph{representative soil
profiles} and the strength of the relationship between the resulting
soil classes and target soil properties. \citet{Qi2006Geoderma}, for
example, recommended that the most representative values for soil
classes can be identified, if many soil profiles are available, by
finding the sampling location that occurs at the grid cell with highest
similarity value for a particular soil class. Soil mappers are now
increasingly looking for ways to combine expert systems with statistical
data mining and regression modelling techniques.

One problem of using a supervised mapping system, as described above, is
that it is difficult to get an objective estimate of the prediction
error (or at least a robust statistical theory for this has not yet been
developed). The only possibility to assess the accuracy of such maps
would be to collect independent validation samples and estimate the
mapping accuracy following the methods described in section
\ref{accuracy-assessment}. So, in fact, expert-based systems also depend
on statistical sampling and inference for evaluation of the accuracy of
the resulting map.

\hypertarget{regression-kriging}{%
\subsection{Geostatistics-driven soil mapping (pedometric
mapping)}\label{regression-kriging}}

Pedometric mapping is based on using statistical models to predict soil
properties, which leads us to the field of geostatistics. Geostatistics
treats the soil as a realization of a \emph{random process}
\citep{Webster2001Wiley}. It uses the point observations and gridded
covariates to predict the random process at unobserved locations, which
yields conditional probability distributions, whose spread
(i.e.~standard deviation, width of prediction intervals) explicitly
characterizes the uncertainty associated with the predictions. As
mentioned previously in section \ref{pedometric-mapping}, geostatistics
is a data-driven approach to soil mapping in which georeferenced point
samples are the key input to map production.

Traditional geostatistics has basically been identified with various
ways of variogram modeling and kriging \citep{Haining2010GEAN780}.
Contemporary geostatistics extends linear models and plain kriging
techniques to non-linear and hybrid models; it also extends purely
spatial models (2D) to 3D and space-time models
\citep{schabenberger2005statistical, Bivand2008Springer, Diggle2007Springer, cressie2011statistics}.
Implementation of more sophisticated geostatistical models for soil
mapping is an ongoing activity and is quite challenging
(computationally), especially in the case of fine-resolution mapping of
large areas \citep{Hengl2017SoilGrids250m}.

Note also that geostatistical mapping is often restricted to
quantitative soil properties. Soil prediction models that predict
categorical soil variables such as soil type or soil colour class are
often quite complex (see e.g. \citet{Hengl2007Geoderma} and
\citet{Kempen2009Geoderma} for a discussion). Most large scale soil
mapping projects also require predictions in 3D, or at least 2D
predictions (layers) for several depth intervals. This can be done by
treating each layer separately in a 2D analysis, possibly by taking
vertical correlations into account, but also by direct 3D geostatistical
modelling. Both approaches are reviewed in the following sections.

Over the last decade statisticians have recommended using
\emph{model-based geostatistics} as the most reliable framework for
spatial predictions. The essence of model-based statistics is that
\emph{``the statistical methods are derived by applying general
principles of statistical inference based on an explicitly declared
stochastic model of the data generating mechanism''}
\citep{Diggle2007Springer, Brown2014JSS}. This avoids \emph{ad hoc},
heuristic solution methods and has the advantage that it yields generic
and portable solutions. Some examples of diverse geostatistical models
are given in \citet{Brown2014JSS}.

The basic geostatistical model treats the soil property of interest as
the sum of a deterministic trend and a stochastic residual
(Eq.\eqref{eq:ukm}):

\begin{equation}
Z({\bf{s}}) = m({\bf{s}}) + \varepsilon({\bf{s}})
\label{eq:ukm-gstat}
\end{equation}

where \(\varepsilon\) and hence \(Z\) are normally distributed
stochastic processes. This is the same model as that presented in
Eq.\eqref{eq:ukm}, with in this case
\(\varepsilon = \varepsilon ' + \varepsilon ''\) being the sum of the
spatially correlated and spatially uncorrelated stochastic components.
The mean of \(\varepsilon\) is taken to be zero. Note that we use
capital letter \(Z\) because we use a probabilistic model, i.e.~we treat
the soil property as an outcome of a stochastic process and define a
model of that stochastic process. Ideally, the spatial variation of the
stochastic residual of Eq.\eqref{eq:ukm-gstat} is much less than that of
the dependent variable.

When the assumption of normality is not realistic, such as when the
frequency distribution of the residuals at observation locations is very
skewed, the easiest solution is to take a Transformed Gaussian approach
\citep[ \$\S\$3.8]{Diggle2007Springer} in which the Gaussian
geostatistical model is formulated for a transformation of the dependent
variable ( e.g.~logarithmic, logit, square root, Box-Cox transform). A
more advanced approach would drop the normal distribution approach
entirely and assume a \emph{Generalized Linear Geostatistical Model}
\citep{Diggle2007Springer, Brown2014JSS} but this complicates the
statistical analysis and prediction process dramatically. The
Transformed Gaussian approach is nearly as simple as the Gaussian
approach although the back-transformation requires attention, especially
when the spatial prediction includes a change of support (leading to
block kriging). If this is the case, it may be necessary to use a
stochastic simulation approach and derive the predictions and associated
uncertainty (i.e.~the conditional probability distribution) using
numerical simulations.

Model-based geostatistics is based on using an explicitly declared
stochastic model of the data generating mechanism. One basic
geostatistical model of soil variation is to treat the soil property of
interest as the sum of a deterministic trend (modelled via some
regression function) and a zero-mean stochastic residual.

The trend part of Eq.\eqref{eq:ukm-gstat} (i.e. \(m\)) can take many
forms. In the simplest case it would be a constant but usually it is
taken as some function of known, exhaustively available covariates. This
is where soil mapping can benefit from other sources of information and
can implement Jenny's \emph{State Factor Model of soil formation}
\citep{Jenny1968, jenny1994factors, Heuvelink2001Geoderma, McBratney2011HSS},
which has been known from the time of Dokuchaev
\citep{Florinsky2012Dokuchaev}. The covariates are often maps of
environmental properties that are known to be related to the soil
property of interest (e.g.~elevation, land cover, geology) but could
also be the outcome of a mechanistic soil process model (such as a soil
acidification model, a soil nutrient leaching model or a soil genesis
model). In the case of the latter one might opt for taking \(m\) equal
to the output of the deterministic model, but when the covariates are
related environmental properties one must define a structure for \(m\)
and introduce parameters to be estimated from paired observations of the
soil property and covariates. One of the simplest approaches is to use
\emph{multiple linear regression} to predict values at some new location
\({\bf{s}}_0\) \citep{kutner2005applied}:

\begin{equation}
m({\bf{s}}_0 ) = \sum\limits_{j = 0}^p { \beta _j \cdot X_j ({\bf{s}}_0 )}
\label{eq:MRK2D}
\end{equation}

where \(\beta _j\) are the regression model coefficients, \(\beta _0\)
is the intercept, \(j=1,\ldots,p\) are \emph{covariates} or explanatory
variables (available at all locations within the study area of interest
\(\mathbb{A}\)), and \(p\) is the number of covariates.
Eq.\eqref{eq:MRK2D} can also include categorical covariates (e.g.~maps of
land cover, geology, soil type) by representing these by as many binary
dummy variables as there are categories (minus one, to be precise, since
an intercept is included in the model). In addition, transformed
covariates may also be included or interactions between covariates. The
latter is achieved by extending the set of covariates with products or
other mixtures of covariates. However, note that this will dramatically
increase the number of covariates. The risk of considering a large
number of covariates is that it may become difficult to obtain reliable
estimates of the regression coefficients. Also one may run the risk of
\emph{multicollinearity} --- the property of covariates being mutually
strongly correlated (as indicated by \citet{Jenny1968} already in
\citeyearpar{Jenny1968}).

The advantage of Eq.\eqref{eq:MRK2D} is that it is linear in the unknown
coefficients, which makes their estimation relatively straightforward
and also permits derivation of the uncertainty about the regression
coefficients (\(\beta\)). However, in many practical cases, the linear
formulation may be too restrictive and that is why alternative
structures have been extensively developed to establish the relationship
between the dependent and covariates. Examples of these so-called
\emph{`statistical learning'} and/or \emph{`machine learning'}
approaches are:

\begin{itemize}
\item
  \emph{artificial neural networks} \citep{yegnanarayana2004artificial},
\item
  \emph{classification and regression trees}
  \citep{breiman1993classification},
\item
  \emph{support vector machines} \citep{hearst1998support},
\item
  \emph{computer-based expert systems},
\item
  \emph{random forests}
  \citep{breiman2001random, meinshausen2006quantile},
\end{itemize}

Statistical treatment of many of these methods is given in
\citet{hastie2009elements}. Care needs to be taken when using machine
learning techniques, such as random forest, because such techniques are
more sensitive to noise and blunders in the data .

Most methods listed above require appropriate levels of expertise to
avoid pitfalls and incorrect use but, when feasible and used properly,
these methods should extract maximal information about the target
variable from the covariates \citep{Statnikov2008, kanevski2009machine}.

The trend (\(m\)) relates covariates to soil properties and for this it
uses a soil-environment correlation model --- the so-called \emph{CLORPT
model}, which was formulated by Jenny in 1941 (a
\citeyearpar{jenny1994factors} reprint from that book is also
available). \citet{McBratney2003Geoderma} further formulated an
extension of the CLORPT model known as the \emph{``SCORPAN''} model.

The CLORPT model may be written as
\citep{jenny1994factors, Florinsky2012Dokuchaev}:

\begin{equation}
S = f (cl, o, r, p, t)
\label{eq:clorpt}
\end{equation}

where \(S\) stands for soil (properties and classes), \(cl\) for
climate, \(o\) for organisms (including humans), \(r\) is relief, \(p\)
is parent material or geology and \(t\) is time. In other words, we can
assume that the distribution of both soil and vegetation (at least in a
natural system) can be at least partially explained by environmental
conditions. Eq.\eqref{eq:clorpt} suggests that soil is a result of
environmental factors, while in reality there are many feedbacks and
soil, in turn, influences many of the factors on the right-hand side of
Eq.\eqref{eq:clorpt}, such as \(cl\), \(o\) and \(r\).

Uncertainty about the estimation errors of model coefficients can fairly
easily be taken into account in the subsequent prediction analysis if
the model is linear in the coefficients, such as in Eq.\eqref{eq:MRK2D}.
In this book we therefore restrict ourselves to this case but allow that
the \(X_j\)'s in Eq.\eqref{eq:MRK2D} are derived in various ways.

Since the stochastic residual of Eq.\eqref{eq:ukm-gstat} is normally
distributed and has zero mean, only its variance-covariance remains to
be specified:

\begin{equation}
C\left[Z({\bf{s}}),Z({\bf{s}}+\bf{h})\right] = \sigma (\bf{s}) \cdot \sigma(\bf{s}+\bf{h}) \cdot \rho (\bf{h})
\end{equation}

where \({\bf{h}}\) is the separation distance between two locations.
Note that here we assumed that the correlation function \(\rho\) is
invariant to geographic translation (i.e., it only depends on the
distance \(\bf{h}\) between locations and not on the locations
themselves). If in addition the standard deviation \(\sigma\) would be
spatially invariant then \(C\) would be \emph{second-order stationary}.
These type of simplifying assumptions are needed to be able to estimate
the variance-covariance structure of \(C\) from the observations. If the
standard deviation is allowed to vary with location, then it could be
defined in a similar way as in Eq.\eqref{eq:MRK2D}. The correlation
function \(\rho\) would be parameterised to a common form
(e.g.~exponential, spherical, Matérn), thus ensuring that the model is
statistically valid and \emph{positive-definite}. It is also quite
common to assume isotropy, meaning that two-dimensional geographic
distance \({\bf{h}}\) can be reduced to one-dimensional Euclidean
distance \(h\).

Once the model has been defined, its parameters must be estimated from
the data. These are the regression coefficients of the trend (when
applicable) and the parameters of the variance-covariance structure of
the stochastic residual. Commonly used estimation methods are least
squares and maximum likelihood. Both methods have been extensively
described in the literature (e.g. \citet{Webster2001Wiley} and
\citet{Diggle2007Springer}). More complex trend models may also use the
same techniques to estimate their parameters, although they might also
need to rely on more complex parameter estimation methods such as
genetic algorithms and \emph{simulated annealing}
\citep{lark2003fitting}.

\begin{rmdnote}
Spatial prediction under the linear Gaussian model with a trend boils
down to \emph{regression-kriging} when the trend coefficients are
determined prior to kriging i.e.~to \emph{universal kriging} or
\emph{kriging with external drift} when they are estimated together with
kriging weights. Both computational approaches --- regression-kriging,
kriging with external drift or universal kriging --- yield exactly the
same predictions if run using the same inputs and assuming the same
(global) geostatistical model {[}@hengl2007regression{]}.
\end{rmdnote}

The optimal spatial prediction in the case of a model
Eq.\eqref{eq:ukm-gstat} with a linear trend Eq.\eqref{eq:MRK2D} and a
normally distributed residual is given by the well-kown \emph{Best
Linear Unbiased Predictor} (BLUP):

\[\label{E:BLUP}
\hat z({{\bf{s}}_0}) = {\bf{X}}_{\bf{0}}^{\bf{T}}\cdot \hat{\bf{\beta}} + \hat{\bf{\lambda}}_{\bf{0}}^{\bf{T}}\cdot({\bf{z}} - {\bf{X}}\cdot \hat{\bf{\beta}} )\]

where the regression coefficients and kriging weights are estimated
using:

\begin{equation}
\begin{aligned}
\hat{\bf{\beta}}  &= {\left( {{{\bf{X}}^{\bf{T}}}\cdot{{\bf{C}}^{ - {\bf{1}}}}\cdot{\bf{X}}} \right)^{ - {\bf{1}}}}\cdot{{\bf{X}}^{\bf{T}}}\cdot{{\bf{C}}^{ - {\bf{1}}}}\cdot{\bf{z}} \\
\hat{\bf{\lambda}}_{\bf{0}} &= \bf{C}^{ - {\bf{1}}} \cdot {\bf{c}}_{\bf{0}} \notag\end{aligned}
\label{eq:betas}
\end{equation}

and where \({\bf{X}}\) is the matrix of \(p\) predictors at the \(n\)
sampling locations, \(\hat{\bf{\beta}}\) is the vector of estimated
regression coefficients, \(\bf{C}\) is the \(n\)\(n\)
variance-covariance matrix of residuals, \(\bf{c}_{\bf{0}}\) is the
vector of \(n\)\(1\) covariances at the prediction location, and
\(\bf{\lambda}_{\bf{0}}\) is the vector of \(n\) kriging weights used to
interpolate the residuals. Derivation of BLUP for spatial data can be
found in many standard statistical books e.g. \citet{Stein1999Springer},
\citet[p.277]{Christensen2001Springer},
\citet[p.425--430]{Venables2002Springer} and/or
\citet{schabenberger2005statistical}.

Any form of kriging computes the conditional distribution of
\(Z({\bf{s}}_0)\) at an unobserved location \({\bf{s}}_0\) from the
observations \(z({\bf{s}}_1 )\),
\(z({\bf{s}}_2 ), \ldots , z({\bf{s}}_n )\) and the covariates
\({\bf{X}}({\bf{s}}_0)\) (matrix of size \(p \times n\)). From a
statistical perspective this is straightforward for the case of a linear
model and normally distributed residuals. However, solving large
matrices and more sophisticated model fitting algorithms such as
restricted maximum likelihood can take a significant amount of time if
the number of observations is large and/or the prediction grid dense.
Pragmatic approaches to addressing constraints imposed by large data
sets are to constrain the observation data set to local neighbourhoods
or to take a multiscale nested approach.

Kriging not only yields optimal predictions but also quantifies the
prediction error with the kriging standard deviation. Prediction
intervals can be computed easily because the prediction errors are
normally distributed. Alternatively, uncertainty in spatial predictions
can also be quantified with spatial stochastic simulation. While kriging
yields the \emph{`optimal'} prediction of the soil property at any one
location, spatial stochastic simulation yields a series of possible
values by sampling from the conditional probability distribution. In
this way a large number of \emph{`realizations'} can be generated, which
can be useful when the resulting map needs to be back-transformed or
when it is used in a spatial uncertainty propagation analysis. Spatial
stochastic simulation of the linear Gaussian model can be done using a
technique known as sequential Gaussian simulation
\citep{Goovaerts1997Oxford, Yamamoto2008}. It is not, in principal, more
difficult than kriging but it is certainly numerically more demanding
i.e. takes significantly more time to compute.

\hypertarget{RK-generic}{%
\subsection{Regression-kriging (generic model)}\label{RK-generic}}

Ignoring the assumptions about the cross-correlation between the trend
and residual components, we can extend the regression-kriging model and
use any type of (non-linear) regression to predict values (
e.g.~regression trees, artificial neural networks and other machine
learning models), calculate residuals at observation locations, fit a
variogram for these residuals, interpolate the residuals using ordinary
or simple kriging, and add the result to the predicted regression part.
This means that RK can, in general, be formulated as:

\begin{equation}
{\rm prediction} \; = \;
\begin{matrix}
{\rm trend} \; {\rm predicted} \\
{\rm using} \; {\rm regression} \end{matrix} \; + \;
\begin{matrix}
{\rm residual} \; {\rm predicted} \\
{\rm using} \; {\rm kriging} \end{matrix}
\label{eq:RKgeneral}
\end{equation}

Again, statistical inference and prediction is relatively simple if the
stochastic residual, or a transformation thereof, may be assumed
normally distributed. Error of the regression-kriging model is likewise
a sum of the regression and the kriging model errors.

\hypertarget{spatial-prediction-using-multiple-linear-regression}{%
\subsection{Spatial Prediction using multiple linear
regression}\label{spatial-prediction-using-multiple-linear-regression}}

The predictor \(\hat Y({{\bf s}_0})\) of \(Y({{\bf s}_0})\) is typically
taken as a function of covariates and the \(Y({\bf s}_i)\) which, upon
substitution of the observations \(y({\bf s}_i)\), yields a
(deterministic) prediction \(\hat y({{\bf s}_0})\). In the case of
multiple linear regression (MLR), model assumptions state that at any
location in \(D\) the dependent variable is the sum of a linear
combination of the covariates at that location and a zero-mean normally
distributed residual. Thus, at the \(n\) observation locations we have:

\begin{equation}
{\bf Y} = {\bf X}^{{\bf T}} \cdot {\bf \beta} + {\bf \varepsilon}
\label{eq:lm}
\end{equation}

where \({\bf Y}\) is a vector of the target variable at the \(n\)
observation locations, \({\bf X}\) is an \(n \times p\) matrix of
covariates at the same locations and \({\bf \beta}\) is a vector of
\(p\) regression coefficients. The stochastic residual
\({\bf \varepsilon}\) is assumed to be independently and identically
distributed. The paired observations of the target variable and
covariates (\({\bf y}\) and \({\bf X}\)) are used to estimate the
regression coefficients using, e.g., Ordinary Least Squares
\citep{Kutner2004McGraw}:

\begin{equation}
\hat{{\bf \beta}}  = \left( {{{\bf X}}^{{\bf T}} \cdot {{\bf X}}} \right)^{ - {{\bf 1}}} \cdot
{{\bf X}}^{{\bf T}} \cdot {{\bf y}}
\label{eq:ols-betas}
\end{equation}

once the coefficients are estimated, these can be used to generate a
prediction at \({\bf s}_0\):

\begin{equation}
\hat y({\bf s}_0) = {\bf x}_0^{\bf T} \cdot {\bf \hat \beta}
\end{equation}

with associated prediction error variance:

\begin{equation}
\sigma ^2 ({\bf s}_0 ) = var\left[ \varepsilon ({\bf s}_0) \right] \cdot \left[ {1 +
{\mathbf x}_0^{\rm T}  \cdot \left(
{{\mathbf X}^{\rm T}  \cdot {\mathbf X}} \right)^{ -
{\mathbf 1}}  \cdot {\mathbf x}_0 } \right]
\label{eq:ols-sigma}
\end{equation}

here, \({\mathbf x}_0\) is a vector with covariates at the prediction
location and \(var\left[ \varepsilon ({\bf s}_0) \right]\) is the
variance of the stochastic residual. The latter is usually estimated by
the mean squared error (MSE):

\begin{equation}
{\mathrm{MSE}} = \frac{\sum\limits_{i = 1}^n {(y_i - \hat y_i)^2}}{n-p}
\end{equation}

The prediction error variance given by Eq.\eqref{eq:ols-sigma} is smallest
at prediction points where the covariate values are in the center of the
covariate (\emph{`feature'}) space and increases as predictions are made
further away from the center. They are particularly large in case of
extrapolation in feature space \citep{Kutner2004McGraw}. Note that the
model defined in Eq.\eqref{eq:lm} is a non-spatial model because the
observation locations and spatial-autocorrelation of the dependent
variable are not taken into account.

\hypertarget{universal-kriging-prediction-error}{%
\subsection{Universal kriging prediction
error}\label{universal-kriging-prediction-error}}

In the case of universal kriging, regression-kriging or Kriging with
External Drift, the prediction error is computed as
\citep{Christensen2001Springer}:

\begin{equation}
\hat \sigma _{\tt{UK}}^2 ({\bf{s}}_0 )  = (C_0  + C_1 ) - {\bf{c}}_{\bf{0}}^{\bf{T}}  \cdot {\bf{C}}^{ - {\bf{1}}}  \cdot
{\bf{c}}_{\bf{0}} + \left( {{\bf{X}}_{\bf{0}}  -
{\bf{X}}^{\bf{T}} \cdot {\bf{C}}^{ - {\bf{1}}} \cdot
{\bf{c}}_{\bf{0}} } \right)^{\bf{T}}  \cdot \left( {{\bf{X}}^{\bf{T}}
\cdot {\bf{C}}^{ - {\bf{1}}} \cdot {\bf{X}}} \right)^{{\bf{ - 1}}} \cdot \left( {{\bf{X}}_{\bf{0}}  - {\bf{X}}^{\bf{T}}  \cdot
{\bf{C}}^{ - {\bf{1}}} \cdot {\bf{c}}_{\bf{0}} } \right)
\label{eq:UKvar}
\end{equation}

where \(C_0 + C_1\) is the sill variation (variogram parameters),
\(\bf{C}\) is the covariance matrix of the residuals, and \({\bf{c}}_0\)
is the vector of covariances of residuals at the unvisited location.

Ignoring the mixed component of the prediction variance in
Eq.\eqref{eq:UKvar}, one can also derive a simplified regression-kriging
variance i.e.~as a sum of the kriging variance and the standard error of
estimating the regression mean:

\begin{equation}
\hat \sigma _{\tt{RK}}^2 ({\bf{s}}_0) = (C_0  + C_1 ) -
{\bf{c}}_{\bf{0}}^{\bf{T}}  \cdot {\bf{C}}^{ - {\bf{1}}}  \cdot
{\bf{c}}_{\bf{0}} + {\it{SEM}}^2
\label{eq:RKvar-simple}
\end{equation}

which is the general approach used in the GSIF package.

Note that there will always be a small difference between results of
Eq.\eqref{eq:UKvar} and Eq.\eqref{eq:RKvar-simple}, and this is a major
disadvantage of using the general regression-kriging framework for
spatial prediction. Although the predicted mean derived by using
regression-kriging or universal kriging approaches might not differ, the
estimate of the prediction variance using Eq.\eqref{eq:RKvar-simple} will
be suboptimal as it ignores product component. On the other hand, the
advantage of running separate regression and kriging predictions is
often worth the sacrifice as the computing time is an order of magnitude
shorter and we have more flexibility to combine different types of
regression models with kriging when regression is run separately from
kriging \citep{hengl2007regression}.

\hypertarget{regression-kriging-examples}{%
\subsection{Regression-kriging
examples}\label{regression-kriging-examples}}

The type of regression-kriging model explained in the previous section
can be implemented here by combining the (\textbf{\emph{WHAT}}) and
(\textbf{\emph{WHAT}}) packages. Consider for example the Meuse case
study:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gstat)}
\KeywordTok{demo}\NormalTok{(meuse, }\DataTypeTok{echo=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can overlay the points and grids to create the regression matrix by:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meuse.ov <-}\StringTok{ }\KeywordTok{over}\NormalTok{(meuse, meuse.grid)}
\NormalTok{meuse.ov <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(meuse), meuse.ov)}
\KeywordTok{head}\NormalTok{(meuse.ov[,}\KeywordTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{,}\StringTok{"y"}\NormalTok{,}\StringTok{"dist"}\NormalTok{,}\StringTok{"soil"}\NormalTok{,}\StringTok{"om"}\NormalTok{)])}
\CommentTok{#>        x      y   dist soil   om}
\CommentTok{#> 1 181072 333611 0.0014    1 13.6}
\CommentTok{#> 2 181025 333558 0.0122    1 14.0}
\CommentTok{#> 3 181165 333537 0.1030    1 13.0}
\CommentTok{#> 4 181298 333484 0.1901    2  8.0}
\CommentTok{#> 5 181307 333330 0.2771    2  8.7}
\CommentTok{#> 6 181390 333260 0.3641    2  7.8}
\end{Highlighting}
\end{Shaded}

which lets us fit a linear model for organic carbon as a function of
distance to river and soil type:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log1p}\NormalTok{(om)}\OperatorTok{~}\NormalTok{dist}\OperatorTok{+}\NormalTok{soil, meuse.ov)}
\KeywordTok{summary}\NormalTok{(m)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = log1p(om) ~ dist + soil, data = meuse.ov)}
\CommentTok{#> }
\CommentTok{#> Residuals:}
\CommentTok{#>     Min      1Q  Median      3Q     Max }
\CommentTok{#> -1.0831 -0.1504  0.0104  0.2098  0.5913 }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>             Estimate Std. Error t value  Pr(>|t|)    }
\CommentTok{#> (Intercept)   2.3421     0.0425   55.05   < 2e-16 ***}
\CommentTok{#> dist         -0.8009     0.1787   -4.48 0.0000147 ***}
\CommentTok{#> soil2        -0.3358     0.0702   -4.78 0.0000041 ***}
\CommentTok{#> soil3         0.0366     0.1247    0.29      0.77    }
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 0.33 on 149 degrees of freedom}
\CommentTok{#>   (2 observations deleted due to missingness)}
\CommentTok{#> Multiple R-squared:  0.384,  Adjusted R-squared:  0.371 }
\CommentTok{#> F-statistic: 30.9 on 3 and 149 DF,  p-value: 1.32e-15}
\end{Highlighting}
\end{Shaded}

Next, we can derive the regression residuals and fit a variogram:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meuse.s <-}\StringTok{ }\NormalTok{meuse[}\OperatorTok{-}\NormalTok{m}\OperatorTok{$}\NormalTok{na.action,]}
\NormalTok{meuse.s}\OperatorTok{$}\NormalTok{om.res <-}\StringTok{ }\KeywordTok{resid}\NormalTok{(m)}
\NormalTok{vr.fit <-}\StringTok{ }\KeywordTok{fit.variogram}\NormalTok{(}\KeywordTok{variogram}\NormalTok{(om.res}\OperatorTok{~}\DecValTok{1}\NormalTok{, meuse.s), }\KeywordTok{vgm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\StringTok{"Exp"}\NormalTok{, }\DecValTok{300}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{vr.fit}
\CommentTok{#>   model psill range}
\CommentTok{#> 1   Nug 0.048     0}
\CommentTok{#> 2   Exp 0.065   285}
\end{Highlighting}
\end{Shaded}

With this, all model parameters (four regression coefficients and three
variogram parameters) for regression-kriging have been estimated and the
model can be used to generate predictions. Note that the regression
model we fitted is significant, and the remaining residuals still show
spatial auto-correlation. The nugget variation is about
(\textbf{\emph{WHAT}}) of the sill variation.

Using the gstat package \citep{Pebesma2004CG, Bivand2013Springer},
regression and kriging can be combined by running universal kriging or
kriging with external drift \citep{hengl2007regression}. First, the
variogram of the residuals is calculated:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{v.s <-}\StringTok{ }\KeywordTok{variogram}\NormalTok{(}\KeywordTok{log1p}\NormalTok{(om)}\OperatorTok{~}\NormalTok{dist}\OperatorTok{+}\NormalTok{soil, meuse.s)}
\NormalTok{vr.fit <-}\StringTok{ }\KeywordTok{fit.variogram}\NormalTok{(v.s, }\KeywordTok{vgm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\StringTok{"Exp"}\NormalTok{, }\DecValTok{300}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{vr.fit}
\CommentTok{#>   model psill range}
\CommentTok{#> 1   Nug 0.048     0}
\CommentTok{#> 2   Exp 0.065   285}
\end{Highlighting}
\end{Shaded}

which gives almost the same model parameter values as the
regression-kriging above. Next, the kriging can be executed with a
single call to the generic \texttt{krige} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{om.rk <-}\StringTok{ }\KeywordTok{krige}\NormalTok{(}\KeywordTok{log1p}\NormalTok{(om)}\OperatorTok{~}\NormalTok{dist}\OperatorTok{+}\NormalTok{soil, meuse.s, meuse.grid, vr.fit)}
\CommentTok{#> [using universal kriging]}
\end{Highlighting}
\end{Shaded}

The package nlme fits the regression model and the variogram of the
residuals concurrently \citep{pinheiro2009mixed}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(nlme)}
\NormalTok{m.gls <-}\StringTok{ }\KeywordTok{gls}\NormalTok{(}\KeywordTok{log1p}\NormalTok{(om)}\OperatorTok{~}\NormalTok{dist}\OperatorTok{+}\NormalTok{soil, meuse.s, }\DataTypeTok{correlation=}\KeywordTok{corExp}\NormalTok{(}\DataTypeTok{nugget=}\OtherTok{TRUE}\NormalTok{))}
\NormalTok{m.gls}
\CommentTok{#> Generalized least squares fit by REML}
\CommentTok{#>   Model: log1p(om) ~ dist + soil }
\CommentTok{#>   Data: meuse.s }
\CommentTok{#>   Log-restricted-likelihood: -26}
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#> (Intercept)        dist       soil2       soil3 }
\CommentTok{#>       2.281      -0.623      -0.244      -0.057 }
\CommentTok{#> }
\CommentTok{#> Correlation Structure: Exponential spatial correlation}
\CommentTok{#>  Formula: ~1 }
\CommentTok{#>  Parameter estimate(s):}
\CommentTok{#>  range nugget }
\CommentTok{#>   2.00   0.07 }
\CommentTok{#> Degrees of freedom: 153 total; 149 residual}
\CommentTok{#> Residual standard error: 0.34}
\end{Highlighting}
\end{Shaded}

In this case, the regression coefficients have been estimated using
Eq.\eqref{eq:betas} i.e.~via \emph{Restricted maximum likelihood} (REML).
The advantage of fitting the regression model and spatial
autocorrelation structure concurrently is that both fits are adjusted:
the estimation of the regression coefficients is adjusted for spatial
autocorrelation of the residual and the variogram parameters are
adjusted for the adjusted trend estimate. A disadvantage of using the
nlme package is that the computational intensity increases with the size
of the data set, so for any data set \textgreater{}1000 points the
computation time can increase to tens of hours of computing. On the
other hand, coefficients fitted by REML methods might not result in
significantly better predictions. Getting the most objective estimate of
the model parameters is sometimes not worth the effort, as demonstrated
by \citet{Minasny2007Geoderma}.

Simultaneous estimation of regression coefficients and variogram
parameters and including estimation errors in regression coefficients
into account by using universal kriging / kriging with external drift is
more elegant from a statistical point of view, but there are
computational and other challenges. One of these is that it is difficult
to implement global estimation of regression coefficients with local
spatial prediction of residuals, which is a requirement in the case of
large spatial data sets. Also, the approach does not extend to more
complex non-linear trend models. In such cases, we recommend separating
trend estimation from kriging of residuals by using the
regression-kriging approach discussed above (Eq.\eqref{eq:RKgeneral}).

\hypertarget{regression-kriging-examples-using-the-gsif-package}{%
\subsection{Regression-kriging examples using the GSIF
package}\label{regression-kriging-examples-using-the-gsif-package}}

In the (\textbf{\emph{GSIF?}}) package, most of the steps described
above (regression modelling and variogram modelling) used to fit
regression-kriging models are wrapped into generic functions. A
regression-kriging model can be fitted in one step by running:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{omm <-}\StringTok{ }\KeywordTok{fit.gstatModel}\NormalTok{(meuse, }\KeywordTok{log1p}\NormalTok{(om)}\OperatorTok{~}\NormalTok{dist}\OperatorTok{+}\NormalTok{soil, meuse.grid)}
\CommentTok{#> Fitting a linear model...}
\CommentTok{#> Fitting a 2D variogram...}
\CommentTok{#> Saving an object of class 'gstatModel'...}
\KeywordTok{str}\NormalTok{(omm, }\DataTypeTok{max.level =} \DecValTok{2}\NormalTok{)}
\CommentTok{#> Formal class 'gstatModel' [package "GSIF"] with 4 slots}
\CommentTok{#>   ..@ regModel :List of 32}
\CommentTok{#>   .. ..- attr(*, "class")= chr [1:2] "glm" "lm"}
\CommentTok{#>   ..@ vgmModel :'data.frame':    2 obs. of  9 variables:}
\CommentTok{#>   .. ..- attr(*, "singular")= logi FALSE}
\CommentTok{#>   .. ..- attr(*, "SSErr")= num 0.00000107}
\CommentTok{#>   .. ..- attr(*, "call")= language gstat::fit.variogram(object = svgm, model = ivgm)}
\CommentTok{#>   ..@ svgmModel:'data.frame':    15 obs. of  6 variables:}
\CommentTok{#>   .. ..- attr(*, "direct")='data.frame': 1 obs. of  2 variables:}
\CommentTok{#>   .. ..- attr(*, "boundaries")= num [1:16] 0 106 213 319 426 ...}
\CommentTok{#>   .. ..- attr(*, "pseudo")= num 0}
\CommentTok{#>   .. ..- attr(*, "what")= chr "semivariance"}
\CommentTok{#>   ..@ sp       :Formal class 'SpatialPointsDataFrame' [package "sp"] with 5 slots}
\end{Highlighting}
\end{Shaded}

the resulting \texttt{gstatModel} class object consists of a (1)
regression component, (2) variogram model for residual, and (3) sample
variogram for plotting, (4) spatial locations of observations used to
fit the model. To predict values of organic carbon using this model, we
can run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{om.rk <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(omm, meuse.grid)}
\CommentTok{#> Subsetting observations to fit the prediction domain in 2D...}
\CommentTok{#> Generating predictions using the trend model (RK method)...}
\CommentTok{#> [using ordinary kriging]}
\CommentTok{#> }
\DecValTok{100}\NormalTok{% done}
\CommentTok{#> Running 5-fold cross validation using 'krige.cv'...}
\CommentTok{#> Creating an object of class "SpatialPredictions"}
\NormalTok{om.rk}
\CommentTok{#>   Variable           : om }
\CommentTok{#>   Minium value       : 1 }
\CommentTok{#>   Maximum value      : 17 }
\CommentTok{#>   Size               : 153 }
\CommentTok{#>   Total area         : 4964800 }
\CommentTok{#>   Total area (units) : square-m }
\CommentTok{#>   Resolution (x)     : 40 }
\CommentTok{#>   Resolution (y)     : 40 }
\CommentTok{#>   Resolution (units) : m }
\CommentTok{#>   GLM call formula   : log1p(om) ~ dist + soil }
\CommentTok{#>   Family             : gaussian }
\CommentTok{#>   Link function      : identity }
\CommentTok{#>   Vgm model          : Exp }
\CommentTok{#>   Nugget (residual)  : 0.048 }
\CommentTok{#>   Sill (residual)    : 0.065 }
\CommentTok{#>   Range (residual)   : 285 }
\CommentTok{#>   RMSE (validation)  : 2.4 }
\CommentTok{#>   Var explained      : 49.4% }
\CommentTok{#>   Effective bytes    : 295 }
\CommentTok{#>   Compression method : gzip}
\NormalTok{## back-transformation:}
\NormalTok{meuse.grid}\OperatorTok{$}\NormalTok{om.rk <-}\StringTok{ }\KeywordTok{expm1}\NormalTok{(om.rk}\OperatorTok{@}\NormalTok{predicted}\OperatorTok{$}\NormalTok{om }\OperatorTok{+}\StringTok{ }\NormalTok{om.rk}\OperatorTok{@}\NormalTok{predicted}\OperatorTok{$}\NormalTok{var1.var}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=0.85\linewidth]{figures/Fig_meuse_om_RK_vs_GLMK} 

}

\caption{Predictions of organic carbon in percent (top soil) for the Meuse data set derived using regression-kriging with transformed values, GLM-kriging, regression tress (rpart) and random forest models combined with kriging. The percentages in brackets indicates amount of variation explained by the models.}\label{fig:meuse-om-rk-glm}
\end{figure}

We could also have opted for fitting a GLM with a link function, which
would look like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{omm2 <-}\StringTok{ }\KeywordTok{fit.gstatModel}\NormalTok{(meuse, om}\OperatorTok{~}\NormalTok{dist}\OperatorTok{+}\NormalTok{soil, meuse.grid, }\DataTypeTok{family=}\KeywordTok{gaussian}\NormalTok{(}\DataTypeTok{link=}\NormalTok{log))}
\CommentTok{#> Fitting a linear model...}
\CommentTok{#> Fitting a 2D variogram...}
\CommentTok{#> Saving an object of class 'gstatModel'...}
\KeywordTok{summary}\NormalTok{(omm2}\OperatorTok{@}\NormalTok{regModel)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> glm(formula = om ~ dist + soil, family = fit.family, data = rmatrix)}
\CommentTok{#> }
\CommentTok{#> Deviance Residuals: }
\CommentTok{#>    Min      1Q  Median      3Q     Max  }
\CommentTok{#> -7.066  -1.492  -0.281   1.635   7.401  }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>             Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept)   10.054      0.348   28.88  < 2e-16 ***}
\CommentTok{#> dist          -8.465      1.461   -5.79    4e-08 ***}
\CommentTok{#> soil2         -2.079      0.575   -3.62  0.00041 ***}
\CommentTok{#> soil3          0.708      1.021    0.69  0.48913    }
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> (Dispersion parameter for gaussian family taken to be 7.2)}
\CommentTok{#> }
\CommentTok{#>     Null deviance: 1791.4  on 152  degrees of freedom}
\CommentTok{#> Residual deviance: 1075.5  on 149  degrees of freedom}
\CommentTok{#>   (2 observations deleted due to missingness)}
\CommentTok{#> AIC: 742.6}
\CommentTok{#> }
\CommentTok{#> Number of Fisher Scoring iterations: 2}
\NormalTok{om.rk2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(omm2, meuse.grid)}
\CommentTok{#> Subsetting observations to fit the prediction domain in 2D...}
\CommentTok{#> Generating predictions using the trend model (RK method)...}
\CommentTok{#> [using ordinary kriging]}
\CommentTok{#> }
 \DecValTok{53}\NormalTok{% done}
\DecValTok{100}\NormalTok{% done}
\CommentTok{#> Running 5-fold cross validation using 'krige.cv'...}
\CommentTok{#> Creating an object of class "SpatialPredictions"}
\end{Highlighting}
\end{Shaded}

or fitting a regression tree:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{omm3 <-}\StringTok{ }\KeywordTok{fit.gstatModel}\NormalTok{(meuse, }\KeywordTok{log1p}\NormalTok{(om)}\OperatorTok{~}\NormalTok{dist}\OperatorTok{+}\NormalTok{soil, meuse.grid, }\DataTypeTok{method=}\StringTok{"rpart"}\NormalTok{)}
\CommentTok{#> Fitting a regression tree model...}
\CommentTok{#> Estimated Complexity Parameter (for prunning): 0.09396}
\CommentTok{#> Fitting a 2D variogram...}
\CommentTok{#> Saving an object of class 'gstatModel'...}
\end{Highlighting}
\end{Shaded}

or a random forest model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{omm4 <-}\StringTok{ }\KeywordTok{fit.gstatModel}\NormalTok{(meuse, om}\OperatorTok{~}\NormalTok{dist}\OperatorTok{+}\NormalTok{soil, meuse.grid, }\DataTypeTok{method=}\StringTok{"quantregForest"}\NormalTok{)}
\CommentTok{#> Fitting a Quantile Regression Forest model...}
\CommentTok{#> Fitting a 2D variogram...}
\CommentTok{#> Saving an object of class 'gstatModel'...}
\end{Highlighting}
\end{Shaded}

All regression-kriging models listed above are valid and the differences
between their respective results are not likely to be large (Fig.
\ref{fig:meuse-om-rk-glm}). Regression tree combined with kriging
(rpart-kriging) seems to produce slightly better results i.e.~smallest
cross-validation error, although the difference between the four
prediction methods is, in fact, not large (±5\% of variance explained).
It is important to run such comparisons nevertheless, as they allow us
to objectively select the most efficient method.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_RK_vs_randomForestK_Meuse} 

}

\caption{Predictions of the organic carbon (log-transformed values) using random forest vs linear regression-kriging. The random forest-kriging variance has been derived using the quantregForest package [@meinshausen2006quantile].}\label{fig:rk-vs-rf-meuse}
\end{figure}

Fig. \ref{fig:rk-vs-rf-meuse} shows the RK variance derived for the
random forest model using the package \citep{meinshausen2006quantile}
and the formula in Eq.\eqref{eq:RKvar-simple}. Note that the
quantregForest package estimates a much larger prediction variance than
simple linear RK for large parts of the study area.

\hypertarget{regression-kriging-and-polygon-averaging}{%
\subsection{Regression-kriging and polygon
averaging}\label{regression-kriging-and-polygon-averaging}}

Although many soil mappers may not realize it, many simpler
regression-based techniques can be viewed as a special case of RK, or
its variants. Consider for example a technique commonly used to generate
predictions of soil properties from polygon maps: weighted averaging.
Here the principal covariate available is a polygon map (showing the
distribution of mapping units). In this model it is assumed that the
trend is constant within mapping units and that the stochastic residual
is spatially uncorrelated. In that case, the Best Linear Unbiased
Predictor of the values is simple averaging of soil properties per unit
\citep[ p.43]{Webster2001Wiley}:

\begin{equation}
\hat z({\bf{s}}_0 ) = \bar \mu _p  = \frac{1}{{n_p }}\sum\limits_{i = 1}^{n_p } {z({\bf{s}}_i )}
\label{eq:regavg}
\end{equation}

The output map produced by polygon averaging will exhibit abrupt changes
at boundaries between polygon units. The prediction variance of this
area-class prediction model is simply the sum of the within-unit
variance and the estimation variance of the unit mean:

\begin{equation}
\hat \sigma^2 ({\bf{s}}_0 ) = \left( 1 + \frac{1}{n_p } \right) \cdot \sigma _p^2
\label{eq:polvar}
\end{equation}

From Eq.\eqref{eq:polvar}, it is evident that the accuracy of the
prediction under this model depends on the degree of within-unit
variation. The approach is advantageous if the within-unit variation is
small compared to the between-unit variation. The predictions under this
model can also be expressed as:

\begin{equation}
\hat z({\bf{s}}_0 ) = \sum\limits_{i = 1}^n {w_i  \cdot z({\bf{s}}_i)}; \qquad w_i  = \left\{ {\begin{array}{*{20}c}
   {1/n_p } & {{\rm for} \; {\bf{s}}_i \in p}  \\
   0 & {{\rm otherwise}}  \\
 \end{array} } \right.
\end{equation}

where \(p\) is the unit identifier. So, in fact, weighted averaging per
unit is a special version of regression-kriging where spatial
autocorrelation is ignored (assumed zero) and all covariates are
categorical variables.

Going back to the Meuse data set, we can fit a regression model for
organic matter using soil types as predictors, which gives:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{omm <-}\StringTok{ }\KeywordTok{fit.gstatModel}\NormalTok{(meuse, }\KeywordTok{log1p}\NormalTok{(om)}\OperatorTok{~}\NormalTok{soil}\DecValTok{-1}\NormalTok{, meuse.grid)}
\CommentTok{#> Fitting a linear model...}
\CommentTok{#> Fitting a 2D variogram...}
\CommentTok{#> Saving an object of class 'gstatModel'...}
\KeywordTok{summary}\NormalTok{(omm}\OperatorTok{@}\NormalTok{regModel)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> glm(formula = log1p(om) ~ soil - 1, family = fit.family, data = rmatrix)}
\CommentTok{#> }
\CommentTok{#> Deviance Residuals: }
\CommentTok{#>     Min       1Q   Median       3Q      Max  }
\CommentTok{#> -1.0297  -0.2087  -0.0044   0.2098   0.6668  }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>       Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> soil1   2.2236     0.0354    62.9   <2e-16 ***}
\CommentTok{#> soil2   1.7217     0.0525    32.8   <2e-16 ***}
\CommentTok{#> soil3   1.9293     0.1006    19.2   <2e-16 ***}
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> (Dispersion parameter for gaussian family taken to be 0.12)}
\CommentTok{#> }
\CommentTok{#>     Null deviance: 672.901  on 153  degrees of freedom}
\CommentTok{#> Residual deviance:  18.214  on 150  degrees of freedom}
\CommentTok{#>   (2 observations deleted due to missingness)}
\CommentTok{#> AIC: 116.6}
\CommentTok{#> }
\CommentTok{#> Number of Fisher Scoring iterations: 2}
\end{Highlighting}
\end{Shaded}

and these regression coefficients for soil classes \texttt{1},
\texttt{2}, \texttt{3} are equal to the mean values per class:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(}\KeywordTok{log1p}\NormalTok{(om) }\OperatorTok{~}\StringTok{ }\NormalTok{soil, meuse, mean) }
\CommentTok{#>   soil log1p(om)}
\CommentTok{#> 1    1       2.2}
\CommentTok{#> 2    2       1.7}
\CommentTok{#> 3    3       1.9}
\end{Highlighting}
\end{Shaded}

Note that this equality can be observed only if we remove the intercept
from the regression model, hence we use:

\begin{verbatim}
log1p(om) ~ soil-1
\end{verbatim}

and NOT:

\begin{verbatim}
log1p(om) ~ soil
\end{verbatim}

The RK model can also be extended to fuzzy memberships, in which case
\({\rm{MU}}\) values are binary variables with continuous values in the
range 0--1. Hence also the SOLIM model (Eq.\eqref{eq:solim}) is in fact
just a special version of regression on mapping units:

\begin{equation}
\hat z({\bf{s}}_0 ) = \sum\limits_{c_j = 1}^{c_p} {\nu _{c_j} ({\bf{s}}_0) \cdot z_{c_j} } = \sum\limits_{j = 1}^p { {\rm{MU}}_j \cdot \hat b_j}  \hspace{.5cm} {\rm {for}}  \hspace{.5cm}  z_{c_j} = \frac{1}{{n_p }}\sum\limits_{i = 1}^{n_p } {z({\bf{s}}_i )}
\label{eq:SOLIMreg}
\end{equation}

where \({\rm{MU}}\) is the mapping unit or soil type, \(z_{c_j}\) is the
modal (or most representative) value of some soil property \(z\) for the
\(c_j\) class, and \(n_p\) is total number of points in some mapping
unit \({\rm{MU}}\).

Ultimately, spatially weighted averaging of values per mapping unit,
different types of regression, and regression kriging are all, in
principle, different variants of the same statistical method. The
differences are related to whether only categorical or both categorical
and continuous covariates are used and whether the stochastic residual
is spatially correlated or not. Although there are different ways to
implement combined deterministic/stochastic predictions, one should not
treat these nominally equivalent techniques as highly different.

\hypertarget{block-support}{%
\subsection{Predictions at point vs block support}\label{block-support}}

The geostatistical model refers to a soil variable that is defined by
the type of property and how it is measured (e.g.~soil pH (KCl), soil pH
(H\(_2\)O), clay content, soil organic carbon measured with
spectroscopy), but also to the size and orientation of the soil samples
that were taken from the field. This is important because the spatial
variation of the dependent variable strongly depends on the support size
(e.g.~due to an averaging out effect, the average organic content of
bulked samples taken from 1 ha plots typically has less spatial
variation than that of single soil samples taken from squares). This
implies that observations at different supports cannot be merged without
taking this effect into account \citep{Webster2001Wiley}. When making
spatial predictions using kriging one can use \emph{block-kriging}
\citep{Webster2001Wiley} or \emph{area-to-point kriging}
\citep{Kyriakidis2004GEAN1135} to make predictions at larger or smaller
supports. Both block-kriging and area-to-point kriging are implemented
in the gstat package via the generic function \texttt{krige}
\citep{Pebesma2004CG}.

\emph{Support} can be defined as the integration volume or aggregation
level at which an observation is taken or for which an estimate or
prediction is given. Support is often used in the literature as a
synonym for \emph{scale} --- large support can be related to coarse or
general scales and vice versa \citep{Hengl2006CG}. The notion of support
is important to characterize and relate different scales of soil
variation \citep{schabenberger2005statistical}. Any research of soil
properties is made with specific support and spatial spacing, the latter
being the distance between sampling locations. If properties are to be
used with different support, e.g.~when model inputs require a different
support than the support of the observations, scaling (aggregation or
disaggregation) becomes necessary \citep{Heuvelink1999Geoderma}.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_confidence_limits} 

}

\caption{Scheme with predictions on point (above) and block support (below). In the case of various versions of kriging, both point and block predictions smooth the original measurements proportionally to the nugget variation. After @Goovaerts1997Oxford.}\label{fig:confidence-limits-block}
\end{figure}

Depending on how significant the nugget variation is, prediction
variance estimated by a model can be significantly reduced by increasing
the support from points to blocks. The block kriging variance is smaller
than the point kriging variance for an amount approximately equal to the
nugget variation. Even if we take a block size of a few meters this
decreases the prediction error significantly, if indeed the nugget
variation occurs within a few meters. Because, by definition, many
kriging-type techniques smooth original sampled values, one can easily
notice that for support sizes smaller than half of the average shortest
distance between the sampling locations, both point and block
predictions might lead to practically the same predictions (see some
examples by \citet[p.158]{Goovaerts1997Oxford},
\citet{Heuvelink1999Geoderma} and/or \citet{Hengl2006CG}).

\begin{rmdnote}
The spatial support is the integration volume or size of the blocks
being sampled and/or predicted. By increasing the support size from
point to block support we decrease the prediction error variance. The
decrease in the prediction error variance is approximately equal to the
nugget variance.
\end{rmdnote}

Consider, for example, point and block predictions and simulations using
the estimates of organic matter content in the topsoil (in dg/kg) for
the Meuse case study. We first generate predictions and simulations on
point support:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{omm <-}\StringTok{ }\KeywordTok{fit.gstatModel}\NormalTok{(meuse, }\KeywordTok{log1p}\NormalTok{(om)}\OperatorTok{~}\NormalTok{dist}\OperatorTok{+}\NormalTok{soil, meuse.grid)}
\CommentTok{#> Fitting a linear model...}
\CommentTok{#> Fitting a 2D variogram...}
\CommentTok{#> Saving an object of class 'gstatModel'...}
\NormalTok{om.rk.p <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(omm, meuse.grid, }\DataTypeTok{block=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\CommentTok{#> Subsetting observations to fit the prediction domain in 2D...}
\CommentTok{#> Generating predictions using the trend model (RK method)...}
\CommentTok{#> [using ordinary kriging]}
\CommentTok{#> }
\DecValTok{100}\NormalTok{% done}
\CommentTok{#> Running 5-fold cross validation using 'krige.cv'...}
\CommentTok{#> Creating an object of class "SpatialPredictions"}
\NormalTok{om.rksim.p <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(omm, meuse.grid, }\DataTypeTok{nsim=}\DecValTok{20}\NormalTok{, }\DataTypeTok{block=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\CommentTok{#> Subsetting observations to fit the prediction domain in 2D...}
\CommentTok{#> Generating 20 conditional simulations using the trend model (RK method)...}
\CommentTok{#> drawing 20 GLS realisations of beta...}
\CommentTok{#> [using conditional Gaussian simulation]}
\CommentTok{#> }
  \DecValTok{4}\NormalTok{% done}
\DecValTok{100}\NormalTok{% done}
\CommentTok{#> Creating an object of class "RasterBrickSimulations"}
\CommentTok{#> Loading required package: raster}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'raster'}
\CommentTok{#> The following object is masked from 'package:nlme':}
\CommentTok{#> }
\CommentTok{#>     getData}
\CommentTok{#> The following objects are masked from 'package:aqp':}
\CommentTok{#> }
\CommentTok{#>     metadata, metadata<-}
\end{Highlighting}
\end{Shaded}

where the argument \texttt{block} defines the support size for the
predictions (in this case points). To produce predictions on block
support for square blocks of (\textbf{\emph{WHAT}})
by(\textbf{\emph{WHAT}}) we run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{om.rk.b <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(omm, meuse.grid, }\DataTypeTok{block=}\KeywordTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{,}\DecValTok{40}\NormalTok{), }\DataTypeTok{nfold=}\DecValTok{0}\NormalTok{)}
\CommentTok{#> Subsetting observations to fit the prediction domain in 2D...}
\CommentTok{#> Generating predictions using the trend model (RK method)...}
\CommentTok{#> [using ordinary kriging]}
\CommentTok{#> }
 \DecValTok{63}\NormalTok{% done}
\DecValTok{100}\NormalTok{% done}
\CommentTok{#> Creating an object of class "SpatialPredictions"}
\NormalTok{om.rksim.b <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(omm, meuse.grid, }\DataTypeTok{nsim=}\DecValTok{2}\NormalTok{, }\DataTypeTok{block=}\KeywordTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{,}\DecValTok{40}\NormalTok{))}
\CommentTok{#> Subsetting observations to fit the prediction domain in 2D...}
\CommentTok{#> Generating 2 conditional simulations using the trend model (RK method)...}
\CommentTok{#> drawing 2 GLS realisations of beta...}
\CommentTok{#> [using conditional Gaussian simulation]}
\CommentTok{#> }
 \DecValTok{14}\NormalTok{% done}
 \DecValTok{23}\NormalTok{% done}
 \DecValTok{31}\NormalTok{% done}
 \DecValTok{37}\NormalTok{% done}
 \DecValTok{45}\NormalTok{% done}
 \DecValTok{52}\NormalTok{% done}
 \DecValTok{59}\NormalTok{% done}
 \DecValTok{65}\NormalTok{% done}
 \DecValTok{72}\NormalTok{% done}
 \DecValTok{79}\NormalTok{% done}
 \DecValTok{85}\NormalTok{% done}
 \DecValTok{91}\NormalTok{% done}
 \DecValTok{98}\NormalTok{% done}
\DecValTok{100}\NormalTok{% done}
\CommentTok{#> Creating an object of class "RasterBrickSimulations"}
\NormalTok{## computationally intensive}
\end{Highlighting}
\end{Shaded}

Visual comparison confirms that the point and block kriging prediction
maps are quite similar, while the block kriging variance is much smaller
than the point kriging variance (Fig.
\ref{fig:meuse-block-predictions}).

Even though block kriging variances are smaller than point kriging
variances this does not imply that block kriging should always be
preferred over point kriging. If the user interest is in point values
rather than block averages, point kriging should be used. Block kriging
is also computationally more demanding than point kriging. Note also
that it is more difficult (read: more expensive) to validate block
kriging maps. In the case of point predictions, maps can be validated to
some degree using cross-validation, which is inexpensive. For example,
via one can estimate the cross-validation error using the
\texttt{krige.cv} function. The (\textbf{\emph{WHAT}}) package reports
automatically the cross-validation error \citep{Hengl2013JAG}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{om.rk.p}
\CommentTok{#>   Variable           : om }
\CommentTok{#>   Minium value       : 1 }
\CommentTok{#>   Maximum value      : 17 }
\CommentTok{#>   Size               : 153 }
\CommentTok{#>   Total area         : 4964800 }
\CommentTok{#>   Total area (units) : square-m }
\CommentTok{#>   Resolution (x)     : 40 }
\CommentTok{#>   Resolution (y)     : 40 }
\CommentTok{#>   Resolution (units) : m }
\CommentTok{#>   GLM call formula   : log1p(om) ~ dist + soil }
\CommentTok{#>   Family             : gaussian }
\CommentTok{#>   Link function      : identity }
\CommentTok{#>   Vgm model          : Exp }
\CommentTok{#>   Nugget (residual)  : 0.048 }
\CommentTok{#>   Sill (residual)    : 0.065 }
\CommentTok{#>   Range (residual)   : 285 }
\CommentTok{#>   RMSE (validation)  : 2.5 }
\CommentTok{#>   Var explained      : 47.3% }
\CommentTok{#>   Effective bytes    : 313 }
\CommentTok{#>   Compression method : gzip}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_meuse_block_predictions} 

}

\caption{Predictions and simulations (2) at point (above) and block (below) support using the Meuse dataset. Note that prediction values produced by point and block methods are quite similar. Simulations on block support produce *smoother* maps than the point-support simulations.}\label{fig:meuse-block-predictions}
\end{figure}

which shows that the mapping accuracy at point support is ca. 53\% of
the original variance (see further Eq.\eqref{eq:normvar}).

Note also that, cross-validation using block support in is not possible
because the input data needed for cross-validation are only available at
point support. This basically means that, for the Meuse example, to
estimate the mapping accuracy at block support we would have to revisit
the study area and collect additional (composite) samples on block
support that match the support size of block predictions.

Although prediction at block support is attractive because it leads to
more \emph{precise} predictions, the amount of variation explained by
predictions at block versus point support might not differ all that much
or even at all. Likewise users might not be interested in block averages
and may require point predictions. Geostatistical simulations on block
support can also be computationally intensive and extra field effort is
almost certain to be necessary to validate these maps.

One can use point samples to produce both point and block predictions,
but it is more difficult to produce point predictions from block
observations. This can be done using area-to-point kriging
\citep{Kyriakidis2004GEAN1135}, but this technique is computationally
intensive, yields large prediction uncertainties, and is hampered by the
fact that it requires the point support variogram which cannot uniquely
be derived from only block observations.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_meuse_block_support_plots1} 

}

\caption{Correlation plots for predictions and prediction variance: point vs block support.}\label{fig:meuse-block-support-plots1}
\end{figure}

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_meuse_block_support_plots2} 

}

\caption{Difference in variograms sampled from the simulated maps: point vs block support.}\label{fig:meuse-block-support-plots2}
\end{figure}

What confuses non-geostatisticians is that both point and block
predictions are normally visualized using raster GIS models, hence one
does not see that the point predictions refer to the centres of the grid
cells \citep{Hengl2006CG}. In the case of soil survey, the available
soil profile data most typically refer to point locations (\(1\times 1\)
meter or smaller horizontal blocks) because soil samples have small
support. In some cases surveyors mix soil samples from several different
profle locations to produce composite estimates of values. Nevertheless,
we can assume that the vast majority of soil profiles that are collected
in the world refer to (lateral) point support. Hence the most typical
combination of support size that we work with is: point support for soil
property observations, block support for covariates and point or block
support for soil property predictions. Modelling at full point support
(both soil sampled, covariates and outputs at point support) is in fact
very rare. Soil covariates are often derived from remote sensing data,
which is almost always delivered at block support.

In principle, there is no problem with using covariates at block support
to predict the soil at point support, except the strength of the
relationship between the covariate and target soil property may be
weakened by a mismatch in the support. Ideally, one should always try to
collect all input data at the finest support possible, then aggregate
based on the project requirements. This is unfortunately not always
possible, as most inputs are often \emph{bulked} already and our
knowledge about the short range variation is often very limited.

Figs. \ref{fig:meuse-block-support-plots1} and
\ref{fig:meuse-block-support-plots2} (correlation plots for Meuse data
set) confirms that: (1) predictions on block and point support show
practically no differences and (2) the difference in the prediction
error variance for point and block kriging effectively equals the nugget
variance.

The targeted support size for the \emph{GlobalSoilMap} project, for
example, is 3--arcsecond (ca. 100 m) horizontal dimensions of the SRTM
and other covariate data layers used to support prediction of spatial
variation in soil properties. This project probably needs predictions at
both point and block support at the target resolution, and then also
provide aggregated values at coarser resolution blocks (250, 500, 1000 m
etc). In any case, understanding consequences of aggregating spatial
data and converting from point to block support is important.

\begin{rmdnote}
In geostatistics, one needs to consider that any input / output spatial
layer refers to some support. In soil mapping, there are three main
support sizes: support size of the soil samples (sampling support; can
refer to point locations or blocks of land), support size of the
covariates (often equivalent to the grid cell size), and support size of
predictions (again point locations or blocks of land).
\end{rmdnote}

\hypertarget{gstat-sims}{%
\subsection{Geostatistical simulations}\label{gstat-sims}}

In statistical terms, the assessment of the uncertainty of produced maps
is equally important as the prediction of values at all locations. As
shown in the previous Section, uncertainty of soil variables can be
assessed in several ways. Three aspects, however, appear to be important
for any type of spatial prediction model:

\begin{itemize}
\item
  What are the \emph{conditional probability distribution functions}
  (PDFs) of the target variable at each location?
\item
  Where does the prediction model exhibit its \emph{largest errors}?
\item
  What is the \emph{accuracy} of the spatial predictions for the entire
  area of interest? And how accurate is the map overall?
\end{itemize}

For situations in which PDFs can be estimated \emph{`reliably'},
\citet{Heuvelink2006Elsevier} argued that they confer a number of
advantages over non-probabilistic techniques. For example, PDFs include
methods for describing interdependence or correlation between
uncertainties, methods for propagating uncertainties through
environmental models and methods for tracing the sources of uncertainty
in environmental data and models \citep{Heuvelink1998a}. By taking a
geostatistical approach, kriging not only yields prediction maps, but
also automatically produces PDFs at prediction points and quantifies the
spatial correlation in the prediction errors. Geostatistical simulation,
as already introduced in previous sections, refers to a method where
realizations are drawn from the conditional PDF using a pseudo-random
number generator. These simulations give a more realistic image of the
spatial correlation structure or spatial pattern of the target variable
because, unlike kriging, they do not smooth out the values.

\textbackslash{}begin\{figure\}{[}t{]}

\{\centering \includegraphics[width=1\linewidth]{figures/Fig_20_sims_cross_section}

\}

\textbackslash{}caption\{20 simulations (at block support) of the soil
organic carbon for the Meuse study area (cross-section from West to East
at Y=330348). Bold line indicates the median value and broken lines
indicate upper and lower quantiles (95\%
probability).\}\label{fig:sims-cross-section} \textbackslash{}end\{figure\}

Estimates of the model accuracy are also provided by the geostatistical
model, i.e.~the kriging variance. It is useful to note that the variance
of a large number of geostatistical simulations will approximate the
kriging variance (and likewise the average of a large number of
simulations will approximate the kriging prediction map).

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_hist_om_predicted_vs_simulated} 

}

\caption{Histogram for the target variable (Meuse data set; log of organic matter) based on the actual observations (left), predictions at all grid nodes (middle) and simulations (right). Note that the histogram for predicted values will always show somewhat narrower distribution (smoothed), depending on the strength of the model, while the simulations should be able to reproduce the original range (see also @Yamamoto2008).}\label{fig:hist-om-predicted-simulated}
\end{figure}

The differences among an ensemble of realizations produced using
geostatistical simulations capture the uncertainty associated with the
prediction map and can be used to communicate uncertainty or used as
input in a spatial uncertainty propagation analysis.

Even though the kriging variance and geostatistical simulations are
valid and valuable means to quantify the prediction accuracy, it is
important to be aware that these assessments of uncertainty are
\emph{model-based}, i.e.~are only valid under the assumptions made by
the geostatistical model. A truly \emph{model-free} assessment of the
map accuracy can (only) be obtained by probability-based validation
\citep{Brus2011EJSS}. For this we need independent sample i.e.~a sample
that was not used to build the model and make the predictions, and that,
in addition, was selected from the study area using a probabilistic
sampling design.

For the regression-kriging model fitted for organic carbon of the Meuse
data set, we can produce 20 simulations by switching the \texttt{nsim}
argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{om.rksim.p <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(omm, meuse.grid, }\DataTypeTok{block=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{nsim=}\DecValTok{20}\NormalTok{)}
\CommentTok{#> Subsetting observations to fit the prediction domain in 2D...}
\CommentTok{#> Generating 20 conditional simulations using the trend model (RK method)...}
\CommentTok{#> drawing 20 GLS realisations of beta...}
\CommentTok{#> [using conditional Gaussian simulation]}
\CommentTok{#> }
 \DecValTok{91}\NormalTok{% done}
\DecValTok{100}\NormalTok{% done}
\CommentTok{#> Creating an object of class "RasterBrickSimulations"}
\KeywordTok{log1p}\NormalTok{(meuse}\OperatorTok{@}\NormalTok{data[}\DecValTok{1}\NormalTok{,}\StringTok{"om"}\NormalTok{])}
\CommentTok{#> [1] 2.7}
\KeywordTok{extract}\NormalTok{(}\KeywordTok{raster}\NormalTok{(om.rk.p}\OperatorTok{@}\NormalTok{predicted), meuse[}\DecValTok{1}\NormalTok{,])}
\CommentTok{#> [1] 2.7}
\KeywordTok{extract}\NormalTok{(om.rksim.p}\OperatorTok{@}\NormalTok{realizations, meuse[}\DecValTok{1}\NormalTok{,])}
\CommentTok{#>      sim1 sim2 sim3 sim4 sim5 sim6 sim7 sim8 sim9 sim10 sim11 sim12 sim13}
\CommentTok{#> [1,]  2.3  2.8  2.8  2.9  2.2  2.4  2.8  2.4  2.4     2   2.3   2.9   2.8}
\CommentTok{#>      sim14 sim15 sim16 sim17 sim18 sim19 sim20}
\CommentTok{#> [1,]   2.7   2.5   2.9   2.7   2.8   2.4   2.5}
\end{Highlighting}
\end{Shaded}

which shows the difference between sampled value (2.681022), predicted
value (2.677931) and simulated values for about the same location i.e.~a
PDF (see also histograms in Fig. \ref{fig:hist-om-predicted-simulated}).
If we average the 20 simulations we obtain an alternative estimate of
the mean:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{extract}\NormalTok{(om.rksim.p}\OperatorTok{@}\NormalTok{realizations, meuse[}\DecValTok{1}\NormalTok{,]))}
\CommentTok{#> [1] 2.6}
\end{Highlighting}
\end{Shaded}

In this case there remains a small difference between the two results,
which is probably due to the small number of simulations (20) used.

\hypertarget{automated-mapping}{%
\subsection{Automated mapping}\label{automated-mapping}}

Applications of geostatistics today suggest that we will be increasingly
using \emph{automated mapping} algorithms for mapping environmental
variables. The authors of the
\href{https://cran.r-project.org/package=intamap}{intamap} package for
R, for example, have produced a wrapper function \texttt{interpolate}
that automatically generates predictions for any given combiination of
input observations and prediction locations
\citep{Pebesma2011CompGeoSci}. Consider the following example for
predicting organic matter content using the Meuse case study:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(intamap)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'intamap'}
\CommentTok{#> The following object is masked from 'package:raster':}
\CommentTok{#> }
\CommentTok{#>     interpolate}
\KeywordTok{demo}\NormalTok{(meuse, }\DataTypeTok{echo=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{meuse}\OperatorTok{$}\NormalTok{value =}\StringTok{ }\NormalTok{meuse}\OperatorTok{$}\NormalTok{zinc}
\NormalTok{output <-}\StringTok{ }\KeywordTok{interpolate}\NormalTok{(meuse, meuse.grid, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{mean=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{variance=}\OtherTok{TRUE}\NormalTok{))}
\CommentTok{#> R 2018-10-16 20:25:14 interpolating 155 observations, 3103 prediction locations}
\CommentTok{#> Warning in predictTime(nObs = dim(observations)[1], nPred = nPred, formulaString = formulaString, : }
\CommentTok{#>  using standard model for estimating time. For better }
\CommentTok{#>  platform spesific predictions, please run }
\CommentTok{#>  timeModels <- generateTimeModels()}
\CommentTok{#>   and save the workspace}
\CommentTok{#> [1] "estimated time for  copula 150.09204981115"}
\CommentTok{#> Checking object ... OK}
\end{Highlighting}
\end{Shaded}

which gives the (presumably) best interpolation method for the problem
at hand (\texttt{value} column), given the time available set with
\texttt{maximumTime} \citep{Pebesma2011CompGeoSci}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(output, }\DataTypeTok{max.level =} \DecValTok{2}\NormalTok{)}
\CommentTok{#> List of 16}
\CommentTok{#>  $ observations       :Formal class 'SpatialPointsDataFrame' [package "sp"] with 5 slots}
\CommentTok{#>  $ formulaString      :Class 'formula'  language value ~ 1}
\CommentTok{#>   .. ..- attr(*, ".Environment")=<environment: 0x55d06084ec88> }
\CommentTok{#>  $ predictionLocations:Formal class 'SpatialPixelsDataFrame' [package "sp"] with 7 slots}
\CommentTok{#>  $ params             :List of 18}
\CommentTok{#>   ..$ doAnisotropy     : logi TRUE}
\CommentTok{#>   ..$ testMean         : logi FALSE}
\CommentTok{#>   ..$ removeBias       : logi NA}
\CommentTok{#>   ..$ addBias          : logi NA}
\CommentTok{#>   ..$ biasRemovalMethod: chr "LM"}
\CommentTok{#>   ..$ nmax             : num 50}
\CommentTok{#>   ..$ nmin             : num 0}
\CommentTok{#>   ..$ omax             : num 0}
\CommentTok{#>   ..$ maxdist          : num Inf}
\CommentTok{#>   ..$ ngrid            : num 100}
\CommentTok{#>   ..$ nsim             : num 100}
\CommentTok{#>   ..$ sMin             : num 4}
\CommentTok{#>   ..$ block            : num(0) }
\CommentTok{#>   ..$ processType      : chr "gaussian"}
\CommentTok{#>   ..$ confProj         : logi TRUE}
\CommentTok{#>   ..$ debug.level      : num 0}
\CommentTok{#>   ..$ nclus            : num 1}
\CommentTok{#>   ..$ significant      : logi TRUE}
\CommentTok{#>   ..- attr(*, "class")= chr "IntamapParams"}
\CommentTok{#>  $ outputWhat         :List of 2}
\CommentTok{#>   ..$ mean    : logi TRUE}
\CommentTok{#>   ..$ variance: logi TRUE}
\CommentTok{#>  $ blockWhat          : chr "none"}
\CommentTok{#>  $ intCRS             : chr "+init=epsg:28992 +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=46"| __truncated__}
\CommentTok{#>  $ lambda             : num -0.27}
\CommentTok{#>  $ anisPar            :List of 4}
\CommentTok{#>   ..$ ratio     : num 1.48}
\CommentTok{#>   ..$ direction : num 56.1}
\CommentTok{#>   ..$ Q         : num [1, 1:3] 3.05e-07 2.29e-07 -9.28e-08}
\CommentTok{#>   .. ..- attr(*, "dimnames")=List of 2}
\CommentTok{#>   ..$ doRotation: logi TRUE}
\CommentTok{#>  $ variogramModel     :Classes 'variogramModel' and 'data.frame':    2 obs. of  9 variables:}
\CommentTok{#>   ..$ model: Factor w/ 20 levels "Nug","Exp","Sph",..: 1 3}
\CommentTok{#>   ..$ psill: num [1:2] 0.00141 0.02527}
\CommentTok{#>   ..$ range: num [1:2] 0 1282}
\CommentTok{#>   ..$ kappa: num [1:2] 0 0}
\CommentTok{#>   ..$ ang1 : num [1:2] 0 33.9}
\CommentTok{#>   ..$ ang2 : num [1:2] 0 0}
\CommentTok{#>   ..$ ang3 : num [1:2] 0 0}
\CommentTok{#>   ..$ anis1: num [1:2] 1 0.674}
\CommentTok{#>   ..$ anis2: num [1:2] 1 1}
\CommentTok{#>   ..- attr(*, "singular")= logi FALSE}
\CommentTok{#>   ..- attr(*, "SSErr")= num 2.84e-08}
\CommentTok{#>   ..- attr(*, "call")= language fit.variogram(object = experimental_variogram, model = vgm(psill = psill,      model = model, range = range, nugg| __truncated__ ...}
\CommentTok{#>  $ sampleVariogram    :Classes 'gstatVariogram' and 'data.frame':    11 obs. of  6 variables:}
\CommentTok{#>   ..$ np     : num [1:11] 7 31 94 132 147 ...}
\CommentTok{#>   ..$ dist   : num [1:11] 67.2 94.2 142.9 193.5 248.9 ...}
\CommentTok{#>   ..$ gamma  : num [1:11] 0.000891 0.005635 0.005537 0.006056 0.010289 ...}
\CommentTok{#>   ..$ dir.hor: num [1:11] 0 0 0 0 0 0 0 0 0 0 ...}
\CommentTok{#>   ..$ dir.ver: num [1:11] 0 0 0 0 0 0 0 0 0 0 ...}
\CommentTok{#>   ..$ id     : Factor w/ 1 level "var1": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>   ..- attr(*, "direct")='data.frame':    1 obs. of  2 variables:}
\CommentTok{#>   ..- attr(*, "boundaries")= num [1:12] 36.8 73.5 110.3 165.5 220.6 ...}
\CommentTok{#>   ..- attr(*, "pseudo")= num 0}
\CommentTok{#>   ..- attr(*, "what")= chr "semivariance"}
\CommentTok{#>  $ methodParameters   : chr "  vmodel = data.frame(matrix(0,nrow =  2 ,ncol =  9 ))\textbackslash{}nnames(vmodel) = c(\textbackslash{}"model\textbackslash{}",\textbackslash{}"psill\textbackslash{}",\textbackslash{}"range\textbackslash{}",\textbackslash{}"kappa"| __truncated__}
\CommentTok{#>  $ predictions        :Formal class 'SpatialPixelsDataFrame' [package "sp"] with 7 slots}
\CommentTok{#>  $ outputTable        : num [1:4, 1:3103] 181180 333740 842 44785 181140 ...}
\CommentTok{#>   ..- attr(*, "dimnames")=List of 2}
\CommentTok{#>   ..- attr(*, "transposed")= logi TRUE}
\CommentTok{#>  $ processPlot        : chr ""}
\CommentTok{#>  $ processDescription : chr "Spatial prediction using the method  transGaussian"}
\CommentTok{#>  - attr(*, "class")= chr "transGaussian"}
\end{Highlighting}
\end{Shaded}

The interpolate function automatically chooses between: (1) kriging, (2)
copula methods, (3) inverse distance interpolation, projected spatial
gaussian process methods in the package, (4) transGaussian kriging or
Yamamoto interpolation.

\begin{rmdnote}
Automated mapping is the computer-aided generation of (meaningful) maps
from measurements. In the context of geostatistical mapping, automated
mapping implies that the model fitting, prediction and visualization can
be run with little or no human interaction / intervention.
\end{rmdnote}

The same idea of automated model fitting and prediction has been
implemented in the package for (\textbf{\emph{WHAT}}) , which extends
simple point-based models to 2D, 3D, 2D+T regression-kriging models.
Some examples of automated soil mapping have been already
(\textbf{\emph{WHAT}}) shown previously.

\begin{figure}[t]

{\centering \includegraphics[width=0.6\linewidth]{figures/Fig_statmodels} 

}

\caption{A modern workflow of predictive soil mapping. This often includes state-of-the-art Machine Learning Algorithms.}\label{fig:scheme-statmodels}
\end{figure}

Automated mapping, as long as it is not a \emph{black-box} system, is
beneficial for soil mapping applications for several reasons: (1) it
saves time and effort needed to get initial results, (2) it allows
generation of maps using current data (live geostatistics) even via a
web-interfaces, (3) it greatly reduces the workload in cases where maps
need to be produced repeatedly, such as when regular updates are needed
or the same model is applied in different subareas. In practice,
automated mapping is typically a three-stage process (Fig.
\ref{fig:scheme-statmodels}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Rapidly generate predictions and a report of analysis} (analyze
  why a particular technique was chosen and how well it performs? Are
  there any outliers or artifacts? Which predictors are most
  significant? etc).
\item
  \emph{Review the results of spatial prediction and fine-tune some
  parameters} if necessary / filter and/or adjust the input maps.
\item
  \emph{Re-run the prediction process and publish the final maps}.
\end{enumerate}

hence geostatisticians are still an essential and active part of the
process. In automated mapping they primarily focus their expertise on
doing interpretation of the results rather than on manually analyzing
the data.

It is unlikely that a simple linear prediction model can be used to fit
every type of soil data. It is more likely that some customized models,
i.e.~models specific for each property, would perform better than if a
single model were used for a diversity of soil properties. This is
because different soil properties have different distributions, they
vary differently at different scales, and are controlled by different
processes. On the other hand, the preferred way to ensure that a single
model can be used to map a variety of soil properties is to develop a
generic framework with multi-thematic, multi-scale predictors that
allows for iterative search for optimal model structure and parameters,
and then implement this model via an automated mapping system.

\hypertarget{selecting-spatial-prediction-models}{%
\subsection{Selecting spatial prediction
models}\label{selecting-spatial-prediction-models}}

The purpose of spatial prediction is to (a) produce a map showing
spatial distribution of the variable of interest for the area of
interest, and (b) to do this in an unbiased way. A comprehensive path to
evaluating spatial predictions is the
\href{http://topepo.github.io/caret/index.html}{caret} approach
\citep{kuhn2013applied}, which wraps up many of the standard processes
such as model training and validation, method comparison and
visualization. Consider, for example, organic matter \% in the topsoil
in the meuse data set:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret); }\KeywordTok{library}\NormalTok{(rgdal)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'caret'}
\CommentTok{#> The following object is masked from 'package:intamap':}
\CommentTok{#> }
\CommentTok{#>     preProcess}
\KeywordTok{demo}\NormalTok{(meuse, }\DataTypeTok{echo=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{meuse.ov <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{over}\NormalTok{(meuse, meuse.grid), meuse}\OperatorTok{@}\NormalTok{data)}
\NormalTok{meuse.ov}\OperatorTok{$}\NormalTok{x0 =}\StringTok{ }\DecValTok{1}
\end{Highlighting}
\end{Shaded}

We can quickly compare performance of using GLM vs random forest vs no
model for predicting organic matter (om) by using the caret package
functionality:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{2}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{2}\NormalTok{)}
\NormalTok{mFit0 <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(om}\OperatorTok{~}\NormalTok{x0, }\DataTypeTok{data=}\NormalTok{meuse.ov, }\DataTypeTok{method=}\StringTok{"glm"}\NormalTok{, }
               \DataTypeTok{family=}\KeywordTok{gaussian}\NormalTok{(}\DataTypeTok{link=}\NormalTok{log), }\DataTypeTok{trControl=}\NormalTok{fitControl, }
               \DataTypeTok{na.action=}\NormalTok{na.omit)}
\CommentTok{#> Warning in predict.lm(object, newdata, se.fit, scale = 1, type =}
\CommentTok{#> ifelse(type == : prediction from a rank-deficient fit may be misleading}

\CommentTok{#> Warning in predict.lm(object, newdata, se.fit, scale = 1, type =}
\CommentTok{#> ifelse(type == : prediction from a rank-deficient fit may be misleading}

\CommentTok{#> Warning in predict.lm(object, newdata, se.fit, scale = 1, type =}
\CommentTok{#> ifelse(type == : prediction from a rank-deficient fit may be misleading}

\CommentTok{#> Warning in predict.lm(object, newdata, se.fit, scale = 1, type =}
\CommentTok{#> ifelse(type == : prediction from a rank-deficient fit may be misleading}
\CommentTok{#> Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info =}
\CommentTok{#> trainInfo, : There were missing values in resampled performance measures.}
\NormalTok{mFit1 <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(om}\OperatorTok{~}\NormalTok{soil, }\DataTypeTok{data=}\NormalTok{meuse.ov, }\DataTypeTok{method=}\StringTok{"glm"}\NormalTok{, }
               \DataTypeTok{family=}\KeywordTok{gaussian}\NormalTok{(}\DataTypeTok{link=}\NormalTok{log), }\DataTypeTok{trControl=}\NormalTok{fitControl, }
               \DataTypeTok{na.action=}\NormalTok{na.omit)}
\NormalTok{mFit2 <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(om}\OperatorTok{~}\NormalTok{dist}\OperatorTok{+}\NormalTok{soil}\OperatorTok{+}\NormalTok{ffreq, }\DataTypeTok{data=}\NormalTok{meuse.ov, }\DataTypeTok{method=}\StringTok{"glm"}\NormalTok{, }
               \DataTypeTok{family=}\KeywordTok{gaussian}\NormalTok{(}\DataTypeTok{link=}\NormalTok{log), }\DataTypeTok{trControl=}\NormalTok{fitControl, }
               \DataTypeTok{na.action=}\NormalTok{na.omit)}
\NormalTok{mFit3 <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(om}\OperatorTok{~}\NormalTok{dist}\OperatorTok{+}\NormalTok{soil}\OperatorTok{+}\NormalTok{ffreq, }\DataTypeTok{data=}\NormalTok{meuse.ov, }\DataTypeTok{method=}\StringTok{"ranger"}\NormalTok{, }
               \DataTypeTok{trControl=}\NormalTok{fitControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}
\end{Highlighting}
\end{Shaded}

This will run repeated Cross-validation with 50\% : 50\% splits training
and validation, which means that in each iteration models will be
refitted from scratch. Next we can compare performance of the three
models by using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resamps <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{Mean=}\NormalTok{mFit0, }\DataTypeTok{Soilmap=}\NormalTok{mFit1, }\DataTypeTok{GLM=}\NormalTok{mFit2, }\DataTypeTok{RF=}\NormalTok{mFit3))}
\KeywordTok{bwplot}\NormalTok{(resamps, }\DataTypeTok{layout =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{metric=}\KeywordTok{c}\NormalTok{(}\StringTok{"RMSE"}\NormalTok{,}\StringTok{"Rsquared"}\NormalTok{), }
       \DataTypeTok{fill=}\StringTok{"grey"}\NormalTok{, }\DataTypeTok{scales =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{relation =} \StringTok{"free"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{Statistical_theory_files/figure-latex/bwplot-meuse-1} 

}

\caption{Comparison of spatial prediction accuracy (RMSE at cross-validation points) for simple averaging (Mean), GLM with only soil map as covariate (Soilmap), GLM and random forest (RF) models with all possible covariates. Error bars indicate range of RMSE values for repeated CV.}\label{fig:bwplot-meuse}
\end{figure}

In the case above, it seems that random forest
(\href{https://github.com/imbs-hl/ranger}{ranger package}) helps reduce
mean RMSE of predicting organic matter by about 32\%:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{((}\DecValTok{1}\OperatorTok{-}\KeywordTok{min}\NormalTok{(mFit3}\OperatorTok{$}\NormalTok{results}\OperatorTok{$}\NormalTok{RMSE)}\OperatorTok{/}\KeywordTok{min}\NormalTok{(mFit0}\OperatorTok{$}\NormalTok{results}\OperatorTok{$}\NormalTok{RMSE))}\OperatorTok{*}\DecValTok{100}\NormalTok{)}
\CommentTok{#> [1] 32}
\end{Highlighting}
\end{Shaded}

There is certainly added value in using spatial covariates (in the case
above: distance to water and flooding frequency maps) and in using
machine learning for spatial prediction, even with smaller data sets.

Note also that the assessment of spatial prediction accuracy for the
three models based on the train function above is model-free,
i.e.~cross-validation of the models is independent of the models used
because, at each cross-validation subset, fitting of the model is
repeated and validation points are maintained separate from model
training. Subsetting point samples is not always trivial however: in
order to consider cross-validation as completely reliable, the samples
ought to be representative of the study area and preferably collected
using objective sampling such as simple random sampling or similar
\citep{Brus2011EJSS}. In the case the sampling locations are clustered
in geographical space i.e.~if some parts of the study area are
completely omitted from sampling, then also the results of
cross-validation will reflect that sampling bias / poor representation.
In all the following examples we will assume that cross-validation gives
a reliable measure of mapping accuracy and we will use it as the basis
of accuracy assessment i.e.~mapping efficiency. In reality,
cross-validation might be tricky to implement and could often lead to
somewhat over-optimistic results if either sampling bias exists or/and
if there are too few points for model validation. For example, in the
case of soil profile data, it is highly recommended that entire profiles
are removed from CV because soil horizons are too strongly correlated
(as discussed in detail in \citet{Gasch2015SPASTA} and
\citet{Brenning2012}).

The whole process of spatial prediction of soil properties could be
summarized in 5 steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initial model comparison (comparison of prediction accuracy and
  computing time).
\item
  Selection of applicable model(s) and estimation of model parameters
  i.e.~model fitting.
\item
  Predictions i.e.~generation of maps for all areas of interest.
\item
  Objective accuracy assessment using independent (cross-)validation.
\item
  Export and sharing of maps and summary documentation explaining all
  processing steps.
\end{enumerate}

Studying the \href{http://topepo.github.io/caret/index.html}{caret
package tutorial} and/or the \href{https://mlr-org.github.io}{mlr
tutorials} is highly recommended for anyone looking for a systematic
introduction to predictive modelling.

\hypertarget{regression-kriging-3D}{%
\subsection{3D regression-kriging}\label{regression-kriging-3D}}

Measurements of soil properties at point support can be thought of as
describing explicit 3D locations (easting, northing and depth), and are
amenable to being dealt with using 3D geostatistics (e.g.~3D kriging).
Application of 3D kriging to soil measurements is cumbersome for several
reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The differences between sampling intervals and spatial correlation in
  the horizontal and vertical dimensions are very large (\textless{}10
  in the vertical v.s. 100's to 1000's of in the horizontal). The
  resulting strong anisotropy must be accounted for when the
  geostatisitcal model is derived. Estimation of the anisotropy may be
  hampered by the relatively small number of observations along the
  vertical profile, although under a stationarity assumption it can
  benefit from the many repetitions of profile data for all profile
  locations.
\item
  Soil property values refer to vertical block support (usually because
  they are composite samples, i.e.~the average over a soil horizon),
  hence some of the local variation (in the vertical dimension) has been
  smoothed out.
\item
  Soil surveyors systematically under-represent lower depths ---
  surveyors tend to systematically take fewer samples as they assume
  that deeper horizons are of less importance for management or because
  deeper horizons are more expensive to collect or because deeper
  horizons are assumed to be more homogeneous and uniform.
\item
  Many soil properties show clear trends in the vertical dimension and,
  if this is ignored, the result can be a very poor geostatistical
  model. It may not be that easy to incorporate a vertical trend because
  such a trend is generally not consistently similar between different
  soil types. On the other hand, soil variables are auto-correlated in
  both horizontal and vertical (depth) dimensions, so that it makes
  sense to treat them using 3D geostatistics whenever we have enough 3D
  soil observations.
\end{enumerate}

\begin{rmdnote}
Because soil variables are auto-correlated in both horizontal and
vertical (depth) dimensions it makes sense to treat them using 3D
geostatistics, as long as there are enough measurements in all spatial
dimensions.
\end{rmdnote}

\begin{figure}[t]

{\centering \includegraphics[width=0.6\linewidth]{figures/Fig_voxel_scheme} 

}

\caption{Spatial 3D prediction locations in a gridded system (voxels). In soil mapping, we often predict for larger blocks of land e.g. 100 to 1000 m, but then for vertical depths of few tens of centimeters, so the output voxels might appear in reality as being somewhat disproportional.}\label{fig:voxel-scheme}
\end{figure}

The fact that there are almost always \textless{}10 soil observations
over the total depth of a soil profile, so that the estimates of the
range in the vertical dimension will be relatively poor, is something
that cannot be improved. The fact that soil samples taken by horizon
refer to block support is a more serious problem, as part of short range
variation has been lost, plus we know that the point values do not refer
to the horizon center but to the whole horizon block, which, in addition
to everything else, tend to be irregular i.e.~do not have constant depth
and width.

To predict in 3D space, we extend the regression model from
Eq.\eqref{eq:ukm-gstat} with a soil depth function:

\begin{equation}
\begin{split}
\hat z({\bf{s}}_0, d_0 ) = \sum\limits_{j = 0}^p {\hat \beta _j \cdot X_j ({\bf{s}}_0, d_0 )} + {\bf{\hat g}}(d_0) + \sum\limits_{i = 1}^n {\hat{\lambda}_i ({\bf{s}}_0, d_0 ) \cdot e({\bf{s}}_i, d_i )}
\end{split}
\label{eq:MRK3D}
\end{equation}

where \(d\) is the 3rd depth dimension expressed in meters from the land
surface, \({\bf{\hat g}}(d_0)\) is the predicted soil depth function,
typically modelled by a spline function. This allows prediction of soil
properties at any depth using observations at other depths but does
require 3D modelling of the covariance structure, which is not easy
because there may be zonal and geometric anisotropies (i.e.~the variance
and correlation lengths may differ between vertical and horizontal
directions). Also, the vertical support of observations becomes
important and it should be realized that observations are the averages
over depth intervals and not values at points along the vertical axis
(Fig. \ref{fig:voxel-scheme}). Spline functions have been proposed and
used as mass-preserving curve fitting methods to derive point and block
values along the vertical axis from observations at given depth
intervals, but the difficulty is that these yield estimates (with
uncertainties) that should not be confused with real observations.

A 3D variogram, e.g.~modelled using an exponential model with three
standard parameters (nugget \(c_0\), partial sill \(c_1\), range
parameter \(r\)):

\begin{equation}
\gamma \left( {\bf{h}} \right) = \left\{
{\begin{array}{*{20}c}
   0 & {{\rm{if}}} & {h = 0}  \\
   {c_0  + c_1  \cdot \left[ {1 - e^{ - \left( {\frac{{h}}
{r}} \right)} } \right]} & {{\rm{if}}} & {h > 0}  \\
 \end{array} } \right. \qquad {\bf{h}} =  \left[ {h_x  , h_y  , h_d } \right]
\label{eq:exp}
\end{equation}

where the scalar \emph{`distance'} \(h\) is calculated by scaling
horizontal and vertical separation distances using three anisotropy
parameters:

\begin{equation}
h = \sqrt {\left( {\frac{{h_x  }}{{a_x  }}} \right)^2  + \left( {\frac{{h_y  }}{{a_y  }}} \right)^2  + \left( {\frac{{h_d }}{{a_d }}} \right)^2 }
\label{eq:anisotropy}
\end{equation}

Typically, in the case of soil data, the anisotropy ratio between
horizontal and vertical distances is high --- spatial variation observed
in a few depth changes may correspond with several or more in horizontal
space, so that the initial settings of the anisotropy ratio (i.e.~the
ratio of the horizontal and vertical variogram ranges) are between
3000--8000, for example. Variogram fitting criteria can then be used to
optimize the anisotropy parameters. In our case we assumed no horizontal
anisotropy and hence assumed \(a_x=a_y=1\), leaving only \(a_d\) to be
estimated. Once the anisotropy ratio is obtained, 3D variogram modelling
does not meaningfully differ from 2D variogram modelling.

The 3D RK framework explained above can be compared to the approach of
\citet{Malone2009Geoderma}, who first fit an equal-area spline function
to estimate the soil properties at a standard depth, and next fit
regression and variogram models at each depth. A drawback of the
approach by \citet{Malone2009Geoderma}, however, is that the separate
models for each depth ignore all vertical correlations. In addition, the
equal-area spline is not used to model soil-depth relationships but only
to estimate the values at standard depths for sampling locations i.e.~it
is implemented for each soil profile (site) separately. In the 3D RK
framework explained above, a single model is used to generate
predictions at any location and for any depth, and which takes into
account both horizontal and vertical relationships simultaneously. The
3D RK approach is both easier to implement, and allows for incorporating
all (vertical) soil-depth relationships including the spatial
correlations.

\hypertarget{multiscale}{%
\subsection{Predicting with multiscale and multisource
data}\label{multiscale}}

Fig. \ref{fig:general-sp-process} indicates that spatial prediction is a
linear processes with one line of inputs and one line of outputs. In
some cases soil mappers have to use methods that can work with
\emph{multi-scale} and/or \emph{multi-source} data i.e.~data with
different extents, resolution and uncertainty. Here by \emph{multiscale
data} we imply covariates used for geostatistical mapping that are
available at two or more (distinctly different) resolutions, but that
cover the same area of interest (see also: \texttt{RasterStack} class in
the package). In the case of the \emph{multisource data}, covariates can
be of any scale, they can have a variable extent, and variable accuracy
(Fig. \ref{fig:multiscale-vs-multisource}b). In other words, when
referring to multiscale data, we assume that the input covariate layers
differ only in their resolution; whereas in referring to multisource
data, we consider that all technical aspects of the input data could
potentially be different.

Organizing (and using) multiscale and multisource data is something that
probably can not be avoided in global soil mapping projects. From the
GIS perspective, and assuming a democratic right to independently
develop and apply spatial prediction models, merging of the multiscale
and multisource data is likely to be inevitable.

\begin{figure}[t]

{\centering \includegraphics[width=0.9\linewidth]{figures/Fig_multiscale_vs_multisource} 

}

\caption{A general scheme for generating spatial predictions using multiscale and multisource data.}\label{fig:multiscale-vs-multisource}
\end{figure}

As a general strategy, for multi-scale data, a statistically robust
approach is to fit a single model to combined covariates downscaled or
upscaled to a single, common resolution (Fig.
\ref{fig:multiscale-vs-multisource}a). For the multi-source data data
assimilation methods i.e.~merging of predictions (Fig.
\ref{fig:multiscale-vs-multisource}b) can be used \citep{CAUBET201999}.
Imagine if we have covariate layers for one whole continent at some
coarse resolution of e.g.~500 m, but for some specific country have
other predictions at a finer resolution of e.g.~100 m. Obviously any
model we develop that uses both sources of data is limited in its
application to just the extent of that country. To ensure that all
covariate and soil data available for that country are used to generate
predictions, we can fit two models at seperate scales and independently
of each other, and then merge the predictions only for the extent of the
country of interest. A statistical framework for merging such
predictions is given, for example, in \citet{CAUBET201999}. In that
sense, methods for multisource data merging are more attractive for
pan-continental and global projects, because for most of the countries
in the world, both soil and covariate data are available at different
effective scales.

\begin{rmdnote}
A sensible approach to merging multiple predictions (usually at multiple
resolutions) is to derive a weighted average of two or more predictions
/ use the per-pixel accuracy to assign relative weights, so that more
accurate predictions receive more weight {[}@Heuvelink19921{]}.
\end{rmdnote}

It is important to emphasize, however, that, in order to combine various
predictors, we do need to have an estimate of the prediction uncertainty
e.g.~derived using cross-validation, otherwise we are not able to assign
the weights. In principle, a linear combination of statistical
techniques using the equation above should be avoided if a theoretical
basis exists that incorporates such a combination.

Combined predictions are especially interesting for situations where:

\begin{itemize}
\item
  predictions are produced using different inputs i.e.~data with
  different coverage,
\item
  there are several prediction methods which are equally applicable,
\item
  where no theory exists that describes a combination of spatial
  prediction methods,
\item
  where fitting and prediction of individual models is faster and less
  problematic than fitting of a hybrid model.
\end{itemize}

Estimation of the prediction variance and confidence interval of
combined or merged predictions is more complex than estimation of the
mean value.

\hypertarget{accuracy-assessment}{%
\section{Accuracy assessment and the mapping
efficiency}\label{accuracy-assessment}}

\hypertarget{mapping-accuracy}{%
\subsection{Mapping accuracy and numeric
resolution}\label{mapping-accuracy}}

Every time a digital soil mapper produces soil maps, soil GIS and soil
geographical databases those products can be evaluated using independent
validation studies. Unfortunately, much evaluation of soil maps in the
world is still done using subjective \emph{`look-good'} assessments and
the inherent uncertainty of the product is often underreported. In this
book, we promote objective assessment of the mapping accuracy,
i.e.~based on statistical testing using ground truth data.

\emph{Mapping accuracy} can be defined as the difference between an
estimated value and the \emph{``true''} value, i.e.~a value of the same
target variable arrived at using a significantly more accurate method.
In the most simple terms, accuracy is the error component of the
perfectly accurate map \citep{mowrer2000quantifying}. Although we know
that soils form under systematic environmental conditions and probably
much of the variation is deterministic (Eq.\eqref{eq:ukm}), we do not yet
have tools that allow us to model soil formation and evolution processes
perfectly (see also section \ref{sources-uncertainty}). The best we can
do is to calibrate some spatial prediction model using field records,
and then generate (the best possible) predictions. The resulting soil
property map, i.e.~what we know about soils, is then a sum of two
\emph{signals}:

\begin{equation}
z^{\rm{map}}({\bf{s}}) = Z({\bf{s}}) + \varepsilon({\bf{s}})
\label{eq:varparts}
\end{equation}

where \(Z({\bf{s}})\) is the \emph{true} variation, and
\(\varepsilon({\bf{s}})\) is the error component i.e.~what we do not
know. The error component, also known as the \emph{error budget},
consists of two parts: (1) \emph{the unexplained part of soil
variation}, and (2) \emph{the pure noise} (sampling and measurement
errors described in section \ref{sources-uncertainty}).

The unexplained part of soil variation is the variation we somehow
failed to explain because we are not using all relevant covariates
and/or due to the limited sampling intensity. For example, the sampling
plan might fail to sample some hot-spots or other important local
features. The unexplained part of variation also includes short-range
variation, which is possibly deterministic but often not of interest or
is simply not feasible to describe at common mapping scales.

The way to determine the error part in Eq.\eqref{eq:varparts} is to
collect additional samples and then determine the average error or the
\emph{Root Mean Square Error}
\citep{goovaerts2001geostatistical, Finke2006Elsevier, LiHeap2010EI}:

\begin{equation}
{\it RMSE} = \sqrt {\frac{1}{l} \cdot \sum\limits_{i = 1}^l {\left[
{\hat z({\bf{s}}_i ) - z ({\bf{s}}_i )} \right]^2 } }
\label{eq:RMSE}
\end{equation}

where \(l\) is the number of validation points, and the expected
estimate of prediction error at sampling locations is equal to the
nugget variation (\(E\{ {\it RMSE} \} = \sigma({\bf{h}}=0)\)). In
addition to \(\it{RMSE}\), it is often interesting to see also whether
the errors are, on average, positive (over-estimation) or negative
(under-estimation) i.e.~whether there is possibly any clear bias in our
predictions:

\begin{equation}
{\rm ME} = \frac{1}{m} \sum_{j=1}^{m} (\hat y ({\bf s}_j) - y ({\bf s}_j))
\label{eq:ME}
\end{equation}

To see how much of the global variation budget has been explained by the
model we can use:

\begin{equation}
 {\Sigma}_{\%} = \left[ 1 - \frac{{\it{SSE}}}{{\it{SSTO}}} \right] = \left[ 1 - \frac{{\it{RMSE}}^2}{\sigma_z^2} \right] [0-100\%]
\label{eq:normvar}
\end{equation}

where \(\it{SSE}\) is the sum of squares for residuals at
cross-validation points (i.e. \({\it{MSE}} \cdot n\)), and \(\it{SSTO}\)
is the total sum of squares. \({\Sigma}_{\%}\) is a global estimate of
the map accuracy, valid only under the assumption that the validation
points are spatially independent from the calibration points,
representative and large enough (e.g. \(l>50\)), and that the error
component is normally distributed around the zero value
(\(E\left\{ {\hat z({{\bf{s}}_i}) - z({{\bf{s}}_i})} \right\} = 0\)).

Once we have estimated \(\it{RMSE}\), we can also determine the
effective \emph{numeric resolution} for the predictions
\citep{Hengl2013JAG}. For example, assuming that the original sampling
variance is 1.85 and that \(\it{RMSE}\)=1 (i.e. \({\Sigma}_{\%}\)=47\%),
the effective numeric resolution for predictions is then 0.5 (as shown
previously in Fig. \ref{fig:sigma-rmse-relationship}). There is probably
no need to code the values with a better precision than 0.5 units.

\hypertarget{accuracy-assessment-methods}{%
\subsection{Accuracy assessment
methods}\label{accuracy-assessment-methods}}

There are three possibilities for estimating the \(\it{RMSE}\) (Fig.
\ref{fig:cross-validation-types}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Run cross-validation using the same input data used for model
  fitting}.
\item
  \emph{Collect new samples using a correct probability sampling design
  to ensure an unbiased estimate of accuracy}.
\item
  \emph{Compare predicted values with more detailed maps for small study
  areas produced at much higher accuracy, usually also at much finer
  level of detail}.
\end{enumerate}

\begin{figure}[t]

{\centering \includegraphics[width=0.7\linewidth]{figures/Fig_cross_validation_types} 

}

\caption{General types of validation procedures for evaluating accuracy of spatial prediction models.}\label{fig:cross-validation-types}
\end{figure}

Although the prediction variance already indicates what the potential
accuracy of the maps is, only by independent validation can we determine
the true accuracy of the maps. \citet{Brus2011EJSS} further show that,
actually, only if the validation points are selected using some
probability-based sampling, like simple random sampling or stratified
sampling, can one determine the true accuracy of any produced gridded
maps. In practice, we can rarely afford to collect new samples, so that
cross-validation is often the only viable option.

\hypertarget{cross-validation-and-its-limitations}{%
\subsection{Cross-validation and its
limitations}\label{cross-validation-and-its-limitations}}

Because collecting additional (independent) samples is often impractical
and expensive, validation of prediction models is commonly done by using
\emph{cross-validation} i.e.~by subsetting the original point set into
two data sets --- calibration and validation --- and then repeating the
analysis. There are several types of cross-validation methods \citep[
pp.221--226]{Bivand2008Springer}:

\begin{itemize}
\item
  the \(k\)--fold cross-validation --- the original sample is split into
  \(k\) equal parts and then each is used for cross-validation;
\item
  \emph{leave-one-out} cross-validation (LOO) --- each sampling point is
  used for cross-validation;
\item
  \emph{Jackknifing} --- similar to LOO, but aims at estimating the bias
  of statistical analysis and not of predictions;
\end{itemize}

\textbackslash{}begin\{figure\}{[}t{]}

\{\centering \includegraphics[width=0.85\linewidth]{figures/Fig_cross_validation_repetitions}

\}

\textbackslash{}caption\{Left: confidence limits for the amount of
variation explained (0--100\%) for two spatial prediction methods:
inverse distance interpolation (IDW) and regression-kriging (RK) for
mapping organic carbon content (Meuse data set). Right: the average
amount of variation explained for two realizations (5-fold
cross-validation) as a function of the number of cross-validation runs
(repetitions). In this case, the RK method is distinctly better than
method IDW, but the cross-validation score seems to stabilize only after
10 runs.\}\label{fig:cross-validation-repetitions}
\textbackslash{}end\{figure\}

\begin{rmdnote}
Cross-validation is a cost-efficient way to get an objective estimate of
the mapping accuracy. Under an assumption that the input samples are
representative of the study area (ideally collected using objective /
probability sampling to avoid any kind of bias).
\end{rmdnote}

Both \(k\)--fold and the leave-one-out cross validation are implemented
in the e.g.~package (\texttt{krige.cv} methods), which makes this type
of assessment convenient to implement. Note also that cross-validation
is not necessarily independent --- points used for cross-validation are
a subset of the original sampling design, hence if the original design
is biased and/or non-representative, then also the cross-validation
might not reveal the true accuracy of a technique. However, if the
sampling design has been generated using some unbiased design based
sampling (e.g.~random sampling), randomly seleced subsets will provide
unbiased estimators of the true mapping accuracy.

\emph{``Models can only be evaluated in relative terms, and their
predictive value is always open to question. The primary value of models
is heuristic.''} \citep{Oreskes04021994} Hence, also in soil mapping,
accuracy assessment should only be considered in relative terms. Each
evaluation of soil mapping accuracy might give somewhat different
numbers, so it is often a good idea to repeat the evaluation multiple
times. Also cross-validation requires enough repetition (at least
\textgreater{}1) otherwise over-positive or over-negative results can be
produced by chance (Fig. \ref{fig:cross-validation-repetitions}). Many
geostatisticians (see e.g. \texttt{krige.cv} function described in
\citet[pp.222--223]{Bivand2008Springer}) suggest that at least 5
repetitions are needed to produce \emph{`stable'} measures of the
mapping accuracy. If only one realization of cross-validation is used,
this can accidentally lead to over-optimistic or over-pessimistic
estimates of the true mapping accuracy.

\hypertarget{accuracy-of-the-predicted-model-uncertainty}{%
\subsection{Accuracy of the predicted model
uncertainty}\label{accuracy-of-the-predicted-model-uncertainty}}

Recall from Eq.\eqref{eq:sp} that the output of the prediction process is
typically (1) predicted mean value at some location
(\(\hat Z({\bf{s}}_0)\)), and (2) predicted prediction variance
i.e.~regression-kriging error (\(\hat{\sigma}({\bf{s}}_0)\)). In the
previous section we have shown some common accuracy measures for the
prediction of the mean value. It might sound confusing but, in
geostatistics, one can also validate the \emph{uncertainty of
uncertainty} i.e.~derive the \emph{error of the estimation error}. In
the case of the Meuse data set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{om.rk.cv <-}\StringTok{ }\KeywordTok{krige.cv}\NormalTok{(}\KeywordTok{log1p}\NormalTok{(om)}\OperatorTok{~}\NormalTok{dist}\OperatorTok{+}\NormalTok{soil, meuse.s, vr.fit)}
\KeywordTok{hist}\NormalTok{(om.rk.cv}\OperatorTok{$}\NormalTok{zscore, }\DataTypeTok{main=}\StringTok{"Z-scores histogram"}\NormalTok{, }
       \DataTypeTok{xlab=}\StringTok{"z-score value"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"grey"}\NormalTok{, }\DataTypeTok{breaks=}\DecValTok{25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics{Statistical_theory_files/figure-latex/z-scores-histogram-1} 

}

\caption{Z-scores for the cross-validation of the soil organic carbon model.}\label{fig:z-scores-histogram}
\end{figure}

Here, the cross-validation function \texttt{krige.cv} reports errors at
validation points (5--fold cross-validation by default), but it also
compares the difference between the regression-kriging error estimated
by the model and the actual error. The ratio between the actual and
expected error is referred to as the \(z\)-scores \citep[
p.225]{Bivand2008Springer}:

\begin{equation}
 \sigma_r ({\bf{s}}_j) = \frac{\hat z({\bf{s}}_j ) - z^* ({\bf{s}}_j )}{\hat{\sigma}({\bf{s}}_j )}; \qquad  E\{var(\sigma_r)\} = 1
\label{eq:z-scores}
\end{equation}

Ideally, the mean value of \(z\)-scores should be around 0 and the
variance of the \(z\)-scores should be around 1. If the \(z\)-score
variance is substantially smaller than \(1\), then the model
overestimates the actual prediction uncertainty. If the \(z\)-score
variance is substantially greater than \(1\), then the model
underestimates the prediction uncertainty. The difference between the
actual and predicted model error can be also referred to as the
\emph{model reliability}. A model can be accurate but then
\emph{`overpessimistic'} if the predicted model uncertainty is wider
than the actual uncertainty, or accurate but \emph{`overoptimistic'} if
the reported confidence limits are too narrow (Fig.
\ref{fig:difference-accuracy-reliability}).

Ideally, we aim to produce prediction, and prediction error, maps that
are both accurate and realistic; or at least realistic. For a review of
methods for assessment of uncertainty in soil maps refer to
\citet[pp.3--26]{goovaerts2001geostatistical} and/or
\citet{Brus2011EJSS}.

\begin{figure}[t]

{\centering \includegraphics[width=0.85\linewidth]{figures/Fig_difference_accuracy_reliability} 

}

\caption{Mapping accuracy and model reliability (accuracy of the prediction intervals vs actual intervals). Although a method can be accurate in predicting the mean values, it could fail in predicting the prediction intervals i.e. the associated uncertainty.}\label{fig:difference-accuracy-reliability}
\end{figure}

In the case discussed above (Fig. \ref{fig:z-scores-histogram}) it
appears that the error estimated by the model is often different from
the actual regression-kriging variance: in this case the estimated
values are often lower than actual measured values (under-estimation),
so that the whole histogram shifts toward 0 value. Because the variance
of the \(z\)-scores is \textless{}1:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(om.rk.cv}\OperatorTok{$}\NormalTok{zscore, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> [1] 0.95}
\end{Highlighting}
\end{Shaded}

we can also say that the regression-kriging variance is slightly
over-pessimistic or too conservative about the actual accuracy of the
model. On the other hand, Fig. \ref{fig:z-scores-histogram} shows that,
at some points, the cross-validation errors are much higher than the
error estimated by the model.

\hypertarget{derivation-and-interpretation-of-prediction-interval}{%
\subsection{Derivation and interpretation of prediction
interval}\label{derivation-and-interpretation-of-prediction-interval}}

Another important issue for understanding the error budget is derivation
of \emph{prediction interval} i.e.~upper and lower values of the target
variable for which we assume that our predictions will fall within, with
a high probability (e.g.~19 out of 20 times or the 95\% probability).
Prediction interval or \emph{confidence limits} are commonly well
accepted by users as the easiest way to communicate uncertainty
\citep{brodlie2012review}. For example, organic carbon in Meuse study
area (based on 153 samples of organic matter) has a 95\% interval of
2--16\%:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{signif}\NormalTok{(}\KeywordTok{quantile}\NormalTok{(meuse}\OperatorTok{$}\NormalTok{om, }\KeywordTok{c}\NormalTok{(.}\DecValTok{025}\NormalTok{, }\FloatTok{.975}\NormalTok{), }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\CommentTok{#> 2.5%  98% }
\CommentTok{#>    2   16}
\end{Highlighting}
\end{Shaded}

We have previously fitted a geostatistical model using two covariates,
which can now be used to generate predictions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{om.rk <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(omm, meuse.grid)}
\CommentTok{#> Subsetting observations to fit the prediction domain in 2D...}
\CommentTok{#> Generating predictions using the trend model (RK method)...}
\CommentTok{#> [using ordinary kriging]}
\CommentTok{#> }
 \DecValTok{50}\NormalTok{% done}
\DecValTok{100}\NormalTok{% done}
\CommentTok{#> Running 5-fold cross validation using 'krige.cv'...}
\CommentTok{#> Creating an object of class "SpatialPredictions"}
\end{Highlighting}
\end{Shaded}

and which allows us to estimate the confidence limits for organic matter
(assuming normal distribution) at any location within the study area
e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pt1 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\DecValTok{179390}\NormalTok{, }\DataTypeTok{y=}\DecValTok{330820}\NormalTok{)}
\KeywordTok{coordinates}\NormalTok{(pt1) <-}\StringTok{ }\ErrorTok{~}\NormalTok{x}\OperatorTok{+}\NormalTok{y}
\KeywordTok{proj4string}\NormalTok{(pt1) =}\StringTok{ }\KeywordTok{proj4string}\NormalTok{(meuse.grid)}
\NormalTok{pt1.om <-}\StringTok{ }\KeywordTok{over}\NormalTok{(pt1, om.rk}\OperatorTok{@}\NormalTok{predicted[}\StringTok{"om"}\NormalTok{])}
\NormalTok{pt1.om.sd <-}\StringTok{ }\KeywordTok{over}\NormalTok{(pt1, om.rk}\OperatorTok{@}\NormalTok{predicted[}\StringTok{"var1.var"}\NormalTok{])}
\KeywordTok{signif}\NormalTok{(}\KeywordTok{expm1}\NormalTok{(pt1.om}\FloatTok{-1.645}\OperatorTok{*}\KeywordTok{sqrt}\NormalTok{(pt1.om.sd)), }\DecValTok{2}\NormalTok{)}
\CommentTok{#>    om}
\CommentTok{#> 1 4.6}
\KeywordTok{signif}\NormalTok{(}\KeywordTok{expm1}\NormalTok{(pt1.om}\FloatTok{+1.645}\OperatorTok{*}\KeywordTok{sqrt}\NormalTok{(pt1.om.sd)), }\DecValTok{2}\NormalTok{)}
\CommentTok{#>    om}
\CommentTok{#> 1 8.9}
\end{Highlighting}
\end{Shaded}

where 4.6--8.9 are the upper and lower confidence limits. This interval
can also be expressed as:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{signif}\NormalTok{((}\KeywordTok{expm1}\NormalTok{(pt1.om}\FloatTok{+1.645}\OperatorTok{*}\KeywordTok{sqrt}\NormalTok{(pt1.om.sd)) }\OperatorTok{-}
\StringTok{       }\KeywordTok{expm1}\NormalTok{(pt1.om}\FloatTok{-1.645}\OperatorTok{*}\KeywordTok{sqrt}\NormalTok{(pt1.om.sd)))}\OperatorTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\CommentTok{#>    om}
\CommentTok{#> 1 2.1}
\end{Highlighting}
\end{Shaded}

or 6.3 ± 2.1 where half the error of estimating organic matter at that
location is about 1 s.d. Note that these are location specific
prediction intervals and need to be computed for each location.

To visualize the range of values within different strata, we can use
simulations that we can generate using the geostatistical model (which
can be time-consuming to compute!):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{om.rksim <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(omm, meuse.grid, }\DataTypeTok{nsim=}\DecValTok{5}\NormalTok{)}
\CommentTok{#> Subsetting observations to fit the prediction domain in 2D...}
\CommentTok{#> Generating 5 conditional simulations using the trend model (RK method)...}
\CommentTok{#> drawing 5 GLS realisations of beta...}
\CommentTok{#> [using conditional Gaussian simulation]}
\CommentTok{#> }
  \DecValTok{5}\NormalTok{% done}
 \DecValTok{14}\NormalTok{% done}
 \DecValTok{20}\NormalTok{% done}
 \DecValTok{26}\NormalTok{% done}
 \DecValTok{31}\NormalTok{% done}
 \DecValTok{35}\NormalTok{% done}
 \DecValTok{39}\NormalTok{% done}
 \DecValTok{44}\NormalTok{% done}
 \DecValTok{47}\NormalTok{% done}
 \DecValTok{51}\NormalTok{% done}
 \DecValTok{55}\NormalTok{% done}
 \DecValTok{61}\NormalTok{% done}
 \DecValTok{66}\NormalTok{% done}
 \DecValTok{69}\NormalTok{% done}
 \DecValTok{73}\NormalTok{% done}
 \DecValTok{75}\NormalTok{% done}
 \DecValTok{78}\NormalTok{% done}
 \DecValTok{81}\NormalTok{% done}
 \DecValTok{85}\NormalTok{% done}
 \DecValTok{89}\NormalTok{% done}
 \DecValTok{92}\NormalTok{% done}
 \DecValTok{96}\NormalTok{% done}
\DecValTok{100}\NormalTok{% done}
\CommentTok{#> Creating an object of class "RasterBrickSimulations"}
\NormalTok{ov <-}\StringTok{ }\KeywordTok{as}\NormalTok{(om.rksim}\OperatorTok{@}\NormalTok{realizations, }\StringTok{"SpatialGridDataFrame"}\NormalTok{)}
\NormalTok{meuse.grid}\OperatorTok{$}\NormalTok{om.sim1 <-}\StringTok{ }\KeywordTok{expm1}\NormalTok{(ov}\OperatorTok{@}\NormalTok{data[,}\DecValTok{1}\NormalTok{][meuse.grid}\OperatorTok{@}\NormalTok{grid.index])}
\NormalTok{meuse.grid}\OperatorTok{$}\NormalTok{om.rk <-}\StringTok{ }\KeywordTok{expm1}\NormalTok{(om.rk}\OperatorTok{@}\NormalTok{predicted}\OperatorTok{$}\NormalTok{om)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(om}\OperatorTok{~}\NormalTok{ffreq, omm}\OperatorTok{@}\NormalTok{regModel}\OperatorTok{$}\NormalTok{data, }\DataTypeTok{col=}\StringTok{"grey"}\NormalTok{,}
    \DataTypeTok{xlab=}\StringTok{"Flooding frequency classes"}\NormalTok{,}
    \DataTypeTok{ylab=}\StringTok{"Organic matter in %"}\NormalTok{,}
    \DataTypeTok{main=}\StringTok{"Sampled (N = 153)"}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{20}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(om.sim1}\OperatorTok{~}\NormalTok{ffreq, meuse.grid, }\DataTypeTok{col=}\StringTok{"grey"}\NormalTok{,}
    \DataTypeTok{xlab=}\StringTok{"Flooding frequency classes"}\NormalTok{,}
    \DataTypeTok{ylab=}\StringTok{"Organic matter in %"}\NormalTok{,}
    \DataTypeTok{main=}\StringTok{"Predicted (spatial simulations)"}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{20}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[t]

{\centering \includegraphics[width=0.9\linewidth]{Statistical_theory_files/figure-latex/confidence-limits-boxplot-1} 

}

\caption{Prediction intervals for three flooding frequency classes for sampled and predicted soil organic matter. The grey boxes show 1st and 3rd quantiles i.e. range where of data falls.}\label{fig:confidence-limits-boxplot}
\end{figure}

Fig. \ref{fig:confidence-limits-boxplot} shows that the confidence
limits for samples and (\textbf{\emph{WHAT}}) based on the
geostatistical model are about the same width (grey boxes in the plot
showing 1st and 3rd quantile), which should be the case because
geostatistical simulations are supposed maintain the original variances
(see also Fig. \ref{fig:hist-om-predicted-simulated}).

What is also often of interest to soil information users is the error of
estimating the mean value i .e. \emph{standard error of the mean}
(\({\rm{SE}}_{\bar{x}}\)), which can be derived using samples only
\citep{kutner2005applied}:

\begin{equation}
{\rm{SE}}_{\bar{x}} = \frac{\sigma_x}{\sqrt{n-1}}
\label{eq:mean-pop}
\end{equation}

or in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sd.om <-}\StringTok{ }\KeywordTok{qt}\NormalTok{(}\FloatTok{0.975}\NormalTok{, }\DataTypeTok{df=}\KeywordTok{length}\NormalTok{(meuse}\OperatorTok{$}\NormalTok{om)}\OperatorTok{-}\DecValTok{1}\NormalTok{) }\OperatorTok{*}
\StringTok{    }\KeywordTok{sd}\NormalTok{(meuse}\OperatorTok{$}\NormalTok{om, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{length}\NormalTok{(meuse}\OperatorTok{$}\NormalTok{om))}
\NormalTok{sd.om}
\CommentTok{#> [1] 0.54}
\end{Highlighting}
\end{Shaded}

Note that this is (only) the error of estimating the population mean,
which is much narrower than the actual variation inside the units. This
number does not mean that we can estimate organic matter at any location
with precision of ±0.54! This number means that, if we would like to
estimate (aggregated) mean value for the whole population, then the
standard error of that mean would be ±0.54. In other words the
population mean for organic matter based on 153 samples is 7.48 ± 0.54,
but if we would know the values of organic matter at specific,
individual locations, then the confidence limits are about 7.48 ± 3.4
(where 3.4 is the standard error).

The actual variation within the units based on simulations is:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lapply}\NormalTok{(}\KeywordTok{levels}\NormalTok{(meuse.grid}\OperatorTok{$}\NormalTok{ffreq), }\ControlFlowTok{function}\NormalTok{(x)\{}
    \KeywordTok{sapply}\NormalTok{(}\KeywordTok{subset}\NormalTok{(meuse.grid}\OperatorTok{@}\NormalTok{data, ffreq}\OperatorTok{==}\NormalTok{x,}
           \DataTypeTok{select=}\NormalTok{om.sim1), sd, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{\})}
\CommentTok{#> [[1]]}
\CommentTok{#> om.sim1 }
\CommentTok{#>       3 }
\CommentTok{#> }
\CommentTok{#> [[2]]}
\CommentTok{#> om.sim1 }
\CommentTok{#>     2.4 }
\CommentTok{#> }
\CommentTok{#> [[3]]}
\CommentTok{#> om.sim1 }
\CommentTok{#>     1.9}
\end{Highlighting}
\end{Shaded}

This can be confusing especially if the soil data producer does not
clearly report if the confidence limits refer to the population mean, or
to individual values. In principle, most users are interested in the
confidence limits of measuring some value at an individual location,
which are always considerably wider than the confidence limits of
estimating the population mean.

Assessment of the confidence limits should be best considered as a
regression problem, in fact. It can easily be shown that, by fitting a
regression model on strata, we automatically get an estimate of
confidence limits for the study area:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{omm0 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(om}\OperatorTok{~}\NormalTok{ffreq}\DecValTok{-1}\NormalTok{, omm}\OperatorTok{@}\NormalTok{regModel}\OperatorTok{$}\NormalTok{data)}
\NormalTok{om.r <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(omm0, meuse.grid, }\DataTypeTok{se.fit=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{meuse.grid}\OperatorTok{$}\NormalTok{se.fit <-}\StringTok{ }\NormalTok{om.r}\OperatorTok{$}\NormalTok{se.fit}
\KeywordTok{signif}\NormalTok{(}\KeywordTok{mean}\NormalTok{(meuse.grid}\OperatorTok{$}\NormalTok{se.fit, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{), }\DecValTok{3}\NormalTok{)}
\CommentTok{#> [1] 0.48}
\end{Highlighting}
\end{Shaded}

This number is similar to 0.54, which we derived directly from the
simulations. The difference in the values is because the regression
model estimates the prediction intervals for the whole study area based
on the covariate data (and not only for the sampling locations). The
value is also different than the previously derived 0.54 because we use
\texttt{ffreq} stratification as a covariate, so that, as long as the
strata is relatively homogenous, the confidence limits get narrower.

\begin{rmdnote}
Prediction intervals (upper and lower ranges of expected values with
some high probability) are possibly the most accepted way to communicate
uncertainty. Users are commonly interested in what the probability
confidence limits are of measuring some value at a specific location, or
the high probability prediction range.
\end{rmdnote}

To estimate the actual prediction intervals of estimating individual
values (estimation error) we need to add the residual scale value which
is a constant number:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(meuse.grid}\OperatorTok{$}\NormalTok{se.fit}\OperatorTok{^}\DecValTok{2}\OperatorTok{+}\NormalTok{om.r}\OperatorTok{$}\NormalTok{residual.scale}\OperatorTok{^}\DecValTok{2}\NormalTok{),}
     \DataTypeTok{by=}\KeywordTok{list}\NormalTok{(meuse.grid}\OperatorTok{$}\NormalTok{ffreq), mean, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>   Group.1   x}
\CommentTok{#> 1       1 3.3}
\CommentTok{#> 2       2 3.3}
\CommentTok{#> 3       3 3.3}
\end{Highlighting}
\end{Shaded}

and if we compare these limits to the confidence bands for the values
predicted by the geostatistical model fitted above:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(meuse.grid}\OperatorTok{$}\NormalTok{om.sim1, }\DataTypeTok{by=}\KeywordTok{list}\NormalTok{(meuse.grid}\OperatorTok{$}\NormalTok{ffreq), sd, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>   Group.1   x}
\CommentTok{#> 1       1 3.0}
\CommentTok{#> 2       2 2.4}
\CommentTok{#> 3       3 1.9}
\end{Highlighting}
\end{Shaded}

we can clearly see that the geostatistical model has helped us narrow
down the confidence limits, especially for class \texttt{3}.

\hypertarget{universal-measures-of-mapping-accuracy}{%
\subsection{Universal measures of mapping
accuracy}\label{universal-measures-of-mapping-accuracy}}

In the examples above, we have seen that mapping accuracy can be
determined by running cross-validation and determining e.g.
\(\it{RMSE}\) and R-square. In addition to R--square, a more universal
measure of prediction success is the Lin's Concordance Correlation
Coefficient (CCC) \citep{steichen2002note}:

\begin{equation}
\rho_c = \frac{2 \cdot \rho \cdot \sigma_{\hat y} \cdot \sigma_y }{ \sigma_{\hat y}^2 + \sigma_y^2 + (\mu_{\hat y} - \mu_y)^2}
\label{eq:CCC}
\end{equation}

where \(\hat y\) are the predicted values and \(y\) are actual values at
cross-validation points, \(\mu_{\hat y}\) and \(\mu_y\) are predicted
and observed means and \(\rho\) is the correlation coefficient between
predicted and observed values. CCC correctly quantifies how far the
observed data deviate from the line of perfect concordance (1:1 line in
Fig. \ref{fig:validation-scheme}). It is usually equal to or somewhat
lower than R--square, depending on the amount of bias in predictions.

\begin{figure}[t]

{\centering \includegraphics[width=0.8\linewidth]{figures/Fig_validation_plots} 

}

\caption{Universal plots of predictive performance: (a) 1:1 predicted vs observed plot, (b) CCC vs standard deviation of the z-scores plot, (c) nominal vs coverage probabilities, and (d) variogram of cross-validation residuals. For more detail see: @Hengl2018RFsp.}\label{fig:validation-scheme}
\end{figure}

CCC and variance or standard deviation of the z-scores are two universal
/ scale-free parameters that can be used to assign multiple spatial
prediction algorithms to work on multiple soil variables. Two additional
measures of the predictive performance of a mapping algoritm are the
spatial dependence structure in the cross-validation residuals and so
called \emph{``accuracy plots''} i.e. \citep{goovaerts1999geostatistics}
(Fig. \ref{fig:validation-scheme}). Ideally, a variogram of the
residuals should show no spatial dependence (i.e. pure nugget effect),
which is a proof that there is no spatial bias in predictions. Likewise,
nominal vs coverage probabilities in the target variable should also
ideally be on a 1:1 line.

So in summary, four universal measures to access predicitive success of
any spatial prediction method are \citep{Hengl2018RFsp}:

\begin{itemize}
\tightlist
\item
  \textbf{Concordance Correlation Coefficient} (0--1): showing
  predictive success of a method on a 1:1 predictions vs observations
  plot,
\item
  \textbf{Variance of the z-scores} (0--\(\infty\)): showing how
  reliable the modeled estimate of the prediction errors is,
\item
  \textbf{Variogram of the cross-validation residuals}: showing whether
  residuals still contain spatial dependence structure,
\item
  \textbf{Accuracy plots}: showing whether the model over- or
  under-estimates either lower or higher values,
\end{itemize}

\hypertarget{mapping-accuracy-and-soil-survey-costs}{%
\subsection{Mapping accuracy and soil survey
costs}\label{mapping-accuracy-and-soil-survey-costs}}

Once the accuracy of some model have been assessed, the next measure of
overall mapping success of interest is the soil information production
costs. Undoubtedly, producing soil information costs money.
\citet{Burrough1971}, \citet{BieUlph1972JAE}, and \citet{Bie1973JSS}
postulated in the early 70s that the survey costs are a direct function
of the mapping scale:

\begin{equation}
\log \begin{Bmatrix}
{\rm cost} \; {\rm per} \; {\rm km}^2\\
{\rm or}\\
{\rm man-days} \; {\rm per} \; {\rm km}^2
\end{Bmatrix}
= a + b \cdot \log( {\rm map} \; {\rm scale} )
\label{eq:burrough}
\end{equation}

To produce soil information costs money. On the other hand soil
information, if used properly, can lead to significant financial
benefits: accurate soil information is a tool to improve decision
making, increase crop and livestock production and help to reduce
investments risk and planning for environmental conservation.

This model typically explains \textgreater{}75\% of the survey costs
\citep{Burrough1971}. Further more, for the given target scale,
\emph{standard soil survey costs} can be commonly expressed as:

\begin{equation}
\theta  = \frac{{\rm X}}{A} \qquad [{\rm USD} \; {\rm km}^{-2} ]
\label{eq:mappingcosts}
\end{equation}

where \({\rm X}\) is the total costs of a survey, \(A\) is the size of
area in km-square. So for example, according to
\citet[p.75]{Legros2006SP}, to map 1 hectare of soil at 1:200,000 scale
(at the beginning of the 21st century), one needs at least 0.48 Euros
(i.e.~48 EUR to map a square-km); to map soil at 1:20 would cost about
25 EUR per ha. These are the all-inclusive costs that include salaries
and time in the office needed for the work of synthesis and editing.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{figures/Fig_scale_costs_ratio} 

}

\caption{Some basic concepts of soil survey: (a) relationship between cartographic scale and pixel size [@Hengl2006CG], (b) soil survey costs and scale relationship based on the empirical data of @Legros2006SP.}\label{fig:scale-costs-ratio}
\end{figure}

Estimated standard soil survey costs per area differ from country to
country. The USDA estimates that the total costs of soil mapping at
their most detailed scale (1:20) are about 1.50 USD per acre i.e.~about
4 USD per ha \citep{eltit2008}; in Canada, typical costs of producing
soil maps at 1:20 are in the range 3--10 CAD per ha
\citep{MacMillan2010DSM}; in the Netherlands 3.4 EUR per ha \citep[
pp.~149--154]{Kempen2011PhDthesis}; in New Zealand 4 USD per ha
\citep{Carrick2010WCSS}. Based on these national-level numbers,
\citet{Hengl2013JAG} undertook to produce a global estimate of soil
survey costs. So for example, to map 1 hectare of land at 1:20 scale,
one would need (at least) 5 USD, and to map soil at 1:200,000 scale
globally would cost about 8 USD per square-kilometer using conventional
soil mapping methods.

A scale of 1:200,000 corresponds approximately to a ground resolution of
100 m (Fig. \ref{fig:scale-costs-ratio}). If we would like to open a
call to map the world's soils (assuming that total land area to map is
about 104 millions of square-km) using contemporary methods at 100 m
resolution, and if we would consider 8 USD per square-kilometer as a
reasonable cost, then the total costs for mapping the total productive
soil areas of the world would be about 872 million USD. Of course, many
countries in the world have already been mapped at a scale of 1:200,000
or finer, so this number could be reduced by at least 30\%, but even
then we would still need a considerable budget. This is just to
illustrate that soil mapping can cost an order of magnitude more than,
for example, land cover mapping.

Producing soil information costs money, but it also leads to financial
benefits. \citet{Pimentel2006Springer} for example shows that the costs
of soil erosion, measured just by the cost of replacing lost water and
nutrients, is on the order of 250 billion USD annually. Soil
information, if used properly, can also lead to increased crop and
livestock production. \citet{Carrick2010WCSS}, for example, show that
soil survey that costs (only) 3.99 USD per hectare, can lead to better
management practices that help retain nitrogen in the soil at a rate of
42.49 USD per kg (17.30 USD per kg for farmers, 25.19 USD per kg for the
community). This also demonstrates that soil mapping can be a profitable
business.

The formula in Eq.\eqref{eq:mappingcosts} is somewhat incomplete as it
tells us only about the cost of mapping per unit area. Obviously,
mapping efficiency has to be expressed within the context of the mapping
objective. Hence, a more informative measure of \emph{mapping
efficiency} is \citep{Hengl2013JAG}:

\begin{equation}
\theta  = \frac{{\rm X}}{{A \cdot {\Sigma}_{\%}}} \qquad [{\rm USD} \; {\rm km}^{-2} \; \%^{-1} ]
\label{eq:efficiency}
\end{equation}

where \({\Sigma}_{\%}\) is the amount of variation explained by the
spatial prediction model (Eq.\eqref{eq:normvar}). In other words, soil
mapping efficiency is the total cost of explaining each percent of
variation in target soil variables for a given area of interest.

\begin{figure}[t]

{\centering \includegraphics[width=0.8\linewidth]{figures/Fig_costs_RMSE_scheme} 

}

\caption{General relationship between the sampling intensity (i.e. survey costs) and amount of variation in the target variable explained by a spatial prediction model. After @Hengl2013JAG.}\label{fig:costs-RMSE-scheme}
\end{figure}

An even more universal measure of mapping efficiency is the Information
Production Efficiency (IPE) \citep{Hengl2013JAG}:

\begin{equation}
\Upsilon = \frac{{\rm X}}{{\rm gzip}} \qquad [{\rm EUR} \; {\rm B}^{-1}]
\label{eq:data-efficiency}
\end{equation}

where \({\rm gzip}\) is the size of data (in Bytes) left after
compression and after recoding the values to match the effective
precision (\(\delta \approx {\rm RMSE}/2\)). Information Production
Efficiency is scale independent as the area is not included in the
equation and hence can be used to compare the efficiency of various
different soil mapping projects.

\begin{rmdnote}
Soil mapping efficiency can be expressed as the cost of producing bytes
of information about the target soil variables for a given area of
interest. This allows for an objective comparison of prediction
efficiency for different soil variables for different study areas.
\end{rmdnote}

\hypertarget{summary-points-2}{%
\subsection{Summary points}\label{summary-points-2}}

Soil mapping processes are increasingly being automated, which is mainly
due to advances in software for statistical computing and growing
processing speed and computing capacity. Fully automated geostatistical
mapping, i.e.~generation of spatial predictions with little to no human
interaction, is today a growing field of geoinformation science
\citep{Pebesma2011CompGeoSci, Brown2014JSS, Hengl2014SoilGrids1km}. Some
key advantages of using automated soil mapping versus more conventional,
traditional expert-based soil mapping are
\citep{heuvelink2010implications, Bivand2013Springer}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  All rules required to produce outputs are formalized. The whole
  procedure is documented (the statistical model and associated computer
  script), enabling reproducible research.
\item
  Predicted surfaces can make use of various information sources and can
  be optimized relative to all available quantitative point and
  covariate data.
\item
  There is more flexibility in terms of the spatial extent, resolution
  and support of requested maps.
\item
  Automated mapping is more cost-effective: once the system is
  operational, maintenance and production of updates are an order of
  magnitude faster and cheaper. Consequently, prediction maps can be
  updated and improved at shorter and shorter time intervals.
\item
  Spatial prediction models typically provide quantitative measures of
  prediction uncertainty (for each prediction location), which are often
  not provided in the case of conventional soil mapping.
\end{enumerate}

A disadvantage of automated soil mapping is that many statistical and
machine learning techniques are sensitive to errors and inconsistencies
in input data. A few typos, misaligned spatial coordinates or
misspecified models can create serious artifacts and reduce prediction
accuracy, more so than with traditional methods. Also, fitting models
using large and complex data sets can be time consuming and selection of
the \emph{`best'} model is often problematic. Explicit incorporation of
conceptual pedological (expert) knowledge, which can be important for
prediction in new situations to address the above issues, can be
challenging as well.

In contemporary soil mapping, traditional concepts such as soil map
scale and size of delineations are becoming increasingly dated or
secondary. The focus of contemporary soil mapping is on minimizing costs
required to explain variation in the target variable, while support size
of the output maps can be set by the user. The amount of variation
explained by a given statistical model gradually increases with sampling
intensity, until it reaches some physical limit and does not result in
any further improvements. Short-range variability and measurement error,
e.g.~the portion of the variation that cannot be captured or expressed
by the model, for many soil variables can be as great as 10--40\% (Fig.
\ref{fig:costs-RMSE-scheme}).

A useful thing for soil mapping teams is to compare a list of valid
competing models and plot the differences for comparison studies using
what we call \emph{``predictograms''} (as illustrated in Fig.
\ref{fig:cost-methods-scheme}). Such comparison studies permit us to
determine the best performing, and most cost effective, pedometric
method for an area of interest and a list of target variables.

\begin{figure}[t]

{\centering \includegraphics[width=0.8\linewidth]{figures/Fig_costs_RMSE_scheme-2} 

}

\caption{An schematic example of a performance plot (*‘predictogram’*) for comparing spatial prediction models. For more details see: @Hengl2013JAG.}\label{fig:cost-methods-scheme}
\end{figure}

In summary, gauging the success of soil mapping basically boils down to
the amount of variation explained by the spatial prediction model
i.e.~quantity of effective bytes produced for the data users. The survey
costs are mainly a function of sampling intensity i.e.~field work and
laboratory data analysis. As we collect more samples for an area of
interest we explain more and more of the total variance, until we reach
some maximum feasible \emph{locked} variation (Fig.
\ref{fig:cost-methods-scheme}). For a given total budget and a list of
target variables an optimal (most efficient) prediction method can be
determined by deriving the mapping efficiency described in
Eq.\eqref{eq:efficiency} or even better Eq.\eqref{eq:data-efficiency}.

\begin{rmdnote}
Modern soil mapping is driven by the objective assessment of accuracy
--- emphasis is put on using methods and covariate layers that can
produce the most accurate soil information given available resources,
and much less on expert opinion or preference.
\end{rmdnote}

By reporting on the RMSE, effective precision, information production
efficiency, and by plotting the prediction variance estimated by the
model, one gets a fairly good idea about the overall added information
value in a given map. In other words, by assessing the accuracy of a map
we can both recommend ways to improve the predictions (i.e.~collect
additional samples), and estimate the resources needed to reach some
target accuracy. By assessing how the accuracy of various methods
changes for various sampling intensities (Fig.
\ref{fig:cost-methods-scheme}), we can distinguish between methods that
are more suited for particular regions, data sets or sizes of area and
optimum methods that outperform all alternatives.

\hypertarget{references}{%
\chapter*{References}\label{references}}


\bibliography{refs.bib}

\backmatter
\printindex

\end{document}
