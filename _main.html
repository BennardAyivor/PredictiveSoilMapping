<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Predictive Soil Mapping with R</title>
  <meta name="description" content="Predictive Soil Mapping aims to produce the most accurate, most objective, and most usable maps of soil variables by using state-of-the-art Statistical and Machine Learning methods. This books explains how to implement common soil mapping procedures within the R programming language.">
  <meta name="generator" content="bookdown <!--bookdown:version--> and GitBook 2.6.7">

  <meta property="og:title" content="Predictive Soil Mapping with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://soilmapper.org" />
  <meta property="og:image" content="http://soilmapper.orgfigures/f0_web.png" />
  <meta property="og:description" content="Predictive Soil Mapping aims to produce the most accurate, most objective, and most usable maps of soil variables by using state-of-the-art Statistical and Machine Learning methods. This books explains how to implement common soil mapping procedures within the R programming language." />
  <meta name="github-repo" content="envirometrix/PredictiveSoilMapping" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Predictive Soil Mapping with R" />
  <meta name="twitter:site" content="@tom_hengl" />
  <meta name="twitter:description" content="Predictive Soil Mapping aims to produce the most accurate, most objective, and most usable maps of soil variables by using state-of-the-art Statistical and Machine Learning methods. This books explains how to implement common soil mapping procedures within the R programming language." />
  <meta name="twitter:image" content="http://soilmapper.orgfigures/f0_web.png" />

<meta name="author" content="Tomislav Hengl and Robert A. MacMillan">


<meta name="date" content="2018-12-19">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-120633517-1', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



<!--bookdown:title:start-->
<div id="header">
<h1 class="title">Predictive Soil Mapping with R</h1>
<p class="author"><em>Tomislav Hengl and Robert A. MacMillan</em></p>
<p class="date"><em>2018-12-19</em></p>
</div>
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul>
<li><a href="#predictive-soil-mapping-for-advanced-r-users">Predictive Soil Mapping for advanced R users</a><ul>
<li><a href="#editors">Editors</a></li>
</ul></li>
<li><a href="#preface">Preface</a><ul>
<li><a href="#connected-publications">Connected publications</a></li>
<li><a href="#contributions">Contributions</a></li>
<li><a href="#reproducibility">Reproducibility</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
</ul></li>
<li><a href="#introduction"><span class="toc-section-number">1</span> Soil resource inventories and soil maps</a><ul>
<li><a href="#introduction"><span class="toc-section-number">1.1</span> Introduction</a></li>
<li><a href="#soils-and-soil-inventories"><span class="toc-section-number">1.2</span> Soils and soil inventories</a><ul>
<li><a href="#soil-a-definition"><span class="toc-section-number">1.2.1</span> Soil: a definition</a></li>
<li><a href="#soil-variables"><span class="toc-section-number">1.2.2</span> Soil variables</a></li>
<li><a href="#primary-and-secondary-soil-variables"><span class="toc-section-number">1.2.3</span> Primary and secondary soil variables</a></li>
</ul></li>
<li><a href="#soil-mapping"><span class="toc-section-number">1.3</span> Soil mapping</a><ul>
<li><a href="#what-are-soil-resource-inventories"><span class="toc-section-number">1.3.1</span> What are soil resource inventories?</a></li>
<li><a href="#soil-mapping-approaches-and-concepts"><span class="toc-section-number">1.3.2</span> Soil mapping approaches and concepts</a></li>
<li><a href="#soil-mapping-theory"><span class="toc-section-number">1.3.3</span> Theoretical basis of soil mapping: in context of the universal model of spatial variation</a></li>
<li><a href="#conventional-mapping"><span class="toc-section-number">1.3.4</span> Traditional (conventional) soil mapping</a></li>
<li><a href="#variants-of-soil-maps"><span class="toc-section-number">1.3.5</span> Variants of soil maps</a></li>
<li><a href="#pedometric-mapping"><span class="toc-section-number">1.3.6</span> Predictive and automated soil mapping</a></li>
<li><a href="#comparison-conventional-pm"><span class="toc-section-number">1.3.7</span> Comparison of conventional and pedometric or predictive soil mapping</a></li>
<li><a href="#top-down"><span class="toc-section-number">1.3.8</span> Top-down versus bottom-up approaches: subdivision versus agglomeration</a></li>
</ul></li>
<li><a href="#sources-of-soil-data-for-soil-mapping"><span class="toc-section-number">1.4</span> Sources of soil data for soil mapping</a><ul>
<li><a href="#soil-data-sources-targeted-by-psm"><span class="toc-section-number">1.4.1</span> Soil data sources targeted by PSM</a></li>
<li><a href="#field-observations"><span class="toc-section-number">1.4.2</span> Field observations of soil properties</a></li>
<li><a href="#legacy-soil-profile-data"><span class="toc-section-number">1.4.3</span> Legacy soil profile data</a></li>
<li><a href="#soil-covariates"><span class="toc-section-number">1.4.4</span> Soil covariates</a></li>
<li><a href="#soil-delineations"><span class="toc-section-number">1.4.5</span> Soil delineations</a></li>
<li><a href="#advantages-and-disadvantages-of-using-soil-delineations"><span class="toc-section-number">1.4.6</span> Advantages and disadvantages of using soil delineations</a></li>
<li><a href="#accuracy-of-conventional-soil-polygon-maps"><span class="toc-section-number">1.4.7</span> Accuracy of conventional soil polygon maps</a></li>
<li><a href="#tacit-knowledge"><span class="toc-section-number">1.4.8</span> Legacy soil expertise (tacit knowledge)</a></li>
<li><a href="#pseudo-observations"><span class="toc-section-number">1.4.9</span> Pseudo-observations</a></li>
</ul></li>
<li><a href="#soil-databases"><span class="toc-section-number">1.5</span> Soil databases and soil information systems</a><ul>
<li><a href="#soil-databases"><span class="toc-section-number">1.5.1</span> Soil databases</a></li>
<li><a href="#a-soil-information-system"><span class="toc-section-number">1.5.2</span> A Soil Information System</a></li>
<li><a href="#soil-information-users"><span class="toc-section-number">1.5.3</span> Soil information users</a></li>
<li><a href="#usability-of-soil-geographical-database"><span class="toc-section-number">1.5.4</span> Usability of soil geographical database</a></li>
</ul></li>
<li><a href="#uncertainty-soil-variables"><span class="toc-section-number">1.6</span> Uncertainty of soil variables</a><ul>
<li><a href="#basic-concepts"><span class="toc-section-number">1.6.1</span> Basic concepts</a></li>
<li><a href="#sources-uncertainty"><span class="toc-section-number">1.6.2</span> Sources of uncertainty</a></li>
<li><a href="#quantifying-the-uncertainty-in-soil-data-products"><span class="toc-section-number">1.6.3</span> Quantifying the uncertainty in soil data products</a></li>
<li><a href="#common-uncertainty-levels-in-soil-maps"><span class="toc-section-number">1.6.4</span> Common uncertainty levels in soil maps</a></li>
</ul></li>
<li><a href="#summary-and-conclusions"><span class="toc-section-number">1.7</span> Summary and conclusions</a></li>
</ul></li>
<li><a href="#software"><span class="toc-section-number">2</span> Software installation and first steps</a><ul>
<li><a href="#list-of-software-in-use"><span class="toc-section-number">2.1</span> List of software in use</a></li>
<li><a href="#installing-software-on-ubuntu-os"><span class="toc-section-number">2.2</span> Installing software on Ubuntu OS</a></li>
<li><a href="#installing-gis-software"><span class="toc-section-number">2.3</span> Installing GIS software</a></li>
<li><a href="#Whitebox"><span class="toc-section-number">2.4</span> WhiteboxTools</a></li>
<li><a href="#Rstudio"><span class="toc-section-number">2.5</span> RStudio</a></li>
<li><a href="#plotkml-and-gsif-packages"><span class="toc-section-number">2.6</span> plotKML and GSIF packages</a></li>
<li><a href="#connecting-r-and-saga-gis"><span class="toc-section-number">2.7</span> Connecting R and SAGA GIS</a></li>
<li><a href="#connecting-r-and-gdal"><span class="toc-section-number">2.8</span> Connecting R and GDAL</a></li>
</ul></li>
<li><a href="#soil-variables-chapter"><span class="toc-section-number">3</span> Soil observations and variables</a><ul>
<li><a href="#basic-concepts-1"><span class="toc-section-number">3.1</span> Basic concepts</a><ul>
<li><a href="#types-of-soil-observations"><span class="toc-section-number">3.1.1</span> Types of soil observations</a></li>
<li><a href="#soil-properties-of-interest-for-global-soil-mapping"><span class="toc-section-number">3.1.2</span> Soil properties of interest for global soil mapping</a></li>
<li><a href="#reference-methods"><span class="toc-section-number">3.1.3</span> Reference methods</a></li>
<li><a href="#standard-soil-variables-of-interest-for-soil-mapping"><span class="toc-section-number">3.1.4</span> Standard soil variables of interest for soil mapping</a></li>
</ul></li>
<li><a href="#descriptive-soil-profile-observations"><span class="toc-section-number">3.2</span> Descriptive soil profile observations</a><ul>
<li><a href="#depth-to-bedrock"><span class="toc-section-number">3.2.1</span> Depth to bedrock</a></li>
<li><a href="#effective-soil-depth-and-rooting-depth"><span class="toc-section-number">3.2.2</span> Effective soil depth and rooting depth</a></li>
</ul></li>
<li><a href="#chemical-soil-properties"><span class="toc-section-number">3.3</span> Chemical soil properties</a><ul>
<li><a href="#soil-organic-carbon"><span class="toc-section-number">3.3.1</span> Soil organic carbon</a></li>
<li><a href="#soil-ph"><span class="toc-section-number">3.3.2</span> Soil pH</a></li>
<li><a href="#soil-nutrients"><span class="toc-section-number">3.3.3</span> Soil nutrients</a></li>
</ul></li>
<li><a href="#physical-and-hydrological-soil-properties"><span class="toc-section-number">3.4</span> Physical and hydrological soil properties</a><ul>
<li><a href="#coarse-fragments"><span class="toc-section-number">3.4.1</span> Coarse fragments</a></li>
<li><a href="#particle-size-class-distribution-sand-silt-and-clay"><span class="toc-section-number">3.4.2</span> Particle size class distribution: sand, silt and clay</a></li>
<li><a href="#bulk-density"><span class="toc-section-number">3.4.3</span> Bulk density</a></li>
<li><a href="#soil-organic-carbon-stock"><span class="toc-section-number">3.4.4</span> Soil organic carbon stock</a></li>
<li><a href="#available-water-capacity"><span class="toc-section-number">3.4.5</span> Available Water Capacity</a></li>
</ul></li>
<li><a href="#harmonisation-of-soil-data-and-pedo-transfer-functions"><span class="toc-section-number">3.5</span> Harmonisation of soil data and pedo-transfer functions</a><ul>
<li><a href="#basic-concepts-of-harmonisation-of-soil-property-values"><span class="toc-section-number">3.5.1</span> Basic concepts of harmonisation of soil property values</a></li>
<li><a href="#example-of-harmonization-using-texture-by-hand-classes"><span class="toc-section-number">3.5.2</span> Example of harmonization using texture-by-hand classes</a></li>
</ul></li>
<li><a href="#soil-class-data"><span class="toc-section-number">3.6</span> Soil class data</a><ul>
<li><a href="#soil-types"><span class="toc-section-number">3.6.1</span> Soil types</a></li>
<li><a href="#other-factor-type-variables"><span class="toc-section-number">3.6.2</span> Other factor-type variables</a></li>
</ul></li>
<li><a href="#importing-and-formatting-soil-data-in-r"><span class="toc-section-number">3.7</span> Importing and formatting soil data in R</a><ul>
<li><a href="#converting-texture-by-hand-classes-to-fractions"><span class="toc-section-number">3.7.1</span> Converting texture-by-hand classes to fractions</a></li>
</ul></li>
<li><a href="#converting-munsell-color-codes-to-other-color-systems"><span class="toc-section-number">3.8</span> Converting Munsell color codes to other color systems</a></li>
<li><a href="#mla-ptfs"><span class="toc-section-number">3.9</span> Using Machine Learning to build Pedo-Transfer-Functions</a><ul>
<li><a href="#ptf-for-bulk-density"><span class="toc-section-number">3.9.1</span> PTF for Bulk Density</a></li>
<li><a href="#ptf-for-correlating-classification-systems"><span class="toc-section-number">3.9.2</span> PTF for correlating classification systems</a></li>
</ul></li>
<li><a href="#summary-points"><span class="toc-section-number">3.10</span> Summary points</a></li>
</ul></li>
<li><a href="#soil-covs-chapter"><span class="toc-section-number">4</span> Preparation of soil covariates for soil mapping</a><ul>
<li><a href="#soil-covariate-data-sources"><span class="toc-section-number">4.1</span> Soil covariate data sources</a><ul>
<li><a href="#types-of-soil-covariates"><span class="toc-section-number">4.1.1</span> Types of soil covariates</a></li>
<li><a href="#soil-covs-30m"><span class="toc-section-number">4.1.2</span> Soil covariate data sources (30–100 m resolution)</a></li>
<li><a href="#soil-covs-250m"><span class="toc-section-number">4.1.3</span> Soil covariate data sources (250 m resolution or coarser)</a></li>
</ul></li>
<li><a href="#preparing-soil-covariate-layers"><span class="toc-section-number">4.2</span> Preparing soil covariate layers</a><ul>
<li><a href="#converting-polygon-maps-to-rasters"><span class="toc-section-number">4.2.1</span> Converting polygon maps to rasters</a></li>
<li><a href="#downscaling-upscaling"><span class="toc-section-number">4.2.2</span> Downscaling or upscaling (aggregating) rasters</a></li>
<li><a href="#deriving-dem-parameters-using-saga-gis"><span class="toc-section-number">4.2.3</span> Deriving DEM parameters using SAGA GIS</a></li>
<li><a href="#filtering-out-missing-pixels-and-artifacts"><span class="toc-section-number">4.2.4</span> Filtering out missing pixels and artifacts</a></li>
<li><a href="#overlaying-and-subsetting-raster-stacks-and-points"><span class="toc-section-number">4.2.5</span> Overlaying and subsetting raster stacks and points</a></li>
<li><a href="#working-with-larger-rasters"><span class="toc-section-number">4.2.6</span> Working with large(r) rasters</a></li>
</ul></li>
<li><a href="#summary-points-1"><span class="toc-section-number">4.3</span> Summary points</a></li>
</ul></li>
<li><a href="#statistical-theory"><span class="toc-section-number">5</span> Statistical theory for predictive soil mapping</a><ul>
<li><a href="#aspects-variability"><span class="toc-section-number">5.1</span> Aspects of spatial variability of soil variables</a><ul>
<li><a href="#modelling-soil-variability"><span class="toc-section-number">5.1.1</span> Modelling soil variability</a></li>
<li><a href="#umsv"><span class="toc-section-number">5.1.2</span> Universal model of soil variation</a></li>
<li><a href="#soil-depth-models"><span class="toc-section-number">5.1.3</span> Modelling the variation of soil with depth</a></li>
<li><a href="#vertical-aggregation"><span class="toc-section-number">5.1.4</span> Vertical aggregation of soil properties</a></li>
</ul></li>
<li><a href="#spatial-prediction-of-soil-variables"><span class="toc-section-number">5.2</span> Spatial prediction of soil variables</a><ul>
<li><a href="#main-principles"><span class="toc-section-number">5.2.1</span> Main principles</a></li>
<li><a href="#soil-sampling"><span class="toc-section-number">5.2.2</span> Soil sampling</a></li>
<li><a href="#sec:expertsystems"><span class="toc-section-number">5.2.3</span> Knowledge-driven soil mapping</a></li>
<li><a href="#regression-kriging"><span class="toc-section-number">5.2.4</span> Geostatistics-driven soil mapping (pedometric mapping)</a></li>
<li><a href="#RK-generic"><span class="toc-section-number">5.2.5</span> Regression-kriging (generic model)</a></li>
<li><a href="#spatial-prediction-using-multiple-linear-regression"><span class="toc-section-number">5.2.6</span> Spatial Prediction using multiple linear regression</a></li>
<li><a href="#universal-kriging-prediction-error"><span class="toc-section-number">5.2.7</span> Universal kriging prediction error</a></li>
<li><a href="#regression-kriging-examples"><span class="toc-section-number">5.2.8</span> Regression-kriging examples</a></li>
<li><a href="#regression-kriging-examples-using-the-gsif-package"><span class="toc-section-number">5.2.9</span> Regression-kriging examples using the GSIF package</a></li>
<li><a href="#regression-kriging-and-polygon-averaging"><span class="toc-section-number">5.2.10</span> Regression-kriging and polygon averaging</a></li>
<li><a href="#block-support"><span class="toc-section-number">5.2.11</span> Predictions at point vs block support</a></li>
<li><a href="#gstat-sims"><span class="toc-section-number">5.2.12</span> Geostatistical simulations</a></li>
<li><a href="#automated-mapping"><span class="toc-section-number">5.2.13</span> Automated mapping</a></li>
<li><a href="#selecting-spatial-prediction-models"><span class="toc-section-number">5.2.14</span> Selecting spatial prediction models</a></li>
<li><a href="#regression-kriging-3D"><span class="toc-section-number">5.2.15</span> 3D regression-kriging</a></li>
<li><a href="#multiscale"><span class="toc-section-number">5.2.16</span> Predicting with multiscale and multisource data</a></li>
</ul></li>
<li><a href="#accuracy-assessment"><span class="toc-section-number">5.3</span> Accuracy assessment and the mapping efficiency</a><ul>
<li><a href="#mapping-accuracy"><span class="toc-section-number">5.3.1</span> Mapping accuracy and numeric resolution</a></li>
<li><a href="#accuracy-assessment-methods"><span class="toc-section-number">5.3.2</span> Accuracy assessment methods</a></li>
<li><a href="#cross-validation-and-its-limitations"><span class="toc-section-number">5.3.3</span> Cross-validation and its limitations</a></li>
<li><a href="#accuracy-of-the-predicted-model-uncertainty"><span class="toc-section-number">5.3.4</span> Accuracy of the predicted model uncertainty</a></li>
<li><a href="#derivation-and-interpretation-of-prediction-interval"><span class="toc-section-number">5.3.5</span> Derivation and interpretation of prediction interval</a></li>
<li><a href="#universal-measures-of-mapping-accuracy"><span class="toc-section-number">5.3.6</span> Universal measures of mapping accuracy</a></li>
<li><a href="#mapping-accuracy-and-soil-survey-costs"><span class="toc-section-number">5.3.7</span> Mapping accuracy and soil survey costs</a></li>
<li><a href="#summary-points-2"><span class="toc-section-number">5.3.8</span> Summary points</a></li>
</ul></li>
</ul></li>
<li><a href="#soilmapping-using-mla"><span class="toc-section-number">6</span> Machine Learning Algorithms for soil mapping</a><ul>
<li><a href="#spatial-prediction-of-soil-properties-and-classes-using-mlas"><span class="toc-section-number">6.1</span> Spatial prediction of soil properties and classes using MLA’s</a><ul>
<li><a href="#loading-the-packages-and-data"><span class="toc-section-number">6.1.1</span> Loading the packages and data</a></li>
<li><a href="#spatial-prediction-of-soil-classes-using-mlas"><span class="toc-section-number">6.1.2</span> Spatial prediction of soil classes using MLA’s</a></li>
<li><a href="#modelling-numeric-soil-properties-using-h2o"><span class="toc-section-number">6.1.3</span> Modelling numeric soil properties using h2o</a></li>
<li><a href="#prediction-3D"><span class="toc-section-number">6.1.4</span> Spatial prediction of 3D (numeric) variables</a></li>
<li><a href="#ensemble-predictions-using-h2oensemble"><span class="toc-section-number">6.1.5</span> Ensemble predictions using h2oEnsemble</a></li>
<li><a href="#ensemble-predictions-using-superlearner-package"><span class="toc-section-number">6.1.6</span> Ensemble predictions using SuperLearner package</a></li>
</ul></li>
<li><a href="#a-generic-framework-for-spatial-prediction-using-random-forest"><span class="toc-section-number">6.2</span> A generic framework for spatial prediction using Random Forest</a><ul>
<li><a href="#general-principle-of-rfsp"><span class="toc-section-number">6.2.1</span> General principle of RFsp</a></li>
<li><a href="#geographical-covariates"><span class="toc-section-number">6.2.2</span> Geographical covariates</a></li>
<li><a href="#spatial-prediction-2d-continuous-variable-using-rfsp"><span class="toc-section-number">6.2.3</span> Spatial prediction 2D continuous variable using RFsp</a></li>
<li><a href="#spatial-prediction-2d-variable-with-covariates-using-rfsp"><span class="toc-section-number">6.2.4</span> Spatial prediction 2D variable with covariates using RFsp</a></li>
<li><a href="#spatial-prediction-of-binomial-variables"><span class="toc-section-number">6.2.5</span> Spatial prediction of binomial variables</a></li>
<li><a href="#spatial-prediction-of-soil-types"><span class="toc-section-number">6.2.6</span> Spatial prediction of soil types</a></li>
</ul></li>
<li><a href="#summary-points-3"><span class="toc-section-number">6.3</span> Summary points</a></li>
</ul></li>
<li><a href="#SOC-chapter"><span class="toc-section-number">7</span> Spatial prediction and assessment of Soil Organic Carbon</a><ul>
<li><a href="#introduction-1"><span class="toc-section-number">7.1</span> Introduction</a></li>
<li><a href="#measurement-and-derivation-of-soil-organic-carbon"><span class="toc-section-number">7.2</span> Measurement and derivation of soil organic carbon</a></li>
<li><a href="#derivation-of-ocs-and-ocd-using-soil-profile-data"><span class="toc-section-number">7.3</span> Derivation of OCS and OCD using soil profile data</a></li>
<li><a href="#estimation-of-bulk-density-using-a-globally-calibrated-ptf"><span class="toc-section-number">7.4</span> Estimation of Bulk Density using a globally-calibrated PTF</a></li>
<li><a href="#generating-maps-of-ocs"><span class="toc-section-number">7.5</span> Generating maps of OCS</a></li>
<li><a href="#predicting-ocs-from-point-data-the-2d-approach"><span class="toc-section-number">7.6</span> Predicting OCS from point data (the 2D approach)</a></li>
<li><a href="#ocs-3d-approach"><span class="toc-section-number">7.7</span> Deriving OCS from soil profile data (the 3D approach)</a></li>
<li><a href="#deriving-ocs-using-spatiotemporal-models"><span class="toc-section-number">7.8</span> Deriving OCS using spatiotemporal models</a></li>
<li><a href="#summary-points-4"><span class="toc-section-number">7.9</span> Summary points</a></li>
</ul></li>
<li><a href="#practical-tips"><span class="toc-section-number">8</span> Practical tips for organizing Predictive Soil Mapping</a><ul>
<li><a href="#critical-aspects-of-predictive-soil-mapping"><span class="toc-section-number">8.1</span> Critical aspects of Predictive Soil Mapping</a><ul>
<li><a href="#psm-main-steps"><span class="toc-section-number">8.1.1</span> PSM main steps</a></li>
<li><a href="#psm-input-and-output-spatial-data-layers"><span class="toc-section-number">8.1.2</span> PSM input and output spatial data layers</a></li>
</ul></li>
<li><a href="#technical-specifications-affecting-the-majority-of-production-costs"><span class="toc-section-number">8.2</span> Technical specifications affecting the majority of production costs</a><ul>
<li><a href="#field-observations-and-measurements"><span class="toc-section-number">8.2.1</span> Field observations and measurements</a></li>
<li><a href="#preparation-of-point-data"><span class="toc-section-number">8.2.2</span> Preparation of point data</a></li>
<li><a href="#preparation-of-covariates"><span class="toc-section-number">8.2.3</span> Preparation of covariates</a></li>
<li><a href="#soil-mask-and-the-grid-system"><span class="toc-section-number">8.2.4</span> Soil mask and the grid system</a></li>
<li><a href="#uncertainty-of-psm-maps"><span class="toc-section-number">8.2.5</span> Uncertainty of PSM maps</a></li>
<li><a href="#computing-costs"><span class="toc-section-number">8.2.6</span> Computing costs</a></li>
</ul></li>
<li><a href="#final-delivery-of-maps"><span class="toc-section-number">8.3</span> Final delivery of maps</a><ul>
<li><a href="#delivery-data-formats"><span class="toc-section-number">8.3.1</span> Delivery data formats</a></li>
<li><a href="#general-recommendations"><span class="toc-section-number">8.3.2</span> General recommendations</a></li>
<li><a href="#technical-specifications-psm-project"><span class="toc-section-number">8.3.3</span> Technical specifications PSM project</a></li>
<li><a href="#standard-soil-data-production-costs"><span class="toc-section-number">8.3.4</span> Standard soil data production costs</a></li>
</ul></li>
<li><a href="#summary-notes"><span class="toc-section-number">8.4</span> Summary notes</a></li>
</ul></li>
<li><a href="#the-future-of-predictive-soil-mapping"><span class="toc-section-number">9</span> The future of predictive soil mapping</a><ul>
<li><a href="#introduction-2"><span class="toc-section-number">9.1</span> Introduction</a></li>
<li><a href="#past-conventional-terrestrial-resource-inventories"><span class="toc-section-number">9.2</span> Past conventional terrestrial resource inventories</a><ul>
<li><a href="#why-have-most-national-resource-inventories-been-discontinued"><span class="toc-section-number">9.2.1</span> Why have most national resource inventories been discontinued?</a></li>
<li><a href="#is-there-a-future-for-conventional-terrestrial-inventory-programs"><span class="toc-section-number">9.2.2</span> Is there a future for conventional terrestrial inventory programs ?</a></li>
<li><a href="#can-terrestrial-inventory-programs-be-renewed-and-revived"><span class="toc-section-number">9.2.3</span> Can terrestrial inventory programs be renewed and revived?</a></li>
<li><a href="#how-can-terrestrial-inventory-programs-be-renewed-and-revived-and-by-whom"><span class="toc-section-number">9.2.4</span> How can terrestrial inventory programs be renewed and revived and by whom?</a></li>
</ul></li>
<li><a href="#the-future-of-psm-embracing-scientific-and-technical-advances"><span class="toc-section-number">9.3</span> The future of PSM: Embracing scientific and technical advances</a><ul>
<li><a href="#overview"><span class="toc-section-number">9.3.1</span> Overview</a></li>
<li><a href="#collection-of-field-observations-and-samples"><span class="toc-section-number">9.3.2</span> Collection of field observations and samples</a></li>
<li><a href="#collecting-new-field-om-data"><span class="toc-section-number">9.3.3</span> Collecting New Field O&amp;M Data</a></li>
<li><a href="#characterization-of-soils-in-the-field-and-the-laboratory"><span class="toc-section-number">9.3.4</span> Characterization of soils in the field and the laboratory</a></li>
<li><a href="#creation-collation-and-distribution-of-effective-environmental-covariates"><span class="toc-section-number">9.3.5</span> Creation, collation and distribution of effective environmental covariates,</a></li>
<li><a href="#automated-spatial-prediction-models-psm"><span class="toc-section-number">9.3.6</span> Automated spatial prediction models (PSM)</a></li>
<li><a href="#hosting-publishing-sharing-and-using-spatial-data"><span class="toc-section-number">9.3.7</span> Hosting, publishing, sharing and using spatial data</a></li>
<li><a href="#new-visualization-and-data-analysis-tools"><span class="toc-section-number">9.3.8</span> New visualization and data analysis tools</a></li>
</ul></li>
<li><a href="#the-future-of-psm-embracing-new-organizational-and-governance-models"><span class="toc-section-number">9.4</span> The future of PSM: Embracing new organizational and governance models</a><ul>
<li><a href="#overview-1"><span class="toc-section-number">9.4.1</span> Overview</a></li>
<li><a href="#open-data-and-platforms-and-procedures-for-acquiring-and-sharing-it"><span class="toc-section-number">9.4.2</span> Open data and platforms and procedures for acquiring and sharing it</a></li>
<li><a href="#open-cloud-based-processing-capabilities"><span class="toc-section-number">9.4.3</span> Open cloud-based processing capabilities</a></li>
<li><a href="#collaborative-production-of-inputs-and-new-outputs"><span class="toc-section-number">9.4.4</span> Collaborative production of inputs and new outputs</a></li>
<li><a href="#crowdsourcing-and-voluntary-collaboration"><span class="toc-section-number">9.4.5</span> Crowdsourcing and voluntary collaboration,</a></li>
<li><a href="#sponsorship-subscription-crowdfunding-and-blockchain-funding-systems"><span class="toc-section-number">9.4.6</span> Sponsorship, subscription, crowdfunding and blockchain funding systems,</a></li>
<li><a href="#a-proposal-for-organizing-and-managing-a-new-open-collective"><span class="toc-section-number">9.4.7</span> A proposal for organizing and managing a new open collective</a></li>
</ul></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Soil Mapping with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->
<div id="predictive-soil-mapping-for-advanced-r-users" class="section level1 unnumbered">
<h1>Predictive Soil Mapping for advanced R users</h1>
<p><img src="figures/f0_web.png" width="33%" style="display: block; margin: auto;" /></p>
<p>This is the online version of the Open Access book: <a href="https://envirometrix.github.io/PredictiveSoilMapping/"><strong>Predictive Soil Mapping with R</strong></a>. Pull requests and general comments are welcome. These materials are based on technical tutorials initially developed for <a href="http://isric.org/">ISRIC’s</a> Global Soil Information Facilities (GSIF) framework for automated soil mapping over the period 2014–2017.</p>
<p><img alt="Under construction" style="border-width:0" src="images/under-construction_640.png" /><br /><strong>This website is under construction</strong>. For news and updates please refer to the <a href="https://github.com/envirometrix/PredictiveSoilMapping/issues">github issues</a>.</p>
<p>Hard copies of this book from will be made available in early 2019.</p>
<div id="editors" class="section level2 unnumbered">
<h2>Editors</h2>
<p><a href="https://opengeohub.org/people/tom-hengl"><strong>Tom Hengl</strong></a> is a Senior Researchers and Vice Chair of the OpenGeoHub Foundation / technical director at Envirometrix Ltd. He has more than 20 years of experience as an environmental modeler, data scientist and spatial analyst. Tom is a passionate advocate for, and supporter of, open data, reproducible science and career development for young scientists. He has designed and implemented the global <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0169748">SoilGrids</a> data set, partially in response to other well known open data projects such as OpenStreetMap, GBIF, GlobalForestWatch and global climate mapping projects. He has been teaching predictive soil mapping at Wageningen University / ISRIC within the “Hands-on-GSIF” block courses. Video tutorials of the soil mapping with R can also be found at <a href="http://youtube.com/c/ISRICorg" class="uri">http://youtube.com/c/ISRICorg</a>. Tom currently leads production of a web mapping system called “LandGIS” (<a href="https://landgis.opengeohub.org" class="uri">https://landgis.opengeohub.org</a>) and which is envisaged as <em>“an OpenStreetMap-type system”</em> for land-related environmental data. The system hosts global, fine spatial resolution data (250 m to 1 km) including various soil classes and soil properties, and is intended for eventual integration with farm-scale data and beyond.</p>
<p><a href="https://opengeohub.org/people/bob-macmillan"><strong>Bob MacMillan</strong></a> is a retired environmental consultant with over 40 years of experience in creating, packaging, delivering and using environmental information on soils, ecosystems, landforms and hydrology. Bob spent 19 years working in public sector research with the Alberta Research Council and Agriculture and Agri-Food Canada and a second 20 years as a private sector consultant offering services in predictive soil and ecological mapping. Since retiring, Bob has remained an active supporter, promoter, advocate, mentor and technical contributor to several continental to global scale efforts to advance the science and technology of mapping soils and other ecosystem components. As Science Coordinator for the GlobalSoilMap project, Bob helped to articulate the vision for the project and led initial activities aimed at achieving this, including authoring technical specifications, promoting the project, recruiting participants/cooperators, and liaising with representatives of national and international soil agencies. Bob continues to contribute on a voluntary basis to OpenGeoHub (<a href="https://opengeohub.org/" class="uri">https://opengeohub.org/</a>), Africa Soil Information Servicce (AfSIS) (<a href="http://africasoils.net/" class="uri">http://africasoils.net/</a>) and the Canadian Digital Soil Data Consortium (CDSDC): (<a href="http://soilinfo.ca" class="uri">http://soilinfo.ca</a>). Throughout his career, Bob has shared his expertise and his enthusiasm freely with dozens of younger scientists interested in learning about, and becoming, practitioners of digital soil mapping. Bob continues to support the next generation of digital soil mappers through his involvement with OpenGeoHub.</p>
</div>
</div>
<div id="preface" class="section level1 unnumbered">
<h1>Preface</h1>
<p>Predictive Soil Mapping (PSM) is based on applying statistical and/or machine learning techniques to fit models for the purpose of producing spatial and/or spatiotemporal predictions of soil variables i.e. maps of soil properties and classes at different resolutions. It is a multidisciplinary field combining statistics, data science, soil science, physical geography, remote sensing, geoinformation science and a number of other sciences <span class="citation">(Scull et al. <a href="#ref-Scul01">2003</a>; McBratney, Mendonça Santos, and Minasny <a href="#ref-MCBRATNEY20033">2003</a>; Henderson et al. <a href="#ref-Henderson2004Geoderma">2004</a>; Boettinger et al. <a href="#ref-Boettinger2010Springer">2010</a>; Zhu et al. <a href="#ref-Zhu2015PSM">2015</a>)</span>. <em>Predictive Soil Mapping with R</em> is about understanding the main concepts behind soil mapping, mastering R packages that can be used to produce high quality soil maps, and about optimizing all processes involved so that also the production costs can be reduced.</p>
<p>The main idea behind predictive vs traditional expert-based soil mapping is that the production of maps: (a) is based on using state-of-the-art statistical methods to ensure objectivity of maps (including objective uncertainty assessment vs expert judgment), and (b) is driven by automation of the processes so that overall soil data production costs can be reduced and updates of the maps implemented without a need for large investments. R, in that sense, is a logical platform to develop PSM workflows and applications, especially thanks to the vibrant and productive R spatial interest group activities and also thanks to the increasingly professional soil data packages such as the soiltexture, aqp, soilprofile, soilDB and similar.</p>
<p>The book is divided into sections covering theoretical concepts, preparation of covariates, model selection and evaluation, prediction and visualization and distribution of final maps. Most of the chapters contain R code examples that try to illustrate the main processing steps and give practical instructions to developers and applied users.</p>
<div id="connected-publications" class="section level2 unnumbered">
<h2>Connected publications</h2>
<p>Most of methods described in this book are based on the following publications:</p>
<ul>
<li><p>Hengl, T., Nussbaum, M., Wright, M. N., Heuvelink, G. B., and Gräler, B. (2018) <a href="https://doi.org/10.7717/peerj.5518">Random Forest as a generic framework for predictive modeling of spatial and spatio-temporal variables</a>. PeerJ 6:e5518.</p></li>
<li><p>Sanderman, J., Hengl, T., Fiske, G., (2017) <a href="http://www.pnas.org/content/early/2017/08/15/1706103114.full">The soil carbon debt of 12,000 years of human land use</a>. PNAS, <a href="doi:10.1073/pnas.1706103114" class="uri">doi:10.1073/pnas.1706103114</a></p></li>
<li><p>Ramcharan, A., Hengl, T., Nauman, T., Brungard, C., Waltman, S., Wills, S., &amp; Thompson, J. (2018). <a href="https://dl.sciencesocieties.org/publications/sssaj/abstracts/82/1/186">Soil Property and Class Maps of the Conterminous United States at 100-Meter Spatial Resolution</a>. Soil Science Society of America Journal, 82(1), 186–201.</p></li>
<li><p>Hengl, T., Leenaars, J. G., Shepherd, K. D., Walsh, M. G., Heuvelink, G. B., Mamo, T., et al. (2017) <a href="https://link.springer.com/article/10.1007/s10705-017-9870-x">Soil nutrient maps of Sub-Saharan Africa: assessment of soil nutrient content at 250 m spatial resolution using machine learning</a>. Nutrient Cycling in Agroecosystems, 109(1), 77–102.</p></li>
<li><p>Hengl T, Mendes de Jesus J, Heuvelink GBM, Ruiperez Gonzalez M, Kilibarda M, Blagotic A, et al. (2017) <a href="http://dx.doi.org/10.1371/journal.pone.0169748">SoilGrids250m: Global gridded soil information based on machine learning</a>. PLoS ONE 12(2): e0169748. <a href="doi:10.1371/journal.pone.0169748" class="uri">doi:10.1371/journal.pone.0169748</a></p></li>
<li><p>Shangguan, W., Hengl, T., de Jesus, J. M., Yuan, H., &amp; Dai, Y. (2017). <a href="https://doi.org/10.1002/2016MS000686">Mapping the global depth to bedrock for land surface modeling</a>. Journal of Advances in Modeling Earth Systems, 9(1), 65-88.</p></li>
<li><p>Hengl, T., Roudier, P., Beaudette, D., &amp; Pebesma, E. (2015) <a href="https://www.jstatsoft.org/article/view/v063i05">plotKML: scientific visualization of spatio-temporal data</a>. Journal of Statistical Software, 63(5).</p></li>
<li><p>Gasch, C. K., Hengl, T., Gräler, B., Meyer, H., Magney, T. S., &amp; Brown, D. J. (2015) <a href="https://doi.org/10.1016/j.spasta.2015.04.001">Spatio-temporal interpolation of soil water, temperature, and electrical conductivity in 3D+ T: The Cook Agronomy Farm data set</a>. Spatial Statistics, 14, 70–90.</p></li>
<li><p>Hengl, T., Nikolic, M., &amp; MacMillan, R. A. (2013) <a href="https://doi.org/10.1016/j.jag.2012.02.005">Mapping efficiency and information content</a>. International Journal of Applied Earth Observation and Geoinformation, 22, 127–138.</p></li>
<li><p>Hengl, T., Heuvelink, G. B., &amp; Rossiter, D. G. (2007) <a href="https://doi.org/10.1016/j.cageo.2007.05.001">About regression-kriging: from equations to case studies</a>. Computers &amp; geosciences, 33(10), 1301-1315.</p></li>
<li><p>Hengl, T. (2006) <a href="https://doi.org/10.1016/j.cageo.2005.11.008">Finding the right pixel size</a>. Computers &amp; geosciences, 32(9), 1283–1298.</p></li>
</ul>
<p>Some other publications / books on the subject of Predictive Soil Mapping and Data Science in general include:</p>
<ul>
<li><p>Malone, B.P, Minasny, B., McBratney, A.B., (2016) <a href="https://www.springer.com/gp/book/9783319443256">Using R for Digital Soil Mapping</a>. Progress in Soil Science ISBN: 9783319443270, 262 pages.</p></li>
<li><p>Hengl, T., &amp; MacMillan, R. A. (2009). <a href="https://doi.org/10.1016/S0166-2481(08)00019-6">Geomorphometry—a key to landscape mapping and modelling</a>. Developments in Soil Science, 33, 433–460.</p></li>
<li><p>California Soil Resource Lab, (2017) <a href="https://casoilresource.lawr.ucdavis.edu/software/">Open Source Software Tools for Soil Scientists</a>, UC Davis.</p></li>
<li><p>McBratney, A.B., Minasny, B., Stockmann, U. (Eds) (2018) <a href="https://www.springer.com/gp/book/9783319634371">Pedometrics</a>. Progress in Soil Science ISBN: 9783319634395, 720 pages.</p></li>
<li><p>FAO, (2018) <a href="https://github.com/FAO-GSP/SOC-Mapping-Cookbook">Soil Organic Carbon Mapping Cookbook</a>. 2nd edt. ISBN: 9789251304402</p></li>
</ul>
<p>Readers are also encouraged to obtain and study the following R books before following some of the more complex exercises in this book:</p>
<ul>
<li><p>Bivand, R., Pebesma, E., Rubio, V., (2013) <a href="http://www.asdar-book.org/">Applied Spatial Data Analysis with R</a>. Use R Series, Springer, Heidelberg, 2nd Ed. 400 pages.</p></li>
<li><p>Irizarry, R.A., (2018) <a href="https://rafalab.github.io/dsbook/">Introduction to Data Science: Data Analysis and Prediction Algorithms with R</a>. HarvardX Data Science Series.</p></li>
<li><p>Kabacoff, R.I., (2011) <a href="http://www.manning.com/kabacoff/">R in Action: Data Analysis and Graphics with R</a>. Manning publications, ISBN: 9781935182399, 472 pages.</p></li>
<li><p>Kuhn, M., Johnson, K. (2013) <a href="http://appliedpredictivemodeling.com/">Applied Predictive Modeling</a>. Springer Science, ISBN: 9781461468493, 600 pages.</p></li>
<li><p>Lovelace, R., Nowosad, J., Muenchow, J., (2018) <a href="https://geocompr.robinlovelace.net/">Geocomputation with R</a>. R Series, CRC Press, ISBN: 9781138304512, 338 pages.</p></li>
<li><p>Reimann, C., Filzmoser, P., Garrett, R., Dutter, R., (2008) <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9780470987605">Statistical Data Analysis Explained Applied Environmental Statistics with R</a>. Wiley, Chichester, 337 pages.</p></li>
</ul>
<p>For the most recent developments in the R-spatial community refer to <a href="https://r-spatial.github.io" class="uri">https://r-spatial.github.io</a>, the R-sig-geo mailing list and/or <a href="https://opengeohub.org" class="uri">https://opengeohub.org</a>.</p>
</div>
<div id="contributions" class="section level2 unnumbered">
<h2>Contributions</h2>
<p>This book is constantly updated and contributions are welcome (through pull requests, but also through adding new chapters) provided that some minimum requirements are met. To contribute a complete new chapter please contact the editors first. Some minimum requirements to contribute a chapter are:</p>
<ol style="list-style-type: decimal">
<li>The data needs to be available for the majority tutorials presented in a chapter. Best is if this is via some R package or web-source.</li>
<li>A chapter should focus on implementing some computing in R (it should be written as an R tutorial).</li>
<li>All examples should be computationally efficient requiring not more than 30 secs of computing time per process on a single core system.</li>
<li>The theoretical basis for methods and interpretation of results should be based on peer-review publications. This book is not intended to report on primary research / experimental results, but only to supplement existing research publications.</li>
<li>A chapter should consist of at least 1500 words and at most 3500 words.</li>
<li>The topic of the chapter must be closely connected to theme of soil mapping, soil geographical databases, methods for processing spatial soil data and similar.</li>
</ol>
<p>In principle, all submitted chapters should also follow closely the <a href="https://en.wikipedia.org/wiki/Wikipedia:Five_pillars">five pillars of Wikipedia</a>, especially: Verifiability, Reproducibility, No original research, Neutral point of view, Good faith, No conflict of interest, and no personal attacks.</p>
</div>
<div id="reproducibility" class="section level2 unnumbered">
<h2>Reproducibility</h2>
<p>To reproduce the book, you need a recent version of <a href="https://cran.r-project.org/">R</a>, and <a href="http://www.rstudio.com/products/RStudio/">RStudio</a> and up-to-date packages, which can be installed with the following command (which requires <a href="https://github.com/hadley/devtools"><strong>devtools</strong></a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">install_github</span>(<span class="st">&quot;envirometrix/PredictiveSoilMapping&quot;</span>)</code></pre></div>
<p>To build the book locally, clone or <a href="https://github.com/envirometrix/PredictiveSoilMapping/archive/master.zip">download</a> the <a href="https://github.com/envirometrix/PredictiveSoilMapping/">PredictiveSoilMapping repo</a>, load R in root directory (e.g. by opening <a href="https://github.com/envirometrix/PredictiveSoilMapping/blob/master/PredictiveSoilMapping.Rproj">PredictiveSoilMapping.Rproj</a> in RStudio) and run the following lines:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bookdown<span class="op">::</span><span class="kw">render_book</span>(<span class="st">&quot;index.Rmd&quot;</span>) <span class="co"># to build the book</span>
<span class="kw">browseURL</span>(<span class="st">&quot;docs/index.html&quot;</span>) <span class="co"># to view it</span></code></pre></div>
</div>
<div id="acknowledgements" class="section level2 unnumbered">
<h2>Acknowledgements</h2>
<p>The authors are grateful to numerous contributions from colleagues around the world, especially for the contributions by the current and former ISRIC — World Soil Information colleagues: Gerard Heuvelink, Johan Leenaars, Jorge Mendes de Jesus, Wei Shangguan, David G. Rossiter, and many others. The authors are grateful to Dutch and European citizens for financing ISRIC and Wageningen University. The authors are also grateful to the support received via the <a href="http://africasoils.net">AfSIS project</a>, which has been funded by the Bill and Melinda Gates Foundation (BMGF) and the Alliance for a Green Revolution in Africa (AGRA). Many soil data processing examples in the book are based on the R code developed by Dylan Beuadette, Pierre Roudier, Alessandro Samuel Rosa, Marcos E. Angelini, Guillermo Federico Olmedo, Julian Moeys, Brandon Malone, and many other developers. Author is also grateful to comments and suggestions to the methods explained in the book by Travis Nauman, Amanda Ramcharan, David G. Rossiter and <a href="http://julienmoeys.info/">Julian Moeys</a>.</p>
<p>LandGIS and SoilGrids are based on numerous soil profile data sets that have been kindly contributed by various national and international agencies: the USA National Cooperative Soil Survey Soil Characterization database (<a href="http://ncsslabdatamart.sc.egov.usda.gov/" class="uri">http://ncsslabdatamart.sc.egov.usda.gov/</a>) and profiles from the USA National Soil Information System, Land Use/Land Cover Area Frame Survey (LUCAS) Topsoil Survey database <span class="citation">(Tóth, Jones, and Montanarella <a href="#ref-Toth2013LUCAS">2013</a>)</span>, Repositório Brasileiro Livre para Dados Abertos do Solo (<a href="https://github.com/febr-team">FEBR</a>), Sistema de Información de Suelos de Latinoamérica y el Caribe (SISLAC), Africa Soil Profiles database <span class="citation">(Leenaars <a href="#ref-Leenaars2012">2014</a>)</span>, Australian National Soil Information by CSIRO Land and Water <span class="citation">(Karssies <a href="#ref-Karssies2011CSIRO">2011</a>; Searle <a href="#ref-searle2014australian">2014</a>)</span>, Mexican National soil profile database <span class="citation">(Instituto Nacional de Estadística y Geografía (INEGI) <a href="#ref-INEGI2000">2000</a>)</span> provided by the Mexican Instituto Nacional de Estadística y Geografía / CONABIO, Brazilian national soil profile database <span class="citation">(Cooper et al. <a href="#ref-cooper2005national">2005</a>)</span> provided by the University of São Paulo, Chinese National Soil Profile database <span class="citation">(Shangguan et al. <a href="#ref-shangguan2013china">2013</a>)</span> provided by the Institute of Soil Science, Chinese Academy of Sciences, soil profile archive from the Canadian Soil Information System <span class="citation">(MacDonald and Valentine <a href="#ref-macdonald1992cansis">1992</a>)</span> and Forest Ecosystem Carbon Database (FECD), ISRIC-WISE <span class="citation">(Batjes <a href="#ref-Batjes2009SUM">2009</a>)</span>, The Northern Circumpolar Soil Carbon Database <span class="citation">(Hugelius et al. <a href="#ref-essd-5-3-2013">2013</a>)</span>, eSOTER profiles <span class="citation">(Van Engelen and Dijkshoorn <a href="#ref-VanEngelen2012">2012</a>)</span>, SPADE <span class="citation">(Hollis et al. <a href="#ref-hollis2006spade">2006</a>)</span>, Unified State Register of soil resources RUSSIA (Version 1.0. Moscow — 2014), National Database of Iran provided by the Tehran University, points from the Dutch Soil Information System (BIS) prepared by Wageningen Environmental Research, and others. We are also grateful to USA’s NASA, USGS and USDA agencies, European Space Agency Copernicus projects, JAXA (Japan Aerospace Exploration Agency) for distributing vast amounts of remote sensing data (especially MODIS, Landsat, Copernicus land products and elevation data), and to the Open Source software developers of the packages rgdal, sp, raster, caret, mlr, ranger, h2o and similar, and without which predictive soil mapping would most likely not be possible.</p>
<p>This book has been inspired by the <a href="https://geocompr.robinlovelace.net/">the Geocomputation with R book</a>, an Open Access book edited by Robin Lovelace, Jakub Nowosad and Jannes Muenchow. Many thanks to Robin Lovelace for helping with rmarkdown and for giving some initial tips for compiling and organizing book. The authors are also grateful to the numerous software/package developers, especially Edzer Pebesma, Roger Bivand, Robert Hijmans, Markus Neteler, Tim Appelhans, and Hadley Wickham, that have enabled a generation of researchers and applied projects.</p>
<p>OpenGeoHub is a not-for-profit research foundation with headquarters in Wageningen, the Netherlands (Stichting OpenGeoHub, KvK 71844570). The main goal of the OpenGeoHub is to promote publishing and sharing of Open Geographical and Geoscientific Data and using and developing of Open Source Software. We believe that the key measure of quality of research in all sciences (and especially in geographical information sciences) is in transparency and reproducibility of the computer code used to generate results. Transparency and reproducibility increase trust in information so that it is eventually also the fastest path to optimal decision making.</p>
<p>Every effort has been made to trace copyright holders of the materials used in these materials. Should we, despite all our efforts have overlooked contributors please contact the author and we shall correct this unintentional omission without any delay and will acknowledge any overlooked contributions and contributors in future updates.</p>
<p><strong>Data availability</strong>: All data used in this book is either available through R packages or is available via the github repository. If not mentioned otherwise, all code presented is available under the <a href="https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html">GNU General Public License v2.0</a>.</p>
<p><strong>Copyright</strong>: © 2018 Authors.</p>
<p><a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>. LandGIS and OpenGeoHub are registered trademarks of the OpenGeoHub Foundation.</p>
<!--chapter:end:index.Rmd-->
</div>
</div>
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Soil resource inventories and soil maps</h1>
<p><em>Edited by: Hengl T. &amp; MacMillan R.A.</em></p>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">1.1</span> Introduction</h2>
<p>This chapter presents a description and discussion of soils and conventional soil inventories framed within the context of Predictive Soil Mapping (PSM). Soils, their associated properties, and their spatial distribution are the central focus of PSM. We discuss how the products and methods associated with conventional soil mapping relate to new, and emerging, methods of PSM and automated soil mapping. We discuss similarities and differences, strengths and weaknesses of conventional soil mapping (and its inputs and products) relative to PSM.</p>
<p>The universal model of soil variation presented in detail in Chapter @ref(statistical-theory) is adopted as a framework for comparison of conventional and PSM. Our aim is to show how the products and methods of conventional soil mapping can complement, and contribute to, PSM and equally, how the theories and methods of PSM can extend and strengthen conventional soil mapping. PSM aims to implement tools and methods that can be supportive of growth, change and improvement in soil mapping and that can stimulate a rebirth and reinvigoration of soil inventory activity globally.</p>
</div>
<div id="soils-and-soil-inventories" class="section level2">
<h2><span class="header-section-number">1.2</span> Soils and soil inventories</h2>
<div id="soil-a-definition" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Soil: a definition</h3>
<p>Soil is a natural body composed of biota and air, water and minerals, developed from unconsolidated or semi-consolidated material that forms the topmost layer of the Earth’s surface <span class="citation">(Chesworth <a href="#ref-chesworth2008encyclopedia">2008</a>)</span>. The upper limit of the soil is either air, shallow water, live plants or plant materials that have not begun to decompose. The lower limit is defined by the presence of hard rock or the lower limit of biologic activity <span class="citation">(Richter and Markewitz <a href="#ref-Richter1995">1995</a>; Soil survey Division staff <a href="#ref-SSDS1993">1993</a>)</span>. Although soil profiles up to tens of meters depths can be found in some tropical areas <span class="citation">(Richter and Markewitz <a href="#ref-Richter1995">1995</a>)</span>, for soil classification and mapping purposes, the lower limit soil is often arbitrarily set to 2 m (<a href="http://soils.usda.gov/education/facts/soil.html" class="uri">http://soils.usda.gov/education/facts/soil.html</a>). Soils are rarely described to depths beyond 2 m and many soil sampling projects put primary focus on the upper (0-100 cm) depths.</p>
<p>The chemical, physical and biological properties of the soil differ from those of unaltered (unconsolidated) parent material from which the soil is derived over a period of time under influence of climate, organisms and relief effects. Soil should show a capacity to support life, otherwise we are dealing with inert unconsolidated parent material. Hence, for purposes of developing statistically based models to predict soil properties using PSM, it proves useful to distinguish between <em>actual</em> and <em>potential</em> soil areas (see further section @ref(soil-covariates)).</p>
<p>A significant aspect of the accepted definition of soil is that it is seen as a <em>natural body</em> that merits study, description, <em>classification</em> and interpretation in, and of, itself. As a <em>natural body</em> a soil is viewed as an object that occupies space, has defined physical dimensions and that is more than the sum of its individual properties or attributes. This concept requires that all properties of soils be considered collectively and simultaneously in terms of a completely integrated natural body <span class="citation">(Soil survey Division staff <a href="#ref-SSDS1993">1993</a>)</span>. A consequence of this, is that one must generally assume that all soil properties covary in space in lockstep with specific named soils and that different soil properties do not exhibit different patterns of spatial variation independently relative to a named soil.</p>
<p>From a management point of view, soil can be seen from at least three perspectives. It is a:</p>
<ul>
<li><p><em>Resource</em> of materials — It contains quantities of unconsolidated materials, rock fragments, texture fractions, organic carbon, nutrients, minerals and metals, water and so on.</p></li>
<li><p><em>Stabilizing medium / ecosystem</em> — It acts as a medium that supports both global and local processes from carbon and nitrogen fixation to retention and transmission of water, to provision of nutrients and minerals and so on.</p></li>
<li><p><em>Production system</em> — Soil is the foundation for plant growth. In fact, it is the basis of all sustainable terrestrial ecosystem services. It is also a source of livelihood for people that grow crops and livestock.</p></li>
</ul>
<p>For <span class="citation">Frossard et al. (<a href="#ref-frossard2006function">2006</a>)</span> there are six key functions of soil:</p>
<ol style="list-style-type: decimal">
<li><p><em>food and other biomass production</em>,</p></li>
<li><p><em>storage, filtering, and transformation of water, gases and minerals</em>,</p></li>
<li><p><em>biological habitat and gene pool</em>,</p></li>
<li><p><em>source of raw materials</em>,</p></li>
<li><p><em>physical and cultural heritage</em> and</p></li>
<li><p><em>platform for man-made structures: buildings, highways</em>.</p></li>
</ol>
<p>Soil is the Earth’s biggest carbon store containing 82% of total terrestrial organic carbon <span class="citation">(Lal <a href="#ref-Lal2004Science">2004</a>)</span>.</p>
</div>
<div id="soil-variables" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Soil variables</h3>
<p>Knowledge about soil is often assembled and cataloged through <em>soil resource inventories</em>. Conventional soil resource inventories describe the geographic distribution of <em>soil bodies</em> i.e. <em>polypedons</em> <span class="citation">(Wysocki, Schoeneberger, and LaGarry <a href="#ref-Wysocki2005Geoderma">2005</a>)</span>. The spatial distribution of soil properties is typically recorded and described through reference to mapped soil individuals and not through separate mapping of individual soil properties. In fact, the definition of a soil map in the US Soil Survey Manual specifically <em>“excludes maps showing the distribution of a single soil property such as texture, slope, or depth, alone or in limited combinations; maps that show the distribution of soil qualities such as productivity or erodibility; and maps of soil-forming factors, such as climate, topography, vegetation, or geologic material”</em> <span class="citation">(Soil survey Division staff <a href="#ref-SSDS1993">1993</a>)</span>.</p>
<p>In contrast to conventional soil mapping, PSM is primarily interested in portraying, in the form of maps, the spatial distribution of <em>soil variables</em> — measurable or descriptive attributes commonly collected through field sampling and then either measured <em>in-situ</em> or <em>a posteriori</em> in laboratory. Soil variables can be roughly grouped into:</p>
<ol style="list-style-type: decimal">
<li><p><em>quantities of some material</em> (<span class="math inline">\(y \in [0 \rightarrow +\infty]\)</span>);</p></li>
<li><p><em>transformed or standardized quantities</em> such as pH (<span class="math inline">\(y \in [-\infty \rightarrow +\infty]\)</span>)</p></li>
<li><p><em>relative percentages</em> such as mass or volume percentages (<span class="math inline">\(y \in [0 \rightarrow 1]\)</span>);</p></li>
<li><p><em>boolean values e.g. showing occurrence and/or non-occurrence</em> of qualitative soil attributes or objects (<span class="math inline">\(y \in [0,1]\)</span>);</p></li>
<li><p><em>categories</em> (i.e. factors) such as soil classes (<span class="math inline">\(y \in [a,b,\ldots,x]\)</span>);</p></li>
<li><p><em>probabilities</em> e.g. probabilities of occurrence of some class or object (<span class="math inline">\(p(y) \in [0 \rightarrow 1]\)</span>).</p></li>
<li><p><em>censored values</em> e.g. depth to bedrock which is often observed only up to 2 m.</p></li>
</ol>
<p>The nature of a soil variable determines how the attribute is modeled and presented on a map in PSM. Some soil variables are normally described as discrete entities (or classes), but classes can also be depicted as a continuous quantities on a map in the form of probabilities or memberships <span class="citation">(de Gruijter, Walvoort, and Gaans <a href="#ref-DeGruijter1997Geoderma">1997</a>; McBratney, Mendoça Santos, and Minasny <a href="#ref-McBratney2003Geoderma">2003</a>; Kempen et al. <a href="#ref-Kempen2009Geoderma">2009</a>; Odgers, McBratney, and Minasny <a href="#ref-Odgers201130">2011</a>)</span>. For example, a binary soil variable (e.g. the presence/absence of a specific layer or horizon) can be modeled as a binomial random variable with a logistic regression model. Spatial prediction (mapping) with this model gives a map depicting (continuous) probabilities in the range of 0–1. These probabilities can be used to determine the most likely presence/absence of a class at each prediction location, resulting in a discrete representation of the soil attribute variation.</p>
<p>In that context, the aims of most soil resource inventories consist of the identification, measurement, modelling, mapping and interpretation of soil variables that represent transformed or standardized quantities of some material, relative percentages, occurrence and/or non-occurrence of qualitative attributes or objects, and/or soil categories.</p>
</div>
<div id="primary-and-secondary-soil-variables" class="section level3">
<h3><span class="header-section-number">1.2.3</span> Primary and secondary soil variables</h3>
<p>Soil properties can be <em>primary</em> or <em>inferred</em> (see further section @ref(soil-variables-chapter)). Primary properties are properties that can be measured directly in the field or in the laboratory. Inferred properties are properties that cannot be measured directly (or are difficult or too expensive to measure) but can be inferred from primary properties, for example through pedotransfer functions <span class="citation">(Wösten, Pachepsky, and Rawls <a href="#ref-Wosten2001JH">2001</a>; Wösten et al. <a href="#ref-wosten2013soil">2013</a>)</span>. <span class="citation">Dobos et al. (<a href="#ref-Dobos2006digital">2006</a>)</span> also distinguish between primary and secondary soil properties and <em>‘functional’</em> soil properties representing <em>soil functions</em> or <em>soil threats</em>. Such soil properties can be directly used for financial assessment or for decision making. For example, soil organic carbon content in grams per kilogram of soil is the primary soil property, while organic carbon sequestration rate in kilograms per unit area per year is a <em>functional</em> soil property.</p>
</div>
</div>
<div id="soil-mapping" class="section level2">
<h2><span class="header-section-number">1.3</span> Soil mapping</h2>
<div id="what-are-soil-resource-inventories" class="section level3">
<h3><span class="header-section-number">1.3.1</span> What are soil resource inventories?</h3>
<p>Soil resource inventories describe the types, attributes and geographic distributions of soils in a given area. They can consist of spatially explicit maps or of non-spatial lists. Lists simply itemize the kinds and amounts of different soils that occupy an area to address questions about what soils and soil properties occur in an area. The resulting answer is often not highly specific in space but rather presents a mainly non-spatial itemization of soils and soil attributes expected to occur in a bounded area. Maps attempt to portray, with some degree of detail, the patterns of spatial variation in soils and soil properties, within limits imposed by mapping scale and resources.</p>
<p>According to the USDA Manual of Soil Survey <span class="citation">(Soil survey Division staff <a href="#ref-SSDS1993">1993</a>)</span>, a soil survey:</p>
<ul>
<li><p>describes the characteristics of the soils in a given area,</p></li>
<li><p>classifies the soils according to a standard system of classification,</p></li>
<li><p>plots the boundaries of the soils on a map, and</p></li>
<li><p>makes predictions about the behavior of soils.</p></li>
</ul>
<p>The information collected in a soil survey helps in the development of land-use plans and evaluates and predicts the effects of land use on the environment. Hence, the different uses of the soils and how the response of management affects them need to be considered.</p>
<p>In conventional soil mapping, the objects of study, whose spatial distributions are portrayed on any resulting map, are <em>soil individuals</em> that are assumed to possess and exhibit a unique set of soil properties with a defined range of values. A fundamental assumption of conventional soil mapping is therefore that, if one maps the pattern of spatial distribution of uniquely defined <em>soil individuals</em>, one can infer the patterns of spatial distribution of the <em>soil properties</em> associated with each defined individual. Thus, conventional soil maps must, by definition, only map soil individuals and not individual soil properties <span class="citation">(Soil survey Division staff <a href="#ref-SSDS1993">1993</a>)</span> and then subsequently infer the distribution of soil properties from the mapped distribution of soil individuals.</p>
<p>This attribute of conventional soil mapping represents a significant difference compared to PSM, where the object of study is frequently an individual soil property and the objective is to map the pattern of spatial distribution of that property (over some depth interval) independently from consideration of the spatial distribution of soil individuals or other soil properties.</p>
<p>Soil maps give answers to three basic questions: (1) what is mapped?, (2) what is the predicted value?, and (3) where is it? Thematic accuracy of a map tells us how accurate predictions of targeted soil properties are overall, while the spatial resolution helps us locate features with some specified level of spatial precision.</p>

<div class="rmdnote">
The most common output of a soil resource inventory is a <em>soil map</em>. Soil maps convey information about the geographic distribution of named soil types in a given area. They are meant to help answer the questions <em>“what is here”</em> and <em>“where is what”</em> <span class="citation">(Burrough and McDonnell <a href="#ref-Burrough1998OUP">1998</a>)</span>.
</div>

<p>Any map is an abstraction and generalization of reality. The only perfect one-to-one representation of reality is reality itself. To fully describe reality one would need a model at 1:1 scale at which 1 m<span class="math inline">\(^2\)</span> of reality was represented by 1 m<span class="math inline">\(^2\)</span> of the model. Since this is not feasible, we condense and abstract reality in such a way that we hope to describe the major differences in true space at a much reduced scale in model (map) space. When this is done for soil maps, it needs to be understood that the map cannot describe all of the variation that is present in reality. It can only describe that portion of the total variation that is systematic and has structure and occurs over distances that are as large as, or larger than, the smallest area that can be feasibly portrayed and described at any given scale. Issues of scale and resolution are discussed in greater detail in chapter @ref(downscaling-upscaling).</p>
<p>An important functionality of PSM is the production and distribution of maps depicting the spatial distribution of soils and, more specifically, soil attributes. In this chapter we, therefore, concentrate on describing processes for producing maps as spatial depictions of the patterns of arrangement of soil attributes and soil types.</p>
</div>
<div id="soil-mapping-approaches-and-concepts" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Soil mapping approaches and concepts</h3>
<p>As mentioned previously, spatial information about the distribution of soil properties or attributes, i.e. soil maps or GIS layers focused on soil, is produced through soil resource inventories, also known as soil surveys or soil mapping projects <span class="citation">(Burrough, Beckett, and Jarvis <a href="#ref-Burrough1971">1971</a>; Avery <a href="#ref-Avery1987">1987</a>; Wysocki, Schoeneberger, and LaGarry <a href="#ref-Wysocki2005Geoderma">2005</a>; Legros <a href="#ref-Legros2006SP">2006</a>)</span>. The main idea of soil survey is, thus, production and dissemination of soil information for an area of interest usually to address a specific question or questions of interest i.e. production of soil maps and soil geographical databases. Although soil surveyors are usually not <em>per se</em> responsible for usage of soil information, how soil survey information is used is increasingly important.</p>
<p>In statistical terms, the main objective of soil mapping is to describe the spatial variability i.e. spatial complexity of soils, then represent this complexity using maps, summary measures, mathematical models and simulations. Some known sources of spatial variability in soil variables are:</p>
<ol style="list-style-type: decimal">
<li><p><em>Natural spatial variability in 2D (different at various scales), mainly due to climate, parent material, land cover and land use</em>;</p></li>
<li><p><em>Variation by depth</em>;</p></li>
<li><p><em>Temporal variation due to regular or periodic changes in the ecosystem</em>;</p></li>
<li><p><em>Measurement error (in situ or in lab)</em>;</p></li>
<li><p><em>Spatial location error</em>;</p></li>
<li><p><em>Small scale variation</em>;</p></li>
</ol>

<div class="rmdnote">
In statistical terms, the main objective of soil mapping is to describe the spatial complexity of soils, then represent this complexity using maps, summary measures, mathematical models and simulations. From the application point of view, the main application objective of soil mapping is to accurately predict response of a soil(-plant) ecosystem to various soil management strategies.
</div>

<p>Soil mappers do their best to try explain the first two items above and minimize, or exclude from modelling, the remaining components: temporal variation, measurement error, spatial location error and small scale variation.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_soil_crop_model_scheme.png" alt="Inputs to soil-plant, soil-hydrology or soil-ecology models and their relationship." width="100%" angle=0 />
<p class="caption">
(#fig:soil-crop-model-scheme)Inputs to soil-plant, soil-hydrology or soil-ecology models and their relationship.
</p>
</div>
<p>From the application point of view, the main objective of soil mapping is to accurately predict soil properties and their response to possible or actual management practices (Fig. @ref(fig:soil-crop-model-scheme)). In other words, if the soil mapping system is efficient, we should be able to accurately predict the behavior of soil-plant, soil-hydrology or similar ecosystems to various soil management strategies, and hence provide useful advice to agronomists, engineers, environmental modelers, ecologists and similar.</p>
<p>We elect here to recognize two main variants of soil mapping which we refer to as <em>conventional soil mapping</em> and <em>pedometric</em> or <em>predictive soil mapping</em> as described and discussed below (Fig. @ref(fig:comparison-dsm)).</p>
<div class="figure" style="text-align: center">
<img src="figures/Table_comparison_DSM.png" alt="Comparison between traditional (primarily expert-based) and automated (data-driven) soil mapping." width="100%" />
<p class="caption">
(#fig:comparison-dsm)Comparison between traditional (primarily expert-based) and automated (data-driven) soil mapping.
</p>
</div>
</div>
<div id="soil-mapping-theory" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Theoretical basis of soil mapping: in context of the universal model of spatial variation</h3>
<p>Stated simply, <em>“the scientific basis of soil mapping is that the locations of soils in the landscape have a degree of predictability”</em> <span class="citation">(Miller, McCormack, and Talbot <a href="#ref-Miller1979">1979</a>)</span>. According to the USDA Soil Survey Manual, <em>“The properties of soil vary from place to place, but this variation is not random. Natural soil bodies are the result of climate and living organisms acting on parent material, with topography or local relief exerting a modifying influence and with time required for soil-forming processes to act. For the most part, soils are the same wherever all elements of the five factors are the same. Under similar environments in different places, soils are similar. This regularity permits prediction of the location of many different kinds of soil”</em> <span class="citation">(Soil survey Division staff <a href="#ref-SSDS1993">1993</a>)</span>. <span class="citation">Hudson (<a href="#ref-Hudson2000SSSAJ">2004</a>)</span> considers that this <em>soil-landscape paradigm</em> provides the fundamental scientific basis for soil survey.</p>
<p>In the most general sense, both conventional soil mapping and PSM represent ways of applying the <em>soil-landscape paradigm</em> via the universal model of spatial variation, which is explained in greater detail in Chapter @ref(statistical-theory). <span class="citation">Burrough and McDonnell (<a href="#ref-Burrough1998OUP">1998</a>, 133)</span> described the universal model of soil variation as a special case of the universal model of spatial variation. This model distinguishes between three major components of soil variation: (1) deterministic component (trend), (2) spatially correlated component and (3) pure noise.</p>
<span class="math display">\[\begin{equation}
Z({\bf{s}}) = m({\bf{s}}) + \varepsilon &#39;({\bf{s}}) + \varepsilon &#39;&#39;({\bf{s}})
(\#eq:univ-var)
\end{equation}\]</span>
<p>where <span class="math inline">\(\bf{s}\)</span> is two-dimensional location, <span class="math inline">\(m({\bf{s}})\)</span> is the deterministic component, <span class="math inline">\(\varepsilon &#39;({\bf{s}})\)</span> is the spatially correlated stochastic component and <span class="math inline">\(\varepsilon &#39;&#39;({\bf{s}})\)</span> is the pure noise (micro-scale variation and measurement error).</p>

<div class="rmdnote">
The <em>universal model of soil variation</em> assumes that there are three major components of soil variation: (1) the deterministic component (function of covariates), (2) spatially correlated component (treated as stochastic) and (3) pure noise.
</div>

<p>The deterministic part of the equation describes that part of the variation in soils and soil properties that can be explained by reference to some model that relates observed and measured variation to readily observable and interpretable factors that control or influence this spatial variation. In conventional soil mapping, this model is the empirical and knowledge-based <em>soil-landscape paradygm</em> <span class="citation">(Hudson <a href="#ref-Hudson2000SSSAJ">2004</a>)</span>. In PSM, a wide variety of statistical, and machine learning, models have been used to capture and apply the soil-landscape paradigm in a quantitative and optimal fashion:</p>
<span class="math display">\[\begin{equation}
S = f (cl, o, r, p, t)
(\#eq:clorpt)
\end{equation}\]</span>
<p>where <span class="math inline">\(S\)</span> stands for soil (properties and classes), <span class="math inline">\(cl\)</span> for climate, <span class="math inline">\(o\)</span> for organisms (including humans), <span class="math inline">\(r\)</span> is relief, <span class="math inline">\(p\)</span> is parent material or geology and <span class="math inline">\(t\)</span> is time. The Eq. @ref(eq:clorpt) is the CLORPT model originally presented by Jenny <span class="citation">(<a href="#ref-jenny1994factors">1994</a>)</span>.</p>
<p><span class="citation">McBratney, Mendonça Santos, and Minasny (<a href="#ref-MCBRATNEY20033">2003</a>)</span> have further conceptualized the so-called <em>“scorpan”</em> model in which soil property is modeled as a function of:</p>
<ul>
<li><p>(auxiliary) <strong>s</strong>oil classes or properties,</p></li>
<li><p><strong>c</strong>limate,</p></li>
<li><p><strong>o</strong>organisms, vegetation or fauna or human activity,</p></li>
<li><p><strong>r</strong>elief,</p></li>
<li><p><strong>p</strong>arent material,</p></li>
<li><p><strong>a</strong>ge i.e. the time factor,</p></li>
<li><p><strong>n</strong> space, spatial conntext or spatial position,</p></li>
</ul>
<p>Pedometric models are quantitative in that they capture relationships between observed soils, or soil properties, and controlling environmental influences (as represented by environmental co-variates) using statistically-formulated expressions. Pedometric models are seen as optimum because, by design, they minimize the variance between observed and predicted values at all locations with known values. So, no better model of prediction exists for that particular set of observed values at that specific set of locations.</p>
<p>Conventional soil mapping has a long history of effective development and application of empirical, knowledge-based, soil landscape models to predict how soil classes vary spatially across landscapes. Such models can be criticized, however, for being neither quantitative nor optimal.</p>
<p>Our essential point is that both conventional and pedometric soil mapping use models to explain the deterministic part of the spatial variation in soils and soil properties and these models differ mainly in terms of whether they are empirical and subjective (conventional) or quantitative and objective (pedometric). Both can be effective and the empirical and subjective models based on expert knowledge have, until recently, proven to be the most cost effective and widely applied for production of soil maps by conventional means.</p>
<p>The spatially correlated part of the observed variation is that part that shows spatial structure that lends itself to prediction through interpolation but that is not explainable, or easily explained, through use of a deterministic model that relates observed values to controlling factors. This part of the variation is typically modeled in pedometric mapping using geostatistics and kriging to interpolate, in an optimal manner, between point locations with known values <span class="citation">(Goovaerts <a href="#ref-goovaerts2001geostatistical">2001</a>; McBratney, Mendoça Santos, and Minasny <a href="#ref-McBratney2003Geoderma">2003</a>)</span>.</p>
<p>It can be argued that conventional soil mapping has an analogue to kriging in situations where there is no clearly apparent relationship between observed values and readily observable controlling environmental variables. In such instances, conventional soil mappers typically resort to an approach in which they make as many closely spaced observations as feasible and then manually <em>“interpolate”</em> between these locations of known soils or soil properties to locate boundaries indicative of locations of significant change in soils or soil properties. In the vernacular of soil surveyors this is often referred to as <em>“digging it out”</em> in which a pattern that is not readily apparent or visible is revealed through interpolation between closely spaced observations. So, under some circumstances, conventional soil surveyors do implement an analogue of spatial interpolation to describe patterns of variation in soils where such patterns are not readily related to a clear soil-landscape model.</p>

<div class="rmdnote">
In its essence, the objective of PSM is to produce optimal unbiased predictions of a mean value at some new location along with the uncertainty associated with the prediction, at the finest possible resolution.
</div>

<p>There is one way in which PSM differs significantly from conventional soil mapping in terms of the universal model of soil variation. This is in the use of statistics and machine learning to quantitatively correct for error in predictions, defined as the difference between predicted and observed values at locations with known values. Conventional soil mapping has no formal or quantitative mechanism for correcting an initial set of predicted values by computing the difference between predicted and observed values at sampled locations and then correcting initial values at all locations in response to these observed differences. PSM uses geostatistics to determine (via the semi-variogram) if the differences between predicted and observed values (the residuals) exhibit spatial structure (e.g. are predictable). If they do exhibit spatial structure, then it is useful and reasonable to interpolate the computed error at known locations to predict the likely magnitude of error of predictions at all locations <span class="citation">(Hengl, Heuvelink, and Rossiter <a href="#ref-hengl2007regression">2007</a>)</span>. This interpolated prediction error can then be systematically subtracted from (or added to) the original predicted value to correct for errors in the initial predictions that are systematic and spatially correlated. This <em>“after the fact”</em> correction of initial predictions is an aspect of PSM that represents an improvement over conventional soil mapping methods and that conventional methods would do well to emulate.</p>
<p>Neither conventional soil mapping nor PSM can do more than simply describe and quantify the amount of variation that is not predictable and has to be treated as pure noise. Conventional soil maps can be criticized for ignoring this component of the total variation and typically treating it as if it did not exist. For many soil properties, short range, local variation in soil properties that cannot be explained by either the deterministic or stochastic components of the universal model of soil variation can often approach, or even exceed, a significant proportion (e.g. 30–40%) of the total observed range of variation in any given soil property. Such variation is simply not mappable but it exists and should be identified and quantified. We do our users and clients a disservice when we fail to alert them to the presence, and the magnitude, of spatial variation that is not predictable. In cases where the local spatial variation is not predictable (or mappable) the best estimate for any property of interest is the mean value for that local area.</p>
</div>
<div id="conventional-mapping" class="section level3">
<h3><span class="header-section-number">1.3.4</span> Traditional (conventional) soil mapping</h3>
<p>Traditional soil resource inventories are largely based on manual application of expert tacit knowledge through the soil-landscape paradigm <span class="citation">(Burrough, Beckett, and Jarvis <a href="#ref-Burrough1971">1971</a>; Hudson <a href="#ref-Hudson2000SSSAJ">2004</a>)</span>. In this approach, soil surveyors develop and apply conceptual models of where and how soils vary in the landscape through a combination of field inspections to establish spatial patterns and photo-interpretation to extrapolate the patterns to similar portions of the landscape (Fig. @ref(fig:soilsurvey-scheme)). Traditional soil mapping procedures mainly address the deterministic part of the universal model of soil variation.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_soilsurvey_scheme.png" alt="Typical soil survey phases and intermediate and final products." width="100%" />
<p class="caption">
(#fig:soilsurvey-scheme)Typical soil survey phases and intermediate and final products.
</p>
</div>
<p>Conventional (traditional) manual soil mapping typically adheres to the following sequence of steps, with minor variations <span class="citation">(McBratney, Mendoça Santos, and Minasny <a href="#ref-McBratney2003Geoderma">2003</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><em>Specify the objective(s) to be served by the soil survey and resulting map</em>;</p></li>
<li><p><em>Identify which attributes of the soil or land need to be observed, described and mapped to meet the specified objectives</em>;</p></li>
<li><p><em>Identify the minimum sized area that must be described and the corresponding scale of mapping to meet the specified objectives</em>;</p></li>
<li><p><em>Collate and interpret existing relevant land resource information (geology, vegetation, climate, imagery) for the survey area</em>;</p></li>
<li><p><em>Conduct preliminary field reconnaissance and use these observations to construct a preliminary legend of conceptual mapping units (described in terms of soil individuals)</em>;</p></li>
<li><p><em>Apply preliminary conceptual legend using available source information to delineate initial map unit boundaries (pre-typing)</em>;</p></li>
<li><p><em>Plan and implement a field program to collect samples and observations to obtain values of the target soil attributes (usually classes) at known locations to test and refine initial conceptual prediction models</em>;</p></li>
<li><p><em>Using field observations, refine the conceptual models and finalize map unit legends and boundaries to generate conventional area–class soil maps</em>;</p></li>
<li><p><em>Conduct a field correlation exercise to match mapping with adjacent areas and to confirm mapping standards were adhered to</em>;</p></li>
<li><p><em>Select and analyse representative soil profile site data to characterize each mapped soil type and soil map unit</em>;</p></li>
<li><p><em>Prepare final documentation that describes all mapped soils and soil map units (legends) according to an accepted format</em>;</p></li>
<li><p><em>Publish and distribute the soil information in the form of maps, geographical databases and reports</em>;</p></li>
</ol>
<p>Expert knowledge about soil-landform patterns is generally used to produce manually drawn polygon maps that outline areas of different dominant soils or combinations of soils — <em>soil map units</em> (see Figs. @ref(fig:smu-aggregation) and @ref(fig:from-photointerpretation-to-soilmap)). Soil map units (polygons of different soil types) are described in terms of the composition of soil classes (and often also landscape attributes) within each unit, with various soil physical and chemical variables attached to each class. Most commonly, the objective of conventional soil mapping is to delineate recognizable portions of a landscape (soil–landform units) as polygons in which the variation of soils and soil properties is describable and usually (but not always) more limited than between polygons. Because most soil mapping projects have limited resources and time, soil surveyors can not typically afford to survey areas in great detail (e.g. 1:5000) so as to map actual <em>polypedons</em>. As a compromise, the survey team generally has to choose some best achievable target scale (e.g. 1:10,000 - 1:50,000). Maps produced at some initial scale can be further generalized, depending on the application and users demands <span class="citation">(Wysocki, Schoeneberger, and LaGarry <a href="#ref-Wysocki2005Geoderma">2005</a>)</span>.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_SMU_aggregation.png" alt="Three basic conceptual scales in soil mapping: (left) most detailed scale showing the actual distribution of soil bodies, (center) target scale i.e. scale achievable by the soil survey budget, (right) generalized intermediate scale or coarse resolution maps. In a conventional soil survey, soils are described and conceptualized as groups of similar pedons (smallest elements of 1–10 square-m), called “polypedons” — the smallest mappable entity. These can then be further generalized to soil map units, which can be various combinations (systematic or random) of dominant and contrasting soils (inclusions)." width="85%" />
<p class="caption">
(#fig:smu-aggregation)Three basic conceptual scales in soil mapping: (left) most detailed scale showing the actual distribution of soil bodies, (center) target scale i.e. scale achievable by the soil survey budget, (right) generalized intermediate scale or coarse resolution maps. In a conventional soil survey, soils are described and conceptualized as groups of similar pedons (smallest elements of 1–10 square-m), called “polypedons” — the smallest mappable entity. These can then be further generalized to soil map units, which can be various combinations (systematic or random) of dominant and contrasting soils (inclusions).
</p>
</div>
<p>Where variation within a polygon is systematic and predictable, the pattern of variation in soils within any given polygon is often described in terms of the most common position, or positions, in the landscape occupied by each named soil class. In other cases, soil patterns are not clearly related to systematic variations in observable landscape attributes and it is not possible to describe where each named soil type is most likely to occur within any polygon or why.</p>
<p>Conventional soil mapping has some limitations related to the fact that mapping concepts (mental models) are not always applied consistently by different mappers. Application of conceptual models is largely manual and it is difficult to automate. In addition, conventional soil survey methods differ from country to country, and even within a single region, depending largely on the scope and level-of-detail of the inventory <span class="citation">(Schelling <a href="#ref-Schelling1970Geoderma">1970</a>; Soil Survey Staff <a href="#ref-SSS1983USDA">1983</a>; Rossiter <a href="#ref-Rossiter2001">2003</a>)</span>. The key advantages of conventional soil maps, on the other hand, are that:</p>
<ul>
<li><p><em>they portray the spatial distribution of stable, recognizable and repeating patterns of soils that occupy identifiable portions of the landscape</em>, and</p></li>
<li><p><em>these patterns can be extracted from legends and maps to model (predict) the most likely soil at any location in the landscape using expert knowledge alone</em> <span class="citation">(Zhu et al. <a href="#ref-Zhu2001">2001</a>)</span>.</p></li>
</ul>
<p>Resource inventories, and in particular soil surveys, have been notoriously reluctant, or unable, to provide objective quantitative assessments of the accuracy of their products. For example, most soil survey maps have only been subjected to qualitative assessments of map accuracy through visual inspection and subjective correlation exercises. In the very few examples of quantitative evaluation <span class="citation">(Marsman and de Gruijter <a href="#ref-Marsman1986ALTERRA">1986</a>; Finke <a href="#ref-Finke2006Elsevier">2006</a>)</span>, the assessments have typically focused on measuring the degree with which predictions of soil classes at specific locations on a map, or within polygonal areas on a map, agreed with on-the-ground assessments of the soil class at these same locations or within these same polygons. Measurement error can be large in assessing the accuracy of soil class maps. <span class="citation">MacMillan, Pettapiece, and Brierley (<a href="#ref-MacMillan2005CJSS">2005</a>)</span>, for example, demonstrated that experts disagreed with each other regarding the correct classification of ecological site types at the same locations about as often as they disagreed with the classifications reported by a map produced using a predictive model.</p>
<p>Assessments of map accuracy that compare the ability of a map to predict classes of soil at specific locations are insufficient to assess the ability of a map to predict spatial variation in soil properties. Maps are increasingly used to predict <em>soil functional properties</em> at specific (point) locations. In traditional soil mapping, all properties are tied to soil classes and all properties are assumed to vary in exactly the same manner as the observed variation in soil types. To predict the value of a soil property at a location, one would first predict the soil class most likely to occupy that location then infer the soil property based on the predicted soil class. This has disadvantages when soil properties do not covary exactly with soil classes and when spatial variation in soil classes is difficult to predict.</p>
</div>
<div id="variants-of-soil-maps" class="section level3">
<h3><span class="header-section-number">1.3.5</span> Variants of soil maps</h3>
<p>In the last 20–30 years, soil maps have evolved from purely 2D polygon maps showing the distribution of soil poly-pedons i.e. named soil classes, to dynamic 3D maps representing predicted or simulated values of various primary or inferred soil properties and/or classes (Fig. @ref(fig:soilmap-types)). Examples of 2D+T and/or 3D+T soil maps are less common but increasingly popular (see e.g. <span class="citation">Rosenbaum et al. (<a href="#ref-Rosenbaum2012WRCR">2012</a>)</span> and <span class="citation">Gasch et al. (<a href="#ref-Gasch2015SPASTA">2015</a>)</span>). In general, we expect that demand for spatio-temporal soil data is likely to grow.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_soilmap_types.png" alt="Classification of types of soil maps based on spatial representation and variable type." width="100%" />
<p class="caption">
(#fig:soilmap-types)Classification of types of soil maps based on spatial representation and variable type.
</p>
</div>

<div class="rmdnote">
A soil map can represent 2D, 3D, 2D+T and/or 3D+T distribution of quantitative soil properties or soil classes. It can show predicted or simulated values of target soil properties and/or classes, or inferred soil-functions.
</div>

<p>The spatial model increasingly used to represent soil spatial information is the <em>gridded or raster data model</em>, where most of the technical properties are defined by the grid cell size i.e. the ground resolution. In practice, vector-based polygon maps can be converted to gridded maps and <em>vice versa</em>, so in practical terms there is really no meaningful difference between the two models. In this book, to avoid any ambiguity, when mentioning soil maps we will often refer to the spatio-temporal reference and support size of the maps at the finest possible level of detail. Below, for example, is a full list of specifications attached to a <em>soil map</em> produced for the African continent <span class="citation">(Hengl <a href="#ref-Hengl2015AfSoilGrids250m">2015</a>)</span>:</p>
<ul>
<li><p><em>target variable</em>: soil organic carbon in permille;</p></li>
<li><p><em>values presented</em>: predictions (mean value);</p></li>
<li><p><em>prediction method</em>: 3D regression-kriging;</p></li>
<li><p><em>prediction depths</em>: 6 standard layers (0–5, 5–15, 15–30, 30–60, 60–100, 100–200 cm);</p></li>
<li><p><em>temporal domain (period)</em>: 1950–2005;</p></li>
<li><p><em>spatial support (resolution) of covariate layers</em>: 250 m;</p></li>
<li><p><em>spatial support of predictions</em>: point support (center of a grid cell);</p></li>
<li><p><em>amount of variation explained by the spatial prediction model</em>: 45%;</p></li>
</ul>
<p>Until recently, maps of individual soil properties, or of soil functions or soil interpretations, were not considered to be true soil maps, but rather, to be single-factor derivative maps or interpretive maps. This is beginning to change and maps of the spatial pattern of distribution of individual soil properties are increasingly being viewed as a legitimate form of soil mapping.</p>
</div>
<div id="pedometric-mapping" class="section level3">
<h3><span class="header-section-number">1.3.6</span> Predictive and automated soil mapping</h3>
<p>In contrast to traditional soil mapping, which is primarily based on applying qualitative expert knowledge, the emerging, <em>‘predictive’</em> approach to soil mapping is generally more quantitative and data-driven and based on the use of statistical methods and technology <span class="citation">(Grunwald <a href="#ref-grunwald2005environmental">2005</a><a href="#ref-grunwald2005environmental">a</a>; Lagacherie, McBratney, and Voltz <a href="#ref-Lagacherie2006Elsevier">2006</a>; Hartemink, McBratney, and Mendonça-Santos <a href="#ref-Hartemink2008Springer">2008</a>; Boettinger et al. <a href="#ref-Boettinger2010Springer">2010</a>)</span>. The emergence of new soil mapping methods is undoubtedly a reflection of new developing technologies and newly available global data layers, especially those that are free and publicly distributed such as MODIS products, SRTM DEM and similar (Fig. @ref(fig:new-technologies)). PSM can be compared to, and shares similar concepts with, other applications of statistics and machine learning in physical geography, for example Predictive Vegetation Mapping <span class="citation">(Franklin <a href="#ref-Fran01">1995</a>; T. Hengl, Walsh, et al. <a href="#ref-Hengl2018PNV">2018</a>)</span>.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_new_technologies.png" alt="Evolution of digital soil mapping parallels the emergence of new technologies and global, publicly available data sources." width="60%" angle=0 />
<p class="caption">
(#fig:new-technologies)Evolution of digital soil mapping parallels the emergence of new technologies and global, publicly available data sources.
</p>
</div>
<p>The objective of using pedometric techniques for soil mapping is to develop and apply objective and optimal sets of rules to predict the spatial distribution of soil properties and/or soil classes. Most typically, rules are developed by fitting statistical relationships between digital databases representing the spatial distribution of selected environmental covariates and observed instances of a soil class or soil property at geo-referenced sample locations. The environmental covariate databases are selected as predictors of the soil attributes on the basis of either expert knowledge of known relationships to soil patterns or through objective assessment of meaningful correlations with observed soil occurrences. The whole process is amenable to complete automation and documentation so that it allows for <em>reproducible research</em> (read more in: <a href="http://en.wikipedia.org/wiki/Reproducibility" class="uri">http://en.wikipedia.org/wiki/Reproducibility</a>).</p>
<p>Pedometric soil mapping typically follows six steps as outlined by <span class="citation">McBratney, Mendoça Santos, and Minasny (<a href="#ref-McBratney2003Geoderma">2003</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><em>Select soil variables (or classes) of interest and suitable measurement techniques (decide what to map and describe)</em>;</p></li>
<li><p><em>Prepare a sampling design (select the spatial locations of sampling points and define a sampling intensity)</em>;</p></li>
<li><p><em>Collect samples in the field and then estimate values of the target soil variables at unknown locations to test and refine prediction models</em>;</p></li>
<li><p><em>Select and implement the most effective spatial prediction (or extrapolation) models and use these to generate soil maps</em>;</p></li>
<li><p><em>Select the most representative data model and distribution system</em>;</p></li>
<li><p><em>Publish and distribute the soil information in the form of maps, geographical databases and reports (and provide support to users)</em>;</p></li>
</ol>

<div class="rmdnote">
Differences among <em>conventional soil mapping</em>, <em>digital soil mapping</em> or <em>technology-driven or data-driven mapping</em> relate primarily to the degree of use of robust statistical methods in developing prediction models to support the mapping process.
</div>

<p>We here recognize four classes of soil mapping methods (B, C, D and E in Fig. @ref(fig:pedometric-mapping-vs-dsm)) which all belong to a continuum of <em>digital soil mapping</em> methods <span class="citation">(Malone, Minasny, and McBratney <a href="#ref-malone2016using">2016</a>; McBratney, Minasny, and Stockmann <a href="#ref-mcbratney2018pedometrics">2018</a>)</span>. We promote in this book specifically the Class E soil mapping approach i.e. which we refer to as the <em>predictive</em> and/or <em>automated soil mapping</em>.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_pedometric_mapping_vs_DSM.png" alt="A classification of approaches to soil mapping: from purely expert driven (Class A), to various types of digital soil mapping including fully automated soil mapping (Class E)." width="85%" />
<p class="caption">
(#fig:pedometric-mapping-vs-dsm)A classification of approaches to soil mapping: from purely expert driven (Class A), to various types of digital soil mapping including fully automated soil mapping (Class E).
</p>
</div>
<p>Some key advantages of the pedometric (statistical) approach to soil mapping are that it is: objective, systematic, repeatable, updatable and represents an optimal expression of statistically validated understanding of soil-environmental relationships in terms of the currently available data.</p>
<p>There are, of course, also limitations with pedometric methods that still require improvement. Firstly, the number of accurately georeferenced locations of reliable soil observations (particularly with analytical data) is often not sufficient to completely capture and describe all significant patterns of soil variation in an area. There may be too few sampled points and the exact location of available point data may not be well recorded. In short, data-driven soil mapping is field-data demanding and collecting field data can require significant expenditures of time, effort and money.</p>
<p>With legacy soil point data the sampling design, or rationale, used to decide where to locate soil profile observation or sampling points is often not clear and may vary from project to project or point to point. Therefore there is no guarantee that available point data are actually representative of the dominant patterns and soil forming conditions in any area. Points may have been selected and sampled to capture information about unusual conditions or to locate boundaries at points of transition and maximum confusion about soil properties. Once a soil becomes recognized as being widely distributed and dominant in the landscape, many conventional field surveys elect not to record observations when that soil is encountered, preferring to focus instead on recording unusual or transition soils. Thus the population of available legacy soil point observations may not be representative of the true population of soils, with some soils being either over or under-represented.</p>

<div class="rmdnote">
We define automated or predictive soil mapping as a data-driven approach to soil mapping with little or no human interaction, commonly based on using optimal (where possible) statistical methods that elucidate relationships between target soil variables (sampled in the field and geolocated) and covariate layers, primarily coming from remote sensing data.
</div>

<p>A second key limitation of the automated approach to soil mapping is that there may be no obvious relationship between observed patterns of soil variation and the available environmental covariates. This may occur when a soil property of interest does, indeed, strongly covary with some mappable environmental covariate (e.g. soil clay content with airborne radiometric data) but data for that environmental covariate are not available for an area. It may also transpire that the pattern of soil variation is essentially not predictable or related to any known environmental covariate, available or not. In such cases, only closely spaced, direct field observation and sampling is capable of detecting the spatial pattern of variation in soils because there is no, or only a very weak, correlation with available covariates <span class="citation">(Kondolf and Piégay <a href="#ref-kondolf2003tools">2003</a>)</span>.</p>
</div>
<div id="comparison-conventional-pm" class="section level3">
<h3><span class="header-section-number">1.3.7</span> Comparison of conventional and pedometric or predictive soil mapping</h3>
<p>There has been a tendency to view conventional soil mapping and automated soil mapping as competing and non-complementary approaches. In fact, they share more similarities than differences. Indeed, they can be viewed as end members of a logical continuum. Both rely on applying the underlying idea that the distribution of soils in the landscape is largely predictable (the deterministic part) and, where it is not predictable, it must be revealed through intensive observation, sampling and interpolation (the stochastic part).</p>
<p>In most cases, the basis of prediction is to relate the distribution of soils, or soil properties, in the landscape to observable environmental factors such as topographic position, slope, aspect, underlying parent material, drainage conditions, patterns of climate, vegetation or land use and so on. This is done manually and empirically (subjectively) in conventional soil survey, while in automated soil mapping it is done objectively and mostly in an automated fashion. At the time it was developed, conventional soil survey lacked both the digital data sets of environmental covariates and the statistical tools required to objectively analyze relationships between observed soil properties and environmental covariates. So, these relationships were, of necessity, developed empirically and expressed conceptually as expert knowledge.</p>
<p>More recently, it has become increasingly possible to obtain both environmental covariate data and field point soil observations in georegistered and digital format and to analyze and express relationships objectively and optimally, using statistical methods <span class="citation">(Pebesma <a href="#ref-Pebesma2006TiG">2006</a>; McBratney et al. <a href="#ref-McBratney2011HSS">2011</a>)</span>. Where the relationship between available environmental covariates and observed soil variation is weak, as in featureless plains or complex flood plains, both methods rely on similar approaches of using densely spaced point observations to reveal the spatial patterns. Conventional soil mappers <em>‘dig out’</em> these patterns while digital soil mappers interpolate using geostatistical procedures, but here too the two methods are quite analogous. Hard facts (point data and covariates) can often be beneficially enhanced using soft data (expert knowledge).</p>
<p>In summary, we suggest that next generation soil surveyors will increasingly benefit from having a solid background in statistics and computer science, especially in Machine Learning and A.I. However, effective selection and application of appropriate statistical sampling and analysis techniques can also benefit from consideration of expert knowledge.</p>
</div>
<div id="top-down" class="section level3">
<h3><span class="header-section-number">1.3.8</span> Top-down versus bottom-up approaches: subdivision versus agglomeration</h3>
<p>There are two fundamentally different ways to approach the production of soil maps for areas of larger extent, whether by conventional or pedometric means. For ease of understanding we refer to these two alternatives here as <em>“bottom-up”</em> versus <em>“top-down”</em>. <span class="citation">Rossiter (<a href="#ref-Rossiter2001">2003</a>)</span> refers to a synthetic approach that he calls the <em>“bottom-up”</em> or <em>“name and then group”</em> approach versus an analytic approach that he calls the <em>“top-down”</em> or <em>“divide and then name”</em> approach.</p>
<p>The bottom up approach is agglomerative and synthetic. It is implemented by first collecting observations and making maps at the finest possible resolution and with the greatest possible level of detail. Once all facts are collected and all possible soils and soil properties, and their respective patterns of spatial distribution, are recorded, these detailed data are generalized at successively coarser levels of generalization to detect, analyse and describe broader scale (regional to continental) patterns and trends. The fine detail synthesized to extract broader patterns leads to the identification and formulation of generalizations, theories and concepts about how and why soils organize themselves spatially. The bottom-up approach makes little, to no, use of generalizations and theories as tools to aid in the conceptualization and delineation of mapping entities. Rather, it waits until all the facts are in before making generalizations. The bottom-up approach tends to be applied by countries and organizations that have sufficient resources (people and finances) to make detailed field surveys feasible to complete for entire areas of jurisdiction. Soil survey activities of the US national cooperative soil survey (NCSS) primarily adopt this bottom-up approach. Other smaller countries with significant resources for field surveys have also adopted this approach (e.g. Netherlands, Denmark, Cuba). The bottom-up approach was, for example, used in the development and elaboration of the US Soil Taxonomy system of classification and of the US SSURGO (1:20,000) and STATSGO (1:250,000) soil maps <span class="citation">(Zhong and Xu <a href="#ref-ZHONG2011491">2011</a>)</span>.</p>
<p>The top-down approach is synoptic, analytic and divisive. It is implemented by first collecting just enough observations and data to permit construction of generalizations and theoretical concepts about how soils arrange themselves in the landscape in response to controlling environmental variables. Once general theories are developed about how environmental factors influence how soils arrange themselves spatially, these concepts and theories are tested by using them to predict what types of soils are likely to occur under similar conditions at previously unvisited sites. The theories and concepts are adjusted in response to initial application and testing until such time as they are deemed to be reliable enough to use for production mapping. Production mapping proceeds in a divisive manner by stratifying areas of interest into successively smaller, and presumably more homogeneous, areas or regions through application of the concepts and theories to available environmental data sets. The procedures begin with a synoptic overview of the environmental conditions that characterize an entire area of interest. These conditions are then interpreted to impose a hierarchical subdivision of the whole area into smaller, and more homogeneous subareas. This hierarchical subdivision approach owes its origins to early Russian efforts to explain soil patterns in terms of the geographical distribution of observed soils and vegetation. The top-down approach tends to be applied preferentially by countries and agencies that need to produce maps for very large areas but that lack the people and resources to conduct detailed field programs everywhere (see e.g. <span class="citation">Henderson et al. (<a href="#ref-Henderson2004Geoderma">2004</a>)</span> and <span class="citation">Mansuy et al. (<a href="#ref-Mansuy201459">2014</a>)</span>). Many of these divisive hierarchical approaches adopt principals and methods associated with the ideas of Ecological Land Classification <span class="citation">(Rowe and Sheard <a href="#ref-rowe1981ecological">1981</a>)</span> (in Canada) or Land Systems Mapping <span class="citation">(Gibbons, Downes, and others <a href="#ref-gibbons1964study">1964</a>; Rowan <a href="#ref-rowan1990land">1990</a>)</span> (in Australia).</p>
<p>As observed by <span class="citation">Rossiter (<a href="#ref-Rossiter2001">2003</a>)</span> <em>“neither approach is usually applied in its pure form”</em> and most approaches to soil mapping use both approaches simultaneously, to varying degrees. Similarly, it can be argued that PSM provides support for both approaches to soil mapping. PSM implements two activities that bear similarities to bottom-up mapping. Firstly, PSM uses <em>all</em> available soil profile data globally as input to initial global predictions at coarser resolutions (<em>“top-down”</em> mapping). Secondly, PSM is set up to ingest finer resolution maps produced via detailed <em>“bottom-up”</em> mapping methods and to merge these more detailed maps with initial, coarser-resolution predictions <span class="citation">(Ramcharan et al. <a href="#ref-ramcharan2018soil">2018</a><a href="#ref-ramcharan2018soil">a</a>)</span>.</p>
</div>
</div>
<div id="sources-of-soil-data-for-soil-mapping" class="section level2">
<h2><span class="header-section-number">1.4</span> Sources of soil data for soil mapping</h2>
<div id="soil-data-sources-targeted-by-psm" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Soil data sources targeted by PSM</h3>
<p>PSM aims at integrating and facilitating exchange of global soil data. Most (global) soil mapping initiatives currently rely on capture and use of <em>legacy soil data</em>. This raises several questions. What is meant by legacy soil data? What kinds of legacy soil data exist? What are the advantages and limitations of the main kinds of legacy soil data?</p>
<p>In its most general sense, a legacy is something of value bequeathed from one generation to the next. It can be said that global soil legacy data consists of the sum of soil data and knowledge accumulated since the first soil investigations 100 or more years ago. More specifically, the concept of a legacy is usually accompanied by an understanding that there is an obligation and duty of the recipient generation to not simply protect the legacy but to make positive and constructive use of it.</p>
<p>The idea is that a legacy is not a priceless artifact, to be hidden away somewhere for static preservation and protection, but a living resource to be invested, improved upon, and grown for the sake of successive generations. The intention of any PSM framework is therefore not simply to rescue and protect the existing accumulation of legacy soil data, but to put it to new and beneficial uses, so that its value is increased and not just preserved.</p>

<div class="rmdnote">
Four main groups of legacy data of interest for global soil mapping are: (1) soil field records, (2) soil polygon maps and legends, (3) soil-landscape diagrams and sketches, (d) soil (profile) photographs.
</div>

<p>In the context of soils, legacy soil data consist of the sum total of data, information and knowledge about soils accumulated since soils were first studied as independent natural objects. At its broadest, this includes information about soil characteristics and classification, soil use and management, soil fertility, soil bio-chemistry, soil formation, soil geography and many other subdisciplines.</p>
<p>In the more focused context of PSM, we are primarily interested in four main kinds of legacy soil data:</p>
<ul>
<li><p><em>Soil field observations and measurements</em> — Observations and analytical data obtained for soils at point locations represent a primary type of legacy soil data. These point source data provide objective evidence of observed soil characteristics at known locations that can be used to develop knowledge and rules about how soils, or individual soil properties, vary across the landscape. The quality and precision of these data can vary greatly. Some data points might be accurately located, or geo-referenced, while others might have very coarse geo-referencing (for example coordinates rounded in decimal minutes or kilometers). Some point data might only have a rough indication of the location obtained from a report (for example <em>‘2 km south of village A’</em>), or might even lack geo-referencing. Soil profile descriptions can be obtained from pits (relatively accurate) or auger bores (less accurate). Soil attributes can be determined in the laboratory (relatively accurate) or by hand-estimation in the field (less accurate). Legacy point data is characterized by great variation in precision, accuracy, completeness, relevance and age. It needs to be used with caution and with understanding of how these issues affect its potential use.</p></li>
<li><p><em>Soil (polygon) maps and legends</em> — Soil maps and legends are one of the primary means by which information and knowledge about how soils vary spatially have been observed, distilled, recorded and presented to users. Soil maps provide lists, or inventories, of soils that occur in mapped regions, illustrate the dominant spatial patterns displayed by these listed soils and provide information to characterize the main properties of these soils. Soil maps can themselves be used as sources of evidence to develop knowledge and quantitative rules about how soils, or individual soil properties, vary across the landscape. On the other hand, similar to soil observations, soil maps also can exhibit significant errors with respect to measurement, classification, generalization, interpretation and spatial interpolation.</p></li>
<li><p><em>Tacit expert soil knowledge</em> — In the context of soils, tacit expert knowledge represents a diffuse domain of information about the characteristics and spatial distribution of soils that has not been captured and recorded formally or explicitly. It may reside in the minds and memories of experts who have conducted field and laboratory studies but have been unable to record all their observations in a formal way. It may be captured informally and partially in maps, legends, conceptual diagrams, block diagrams, generalized decision rules and so on. Tacit knowledge represents soft data, in comparison to the more hard data of point observations and maps.</p></li>
<li><p><em>Photographs</em> — Traditional soil survey is heavily based on use of aerial photographs. Older aerial photographs (even if not stereoscopic) are an important resource for land degradation monitoring and vegetation succession studies. Field photographs of soil profiles, soil sites and soil processes are another important source of information that has been under-used for soil mapping. ISRIC for example has an archive of over 30 thousand photographs from various continents. Most of these can be geo-coded and distributed via image sharing web-services such as WikiMedia, Instagram and/or Flickr. In theory, even a single photograph of a soil profile could be used to (automatically?) identify soil types, even extract analytical soil properties. Although it is very likely that prediction by using photographs only would be fairly imprecise, such data could potentially help fill large gaps for areas where there are simply no soil observations.</p></li>
</ul>
</div>
<div id="field-observations" class="section level3">
<h3><span class="header-section-number">1.4.2</span> Field observations of soil properties</h3>
<p>Perhaps the most significant, certainly the most reliable, inputs to soil mapping are the <em>field observations</em> (usually at point locations) of descriptive and analytical soil properties <span class="citation">(Soil survey Division staff <a href="#ref-SSDS1993">1993</a>; Schoeneberger et al. <a href="#ref-Schoeneberger1998">1998</a>)</span>. This is the <em>hard data</em> or <em>ground truth</em> in soil mapping <span class="citation">(Rossiter <a href="#ref-Rossiter2001">2003</a>)</span>. Field observations are also the main input to spatial prediction modelling and the basis for assessment of mapping accuracy. Other synthetically or empirically generated estimates of values of target variables in the field are considered as <em>soft data</em>. Soft data are less desirable as the primary input to model estimation, but sometimes there is no alternative. It is in any case important to recognize differences between <em>hard</em> and <em>soft</em> data and to suggest ways to access the uncertainty of models that are based on either or both.</p>
<p>The object of observation and description of a soil is almost always a soil profile or <em>pedon</em>. Officially, a soil pedon is defined as a body of soil having a limited horizontal extent of no more than 1–2 m in horizontal and a vertical dimension (<span class="math inline">\(d\)</span>) that typically extends to only 1–2 m but may occasionally extend to greater depths. In practice, the vast majority of soil profile data pertain to soil observations and samples collected over very limited horizontal dimensions (10–50 cm) and down to maximum depths of 1–2 m.</p>
<p>In geostatistical terms, soil observations are most commonly collected at point support, meaning that they are representative of a point in space with very limited horizontal extent. It is relatively rare to encounter legacy soil profile data collected over larger horizontal extents and bulked to create a sample representative of a larger volume of soil that can be treated as providing block support for statistical purposes. On the other hand, there is an increasing interest in soil predictions at varying support sizes e.g. 1 ha for which composite sampling can be used.</p>
<p>In the vertical dimension, soil profiles are usually described and sampled with respect to <em>genetic soil horizons</em>, which are identifiable layers in the soil that reflect differences in soil development or depositional environments. Less frequently, soils are described and sampled in the vertical dimension with respect to arbitrary depth intervals or layers e.g. at fixed depths intervals e.g. 10, 20, 30, 40, <span class="math inline">\(\ldots\)</span> cm.</p>

<div class="rmdnote">
A soil profile record is a set of field observations of the soil at a location — a collection of descriptive and analytical soil properties attached to a specific location, depth and sampling support size (volume of soil body).
</div>

<p>Soil profile descriptions in the vertical dimension are usually accompanied by additional soil site descriptions that describe attributes of the site in the horizontal dimension for distances of a few meters to up 10 m to surrounding the location where the vertical profile was sampled and described. Site attributes described typically characterize the immediately surrounding landscape, including slope gradient, aspect, slope position, surface shape, drainage condition, land use, vegetation cover, stoniness and unusual or site specific features.</p>
<p>Two main types of information are typically recorded for point soil profiles. The first consists of field observations and classifications of observable profile and <em>site characteristics</em>. Profile attributes usually include the location and thickness of observably different horizons or layers, the color, texture, structure and consistence of each recognized horizon or layer and other observable attributes such as stone content, presence, size and abundance of roots, pores, mottles, cracks and so on. Despite their potential for subjectivity, these field observations provide much useful information at a relatively low cost, since there is no need to sample or transport the soil or analyze it at considerable cost in a distant laboratory.</p>
<p>The second main type of information collected to describe soil profiles consists of various types of objective measurements and analyses. Some objective measurements can be taken on-site, in the field. Examples of field measurements include <em>in-situ</em> assessment of bulk density, infiltration rate, hydraulic conductivity, electrical conductivity, penetration resistance and, more recently, spectral analysis of soil reflectance <span class="citation">(Kondolf and Piégay <a href="#ref-kondolf2003tools">2003</a>; Gehl and Rice <a href="#ref-GehlRice2005">2007</a>; Shepherd and Walsh <a href="#ref-ShepherdWalsh2007JNIS">2007</a>)</span>. The most frequently obtained and reported objective measurements are obtained by off-site <em>laboratory analysis of soil samples</em> collected from soil profiles at sampled locations. A wide variety of chemical and physical laboratory analyses can be, and have been, carried out on soil samples included in legacy soil profile data bases.</p>
<p>Within PSM we are mainly interested in a core set of laboratory analyses for e.g. pH, organic carbon, sand, silt, clay, coarse fragment content, bulk density, available water capacity, exchangeable cations and acidity and electrical conductivity. This core set was selected partly because it is considered to represent the key soil functional properties of most interest and use for interpretation and analysis and partly because these soil properties are the most widely analyzed and reported in the soil legacy literature <span class="citation">(Sanchez et al. <a href="#ref-Sanchez2009Science">2009</a>; Hartemink et al. <a href="#ref-Hartemink2010Springer">2010</a>)</span>. The significant feature of objective measurements is that they are expected to be consistent, repeatable and comparable across time and space. We will see in the following chapter that this is not always the case.</p>

<div class="rmdnote">
An advantage of descriptive field observations such as soil color, stone content, presence, size and abundance of roots, pores, mottles, cracks, diagnostic horizons etc. is that they provide much useful information at a relatively low cost, since there is no need to sample or transport the soil or analyze it at considerable cost in a distant laboratory.
</div>

</div>
<div id="legacy-soil-profile-data" class="section level3">
<h3><span class="header-section-number">1.4.3</span> Legacy soil profile data</h3>
<p>The principal advantage of legacy soil profile data at point locations is simply that the observations and measurements are referenced to a known location in space (and usually also time). Knowledge of the spatial location of soil profile data provides the opportunity to analyze relationships between known data values at a location and other covariate (predictor) data sets. It also becomes possible to simply analyze spatial patterns i.e. represent spatial variability using values at known point locations. In the first instance, knowing the location of a point at which a soil property has been described or measured permits that location to be overlaid onto other spatially referenced digital data layers to produce data sets of related environmental values that all occur at the same site.</p>
<p>The known point values of soil properties (or classes) can be analyzed relative to the known values of environmental covariates at corresponding locations. If a statistically significant relationship can be established between the value of a soil property at numerous locations and the corresponding values of a environmental variables at the same locations, a predictive model can be developed. Development of predictive models based on such observed environmental correlations is a fundamental aspect of modern pedometric soil mapping.</p>
<p>A second main advantage of point profile data is that the data values are, more or less, objective assessments of a soil property or characteristic at a location. Objective values are more amenable to exploration using statistical techniques than subjective observations and classifications. They typically (but not always) exhibit less measurement error.</p>
<p>As important and useful as soil point data are, they also possess limitations and problems that must be recognized and addressed. One common limitation of legacy soil point data is lack of accurate geo-referencing information. The location information provided for older soil legacy profile data is often poor. Prior to the widespread adoption of the Global Positioning Systems (GPS) the locations of most soil sampling points were obtained and described in terms of estimated distances and directions from some known local reference point (Fig. @ref(fig:gps-evolution)). Even the best located of such older (prior to 1990’s) sampling points cannot be expected to be located with an accuracy of better than 50–100 m. Some widely used profile data from developing countries cannot be reliably located to within 1 km <span class="citation">(Leenaars <a href="#ref-Leenaars2012">2014</a>)</span>.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_GPS_evolution.png" alt="Evolution of the Open Access Navigation and positioning technologies (left) and the open access remote sensing monitoring systems (right). API — Aerial photo-interpretation; S.A. — Selective Availability; L.R.S.P.A. — Land Remote Sensing Policy Act (made Landsat digital data and images available at the lowest possible cost)." width="85%" />
<p class="caption">
(#fig:gps-evolution)Evolution of the Open Access Navigation and positioning technologies (left) and the open access remote sensing monitoring systems (right). API — Aerial photo-interpretation; S.A. — Selective Availability; L.R.S.P.A. — Land Remote Sensing Policy Act (made Landsat digital data and images available at the lowest possible cost).
</p>
</div>
<p>This relatively poor positional accuracy has implications when intersecting legacy point data with covariate data layers to discover and quantify statistical relationships. It can be difficult to impossible to develop meaningful relationships between soil properties at point locations and environmental covariates that vary significantly over short horizontal distances. Consider, for example, topography, in which the largest portion of significant variation is often local and is related to individual hill slopes from ridge line to channel. Many hill slopes, especially in agricultural landscapes, have total lengths of from 50–100 m. If the location of a point soil profile is only known with an accuracy of 100 m, then, when overlaid on topographic data, that point may fall at almost any point on a typical hill slope from channel bottom to ridge top.</p>
<p>In such cases, it is unlikely that statistical analysis of the relationship between soil properties and slope position will reveal anything meaningful. Even if a strong relationship does exist in reality, it will not be apparent in the poorly geo-referenced data. The likelihood of establishing a meaningful relationship becomes even smaller when the accuracy of the point location is ±1 km. In such cases, subjective information on the conceptual location of the soil in the landscape (e.g. manually observed slope position) may be more useful for establishing rules and patterns than intersection of the actual point data with fine resolution covariates.</p>
<p>Another common limitation of legacy soil point data is that the criteria used to select locations at which to sample soils have not always been consistent. This can lead to bias in which soils and which parts of the landscape get sampled in any given area. So, available information on soil classes or soil properties at known points in the landscape may, or may not, be representative of the dominant or actual landscape conditions. Sometimes soils are sampled because they are believed to be representative of the dominant conditions in a landscape. At other times, soils are sampled because they are unusual and stand out or because they occupy a transitional position and the sampler is trying to identify a boundary. Most statistical techniques for extracting patterns and relationships from analysis of soil point data assume that the point data are somewhat representative of the landscape and cover the full range of both covariate space and physical space. This assumption is often not met and point samples, in many areas, may not be fully representative of the full range of conditions in an area.</p>
<p>In analyzing legacy soil profile data to develop rules and relationships, it is usually also assumed that the values reported for any soil property for all sites are comparable and consistent. Differences in methods used to sample and analyze soils lead to considerable differences in the values reported for any given soil property depending upon such factors as method of analysis, laboratory at which the analysis was done, time of analysis (results vary year to year), person doing the analysis and so on. These differences in values for what should be the same soil property produce noise that confounds the ability to discern and quantify statistical relationships between observed soil property values and values for covariates at the same locations.</p>
<p>In the case of automated soil mapping, efforts are usually made to try to harmonize values produced using different laboratory methods to achieve roughly equivalent values relative to a single standard reference method. Even where harmonization is applied, some noise and inconsistency always remains and the ability to establish statistical relationships is often somewhat compromised.</p>

<div class="rmdnote">
If not collected using probability sampling and with high location accuracy, soil field records are often only marginally suitable for building spatial prediction models, especially at fine spatial resolution. Legacy data can carry significant positional and attribute error, and is possibly not representative of all soil forming conditions in an area of interest. All these limitations can seriously degrade the final map accuracy, so that sometimes better accuracy cannot be achieved without collecting new field data.
</div>

<p>What needs to be emphasized is that much of the legacy soils profile data in the world is under used. It tends to be fragmented, non-standard between countries and often even within countries. Many original field observations are still not converted into digital format and these data are in considerable danger of being lost to effective use forever (!) as government sponsored soil institutions lose support and close and the current generation of experienced soil specialists retire and are not replaced. Even where these data are in digital format, it is not easy to share or exchange data across national, state or even project borders because of significant differences in standards, methods, definitions, ownership and legends <span class="citation">(Omuto, Nachtergaele, and Vargas Rojas <a href="#ref-Omuto2012GSP">2012</a>)</span>.</p>
</div>
<div id="soil-covariates" class="section level3">
<h3><span class="header-section-number">1.4.4</span> Soil covariates</h3>
<p>Following the work of Jenny <span class="citation">(White <a href="#ref-white2009principles">2009</a>)</span> and further <span class="citation">McBratney et al. (<a href="#ref-McBratney2011HSS">2011</a>)</span>, we recognize six main groups of soil covariates of interest for pedometric soil mapping:</p>
<ol style="list-style-type: decimal">
<li><p><em>Raw spectral and multi-spectral images</em> of the land surface (remote sensing bands),</p></li>
<li><p><em>DEM-derived covariates</em>,</p></li>
<li><p><em>Climatic images</em>,</p></li>
<li><p><em>Vegetation and land-cover</em> based covariates,</p></li>
<li><p><em>Land survey and land use information</em> — human-made objects, manageemnt, fertilization and tillage practice maps etc,</p></li>
<li><p><em>Expert-based covariates</em> — soil delineations or delineations of soil parent material or geology (manually or semi-automatically prepared); empirical maps of soil processes and features (e.g. catena sequences etc).</p></li>
</ol>
<div class="figure" style="text-align: center">
<img src="figures/Fig_DEM_evolution.png" alt="Evolution of global DEM data sources: (right) SRTM DEM released in 2002, as compared to (left) WorldDEM released in 2014 (Baade et al., 2014). Sample data set for city of Quorn in South Australia. As with many digital technologies, the level of detail and accuracy of GIS and remote sensing data is exhibiting exponential growth." width="100%" />
<p class="caption">
(#fig:dem-evolution)Evolution of global DEM data sources: (right) SRTM DEM released in 2002, as compared to (left) WorldDEM released in 2014 (Baade et al., 2014). Sample data set for city of Quorn in South Australia. As with many digital technologies, the level of detail and accuracy of GIS and remote sensing data is exhibiting exponential growth.
</p>
</div>

<div class="rmdnote">
The most common environmental covariates typically used in soil mapping are: (1) Raw spectral and multi-spectral images of the land surface, (2) DEM-derivatives, (3) Climatic maps, (4) Vegetation and land-cover based covariates, (5) Land survey and land use information, and (6) Expert-based covariates e.g. soil or surficial geology maps.
</div>

<p>Different environmental covariates will be the dominant spatial predictors of targeted soil properties and this relationship is often scale dependent. Often, only a few key covariates can explain over 50% of the fitted model, but these are unknown until we fit the actual models. The only way to ensure that the most relevant environmental covariates are included in the modelling process is to start with the most extensive list of all possible environmental covariates, then subset and prioritize.</p>
</div>
<div id="soil-delineations" class="section level3">
<h3><span class="header-section-number">1.4.5</span> Soil delineations</h3>
<p><em>Soil delineations</em> are manually drawn entities — soil mapping units — that portray boundaries between soil bodies. Soil polygons are usually assumed to differ across boundaries and to be relatively homogeneous within boundaries, but other criteria are sometimes used <span class="citation">(Simonson <a href="#ref-Simonson1968AA">1968</a>; Schelling <a href="#ref-Schelling1970Geoderma">1970</a>)</span>. They are commonly generated through photo-interpretation i.e. stereoscopic interpretation of aerial photographs of the area of interest (Fig. @ref(fig:from-photointerpretation-to-soilmap)). Soil delineations based on expert knowledge about an area are the main output of conventional soil mapping. If available imagery is of high detail (scales &gt;1:25k), and if the soil surveyor has developed an extensive knowledge of the soil—land-use—topography relations in an area, soil delineations can produce useful and relatively accurate maps of soil bodies and are, in a way, irreplaceable <span class="citation">(Soil Survey Staff <a href="#ref-SSS1983USDA">1983</a>)</span>. However, in many parts of the world, soil delineations have been produced using relatively weak source materials and these can be of variable accuracy.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_from_photointerpretation_to_soilmap.png" alt="In conventional soil mapping, soil delineations are usually manually drawn polygons representing (assumed) bodies of homogenous soil materials (often geomorphological units). These are first validated in the field before a final area-class map is produced, which can then be generalized and used to extract soil property maps. After USDA Soil Survey Manual." width="95%" />
<p class="caption">
(#fig:from-photointerpretation-to-soilmap)In conventional soil mapping, soil delineations are usually manually drawn polygons representing (assumed) bodies of homogenous soil materials (often geomorphological units). These are first validated in the field before a final area-class map is produced, which can then be generalized and used to extract soil property maps. After USDA Soil Survey Manual.
</p>
</div>
<p>In soil mapping terms, soil map delineations can be considered to be expert-based covariates. They can be used as input to spatial prediction in the same way as DEM-derived predictors or remote sensing indices. This is assuming that a standardized legend is attached to the soil polygon map systematically describing types of polygons (e.g. soil-geomorphological units). Soil delineations, in combination with with other auxiliary predictors, can generate soil property maps that exhibit both abrupt and smooth transitions in values. An analyst can objectively assess the utility and importance of hybrid covariates and then try to obtain optimal covariates that can be clearly demonstrated to be significant predictors. In practice, expert-based predictors can sometimes perform better than alternatives such as DEM-derived predictors or remote sensing indices. <em>“Perform better”</em> in this case indicates that the predictors will be more distinctly correlated with target soil properties. In all applications of PSM methods, it is advisable to obtain and assess the utility of available soil polygon maps.</p>
<p>Most legacy polygon soil maps represent a distillation and summary of expert knowledge about the main spatial patterns of variation in soil types (classes) within an area. This knowledge has been abstracted and generalized in order to convey dominant patterns at specific scales. Thus, it is often not reasonable to expect to be able to go to a specific point portrayed on a soil map and find a single specific soil class or soil property value (see Fig. @ref(fig:smu-aggregation)). Most often, soil maps provide lists or inventories of soil classes that occur within a given map area and give outlines of areas (polygons) within which lists of specific soils are predicted to occur with specified frequencies or possibilities. Soils are conceptualized as objects that belong to defined soil classes.</p>

<div class="rmdnote">
Soil delineations are manually drawn entities that portray boundaries between soil bodies assumed to be internally homogeneous. Soil delineations can be considered to be expert-based soil covariates.
</div>

<p>Each class of soil (often a soil series or taxonomic class) is assumed to have a limited and describable range of characteristics i.e. physical and chemical properties that can be used to characterize it. Within mapped polygons, the manner in which soils vary horizontally across the landscape is usually not explicitly portrayed (Fig. @ref(fig:smu-aggregation)). At best, such internal polygon variation may be described in conceptual terms relative to how different soils may be more likely to occupy specific landscape positions or occur on specific parent materials or under different drainage conditions. For example the USDA’s Soil Survey Manual distinguishes between <em>consociations</em> (relatively homogeneous polypedons), <em>associations</em> (heterogeneous unit with two or more similar polypedons), and <em>complexes</em> (mix of two or more contrasting polypedons), but in most cases none of the described components is actually mapped separately.</p>
<p>Variation of soil properties in the vertical dimension is usually described in terms of variation in the type, thickness and arrangement of various different soil horizons. Soil horizons are themselves a collection of class objects, with each class also expected to display a characteristic range of attributes and soil property values. All soils do not always have the same types or sequences of horizons and so, most horizons are not laterally continuous and mappable. So, most legacy soil maps portray abstract representations of how various classes of soils vary horizontally between soil polygons and vertically by soil horizons.</p>
<p>Interpretation of most maps of soil classes often requires a considerable amount of knowledge and understanding of both underlying soil mapping concepts and of local classes of soils and soil horizons. This restricts effective use of many soils maps to persons with the necessary background knowledge.</p>
</div>
<div id="advantages-and-disadvantages-of-using-soil-delineations" class="section level3">
<h3><span class="header-section-number">1.4.6</span> Advantages and disadvantages of using soil delineations</h3>
<p>One of the key advantages of conventional soil polygon map data is its availability. In many parts of the world, the number of instances of reliably located soil profile observations is quite low and the spatial extent of areas for which sufficient point data are available can be small <span class="citation">(Hartemink <a href="#ref-Hartemink2008SMD">2008</a>)</span>. However, many areas with only limited amounts of geo–referenced point data are covered by soil maps of various types and scales. So, conventional soil polygon maps are often available for areas that lack sufficient amounts of soil point data.</p>
<p>For most of the last 80–100 years, conventional polygonal (area-class) soil maps have been seen as the most effective way to convey information about horizontal and vertical variation in soils and soil properties across the landscape <span class="citation">(Wysocki, Schoeneberger, and LaGarry <a href="#ref-Wysocki2005Geoderma">2005</a>)</span>. Conventional soil maps do manage to achieve some partitioning of the total amount of variation in soils and soil properties in the horizontal dimension. Soil maps have always acknowledged that they are unable to capture and explicitly portray variation that occurs at distances shorter than some minimum sized area that is feasible to display at any particular scale of mapping.</p>
<p>Since soil types and soil properties can exhibit a significant amount of variation over rather short distances, there is always a relatively large amount of total variation in soils and soil properties that is not explicitly captured or described by polygonal soil maps. For some highly variable soil properties, as much as 40–60% of the total variation in that soil property within a mapped area can occur over distances of meters to tens of meters. This means that most soil maps cannot explicitly display this portion of the variation and can only try to portray the remaining portion of the variation (40–60%) that occurs over longer distances <span class="citation">(Heuvelink and Webster <a href="#ref-Heuvelink2001Geoderma">2001</a>)</span>. Much of this longer range variation is often related to observable and mappable physical or landscape features such as slope gradient, slope position, landform elements, definable bodies of different surficial geological materials, readily apparent differences in moisture or drainage conditions or observable changes in soil color, accumulation of surface salts or visible erosion.</p>
<p>Soil surveyors make use of these correlations to manually delineate soil polygon boundaries that outline areas that display different soil assemblages in response to observable differences in landscape or environmental conditions. These manually drawn polygon boundaries can, and do, provide much useful information about variation in readily observable soil and landscape attributes. So, soil maps are often one of the best sources of information on local variation in surficial geological materials, because soil surveyors have observed, recorded and mapped this variation in delineating their polygons.</p>
<p>Likewise, soil maps are often able to be quite successful in outlining areas of significantly different moisture or drainage conditions, climate or vegetation related conditions, depth to bedrock, slope or slope position, salinity or calcareousness. Where they exist, conventional soil polygon maps can act as one of the most effective sources of covariate information describing medium to long range variation in key environmental factors such as parent material, drainage, climate, vegetation and topography.</p>
<p>In terms of automated soil mapping, one of the key advantages of conventional soil maps is that they provide a useful initial indication of the main soils that are likely to be encountered within any given area (map sheet or individual polygon). This listing limits the number of soils that need to be considered as possible or likely to occur at any point or within any area to a much smaller and more manageable number than a full list of all possible soils in a region. Most soil maps provide a hierarchical stratification of an area into smaller areas of increasing homogeneity and more limited soil and environmental conditions.</p>
<p>Many soil maps, or their accompanying reports, also provide some indication about how named soils within polygons or map units vary spatially, within the polygon, in response to changes in slope, landform position, parent material, drainage and so on <span class="citation">(Soil survey Division staff <a href="#ref-SSDS1993">1993</a>; Wysocki, Schoeneberger, and LaGarry <a href="#ref-Wysocki2005Geoderma">2005</a>)</span>. This information on which soils are most likely to occur within a given geographic area and under what environmental conditions (slope position, drainage, parent material) each listed soil is most likely to occur, can provide a foundation for heuristic (or expert-based) modeling of the more detailed and shorter range variation in soil types that lies at the heart of DSM methods of <em>soil polygon disaggregation</em>. Disaggregation of conventional soil polygon maps into more detailed representations of the most likely finer scale spatial pattern of variation of the named component soils is an attractive and feasible method of producing more detailed estimates of the spatial distribution of soils and soil properties for many areas for which point data are scarce and conventional soil polygon maps are available (Fig. @ref(fig:smu-aggregation)).</p>
<p>The list of limitations and potential problems with using conventional soil polygon map data is long and must be acknowledged and dealt with. Two of the most serious issues are completeness and consistency. It is extremely rare to have entire regions or countries for which there is complete coverage with a consistent set of soil polygon maps of consistent scale, content and vintage. In fact, the normal situation for most regions and countries is one of incomplete coverage with patches of maps of different scale, content, design and vintage covering portions of areas of interest with large gaps of unmapped areas between mapped areas.</p>
<p>Only a very few countries or regions (e.g. USA, UK, Japan, western European countries, Jamaica, Gambia etc) have achieved anywhere near complete national coverage at scales more detailed than 1:50,000 <span class="citation">(Rossiter <a href="#ref-Rossiter2004SUM">2004</a>; Hartemink <a href="#ref-Hartemink2008SMD">2008</a>)</span>. Most smaller scale (1:1M or smaller) national or continental soil maps are based on manual interpolation and extrapolation of scattered and incomplete maps that provide only partial coverage for these mapped areas. Even where coverage is complete, or nearly complete, consistency is often a significant issue.</p>

<div class="rmdnote">
Conventional soil polygon maps (manually-drawn delineations) are often one of the best sources of information on local variation in soil polypedons. On the other hand, conventional soil polygon maps often suffer from incompleteness, inconsistency and low accuracy of thematic content, as well as from suspect positional accuracy.
</div>

<p>Mapping concepts change across time and vary among different mappers and agencies. Consequently, the normal situation is that no two maps are entirely comparable and many collections of maps exhibit very marked and significant differences in what has been mapped and described, the concepts and legends used to map and describe, the classification rules and taxonomies and the scale and level of detail of mapping. Joining maps of different scales, vintages and legend concepts into consistent compilations that cover large regions is challenging and not always entirely successful.</p>
<p>Even in the USA, where a single set of mapping guidelines and specifications is ostensibly in place for national mapping programs, there are readily apparent differences in the concepts used to produce maps in different areas and visible differences in the naming and description of dominant mapped soils on the same landforms and landform positions in adjoining map sheets <span class="citation">(Lathrop Jr., Aber, and Bognar <a href="#ref-LathropJr19951">1995</a>; Zhong and Xu <a href="#ref-ZHONG2011491">2011</a>)</span>.</p>
<p>For conventional soil polygon maps to be of maximum utility for automated soil mapping, they really benefit from being compiled and harmonized into regional maps that have a common legend, common scale, common list of described landform and soil attributes and consistent application of terminologies and methods. There have been some successes in developing and demonstrating methods for compiling harmonized soil polygon maps at regional to continental scales from scattered and disparate collections of available soil polygon maps <span class="citation">(Bui <a href="#ref-Bui2003Geoderma">2003</a>; Grinand et al. <a href="#ref-Grinand2008Geoderma">2008</a>)</span> but these methods have not yet been formalized or widely adopted for global use. If soil polygon maps are not harmonized to produce complete and consistent regional to national coverages, then each map needs to be treated as a separate entity which complicates use of soil maps to build consistent rules for predicting soils or soil properties across large areas.</p>
</div>
<div id="accuracy-of-conventional-soil-polygon-maps" class="section level3">
<h3><span class="header-section-number">1.4.7</span> Accuracy of conventional soil polygon maps</h3>
<p>The spatial accuracy of conventional soil polygon maps is also a frequent concern. Most legacy soil maps were prepared before the advent of ortho-rectified digital base maps and GPS. Many legacy maps exist only on non-stable media (e.g. paper), are of unknown or uncertain projection and datum and were compiled onto uncontrolled base maps, usually in paper format. Even though the boundaries of soil polygons are generally subjective and fuzzy, the correct location of many polygon boundaries on legacy soil maps is compromised by problems related to unknown or unstable geo-referencing. It is very common to encounter highly obvious discrepancies between the observed location of soil polygon boundaries on newly digitized soil polygon maps and the obviously intended location of those same boundaries. For example, polygon boundaries, clearly intended to delineate drainage channels are often displaced relative to the channels or cut back and forth across the channels.</p>
<p>Similarly, boundaries intended to delineate an obvious break in slope are often strongly displaced relative to the actual location of the slope break in correct geographic space. The mismatch between observed geographic features and soil polygon map boundary locations is often compounded when boundaries delineated by hand at a coarse resolution are overlain onto, and compared to, landscape features observable at finer resolution on newer digital base maps and digital elevation models.</p>
<p>The displacements in boundary locations and level of generalization can be disturbing and reduce confidence in the accuracy of the polygon soil map, even when the original polygon boundaries were significant and reflected legitimate changes in soil properties at locations of likely change in soils. There are also numerous instances where boundaries on conventional soil polygons maps do not define locations of significant real change in soils or soil properties and simply reflect an arbitrary subdivision of the landscape.</p>
<p>Several soil survey cross-validation studies <span class="citation">(Marsman and de Gruijter <a href="#ref-Marsman1986ALTERRA">1986</a>; Hengl and Husnjak <a href="#ref-Hengl2006SSSAJ">2006</a>)</span> have shown that traditional polygon-based maps can be of limited accuracy and usability. First, they are created using irreproducible methods and hence difficult to update. Second, at broader scales, polygon maps produced by different teams are often incompatible and can not be merged without harmonization. A non-soil scientist introduced to a continental-scale soil map where soil boundaries follow country boundaries will potentially lose confidence and look for another source of information <span class="citation">(D’Avello and McLeese <a href="#ref-DAvello1998SSH">1998</a>)</span>. Consider for example the Harmonized World Soil Database product. On the HWSD-derived maps one can still notice numerous soil borders that match country borders (most often an artifact), but also inconsistent effective scale within continents. All these limitations reduce confidence in the final product and its usage.</p>

<div class="rmdnote">
For legacy soil maps to be of maximum possible utility for digital soil mapping they need to be harmonized with respect to thematic content and accuracy, and they need to be corrected with respect to positional accuracy.
</div>

<p>So, conventional soil polygon maps suffer from issues related to completeness, consistency and accuracy of thematic content as well as from issues related to positional accuracy and relevance of soil polygon boundaries. If these issues are not dealt with, and corrections are not implemented, the likelihood of extracting meaningful and consistent patterns and rules for use in soil mapping is considerably compromised.</p>
</div>
<div id="tacit-knowledge" class="section level3">
<h3><span class="header-section-number">1.4.8</span> Legacy soil expertise (tacit knowledge)</h3>
<p>The dominant characteristic of most legacy soil expert knowledge is that it has often not been formalized or made explicit and systematic. <span class="citation">Hudson (<a href="#ref-Hudson2000SSSAJ">2004</a>)</span> refers to the vast amount of soils knowledge that exists in tacit form, as <em>“unstated and unformalized rules and understanding that exists mainly in the minds and memories of the individuals who conducted field studies and mapping”</em>. Soil maps are one mechanism by which experts try to capture and portray their understanding of how and why soils vary across the landscape <span class="citation">(Bui <a href="#ref-Bui2004Geoderma">2004</a>)</span>. Other methods include:</p>
<ul>
<li><p><em>2D cross sections</em>,</p></li>
<li><p><em>random catenas</em> <span class="citation">(McBratney, Odgers, and Minasny <a href="#ref-McBratney2006WCSS">2006</a>)</span>,</p></li>
<li><p><em>3D block diagrams</em>,</p></li>
<li><p><em>decision trees or rules</em>,</p></li>
<li><p><em>mapping keys and textual descriptions of where, how and why soils have been observed to vary in particular areas or under particular conditions</em>.</p></li>
</ul>
<p>All of these methods are imperfect and all leave some portion of expert knowledge un-expressed and uncaptured. Modern methods of digital soil mapping often represent attempts to capture expert knowledge in a systematic and formal way <span class="citation">(Zhu et al. <a href="#ref-Zhu2001">2001</a>; McBratney, Mendoça Santos, and Minasny <a href="#ref-McBratney2003Geoderma">2003</a>; Bui <a href="#ref-Bui2004Geoderma">2004</a>; MacMillan, Pettapiece, and Brierley <a href="#ref-MacMillan2005CJSS">2005</a>)</span>.</p>
<p>Integration of expert pedological knowledge into soil mapping methods provides the opportunity of potentially improving both the predictions themselves and understanding of the reasons or rationale for the success (or failure) of predictions <span class="citation">(Walter, Lagacherie, and Follain <a href="#ref-Walter2006DSS">2006</a>; Lagacherie <a href="#ref-Lagacherie1995Geoderma">1995</a>; Lagacherie <a href="#ref-Lagacherie2001Geoderm">2001</a>)</span>. There is increasing realization of the benefits of incorporating both hard and soft knowledge into prediction and decision making procedures <span class="citation">(Christakos, Bogaert, and Serre <a href="#ref-christakos2001temporal">2001</a>)</span>. Soft knowledge can help to smooth out or generalize patterns that are incompletely represented by hard data or that are noisy when assessed using hard data. A definite advantage of expert tacit knowledge is that a significant amount of it exists. Conceptual understanding of where, how and why soils and soil properties vary across landscapes is relatively widespread, if not always well documented or expressed.</p>
<p>In the absence of any hard data, in the form of point profile observations or even soil polygon maps, expert knowledge of the main patterns of variation in soils can represent the only feasible way of producing a first approximation model of soil spatial variation for an area. There will be vast tracts of the world for which both soil point data and soil maps will be lacking (e.g. remote portions of Russia and northern Canada) but for which there is considerable expert knowledge of the main kinds of soils, their properties and the patterns in which they vary across the landscape, at least at a conceptual level. It may be possible to capture and apply this expert tacit knowledge in such as way as to permit creation of initial prediction rules that can subsequently be modified and improved upon.</p>
<p>As with much legacy soils data, one of the main limitations of legacy soil tacit knowledge is — its accessibility. By definition, tacit knowledge has not been formalized and has often not even been written down. So, a challenge exists to simply locate legacy soil expert knowledge. Once located, a second challenge is how to best capture and formalize it i.e. how to turn it into rules for a mapping algorithm.</p>

<div class="rmdnote">
The first challenge to using legacy soil expert knowledge is to locate it. Once located, a second challenge is how to best capture and formalize it i.e. how to turn it into rules for a mapping algorithm.
</div>

<p>Common approaches to codifying expert knowledge about soil-landscape patterns include construction of <em>decision trees</em> <span class="citation">(Walter, Lagacherie, and Follain <a href="#ref-Walter2006DSS">2006</a>; Zhou, Zhang, and Wang <a href="#ref-Zhou2004JZUS">2004</a>)</span>, <em>fuzzy logic rule</em> bases <span class="citation">(Zhu et al. <a href="#ref-Zhu2001">2001</a>)</span> or Bayesian maximum likelihood equations <span class="citation">(Zhou, Zhang, and Wang <a href="#ref-Zhou2004JZUS">2004</a>)</span>. A less sophisticated, but more generalized, approach is to apply general conceptual understanding of soil-landscape relationships to existing databases of soils and landform data to automatically associate named soil classes with conceptual landform positions <span class="citation">(MacMillan, Pettapiece, and Brierley <a href="#ref-MacMillan2005CJSS">2005</a>)</span>. Expert tacit knowledge is often inexact and incomplete but it can express and reveal widely recognized general patterns and can provide a reasonable first approximation of soil-landscape patterns. In order to be used effectively, for activities such as PSM, platforms and procudures need to be agreed upon, and put in place, to support knowledge capture and application. Agreement on such platforms and procedures is not yet widespread.</p>
<p>To integrate all available tacit knowledge systems into a one, all encompassing, prediction algorithm is probably beyond human capacities, but it could well be assisted using e.g. web crawling applications for legacy soils data i.e. by scanning documents, soil survey reports and books and then extracting rules and procedures using automated methods. Alternately, different methods, using different types of expert knowledge, could be implemented regionally to locally and the resulting maps merged using harmonization procedures.</p>
</div>
<div id="pseudo-observations" class="section level3">
<h3><span class="header-section-number">1.4.9</span> Pseudo-observations</h3>
<p>When applying Statistical or Machine Learning methods to larger (global to continental) sized areas, one thing that often limits the success of predictions is the existance of very extensive areas with extreme climatic conditions and/or very restricted access, that are consequently significantly under-sampled. This occurs largely in the following five types of areas <span class="citation">(Hengl, Mendes de Jesus, et al. <a href="#ref-Hengl2017SoilGrids250m">2017</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Semi-arid and arid lands, deserts and sand dunes,</p></li>
<li><p>Mountain tops, steep slopes of mountains and similar inaccessible areas,</p></li>
<li><p>Areas covered by ice and/or snow, i.e. glaciers,</p></li>
<li><p>Inaccessible tropical forest,</p></li>
<li><p>Areas governed by totalitarian and hostile regimes, with military conflicts or war.</p></li>
</ol>
<p>It might seem obvious to soil surveyors that there is no soil organic carbon on the top of the active sand dunes in the Sahara, but any model fitted without observations from the Sahara could result in dubious extrapolation and questionable predictions. In addition, relationships across transitional areas — from semi-arid zones to deserts — can be difficult to represent without enough points at both edges of the feature space. Some sand dunes in the USA have fortunately been sampled and analyzed in the laboratory. For example, <span class="citation">Lei (<a href="#ref-Lei1998">1998</a>)</span> has shown that sand dunes in the Mojave desert have an average pH of 8.1. Again, although it might seem obvious that deserts consist mainly of sand, and that steep slopes without vegetation are either very shallow or show bedrock at the surface, prediction models may not be aware of such expert knowledge and hence such unsampled features need to be ‘numerically represented’ in the calibration dataset.</p>
<p>Instead of masking out all such areas from soil mapping, one can alternatively generate a number of pseudo-observations to fill sampling gaps in the feature space. Pseudo-observations can be generated by photo-interpretation of high resolution imagery or by using very detailed land cover, soil or similar maps. <span class="citation">Hengl, Mendes de Jesus, et al. (<a href="#ref-Hengl2017SoilGrids250m">2017</a>)</span> use the following data sources to delineate sand dunes, bare rock and glaciers:</p>
<ul>
<li><p>Mean annual long-term surface temperature generated from the MODIS LST data product (MOD11A2), long-term MODIS Mid-Infrared (MIR) band (MCD43A4) and slope map can be used to delineate — sand dunes mask.</p></li>
<li><p>The MODIS MIR band (MCD43A4) and a slope map can be used to delineate — bare rock areas. Bare rock or dominantly rocky areas show high MIR surface reflectance and are associated with steep slopes.</p></li>
<li><p>Global distribution of glaciers i.e. the GLIMS Geospatial Glacier Database <span class="citation">(Raup et al. <a href="#ref-raup2007glims">2007</a>)</span> can be used to delineate — glaciers and permafrost.</p></li>
</ul>
<p>For each of these three masks <span class="citation">Hengl, Mendes de Jesus, et al. (<a href="#ref-Hengl2017SoilGrids250m">2017</a>)</span> generated randomly 100–400 points based on their relative global extent and assigned soil properties and soil classes accordingly (e.g. in the case of WRB’s Protic Arenosols for sand dunes, Lithic and Rendzic Leptosols for bare rock areas, Cryosols for areas adjacent to glaciers; in the case of USDA’s Psamments for sand dunes, Orthents for bare rock areas and Turbels for glaciers; for sand dunes they also inserted estimated values of 0 for soil organic carbon, sand and coarse fragments).</p>
<p>When inserting pseudo-observations one should try to follow some basic rules (to minimize any negative effects):</p>
<ul>
<li><p>keep the relative percentage of pseudo-points small i.e. try not to exceed 1–5% of the total number of training points,</p></li>
<li><p>only insert pseudo-points for which the actual ground value is known with high confidence, e.g. sand content in sand dune areas,</p></li>
<li><p>if polygon maps are used to insert pseudo-observations, try to use the most detailed soil polygon maps and focus on polygons with the very highest thematic purity.</p></li>
</ul>
</div>
</div>
<div id="soil-databases" class="section level2">
<h2><span class="header-section-number">1.5</span> Soil databases and soil information systems</h2>
<div id="soil-databases" class="section level3">
<h3><span class="header-section-number">1.5.1</span> Soil databases</h3>
<p>To facilitate usage of soil data, soil field records and soil delineations can be digitized and organized into databases. Soil profiles are commonly put into a <em>Soil–Profile (geographical) Database</em> (SPDB); soil delineations are digitized and represented as polygon maps with attributes attached via mapping units and soil classes <span class="citation">(Rossiter <a href="#ref-Rossiter2004SUM">2004</a>)</span>. Soil profile databases and soil polygon maps can be combined to produce attribute maps of soil properties and classes to answer soil or soil–land use specific questions. Once the data are in a database, one can generate maps and statistical plots by running spatial queries <span class="citation">(Beaudette and O’Geen <a href="#ref-Beaudette2009CG">2009</a>)</span>.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_SITE_HORIZON_structure.png" alt="An example of a basic soil profile geographical database, which commonly consists of four tables: SITE, HORIZON, DESCRIPTION and NAMES tables (a). To facilitate rapid display and use of soil variables, SITE and HORIZON tables can be combined into a single (wide) table structure (b)." width="80%" />
<p class="caption">
(#fig:site-horizon-structure)An example of a basic soil profile geographical database, which commonly consists of four tables: SITE, HORIZON, DESCRIPTION and NAMES tables (a). To facilitate rapid display and use of soil variables, SITE and HORIZON tables can be combined into a single (wide) table structure (b).
</p>
</div>
<p>A common database model used for SPDB is one where soil site, soil horizon data and metadata are split into separate tables (Fig. @ref(fig:site-horizon-structure)a; here referred to as the <em>horizon-site</em> or layer-site database model. Note that soil surveyors typically like to include in the database also meta data that describe column names and classes for factor type variables, because these are often area/project specific and need to be attached to the original soil data. Many variations on this horizon-site database model exist, so that each new user of SPDB typically requires some initial training to understand where soil variables of interest are located and how they can be exported and visualized.</p>
<p>Any horizon-site database model can be converted to a single table where each soil profile becomes one record (Fig. @ref(fig:site-horizon-structure)b). The single-table database model simplifies subsequent efforts to visualize sampled values and to import them to a platform to run spatial analysis. Note also that conversion from one data model to the other in software for statistical computing is relatively easy to accomplish.</p>
</div>
<div id="a-soil-information-system" class="section level3">
<h3><span class="header-section-number">1.5.2</span> A Soil Information System</h3>
<p>A <em>Soil Information System</em> (SIS) consists of a combination of input soil data (soil profiles, soil polygon maps, soil covariates), output predictions (soil properties and classes) and software to browse these data. A SIS is basically a thematic GIS focused on soil resources and offering the best possible soil information at some given scale(s). A SIS is often the end product of a soil survey. In the ideal case, it should meet some common predefined soil survey specifications, for example:</p>
<ul>
<li><p><em>It corresponds to a specified soil survey scale</em>.</p></li>
<li><p><em>It provides spatial information about a list of targeted soil variables which can be used directly for spatial planning and environmental modelling</em>.</p></li>
<li><p><em>It provides enough meta-information to allow use by a non-soil science specialist</em>.</p></li>
<li><p><em>It has been cross-checked and validated by an independent assessment</em>.</p></li>
<li><p><em>It follows national and/or international data standards</em>.</p></li>
<li><p><em>It has a defined information usage and access policy</em>.</p></li>
</ul>
<p>Many soil data production agencies are often unclear about where the work of a soil surveyor stops. Is a SPDB and a soil polygon map an intermediate product or can it be delivered as a soil information system? Does a SIS need to already hold all predictions or only inputs to prediction models? In this book we will adhere to a strict definition of a SIS as a complete and standardized geographical information system that contains both initial inputs and final outputs of spatial predictions of soil variables, and which is fully documented and ready to be used for spatial planning. The PSM tools described in this book, in that context, have been designed as a step forward to producing more complete soil information systems.</p>

<div class="rmdnote">
A Soil Information System is an end product of soil mapping — a standardized collection of (usually gridded) soil property and class maps of an area that can be used for spatial planning, environmental modelling, agricultural engineering, land degradation studies, biodiversity assessment and similar. A SIS tries to provide the best possible soil information at some given scale for the spatial domain of interest.
</div>

<p>Another important point is that a modern SIS needs to be user-oriented. As <span class="citation">Campbell (<a href="#ref-Campbell2008NCST">2008</a>)</span> argues: <em>“Soil science, soil classification, mapping and monitoring systems and resources are not ends in themselves, they are means to an end. The objective is more sustainable management of soil.”</em> We envisage that in the near future soil surveyors will have to completely open soil information systems to users so that they can also contribute to construction and influence content. <span class="citation">Goodchild (<a href="#ref-Goodchild2008Accuracy">2008</a>)</span> calls this <em>“Web 2.0”</em> (read and write) and/or <em>“Web 3.0”</em> (read, write and execute) approaches to content creation. We also envisage that soil information will increasingly be produced using global vs local models and increasingly using distributed data and computing (Fig. @ref(fig:automap-future)).</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_automap_future.png" alt="The future of global mapping and environmental monitoring activities is expected to be increasingly automated and distributed." width="75%" angle=0 />
<p class="caption">
(#fig:automap-future)The future of global mapping and environmental monitoring activities is expected to be increasingly automated and distributed.
</p>
</div>
<p>One example of a web-interface, provided to make access to input and output soil data more efficient, is the California Soil Resource Lab SoilWeb <span class="citation">(O’Geen, Walkinshaw, and Beaudette <a href="#ref-OGeen2017soilweb">2017</a>)</span>. Here, a series of web-apps and simple interfaces to PostGIS and similar databases are used to empower users, including developers, to access soil data without using a sophisticated GIS or similar.</p>
<p>There is also increasing interest in the economic aspects of soil functions in relation to soil mapping and soil information use. For a soil mapper to justify the importance of producing spatial soil information there is no better argument that a thorough economic assessment of its use.</p>

<div class="rmdnote">
There is an increasing need to quantify economic aspects of soil functions in relation to soil mapping and soil information use: What is the value of soil information for food production? How much does some sophisticated geostatistical mapping method reduce costs (while producing equally accurate information)? How much does soil (environmental) remediation cost? What is the cost-benefit ratio between soil mapping and soil exploitation? What is the global value of soil for fixation of atmospheric gasses or for water filtering or retention?
</div>

</div>
<div id="soil-information-users" class="section level3">
<h3><span class="header-section-number">1.5.3</span> Soil information users</h3>
<p>Typical <em>user groups of soil information</em> include <span class="citation">(Soil survey Division staff <a href="#ref-SSDS1993">1993</a>; Harpstead, Sauer, and Bennett <a href="#ref-harpstead2001soil">2001</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><em>At local/farm level</em>:</p>
<ol style="list-style-type: decimal">
<li><p>farmers and ranchers who want to maximize sustainability and/or production efficiency;</p></li>
<li><p>fertilizer dealers and agricultural consulting companies, who want to sell competitive products and services;</p></li>
<li><p>civil engineers who plan roads, airports and similar;</p></li>
<li><p>land development agencies who must consider the soil foundations, streets, lawns and e.g. locations for septic systems,</p></li>
<li><p>bankers and financial agencies who give loans, provide insurance or buy or sell land;</p></li>
<li><p>foresters who plan harvesting or reforestation operations and must know the relevant conditions and capabilities of the soil;</p></li>
<li><p>tax assessors who assign potential value for a given piece of farmland and/or ranch land;</p></li>
</ol></li>
<li><p><em>At national level</em>:</p>
<ol style="list-style-type: decimal">
<li><p>agricultural ministries and land use planning agencies (for developing and implementing policies and plans);</p></li>
<li><p>environmental protection agencies, who develop and enforce management plans for protected areas or areas of special value;</p></li>
<li><p>environmental impact assessment companies and agencies, who model various management scenarios;</p></li>
<li><p>agricultural extension agencies;</p></li>
<li><p>natural hazard (e.g. flooding or landslide) monitoring agencies;</p></li>
</ol></li>
<li><p><em>At continental or global levels</em>:</p>
<ol style="list-style-type: decimal">
<li><p>agricultural development organizations such as FAO, CGIAR (Consortium of International Agricultural Research Centers) research institutes;</p></li>
<li><p>international environmental protection agencies, such as UNEP;</p></li>
<li><p>global financial organizations and trading entties, such as the World Bank;</p></li>
<li><p>global biogeochemical cycle modelers;</p></li>
<li><p>climate change modelers;</p></li>
</ol></li>
</ol>
<p>The future for digital soil data may well lie in <em>task-oriented Soil Information Systems</em> (as proposed by Gerard Heuvelink at the DSM 2010 conference in Rome), in which only input data and analytical models are stored, permitting an infinite number of maps and visualizations to be generated on-demand by users. This implies that future soil mappers will eventually evolve from people that draw maps to <em>process moderators</em>, and the maps will evolve from static to <em>interactive, on-demand created</em> maps. Likewise, if the soil mapping tools are exposed to the public, anyone will be able to evolve from a passive user into an active soil mapper. In that sense, there is also an increasing potential in crowd-sourcing soil mapping to a wider possible community.</p>
</div>
<div id="usability-of-soil-geographical-database" class="section level3">
<h3><span class="header-section-number">1.5.4</span> Usability of soil geographical database</h3>
<p>Through PSM, a soil data production agency aims at delivering products of known and reported quality. The quality of a soil geographical database is a product of a number of factors (Fig. @ref(fig:usability-scheme)):</p>
<ol style="list-style-type: decimal">
<li><p><em>Attribute and thematic accuracy</em> — How well do the attribute data correspond to reality? How well do map legends correspond to reality?</p></li>
<li><p><em>Adequacy and consistency</em> — How adequate is the produced map for its intended use? How consistent is the mapping methodology (sampling intensity, thematic coverage, lab analysis techniques)?</p></li>
<li><p><em>Geographical coverage and completeness</em> — Does the GIS provide information for the whole area of interest? How many areas are missing and when will they be made available? Are all requested variables available?</p></li>
<li><p><em>Completeness and accuracy of the metadata</em> — How exactly was the map produced? What do certain abbreviations mean and where can more technical information about data processing steps be found?</p></li>
<li><p><em>Data integrity and interoperability</em> — How can the data be integated within an existing GIS? Are the data optimized for distribution and import?</p></li>
<li><p><em>Accessibility and data sharing capacity</em> — Are the data available for download and are they easy to obtain? How many users can access the data at the same time? Are the data free and easily obtained?</p></li>
</ol>
<div class="figure" style="text-align: center">
<img src="figures/Fig_usability_scheme.png" alt="Usability of a Soil Information System is basically a function of a number of data usability measures from which the following four (C’s) are essential: completeness, consistency, correctness and currency." width="70%" angle=0 />
<p class="caption">
(#fig:usability-scheme)Usability of a Soil Information System is basically a function of a number of data usability measures from which the following four (C’s) are essential: completeness, consistency, correctness and currency.
</p>
</div>
<p>By maximizing each of the usability measures listed above we can be confident of achieving the maximum quality for output products. In reality, we can only improve each of the listed factors up to a certain level. Then, due to practical limits, we reach some best possible performance given the available funds and methods, beyond which no further improvement is feasible. For example, the capacity to serve geodata is determined by the technical capacity of the server system. In order to improve this performance we either have to invest more money to get better computers or re-design the data model so that it is more efficient in fulfilling some operation.</p>
<p>While the objective of PSM (as outlined in this book) is to increase measures such as adequacy, coverage and completeness, inherent properties of the legacy data unfortunately can not be as easily improved. We can at least assess, and report on, the input data consistency, and evaluate and report the final accuracy of the output products. Once we have estimated the true mapping accuracy, and under the assumption that mapping accuracy can be linearly improved by increasing the sampling intensity, we can estimate the total number of additional samples necessary to reach a desired level of accuracy (e.g. even approaching 100% accuracy).</p>
<p>For Keith Shepherd (ICRAF; personal communication) the key to optimization of decision making is to accurately account for uncertainty — to make sense out of measurements one needs to:</p>
<ul>
<li><p><em>Know the decision you are trying to make</em>,</p></li>
<li><p><em>Know the current state of uncertainty (your priors)</em>,</p></li>
<li><p><em>Measure where it matters and only enough to make a sound decision</em>.</p></li>
</ul>

<div class="rmdnote">
The quality of a geospatial database is a function of accuracy, adequacy, consistency, completeness, interoperability, accessibility and serving capacity. Each of these usability measures can be optimized up to a certain level depending on the available resources.
</div>

<p>In practice, soil surveyors rarely have the luxury of returning to the field to collect additional samples to iteratively improve predictions and maps, but the concept of iterative modeling of spatial variation is now increasingly accepted.</p>
</div>
</div>
<div id="uncertainty-soil-variables" class="section level2">
<h2><span class="header-section-number">1.6</span> Uncertainty of soil variables</h2>
<div id="basic-concepts" class="section level3">
<h3><span class="header-section-number">1.6.1</span> Basic concepts</h3>
<p>An important aspect of more recent soil mapping projects, such as the <em>GlobalSoilmap</em> project, is a commitment to estimating and reporting the uncertainty associated with all predictions. This is a recent improvement to soil data, as uncertainty in traditional soil maps has often been reported (if given at all) only using global estimates. Maps of uncertainty (confidence limits or prediction error) of soil properties is a new soil data product and there is an increasing demand for such maps. But what is <em>‘uncertainty’</em> and how do we measure and describe it, particularly for specific point locations?</p>
<p><span class="citation">Walker et al. (<a href="#ref-Walker2003IA">2003</a>)</span> define uncertainty as <em>“any deviation from the unachievable ideal of completely deterministic knowledge of the relevant system”</em>. The purpose of measurement is to reduce decision uncertainty; the purpose of planning soil sampling campaigns is to find an optimum between project budget and targeted accuracy. A general framework for assessing and representing uncertainties in general environmental data is reviewed by <span class="citation">Refsgaard et al. (<a href="#ref-Refsgaard2007UEM">2007</a>)</span>. In this framework, a distinction is made regarding how uncertainty can be described, i.e. whether this can be done by means of:</p>
<ul>
<li><p><em>probability distributions</em> or upper and lower bounds,</p></li>
<li><p>some <em>qualitative indication of uncertainty</em>,</p></li>
<li><p>or <em>scenarios</em>, in which a partial (not exhaustive) set of possible outcomes is simulated.</p></li>
</ul>
<p>Further, the <em>methodological quality</em> of an uncertain variable can be assessed by expert judgement, e.g. whether or not instruments or methods used are reliable and to what degree, or whether or not an experiment for measuring an uncertain variable was properly conducted. Finally, the <em>“longevity”</em>, or presistence, of uncertain information can be evaluated, i.e. to what extent does the information on the uncertainty of a variable change over time.</p>

<div class="rmdnote">
Estimates of uncertainty of soil property and soil class predictions are an increasingly important extension to soil mapping outputs. Maps of spatial variation in uncertainty can be submitted as maps of upper and lower confidence limits, probability distributions or density functions, prediction error maps and/or equiprobable simulations.
</div>

<p><span class="citation">Heuvelink and Brown (<a href="#ref-Heuvelink2006Elsevier">2006</a>)</span> observed that soil data are rarely certain or <em>‘error free’</em>, and these errors may be difficult to quantify in practice. Indeed, the quantification of error, defined here as a <em>‘departure from reality’</em>, implies that the <em>‘true’</em> state of the environment is known, which is often not possible.</p>
</div>
<div id="sources-uncertainty" class="section level3">
<h3><span class="header-section-number">1.6.2</span> Sources of uncertainty</h3>
<p>There are several sources of uncertainty in soil data. For soil profile data the sources of error are for example:</p>
<ol style="list-style-type: decimal">
<li><p><em>sampling (human) bias or omission of important areas</em>;</p></li>
<li><p><em>positioning error (location accuracy)</em>;</p></li>
<li><p><em>sampling error (at horizon level i.e. in a pit)</em>;</p></li>
<li><p><em>measurement error (in the laboratory)</em>;</p></li>
<li><p><em>temporal sampling error (changes in property value with time are ignored)</em>;</p></li>
<li><p><em>data input error (or typing error)</em>;</p></li>
<li><p><em>data interpretation error</em>;</p></li>
</ol>
<div class="figure" style="text-align: center">
<img src="figures/Fig_lines_Legros1997.png" alt="20 photo-interpretations done independently using the same aerial photograph overlaid on top of each other. Image credit: Legros (1997)." width="50%" />
<p class="caption">
(#fig:lines-legros1997)20 photo-interpretations done independently using the same aerial photograph overlaid on top of each other. Image credit: Legros (1997).
</p>
</div>
<p>For soil delineations, the common sources of error (as illustrated in Fig. @ref(fig:lines-legros1997)) are:</p>
<ol style="list-style-type: decimal">
<li><p><em>human bias (under or over representation) / omission of important areas</em>;</p></li>
<li><p><em>artifacts and inaccuracies in the aerial photographs and other covariate data sources</em>;</p></li>
<li><p><em>weak or non-obvious relationships between environmental conditions and observed spatial distributions of soils</em>;</p></li>
<li><p><em>use of inconsistent mapping methods</em>;</p></li>
<li><p><em>digitizing error</em>;</p></li>
<li><p><em>polygonization (mapping unit assignment) error</em>;</p></li>
</ol>
<p>Another important source of uncertainty is the diversity of laboratory methods (see further chapter @ref(statistical-theory)). Many columns in the soil profile databases in pan-continental projects where produced by merging data produced using a diversity of methods for data collection and analysis (see e.g. <span class="citation">Panagos et al. (<a href="#ref-Panagos2013439">2013</a>)</span>). So even if all these are quite precise, if we ignore harmonization of this data we introduce intrinsic uncertainty which is practically invisible but possibly significant.</p>
<p><span class="citation">Kuhn and Johnson (<a href="#ref-kuhn2013applied">2013</a>)</span> lists the four most common reasons why a predictive model fails:</p>
<ol style="list-style-type: decimal">
<li><p>inadequate pre-processing of the input data,</p></li>
<li><p>inadequate model validation,</p></li>
<li><p>unjustified extrapolation (application of the model to data that reside in a space unknown to the model),,</p></li>
<li><p>over-fitting of the model to the existing data,</p></li>
</ol>
<p>Each of these is addressed in further chapters and can often be tracked back with repeated modeling and testing.</p>
</div>
<div id="quantifying-the-uncertainty-in-soil-data-products" class="section level3">
<h3><span class="header-section-number">1.6.3</span> Quantifying the uncertainty in soil data products</h3>
<p>To quantify the uncertainty we must derive probability distributions. There are three main approaches to achieve this <span class="citation">(Brus, Kempen, and Heuvelink <a href="#ref-Brus2011EJSS">2011</a>; Heuvelink <a href="#ref-Heuvelink2014GSM">2014</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Direct uncertainty quantification through geostatistical modelling of soil properties.</p></li>
<li><p>Geostatistical modelling of the error in existing soil property maps.</p></li>
<li><p>Expert judgement/heuristic approaches.</p></li>
</ol>
<p>In the first case uncertainty is directly reported by a geostatistical model. However, any model is a simplified representation of reality, and so is the geostatistical model, so that if our assumptions are incorrect then also the estimate of the uncertainty will also be poor. A model-free assessment of uncertainty can be produced by collecting independent samples, preferably by using some pre-defined probability sampling <span class="citation">(Brus, Kempen, and Heuvelink <a href="#ref-Brus2011EJSS">2011</a>)</span>. This procedure basically works the same way as for geostatistical modelling of the soil property itself. The problem with model-free assessment of uncertainty is that this is often the most expensive approach to quantification of uncertainty as new soil samples need to be collected. Also, there is a difference between global assessment of uncertainty and producing maps that depict spatial patterns of uncertainty. To assess mean error over an entire study area we might need only 50–100 points, but to accurately map the spatial pattern of actual errors we might need an order of magnitude more points.</p>

<div class="rmdnote">
Uncertainty in soil data products can be quantified either via the geostatistical model, or by using a model-free assessment of uncertainty (independent validation), or by relying on expert judgement.
</div>

</div>
<div id="common-uncertainty-levels-in-soil-maps" class="section level3">
<h3><span class="header-section-number">1.6.4</span> Common uncertainty levels in soil maps</h3>
<p>Even small errors can compound and propagate to much larger errors, so that predictions can exceed realistic limits. In some cases, even though we spend significant amounts of money to collect field data, we can still produce statistically insignificant predictions. For example, imagine if the location accuracy for soil profiles is ±5 km or poorer. Even if all other data collection techniques are highly accurate, the end result of mapping will be relatively poor because we are simply not able to match the environmental conditions with the actual soil measurements.</p>
<p>Already at that site level, soil survey can result in significant uncertainty. <span class="citation">Pleijsier (<a href="#ref-Pleijsier1986ISRIC">1986</a>)</span> sent the same soil samples to a large number of soil labs in the world and then compared results they got independently. This measure of uncertainty is referred to as the <em>“inter-laboratory variation”</em>. Soil lab analysis studies by <span class="citation">Pleijsier (<a href="#ref-Pleijsier1986ISRIC">1986</a>)</span> and <span class="citation">van Reeuwijk (<a href="#ref-vanReeuwijk1982">1982</a>; Pleijsier <a href="#ref-vanReeuwijk1984ISRIC">1984</a>)</span> have shown that inter-laboratory variation in analytical results is much greater than previously suspected.</p>
<p>As mentioned previously, if all other sources of error in the soil mapping framework have been reduced, the only remaining strategy to reduce uncertainty in soil maps is to increase sampling intensity (Fig. @ref(fig:lagacherie1992)). This is again possible only up to a certain degree — even if we would sample the whole study area with an infinite number of points, we would still not be able to explain some significant portion of uncertainty. A map can never be 100% valid <span class="citation">(Oreskes, Shrader-Frechette, and Belitz <a href="#ref-Oreskes04021994">1994</a>)</span>.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_Lagacherie1992.png" alt="Reduction of prediction error as a function of sampling intensity (for three control areas). Based on @Lagacherie1992PhD." width="85%" angle=0 />
<p class="caption">
(#fig:lagacherie1992)Reduction of prediction error as a function of sampling intensity (for three control areas). Based on <span class="citation">Lagacherie (<a href="#ref-Lagacherie1992PhD">1992</a>)</span>.
</p>
</div>
<p>Soil mapping is not a trivial task. Validation results for soil maps can often be discouraging. <span class="citation">Kempen, Brus, and Stoorvogel (<a href="#ref-Kempen2011Geoderma">2011</a>)</span> for example use the highest quality soil (17 complete profiles per square-km) and auxiliary data (high quantity of 25 m resolution maps) to map the distribution of soil organic matter in a province of the Netherlands. The validation results showed that, even with such high quality and density of input data and extensive modeling, they were able to explain only an average of 50% of the variability in soil organic carbon (at 3D prediction locations). This means that commonly, at the site level, we might encounter a significant short-range variability, which is unmappable at a feasible resolution resolution, that we will not be able to model even with the most sophisticated methods.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_sigma_RMSE_relationship.png" alt="Relationship between the numeric resolution (visualized using a histogram plot on the left) and amount of variation explained by the model for soil pH." width="100%" />
<p class="caption">
(#fig:sigma-rmse-relationship)Relationship between the numeric resolution (visualized using a histogram plot on the left) and amount of variation explained by the model for soil pH.
</p>
</div>
<p>As a rule of thumb, the amount of variation explained by a model, when assessed using validation, can be used to determine the numeric resolution of the map. For example, if the sampling (or global) variance of soil pH is 1.85 units (i.e. s.d. = 1.36), then to be able to provide an effective numeric resolution of 0.5 units, we need a model that can explain at least 47% of the original variance (Fig. @ref(fig:sigma-rmse-relationship)). However, to be able to provide an effective numeric resolution of 0.2 units, we would need a model that explains 91% of variability, which would be fairly difficult to achieve.</p>
</div>
</div>
<div id="summary-and-conclusions" class="section level2">
<h2><span class="header-section-number">1.7</span> Summary and conclusions</h2>
<p>In this chapter we have presented and described conventional soil resource inventories and soil data products and discussed how these are related to new and emerging methods for automated soil mapping. We have identified, reviewed and discussed the scientific theory and methods that underlie both conventional and pedometric soil mapping and discussed how each is related to the other within a framework of the universal model of soil variation. We have provided an in-depth review of the major sources of legacy soils data as collected by conventional soil survey activities (point profile data, maps and expert knowledge) and discussed the strengths and limitations of each source for supporting current efforts to produce new soils information (within PSM) using state-of-the-art Statistical and Machine Learning methods. We have also outlined a vision of what a Soil Information System is and how such systems can be configured and used to support production and distribution of global maps of soil properties and soil classes using PSM.</p>
<p>The main point of this chapter is to provide full documentation of, and justification for, the choices that have been made in designing and implementing the PSM framework (a more practical steps on how to organize PSM projects are further given in chapter @ref(practical-tips)). At present, PSM is designed to produce local to global maps of soil properties and soil classes using legacy soil data (point profile data, maps and expert knowledge), along with available global covariate data, as inputs to multi-scale, hierarchical, quantitative, global prediction models. At some future date, it is hoped, and expected, that PSM will be able to make increasing use of newly collected (likely crowd-sourced) field observations and laboratory analysis data that are accurately geo-referenced, consistent, widespread and of sufficient density to support production of accurate predictions at finer spatial resolutions (e.g. 10’s to 100’s of m). In the meantime, in order to produce interim products immediately, it is necessary, and desirable, to make use of existing legacy soil data and existing covariates. It is important to acknowledge and understand the capabilities and limitations of the existing legacy data sources at our disposal presently and of the methods that we currently possess to process and use these data.</p>
<p>Each cycle of production in PSM is also a learning cycle that should lead to improved methods, improved products and lower costs. PSM is not a static process but, rather, it is a dynamic endeavour meant to grow, evolve and improve through time. Initial products, produced using existing legacy soil information sources, will increasingly evolve into new products produced using a combination of existing legacy data and newly collected data.</p>
<!--chapter:end:Introduction.Rmd-->
</div>
</div>
<div id="software" class="section level1">
<h1><span class="header-section-number">2</span> Software installation and first steps</h1>
<p><em>Edited by: T. Hengl</em></p>
<p>This section contains instructions on how to install and use software to run predictive soil mapping and export results to GIS or web applications. It has been written (as has most of the book) for Linux users, but should not be too much of a problem to adoat to Microsoft Windows OS and/or Mac OS.</p>
<div id="list-of-software-in-use" class="section level2">
<h2><span class="header-section-number">2.1</span> List of software in use</h2>
<div class="figure" style="text-align: center">
<img src="figures/software_triangle.png" alt="Software combination used in this book." width="60%" />
<p class="caption">
(#fig:software-triangle)Software combination used in this book.
</p>
</div>
<p>For processing the covariates we used a combination of Open Source GIS software, primarily SAGA GIS <span class="citation">(Conrad et al. <a href="#ref-gmd-8-1991-2015">2015</a>)</span>, packages raster <span class="citation">(Hijmans and van Etten <a href="#ref-raster">2017</a>)</span>, sp <span class="citation">(Pebesma and Bivand <a href="#ref-pebesma2005classes">2005</a>)</span>, and GDAL <span class="citation">(Mitchell and GDAL Developers <a href="#ref-mitchell2014geospatial">2014</a>)</span> for reprojecting, mosaicking and merging tiles. GDAL and parallel packages in R are highly suitable for processing large data.</p>
<p>Software (required):</p>
<ul>
<li><p><a href="http://cran.r-project.org/bin/windows/base/">R</a> or <a href="https://mran.microsoft.com/download/">MRO</a>;</p></li>
<li><p><a href="http://www.rstudio.com/products/RStudio/">RStudio</a>;</p></li>
<li><p>R packages: GSIF, plotKML, aqp, ranger, caret, xgboost, plyr, raster, gstat, randomForest, ggplot2, e1071 (see: <a href="http://www.r-bloggers.com/installing-r-packages/">how to install R package</a>)</p></li>
<li><p><a href="http://sourceforge.net/projects/saga-gis/">SAGA GIS</a> (on Windows machines run windows installer);</p></li>
<li><p>Google Earth or Google Earth Pro;</p></li>
<li><p><a href="https://trac.osgeo.org/gdal/wiki/DownloadingGdalBinaries">GDAL v2.x</a> for Windows machines use e.g. <a href="http://download.gisinternals.com/sdk/downloads/release-1800-x64-gdal-2-1-3-mapserver-7-0-4/gdal-201-1800-x64-core.msi">“gdal-*-1800-x64-core.msi“</a>;</p></li>
</ul>
<p>R script used in this tutorial can be downloaded from the <strong><a href="https://github.com/envirometrix/PredictiveSoilMapping">github</a></strong>. As a gentle introduction to the R programming language and to soil classes in R we recommend the chapter on importing and using soil data. Some more examples of SAGA GIS + R usage can be found in the soil covariates chapter. To visualize spatial predictions in a web-browser or Google Earth you could also consider following the soil web-maps tutorial. As a gentle introduction to the R programming language and spatial classes in R we recommend following <a href="https://geocompr.robinlovelace.net/">the Geocomputation with R book</a>. Obtaining also the <a href="https://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf">R reference card</a> is highly recommended.</p>
</div>
<div id="installing-software-on-ubuntu-os" class="section level2">
<h2><span class="header-section-number">2.2</span> Installing software on Ubuntu OS</h2>
<p>On Ubuntu (often the recommended standard for the GIS community) the main required software can be installed within 10–20 minutes. We start with installing GDAL, proj4 and some packages that you might need later on:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="fu">sudo</span> apt-get install libgdal-dev libproj-dev libjasper-dev
<span class="fu">sudo</span> apt-get install gdal-bin python-gdal</code></pre></div>
<p>Next, we can install R and RStudio. For R studio you can use the CRAN distribution or the optimized distribution provided by (the former REvolution company; now Microsoft):</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="fu">wget</span> https://mran.blob.core.windows.net/install/mro/3.4.3/microsoft-r-open-3.4.3.tar.gz
<span class="fu">tar</span> -xf microsoft-r-open-3.4.3.tar.gz
<span class="bu">cd</span> microsoft-r-open/
<span class="fu">sudo</span> ./install.sh</code></pre></div>
<p>Note that R versions are constantly being updated so you will need to replace the URL above based on information provided on the home page (<a href="http://mran.microsoft.com" class="uri">http://mran.microsoft.com</a>). Once you run <code>install.sh</code> you will have to accept the license terms two times before the installation can be completed. If everything completes successfully, you can get the session info by:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sessionInfo</span>()
<span class="co">#&gt; R version 3.5.1 (2018-07-02)</span>
<span class="co">#&gt; Platform: x86_64-redhat-linux-gnu (64-bit)</span>
<span class="co">#&gt; Running under: Fedora 29 (Workstation Edition)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Matrix products: default</span>
<span class="co">#&gt; BLAS/LAPACK: /usr/lib64/R/lib/libRblas.so</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; locale:</span>
<span class="co">#&gt;  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              </span>
<span class="co">#&gt;  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    </span>
<span class="co">#&gt;  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   </span>
<span class="co">#&gt;  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 </span>
<span class="co">#&gt;  [9] LC_ADDRESS=C               LC_TELEPHONE=C            </span>
<span class="co">#&gt; [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; attached base packages:</span>
<span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; other attached packages:</span>
<span class="co">#&gt; [1] microbenchmark_1.4-6</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; loaded via a namespace (and not attached):</span>
<span class="co">#&gt;  [1] compiler_3.5.1   magrittr_1.5     bookdown_0.7     tools_3.5.1     </span>
<span class="co">#&gt;  [5] htmltools_0.3.6  yaml_2.2.0       Rcpp_1.0.0       codetools_0.2-15</span>
<span class="co">#&gt;  [9] stringi_1.2.4    rmarkdown_1.11   highr_0.7        knitr_1.21      </span>
<span class="co">#&gt; [13] stringr_1.3.1    xfun_0.4         digest_0.6.18    evaluate_0.12</span>
<span class="kw">system</span>(<span class="st">&quot;gdalinfo --version&quot;</span>)</code></pre></div>
<p>This shows, for example, that the this installation of R is based on the Ubuntu 16.* LTS and the version of GDAL is up to date. Using an optimized distribution of R (read more about <a href="https://mran.microsoft.com/documents/rro/multithread">“The Benefits of Multithreaded Performance with Microsoft R Open”</a>) is especially important if you plan to use R for production purposes i.e. to optimize computing and generation of soil maps for large numbers of pixels.</p>
<p>To install RStudio we can run:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="fu">sudo</span> apt-get install gdebi-core
<span class="fu">wget</span> https://download1.rstudio.org/rstudio-1.1.447-amd64.deb 
<span class="fu">sudo</span> gdebi rstudio-1.1.447-amd64.deb
<span class="fu">sudo</span> rm rstudio-1.1.447-amd64.deb</code></pre></div>
<p>Again, RStudio is constantly updated so you might have to adjust the rstudio version and distribution. To learn more about doing first steps in R and RStudio and to learn to improve your scripting skills more efficiently, consider studying the following two Open Access books:</p>
<ul>
<li><p>Grolemund, G., (2014) <a href="https://rstudio-education.github.io/hopr/">Hands-On Programming with R</a>. O’Reilly, ISBN: 9781449359010, 236 pages.</p></li>
<li><p>Gillespie, C., Lovelace, R., (2016) <a href="https://csgillespie.github.io/efficientR/">Efficient R programming</a>. O’Reilly, ISBN: 9781491950753, 222 pages.</p></li>
</ul>
</div>
<div id="installing-gis-software" class="section level2">
<h2><span class="header-section-number">2.3</span> Installing GIS software</h2>
<p>Predictive soil mapping is about making maps, and working with maps requires use of GIS software to open, view overlay and analyze the data sptially. GIS software recommended for soil mapping in this book consists of SAGA GIS, QGIS, GRASS GIS and Google Earth. QGIS comes with an <a href="https://www.qgis.org/en/docs/">extensive literature</a> and can be used to publish maps and combine layers served by various organizations. SAGA GIS, being implemented in C++, is highly suited to run geoprocessing on large data sets. To, install SAGA GIS on Ubuntu we can use:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="fu">sudo</span> add-apt-repository ppa:ubuntugis/ubuntugis-unstable
<span class="fu">sudo</span> apt-get update
<span class="fu">sudo</span> apt-get install saga</code></pre></div>
<p>If installation was successful, you should be able to access SAGA command line also from R by using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system</span>(<span class="st">&quot;saga_cmd --version&quot;</span>)
<span class="co">#&gt; Warning in system(&quot;saga_cmd --version&quot;): error in running command</span></code></pre></div>
<p>To install QGIS (<a href="https://download.qgis.org/" class="uri">https://download.qgis.org/</a>) you might first have to add the location of the debian libraries:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="fu">sudo</span> sh -c <span class="st">&#39;echo &quot;deb http://qgis.org/debian xenial main&quot; &gt;&gt; /etc/apt/sources.list&#39;</span>  
<span class="fu">sudo</span> sh -c <span class="st">&#39;echo &quot;deb-src http://qgis.org/debian xenial main &quot; &gt;&gt; /etc/apt/sources.list&#39;</span>  
<span class="fu">sudo</span> apt-get update 
<span class="fu">sudo</span> apt-get install qgis python-qgis qgis-plugin-grass</code></pre></div>
<p>Other utility software that you might need include <code>htop</code> that allows you to track processing progress:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="fu">sudo</span> apt-get install htop iotop</code></pre></div>
<p>and some additional libraries use <code>devtools</code>, <code>geoR</code> and similar, which can be installed via:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="fu">sudo</span> apt-get install build-essential automake<span class="kw">;</span> 
        <span class="ex">libcurl4-openssl-dev</span> pkg-config libxml2-dev<span class="kw">;</span>
        <span class="ex">libfuse-dev</span> mtools libpng-dev libudunits2-dev</code></pre></div>
<p>You might also need the <code>7z</code> software for easier compression and <code>pigz</code> for parallelized compression:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="fu">sudo</span> apt-get install pigz zip unzip p7zip-full </code></pre></div>
</div>
<div id="Whitebox" class="section level2">
<h2><span class="header-section-number">2.4</span> WhiteboxTools</h2>
<p>WhiteboxTools (<a href="http://www.uoguelph.ca/~hydrogeo/WhiteboxTools/" class="uri">http://www.uoguelph.ca/~hydrogeo/WhiteboxTools/</a>), contributed by John Lindsay, is an extensive suite of functions and tools for DEM analysis which is especially useful for extending the hydrological and morphometric analysis tools available in SAGA GIS and GRASS GIS <span class="citation">(Lindsay <a href="#ref-lindsay2016whitebox">2016</a>)</span>. Probably the easiest way to use WhiteboxTools is to install a QGIS plugin (kindly maintained by Alexander Bruy: <a href="https://plugins.bruy.me/" class="uri">https://plugins.bruy.me/</a>) and then learn and extend the WhiteboxTools scripting language by testing things out in QGIS (see below).</p>
<div class="figure" style="text-align: center">
<img src="figures/whiteboxtools-preview.jpg" alt="Calling WhiteboxTools from QGIS via the WhiteboxTools plugin." width="100%" />
<p class="caption">
(#fig:whiteboxtools-preview)Calling WhiteboxTools from QGIS via the WhiteboxTools plugin.
</p>
</div>
<p>The function <code>FlowAccumulationFullWorkflow</code> is, for example, a wrapper function to filter out all spurious sinks and to derive a hydrological flow accumulation map in the same step. To run it from command line we can use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system</span>(<span class="kw">paste0</span>(<span class="st">&#39;&quot;/home/tomislav/software/WBT/whitebox_tools&quot; &#39;</span>,
  <span class="st">&#39;--run=FlowAccumulationFullWorkflow --dem=&quot;./extdata/DEMTOPx.tif&quot; &#39;</span>,
  <span class="st">&#39;--out_type=&quot;Specific Contributing Area&quot; --log=&quot;False&quot; --clip=&quot;False&quot; --esri_pntr=&quot;False&quot; &#39;</span>,
  <span class="st">&#39;--out_dem=&quot;./extdata/DEMTOPx_out.tif&quot; &#39;</span>,
  <span class="st">&#39;--out_pntr=&quot;./extdata/DEMTOPx_pntr.tif&quot; &#39;</span>,
  <span class="st">&#39;--out_accum=&quot;./extdata/DEMTOPx_accum.tif&quot; -v&#39;</span>))
<span class="co">#&gt; Warning in system(paste0(&quot;\&quot;/home/tomislav/software/WBT/whitebox_tools\&quot;</span>
<span class="co">#&gt; &quot;, : error in running command</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/eberg_hydroflow_preview_3d.jpg" alt="Hydrological flow accummulation map based on the Ebergotzen DEM derived using WhiteboxTools." width="100%" />
<p class="caption">
(#fig:eberg-hydroflow-preview-3d)Hydrological flow accummulation map based on the Ebergotzen DEM derived using WhiteboxTools.
</p>
</div>
<p>This produces a number of maps, from which the hydrological flow accumulation map is usually the most useful. It is highly recommended that, before running analysis on large DEM’s using WhiteboxTools and/or SAGA GIS, you test functionality using smaller data sets i.e. either a subset of the original data or using a DEM at very coarse resolutions (so that width and height of a DEM are only few hundred pixels). Also note that WhiteboxTools do not presently work with GeoTIFs that use the <code>COMPRESS=DEFLATE</code> creation options.</p>
</div>
<div id="Rstudio" class="section level2">
<h2><span class="header-section-number">2.5</span> RStudio</h2>
<p>RStudio is, in principle, the main R scripting environment and can be used to control all other software used in this tutorial. A more detailed RStudio tutorial is available at: <a href="http://www.rstudio.com/resources/training/online-learning/">RStudio — Online Learning</a>. Consider also following some spatial data tutorials e.g. by James Cheshire (<a href="http://spatial.ly/r/" class="uri">http://spatial.ly/r/</a>). Below is an example of RStudio session with R editor on right and R console on left.</p>
<div class="figure" style="text-align: center">
<img src="figures/rstudio_example.png" alt="RStudio is a commonly used R editor written in C++." width="100%" />
<p class="caption">
(#fig:rstudio-example)RStudio is a commonly used R editor written in C++.
</p>
</div>
<p>To install all required R packages used in this tutorial at once, you can use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ls &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;rgdal&quot;</span>, <span class="st">&quot;raster&quot;</span>, <span class="st">&quot;GSIF&quot;</span>, <span class="st">&quot;plotKML&quot;</span>, 
        <span class="st">&quot;nnet&quot;</span>, <span class="st">&quot;plyr&quot;</span>, <span class="st">&quot;ROCR&quot;</span>, <span class="st">&quot;randomForest&quot;</span>, <span class="st">&quot;quantregForest&quot;</span>, 
        <span class="st">&quot;psych&quot;</span>, <span class="st">&quot;mda&quot;</span>, <span class="st">&quot;h2o&quot;</span>, <span class="st">&quot;h2oEnsemble&quot;</span>, <span class="st">&quot;dismo&quot;</span>, <span class="st">&quot;grDevices&quot;</span>, 
        <span class="st">&quot;snowfall&quot;</span>, <span class="st">&quot;hexbin&quot;</span>, <span class="st">&quot;lattice&quot;</span>, <span class="st">&quot;ranger&quot;</span>, 
        <span class="st">&quot;soiltexture&quot;</span>, <span class="st">&quot;aqp&quot;</span>, <span class="st">&quot;colorspace&quot;</span>, <span class="st">&quot;Cubist&quot;</span>,
        <span class="st">&quot;randomForestSRC&quot;</span>, <span class="st">&quot;ggRandomForests&quot;</span>, <span class="st">&quot;scales&quot;</span>,
        <span class="st">&quot;xgboost&quot;</span>, <span class="st">&quot;parallel&quot;</span>, <span class="st">&quot;doParallel&quot;</span>, <span class="st">&quot;caret&quot;</span>, 
        <span class="st">&quot;gam&quot;</span>, <span class="st">&quot;glmnet&quot;</span>, <span class="st">&quot;matrixStats&quot;</span>, <span class="st">&quot;SuperLearner&quot;</span>,
        <span class="st">&quot;quantregForest&quot;</span>, <span class="st">&quot;LITAP&quot;</span>, <span class="st">&quot;intamap&quot;</span>)
new.packages &lt;-<span class="st"> </span>ls[<span class="op">!</span>(ls <span class="op">%in%</span><span class="st"> </span><span class="kw">installed.packages</span>()[,<span class="st">&quot;Package&quot;</span>])]
<span class="cf">if</span>(<span class="kw">length</span>(new.packages)) <span class="kw">install.packages</span>(new.packages)</code></pre></div>
<p>This will basically check if any package is installed already, then install it only if it is missing. You can put this line at the top of each R script that you share so that anybody using that script will automatically get all required packages.</p>
<p>The h2o package requires Java libraries, so you should first install Java by using e.g.:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="fu">sudo</span> add-apt-repository ppa:webupd8team/java
<span class="fu">sudo</span> apt-get update
<span class="fu">sudo</span> apt-get install oracle-java8-installer
<span class="ex">java</span> -version</code></pre></div>
</div>
<div id="plotkml-and-gsif-packages" class="section level2">
<h2><span class="header-section-number">2.6</span> plotKML and GSIF packages</h2>
<p>Many examples in this course rely on the top 5 most commonly used packages for spatial data: (1) <a href="https://cran.r-project.org/web/views/Spatial.html">sp and rgdal</a>, (2) <a href="https://cran.r-project.org/web/packages/raster/">raster</a>, (3) <a href="http://plotkml.r-forge.r-project.org/">plotKML</a> and (4) <a href="http://gsif.r-forge.r-project.org/">GSIF</a>. To install the most up-to-date version of plotKML/GSIF, you can also use the R-Forge versions of the package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">if</span>(<span class="op">!</span><span class="kw">require</span>(GSIF)){
  <span class="kw">install.packages</span>(<span class="st">&quot;GSIF&quot;</span>, <span class="dt">repos=</span><span class="kw">c</span>(<span class="st">&quot;http://R-Forge.R-project.org&quot;</span>), 
                 <span class="dt">type =</span> <span class="st">&quot;source&quot;</span>, <span class="dt">dependencies =</span> <span class="ot">TRUE</span>)
}
<span class="co">#&gt; Loading required package: GSIF</span>
<span class="co">#&gt; GSIF version 0.5-4 (2017-04-25)</span>
<span class="co">#&gt; URL: http://gsif.r-forge.r-project.org/</span></code></pre></div>
<p>A copy of the most-up-to-date and stable versions of plotKML and GSIF is also available on <a href="https://github.com/cran/GSIF">github</a>. To run only some specific function from the GSIF package you could do for example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">source_https &lt;-<span class="st"> </span><span class="cf">function</span>(url, ...) {
   <span class="co"># load package</span>
   <span class="kw">require</span>(RCurl)
   <span class="co"># download:</span>
   <span class="kw">cat</span>(<span class="kw">getURL</span>(url, <span class="dt">followlocation =</span> <span class="ot">TRUE</span>, 
       <span class="dt">cainfo =</span> <span class="kw">system.file</span>(<span class="st">&quot;CurlSSL&quot;</span>, <span class="st">&quot;cacert.pem&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;RCurl&quot;</span>)), 
       <span class="dt">file =</span> <span class="kw">basename</span>(url))
   <span class="kw">source</span>(<span class="kw">basename</span>(url))
}
<span class="kw">source_https</span>(<span class="st">&quot;https://raw.githubusercontent.com/cran/GSIF/master/R/OCSKGM.R&quot;</span>)</code></pre></div>
<p>To test if these packages work properly, create soil maps and visualize them in Google Earth by running the following lines of code (see also function: <a href="http://gsif.r-forge.r-project.org/fit.gstatModel.html">fit.gstatModel</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(GSIF)
<span class="kw">library</span>(sp)
<span class="kw">library</span>(boot)
<span class="kw">library</span>(aqp)
<span class="co">#&gt; This is aqp 1.16-3</span>
<span class="kw">library</span>(plyr)
<span class="kw">library</span>(rpart)
<span class="kw">library</span>(splines)
<span class="kw">library</span>(gstat)
<span class="kw">library</span>(quantregForest)
<span class="co">#&gt; Loading required package: randomForest</span>
<span class="co">#&gt; randomForest 4.6-14</span>
<span class="co">#&gt; Type rfNews() to see new features/changes/bug fixes.</span>
<span class="co">#&gt; Loading required package: RColorBrewer</span>
<span class="kw">library</span>(plotKML)
<span class="co">#&gt; plotKML version 0.5-8 (2017-05-12)</span>
<span class="co">#&gt; URL: http://plotkml.r-forge.r-project.org/</span>
<span class="kw">demo</span>(meuse, <span class="dt">echo=</span><span class="ot">FALSE</span>)
omm &lt;-<span class="st"> </span><span class="kw">fit.gstatModel</span>(meuse, om<span class="op">~</span>dist<span class="op">+</span>ffreq, meuse.grid, <span class="dt">method=</span><span class="st">&quot;quantregForest&quot;</span>)
<span class="co">#&gt; Fitting a Quantile Regression Forest model...</span>
<span class="co">#&gt; Fitting a 2D variogram...</span>
<span class="co">#&gt; Saving an object of class &#39;gstatModel&#39;...</span>
om.rk &lt;-<span class="st"> </span><span class="kw">predict</span>(omm, meuse.grid)
<span class="co">#&gt; Subsetting observations to fit the prediction domain in 2D...</span>
<span class="co">#&gt; Prediction error for &#39;randomForest&#39; model estimated using the &#39;quantreg&#39; package.</span>
<span class="co">#&gt; Generating predictions using the trend model (RK method)...</span>
<span class="co">#&gt; [using ordinary kriging]</span>
<span class="co">#&gt; </span>
<span class="dv">100</span>% done
<span class="co">#&gt; Running 5-fold cross validation using &#39;krige.cv&#39;...</span>
<span class="co">#&gt; Creating an object of class &quot;SpatialPredictions&quot;</span>
om.rk
<span class="co">#&gt;   Variable           : om </span>
<span class="co">#&gt;   Minium value       : 1 </span>
<span class="co">#&gt;   Maximum value      : 17 </span>
<span class="co">#&gt;   Size               : 153 </span>
<span class="co">#&gt;   Total area         : 4964800 </span>
<span class="co">#&gt;   Total area (units) : square-m </span>
<span class="co">#&gt;   Resolution (x)     : 40 </span>
<span class="co">#&gt;   Resolution (y)     : 40 </span>
<span class="co">#&gt;   Resolution (units) : m </span>
<span class="co">#&gt;   Vgm model          : Exp </span>
<span class="co">#&gt;   Nugget (residual)  : 2.32 </span>
<span class="co">#&gt;   Sill (residual)    : 4.76 </span>
<span class="co">#&gt;   Range (residual)   : 2930 </span>
<span class="co">#&gt;   RMSE (validation)  : 1.75 </span>
<span class="co">#&gt;   Var explained      : 73.8% </span>
<span class="co">#&gt;   Effective bytes    : 1203 </span>
<span class="co">#&gt;   Compression method : gzip</span>
<span class="co">#plotKML(om.rk)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/ge_preview.jpg" alt="Example of plotKML output." width="90%" />
<p class="caption">
(#fig:ge-preview)Example of plotKML output.
</p>
</div>
</div>
<div id="connecting-r-and-saga-gis" class="section level2">
<h2><span class="header-section-number">2.7</span> Connecting R and SAGA GIS</h2>
<p>SAGA GIS is an extensive GIS geoprocessing software with over <a href="http://www.saga-gis.org/saga_tool_doc/index.html">600 functions</a>. SAGA GIS can not be installed from RStudio (it is not a package for R). Instead, you need to install SAGA GIS using the installation instructions from the <a href="https://sourceforge.net/projects/saga-gis/">software homepage</a>. After you have installed SAGA GIS, you can send processes from R to SAGA GIS by using the <code>saga_cmd</code> command line interface:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">if</span>(<span class="op">!</span><span class="kw">Sys.info</span>()[<span class="st">&#39;sysname&#39;</span>]<span class="op">==</span><span class="st">&quot;Linux&quot;</span>){
  saga_cmd =<span class="st"> &quot;C:/Progra~1/SAGA-GIS/saga_cmd.exe&quot;</span>
} <span class="cf">else</span> {
  saga_cmd =<span class="st"> &quot;saga_cmd&quot;</span>
}
<span class="kw">system</span>(<span class="kw">paste</span>(saga_cmd, <span class="st">&quot;-v&quot;</span>))
<span class="co">#&gt; Warning in system(paste(saga_cmd, &quot;-v&quot;)): error in running command</span></code></pre></div>
<p>To use some SAGA GIS function you need to carefully follow the <a href="http://www.saga-gis.org/saga_tool_doc/index.html">SAGA GIS command line arguments</a>. For example,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(plotKML)
<span class="kw">library</span>(rgdal)
<span class="co">#&gt; rgdal: version: 1.3-6, (SVN revision 773)</span>
<span class="co">#&gt;  Geospatial Data Abstraction Library extensions to R successfully loaded</span>
<span class="co">#&gt;  Loaded GDAL runtime: GDAL 2.3.2, released 2018/09/21</span>
<span class="co">#&gt;  Path to GDAL shared files: /usr/share/gdal</span>
<span class="co">#&gt;  GDAL binary built with GEOS: TRUE </span>
<span class="co">#&gt;  Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493]</span>
<span class="co">#&gt;  Path to PROJ.4 shared files: (autodetected)</span>
<span class="co">#&gt;  Linking to sp version: 1.3-1</span>
<span class="kw">library</span>(raster)
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: &#39;raster&#39;</span>
<span class="co">#&gt; The following objects are masked from &#39;package:aqp&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     metadata, metadata&lt;-</span>
<span class="kw">data</span>(<span class="st">&quot;eberg_grid&quot;</span>)
<span class="kw">gridded</span>(eberg_grid) &lt;-<span class="st"> </span><span class="er">~</span>x<span class="op">+</span>y
<span class="kw">proj4string</span>(eberg_grid) &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="st">&quot;+init=epsg:31467&quot;</span>)
<span class="kw">writeGDAL</span>(eberg_grid[<span class="st">&quot;DEMSRT6&quot;</span>], <span class="st">&quot;./extdata/DEMSRT6.sdat&quot;</span>, <span class="st">&quot;SAGA&quot;</span>)
<span class="kw">system</span>(<span class="kw">paste</span>(saga_cmd, <span class="st">&#39;ta_lighting 0 -ELEVATION &quot;./extdata/DEMSRT6.sgrd&quot; </span>
<span class="st">             -SHADE &quot;./extdata/hillshade.sgrd&quot; -EXAGGERATION 2&#39;</span>))
<span class="co">#&gt; Warning in system(paste(saga_cmd, &quot;ta_lighting 0 -ELEVATION \&quot;./extdata/</span>
<span class="co">#&gt; DEMSRT6.sgrd\&quot; \n -SHADE \&quot;./extdata/hillshade.sgrd\&quot; -EXAGGERATION 2&quot;)):</span>
<span class="co">#&gt; error in running command</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/rstudio_saga_gis.png" alt="Deriving hillshading using SAGA GIS and then visualizing the result in R." width="100%" />
<p class="caption">
(#fig:rstudio-saga-gis)Deriving hillshading using SAGA GIS and then visualizing the result in R.
</p>
</div>
</div>
<div id="connecting-r-and-gdal" class="section level2">
<h2><span class="header-section-number">2.8</span> Connecting R and GDAL</h2>
<p>Another very important software for handling spatial data (and especially for exchanging / converting spatial data) is GDAL. GDAL also needs to be installed separately (for Windows machines use e.g. <a href="http://download.gisinternals.com/sdk/downloads/">“gdal-201-1800-x64-core.msi”</a>) and then can be called from command line:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">if</span>(.Platform<span class="op">$</span>OS.type <span class="op">==</span><span class="st"> &quot;windows&quot;</span>){
  gdal.dir &lt;-<span class="st"> </span><span class="kw">shortPathName</span>(<span class="st">&quot;C:/Program files/GDAL&quot;</span>)
  gdal_translate &lt;-<span class="st"> </span><span class="kw">paste0</span>(gdal.dir, <span class="st">&quot;/gdal_translate.exe&quot;</span>)
  gdalwarp &lt;-<span class="st"> </span><span class="kw">paste0</span>(gdal.dir, <span class="st">&quot;/gdalwarp.exe&quot;</span>) 
} <span class="cf">else</span> {
  gdal_translate =<span class="st"> &quot;gdal_translate&quot;</span>
  gdalwarp =<span class="st"> &quot;gdalwarp&quot;</span>
}
<span class="kw">system</span>(<span class="kw">paste</span>(gdalwarp, <span class="st">&quot;--help&quot;</span>))</code></pre></div>
<p>We can use GDAL to reproject the grid from the previous example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system</span>(<span class="kw">paste</span>(<span class="st">&#39;gdalwarp ./extdata/DEMSRT6.sdat ./extdata/DEMSRT6_ll.tif&#39;</span>,  
             <span class="st">&#39;-t_srs </span><span class="ch">\&quot;</span><span class="st">+proj=longlat +datum=WGS84</span><span class="ch">\&quot;</span><span class="st">&#39;</span>))
<span class="kw">library</span>(raster)
<span class="kw">plot</span>(<span class="kw">raster</span>(<span class="st">&quot;./extdata/DEMSRT6_ll.tif&quot;</span>))</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Software_files/figure-html/plot-eberg-ll-1.png" alt="Ebergotzen DEM reprojected in geographical coordinates." width="100%" />
<p class="caption">
(#fig:plot-eberg-ll)Ebergotzen DEM reprojected in geographical coordinates.
</p>
</div>
<p>The following two books are highly recommended for improving programming skills in R and specially for the purpose of geographical computing:</p>
<ul>
<li><p>Bivand, R., Pebesma, E., Rubio, V., (2013) <a href="http://www.asdar-book.org/">Applied Spatial Data Analysis with R</a>. Use R Series, Springer, Heidelberg, 2nd Ed. 400 pages.</p></li>
<li><p>Lovelace, R., Nowosad, J., Muenchow, J., (2018) <a href="https://geocompr.robinlovelace.net/">Geocomputation with R</a>. R Series, CRC Press, ISBN: 9781138304512, 338 pages.</p></li>
</ul>
<!--chapter:end:Software.Rmd-->
</div>
</div>
<div id="soil-variables-chapter" class="section level1">
<h1><span class="header-section-number">3</span> Soil observations and variables</h1>
<p><em>Edited by: Hengl T., MacMillan R.A. and Leenaars J.G.B.</em></p>
<p>This chapter identifies, and provides comprehensive definitions and descriptions for, a standardized set of soil properties (and classes) that are commonly predicted using PSM. We first discuss the complexity of measuring and standardizing (or harmonizing) soil attributes, then focus on the key soil properties and classes of interest for global soil mapping. The purpose of this chapter is to serve as a reference, and background, for other chapters where the focus is on generating soil maps, interpreting accuracy results and similar.</p>
<p>The R tutorial at the end of the chaper reviews soil data classes and functions for R. It illustrates how to organize and reformat soil data in R for spatial analysis, how to import soil data to R and how to export data and plot it in Google Earth. To learn more about the Global Soil Information Facilities (GSIF) package, visit the main <a href="http://gsif.r-forge.r-project.org/00Index.html">documentation page</a>.</p>
<div id="basic-concepts-1" class="section level2">
<h2><span class="header-section-number">3.1</span> Basic concepts</h2>
<div id="types-of-soil-observations" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Types of soil observations</h3>
<p>As mentioned in the previous chapter, values for soil properties or attributes are obtained through observation and/or measurement of a soil feature, using a specified method. We refer to observations and measurements of the characteristics of soil properties and/or feature attributes as <em>soil observations</em> (see also the <a href="http://www.opengeospatial.org/standards/om">Observation and Measurements OGC standard</a>; ISO/DIS 19156). From the perspective of the technology used, soil observations can be grouped as follows (see also Fig. @ref(fig:soil-vars)):</p>
<ol style="list-style-type: decimal">
<li><p><em>Direct measurements obtained using analytical instruments and procedures in a laboratory or in the field</em> — the results of measurements are analytical values considered representative for a given soil property.</p></li>
<li><p><em>Indirect soil measurements obtained using mechanical devices, analytical instruments and procedures</em> — measurement of soil properties that can be used to infer information about a different target soil property. These can be based on soil spectroscopy and similar close-range or remote sensing systems <span class="citation">(Shepherd and Walsh <a href="#ref-ShepherdWalsh2007JNIS">2007</a>; Viscarra Rossel, McBratney, and Minasny <a href="#ref-ViscarraRossel2010DSS">2010</a>)</span>.</p></li>
<li><p><em>Direct observations of soil properties and interpretations</em> — subjectively assessed values based on protocols for soil description as presented in manuals i.e. abundance of mottles, soil drainage class, soil colour.</p></li>
<li><p><em>Indirect or derived interpretations</em> — subjectively assessed values or conditions based mainly on an expert’s knowledge and interpretation of observations e.g. soil classification, soil fertility class.</p></li>
</ol>
<div class="figure" style="text-align: center">
<img src="figures/Fig_types_observations.png" alt="Types of soil observations in relation to data usage and production costs. Descriptive soil observations (e.g. manual texture or diagnostic soil horizons) are often not directly useable by end users, who are often more interested in specific secondary soil properties (e.g. water holding capacity, erosion index, soil fertility) as inputs to their modeling. However, descriptive field observations are often some orders of magnitude more affordable to obtain than laboratory analysis." width="65%" angle=0 />
<p class="caption">
(#fig:soil-vars)Types of soil observations in relation to data usage and production costs. Descriptive soil observations (e.g. manual texture or diagnostic soil horizons) are often not directly useable by end users, who are often more interested in specific secondary soil properties (e.g. water holding capacity, erosion index, soil fertility) as inputs to their modeling. However, descriptive field observations are often some orders of magnitude more affordable to obtain than laboratory analysis.
</p>
</div>

<div class="rmdnote">
Soil can be assessed quantitatively based on direct or indirect measurements using analytical techniques (in a laboratory or in the field) and qualitatively or descriptively based on observations in the field that adhere to some soil description guidelines. Examples of subjective observations are: diagnostic soil materials and horizons, soil classes, Munsell color classes, manual texture assessment (texture-by-hand), structure, compaction, root abundance and similar.
</div>

<p>Field campaigns are usuallly the most costly part of soil surveys. Large numbers of soil observations are made in the field to assess the spatial distribution of readily observable soil properties to provide empirical evidence for soil mapping. Because a soil analytical measurement in the laboratory is generally much more costly than a soil observation in the field, only a smaller subset of soil samples is taken from the larger number of field soil observations and brought to the laboratory for subsequent analysis. Ideally, every soil observation would be accompanied by corresponding soil analytical measurements to produce the most accurate and comprehensive soil information possible.</p>
<p>It is important to emphasize that soil properties, and the methods used to assess soil properties, are two distinctly different concepts. The two can be defined together (functional definition) or can be defined separately, as given by numerous national and international manuals and guidelines for analytical procedures and soil description: e.g. in <span class="citation">Natural Resources Conservation Service (<a href="#ref-Burt2004SSIR">2004</a>; Carter and Gregorich <a href="#ref-carter2007soil">2007</a>; Food and United Nations <a href="#ref-food2006guidelines">2006</a>)</span>, and/or <span class="citation">Van Reeuwijk (<a href="#ref-VanReeuwijk2002">2002</a>)</span>. Also in this chapter we make a distinction between the <em>‘target variable’</em> (i.e. target soil properties) and <em>‘paths’</em> (i.e. determination methods).</p>
<p>Soil analytical data obtained in a laboratory are typically an order of magnitude more expensive to produce than descriptive field observations <span class="citation">(Burrough, Beckett, and Jarvis <a href="#ref-Burrough1971">1971</a>; Gehl and Rice <a href="#ref-GehlRice2005">2007</a>; Kempen <a href="#ref-Kempen2011PhDthesis">2011</a>)</span>. To reduce these high costs, surveyors collect descriptive soil observations (Fig. @ref(fig:soil-vars)), which can subsequently be interpreted and linked to soil types and soil classes, which are then assumed to be characterised by a limited and definable range of soil properties <span class="citation">(Bouma, Batjes, and Groot <a href="#ref-bouma1998exploring">1998</a>)</span>. It is also possible to convert observed values for certain soil properties to values comparable to those measured by analytical methods (albeit with unknown precision) by using various calibration models or <em>conversion functions</em>. For example, <em>manual texturing</em> analysis <span class="citation">(FAO <a href="#ref-FAO1990">1990</a>; Soil survey Division staff <a href="#ref-SSDS1993">1993</a>)</span> can be used as a basis for estimating soil texture fractions with a precision of ±5 % at fraction of the cost of laboratory analysis.</p>
<p>Soils are usually sampled per depth interval or layer, generally using a genetic A-B-C-R system i.e. corresponding to a <em>soil horizon</em> — a relatively homogeneous layer of soil (with upper and lower depth) that is <em>“distinctly different from other layers and informative for the soil’s nature”</em> <span class="citation">(Harpstead, Sauer, and Bennett <a href="#ref-harpstead2001soil">2001</a>)</span>. Actual soil samples are either taken from the centre of a soil horizon or are mixed samples of the material from the whole horizon (Fig. @ref(fig:soi-var-depth)). Decades of soil survey have shown that soil horizons can be fuzzy objects. They may be difficult for different surveyors to distinguish and delineate consistently <span class="citation">(Burrough <a href="#ref-Burrough1989JSS">1989</a>; de Gruijter, Walvoort, and Gaans <a href="#ref-DeGruijter1997Geoderma">1997</a>)</span>. Soil correlation exercises try (not always successfully) to help different surveyors consistently recognize similar soil horizons and assign similar codes with comparable upper and lower boundaries so as to produce similar descriptions and classifications for any observed soil.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_soi_var_depth.png" alt="Soil observations can refer to genetic horizons (left), fixed depths i.e. point support (center) and/or can be aggregate values for the complete profile (right)." width="70%" />
<p class="caption">
(#fig:soi-var-depth)Soil observations can refer to genetic horizons (left), fixed depths i.e. point support (center) and/or can be aggregate values for the complete profile (right).
</p>
</div>
<p>An emerging approach to soil characterization is to scan the complete soil profile in different parts of the spectra, and then decide on vertical stratification <em>a posteriori</em> <span class="citation">(Viscarra Rossel, McBratney, and Minasny <a href="#ref-ViscarraRossel2010DSS">2010</a>)</span>. Nevertheless, much of the analytical data available in existing legacy soil profile databases is sampled per soil layer and described by soil horizon.</p>
<p>Soil observations are taken at a geographic position and at a specific depth (or depth interval), which is either 3D or refers to the whole solum. The 3D (longitude, latitude, depth) position implies that the property varies not only in geographic space, but also with depth. Soil properties that describe an entire site are by implication 2D, as are soil properties that summarise or refer to the soil profile as a whole (2D). For example, soil type does not change with depth. Also rock outcrops, depth to bedrock and depth to ground water table are single attributes that apply to an entire profile.</p>
</div>
<div id="soil-properties-of-interest-for-global-soil-mapping" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Soil properties of interest for global soil mapping</h3>
<p>There are many soil properties, possibly hundreds, used in the international domain of soil science including pedology, soil survey, soil fertility, soil hydrology, soil biology, etc. Not all of these can be mapped globally, nor are all of explicit interest for global applications or use.</p>
<p>Soil data have been, and are, collected and compiled into maps at various scales for various purposes and soil inventory projects typically begin by first carefully identifying the specific list of soil properties that are of interest for the anticipated uses of the planned survey. Different soil data are required for different purposes, such as applying different models with different data requirements.</p>
<p>In the past, soil surveys typically elected to focus on observing and measuring soil attributes and properties that were considered to be relatively stable, or static, in time. For example the particle size distribution of a soil, or its depth to bedrock, were considered to be relatively stable and not subject to large changes over relatively short time periods (e.g. decades). Even attributes that were known to change with management and time, such as topsoil thickness, organic carbon or pH, were treated as relatively stable properties for the purposes of mapping.</p>
<p>This choice to emphasize relatively stable soil properties and attributes was a logical consequence of the fact that it could take years to produce a single soil map and decades to complete mapping for an entire area of interest. Consequently, for maps to be relevant, and to remain relevant and useful for their anticipated lifetime of use, they had to restrict themselves to trying to describe the variation in space only (not time) of properties that could be considered stable and static.</p>
<p>The idea that soil properties could be assumed to remain relatively stable through time was partially based on an assumption that most soils had achieved a relatively stable condition that was in equilibruim with their current environment. If a soil is in equilibruim with its enviromment, it can be assumed that it will retain its present attributes, since there are no strong drivers for change. This may well apply to undisturbed soils in their natural environment, but it is not valid for disturbed or managed soils. It is well established that human management practices can, and do, significantly alter some key soil properties, such as pH, organic matter and topsoil thickness. Most conventional soil maps recognized, and reported on, differences in soil properties, such as pH or organic matter, between natural soils and managed soils. However, it was never a common practice to name, map and characterize managed soils seperately from natural soils.</p>
<p>Local or national soil survey projects are of direct relevance to global soil mapping initiatives if the range of data collected encompasses the minimum data set as specified for global initiatives. For example, completion of an update to the SOTER database for the World requires an extensive range of soil property data as specified in the procedures manual <span class="citation">(Van Engelen and Dijkshoorn <a href="#ref-VanEngelen2012">2012</a>)</span>. An update of the Harmonised World Soil Database <span class="citation">(FAO/IIASA/ISRIC/ISS-CAS/JRC <a href="#ref-FAO2012HWSD">2012</a>)</span> requires a smaller range of attributes. The <em>GlobalSoilMap</em> project <span class="citation">(Arrouays et al. <a href="#ref-Arrouays201493">2014</a>)</span> selected a list of only <em>twelve soil properties</em> considered relevant for global analyses and feasible to map globally. This list includes seven basic attributes, assessed through primary observation or measurement, and three derived attributes which are calculated from the primary soil properties (Tbl. @ref(tab:globalsoilmap)). These attributes are being mapped (as and where possible) at a fine resolution of six depth intervals in the vertical and, 3–arcseconds in the horizontal dimension (ca. 100 m) (Fig. @ref(fig:scheme-solum)).</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_scheme_solum.png" alt="Standard soil horizons, solum thickness and depth to bedrock (left), six standard depths used in the GlobalSoilMap project (right)." width="75%" />
<p class="caption">
(#fig:scheme-solum)Standard soil horizons, solum thickness and depth to bedrock (left), six standard depths used in the GlobalSoilMap project (right).
</p>
</div>
<table>
<caption>(#tab:globalsoilmap)The <em>GlobalSoilMap</em> project has selected seven primary (depth to bedrock, organic carbon content, pH, soil texture fractions, coarse fragments), three derived (effective soil depth, bulk density and available water capacity) and two optional (effective cation exchange capacity and electrical conductivity) target soil properties of interest for global soil mapping and modelling.</caption>
<thead>
<tr class="header">
<th align="left">Variable.name</th>
<th align="left">Units</th>
<th align="left">Reference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Total profile depth (depth to bedrock)</td>
<td align="left">cm</td>
<td align="left">(SSDS, <span class="citation">(<a href="#ref-SSDS1993">1993</a>)</span>, p.5)</td>
</tr>
<tr class="even">
<td align="left">Plant exploitable (effective depth)</td>
<td align="left">cm</td>
<td align="left">(SSDS, <span class="citation">(<a href="#ref-SSDS1993">1993</a>)</span>, p.60)</td>
</tr>
<tr class="odd">
<td align="left">Soil organic carbon (dry combustion)</td>
<td align="left">permille</td>
<td align="left">ISO 10694</td>
</tr>
<tr class="even">
<td align="left">pH index (the 1:5 H<span class="math inline">\(_2\)</span>O solution)</td>
<td align="left">–</td>
<td align="left">ISO 10390</td>
</tr>
<tr class="odd">
<td align="left">Sand content (gravimetric)</td>
<td align="left">%</td>
<td align="left">(NRCS, <span class="citation">(<a href="#ref-Burt2004SSIR">2004</a>)</span>, p.347)</td>
</tr>
<tr class="even">
<td align="left">Silt content (gravimetric)</td>
<td align="left">%</td>
<td align="left">(NRCS, <span class="citation">(<a href="#ref-Burt2004SSIR">2004</a>)</span>, p.347)</td>
</tr>
<tr class="odd">
<td align="left">Clay content (gravimetric)</td>
<td align="left">%</td>
<td align="left">(NRCS, <span class="citation">(<a href="#ref-Burt2004SSIR">2004</a>)</span>, p.347)</td>
</tr>
<tr class="even">
<td align="left">Coarse fragments (volumetric)</td>
<td align="left">%</td>
<td align="left">(NRCS, <span class="citation">(<a href="#ref-Burt2004SSIR">2004</a>)</span>, p.36)</td>
</tr>
<tr class="odd">
<td align="left">Effective Cation Exchange Capacity</td>
<td align="left">cmol</td>
<td align="left">ISO 11260</td>
</tr>
<tr class="even">
<td align="left">Bulk density of the whole soil</td>
<td align="left">kg/m<span class="math inline">\(^3\)</span></td>
<td align="left">ISO 11272</td>
</tr>
</tbody>
</table>
</div>
<div id="reference-methods" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Reference methods</h3>
<p>A pragmatic solution to ensuring efficient exchange, sharing and interpretation of global soil data is to establish reference methods for soil measurement and description. The <em>GlobalSoilMap</em> project agreed that their target soil properties would be assessed and reported relative to specific, designated <em>reference methods</em>. For example, soil organic carbon content of the fine earth fraction is to be assessed and reported according to ISO10694 dry combustion method <span class="citation">(Sleutel et al. <a href="#ref-Sleutel2007CSSPA">2007</a>)</span>. Values for pH are to be be reported for a 1:5 suspension of soil in water or using the CaCl<span class="math inline">\(_2\)</span> solution, with a precision of 1 decimal place. It has also been recommended that ISO TC 190 — soil quality standards — should be used to assess and report all data measured from air-dried soil samples.</p>
<p>Soil properties designated as optional for the <em>GlobalSoilMap</em> consortium include Effective Cation Exchange Capacity assessed and reported according to ISO11260 Barium Chloride (cmol+/kg = centi-mole+ per kilogram) and Electrical conductivity in 1:1 soil–water solution (dS/m = deci-siemens per metre). The list of soil properties identified for global soil mapping and modelling is likely to grow in the years to come. Initially, GSIF has elected to simply accept and adopt the list of soil properties specified for the <em>GlobalSoilMap</em> project and to extend this list through time in consultation with this and other global soil entities.</p>
<p>The International Organisation for Standardisation (ISO) provides international standard definitions of soil properties, and of associated methods to assess those soil properties, through <code>ISO TC-190</code> and <code>ISO TC-345</code>. Such unambiguously defined international standards are required for purposes as such as multi-partner global soil mapping.</p>
<p>In the following sections we focus our discussion on the soil properties that have been mapped for the <a href="www.soilgrids.org" class="uri">www.soilgrids.org</a> project: depth to bedrock, occurrence of the <code>R</code> horizon, organic carbon content of the fine earth fraction, pH of the fine earth fraction, particle size class contents (sand, silt, clay) of the fine earth fraction, gravel content of the whole soil, bulk density of the whole soil (and subsequently of the fine earth fraction) and Cation Exchange Capacity of the fine earth fraction. We define those attributes as completely and unambiguously as possible, including the associated reference method. For each soil property the following will be discussed:</p>
<ul>
<li><p><em>Brief introduction to the soil property (what is it, what does it reflect, why is it of interest, considerations; in general terms)</em>;</p></li>
<li><p><em>Definition of the soil feature related to the soil property and it’s spatial domain (2D, 3D)</em>;</p></li>
<li><p><em>Definition of the reference method to assess the soil property value</em>;</p></li>
<li><p><em>Definition of the convention used to express the soil property value (units, precision, range)</em>;</p></li>
<li><p><em>Review of the variation in soil property definitions and in methods to assess the attribute, including listings of several of the most widely used conversion functions cited from literature and with emphasis on harmonisation or conversion to the reference method</em>.</p></li>
</ul>
<p>We also identify, and review, a number of other widely used measurement methods, in addition to our selected standard methods. We describe if and how these other methods relate to the selected reference methods and discuss issues related to harmonization and standardization for attributes of current interest for global mapping.</p>
</div>
<div id="standard-soil-variables-of-interest-for-soil-mapping" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Standard soil variables of interest for soil mapping</h3>
<p>Some standard soil legends for listed soil properties are embedded within the GSIF package and can be loaded by:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(GSIF)
<span class="co">#&gt; GSIF version 0.5-4 (2017-04-25)</span>
<span class="co">#&gt; URL: http://gsif.r-forge.r-project.org/</span>
<span class="kw">data</span>(soil.legends)
<span class="kw">str</span>(soil.legends)
<span class="co">#&gt; List of 12</span>
<span class="co">#&gt;  $ ORCDRC  :&#39;data.frame&#39;:    40 obs. of  4 variables:</span>
<span class="co">#&gt;   ..$ MIN  : num [1:40] 0 0.2 0.4 0.6 0.8 1.1 1.5 1.9 2.4 3 ...</span>
<span class="co">#&gt;   ..$ MAX  : num [1:40] 0.2 0.4 0.6 0.8 1.1 1.5 1.9 2.4 3 3.6 ...</span>
<span class="co">#&gt;   ..$ CPROB: num [1:40] 0.0161 0.0301 0.0518 0.0717 0.113 0.159 0.203 0.264 0.328 0.373 ...</span>
<span class="co">#&gt;   ..$ COLOR: chr [1:40] &quot;#000180&quot; &quot;#000393&quot; &quot;#0006A6&quot; &quot;#000FB7&quot; ...</span>
<span class="co">#&gt;  $ PHIHOX  :&#39;data.frame&#39;:    40 obs. of  4 variables:</span>
<span class="co">#&gt;   ..$ MIN  : num [1:40] 20 42 45 46 48 49 50 51 52 53 ...</span>
<span class="co">#&gt;   ..$ MAX  : num [1:40] 42 45 46 48 49 50 51 52 53 54 ...</span>
<span class="co">#&gt;   ..$ CPROB: num [1:40] 0.0125 0.0375 0.0625 0.0875 0.1125 ...</span>
<span class="co">#&gt;   ..$ COLOR: chr [1:40] &quot;#FF0000&quot; &quot;#FF1C00&quot; &quot;#FF3900&quot; &quot;#FF5500&quot; ...</span>
<span class="co">#&gt;  $ PHIKCL  :&#39;data.frame&#39;:    40 obs. of  4 variables:</span>
<span class="co">#&gt;   ..$ MIN  : num [1:40] 20 33 35 36 37 38 38.5 39 40 40.5 ...</span>
<span class="co">#&gt;   ..$ MAX  : num [1:40] 33 35 36 37 38 38.5 39 40 40.5 41 ...</span>
<span class="co">#&gt;   ..$ CPROB: num [1:40] 0.0125 0.0375 0.0625 0.0875 0.1125 ...</span>
<span class="co">#&gt;   ..$ COLOR: chr [1:40] &quot;#FF0000&quot; &quot;#FF1C00&quot; &quot;#FF3900&quot; &quot;#FF5500&quot; ...</span>
<span class="co">#&gt;  $ BLDFIE  :&#39;data.frame&#39;:    40 obs. of  4 variables:</span>
<span class="co">#&gt;   ..$ MIN  : num [1:40] 200 850 1000 1100 1150 1200 1220 1260 1300 1310 ...</span>
<span class="co">#&gt;   ..$ MAX  : num [1:40] 850 1000 1100 1150 1200 1220 1260 1300 1310 1340 ...</span>
<span class="co">#&gt;   ..$ CPROB: num [1:40] 0.0125 0.0375 0.0625 0.0875 0.1125 ...</span>
<span class="co">#&gt;   ..$ COLOR: chr [1:40] &quot;#3D3FFF&quot; &quot;#3A42FF&quot; &quot;#3745FF&quot; &quot;#304CFF&quot; ...</span>
<span class="co">#&gt;  $ CECSOL  :&#39;data.frame&#39;:    40 obs. of  4 variables:</span>
<span class="co">#&gt;   ..$ MIN  : num [1:40] 0 5 5.2 5.3 5.5 5.8 6 6.3 6.7 7.1 ...</span>
<span class="co">#&gt;   ..$ MAX  : num [1:40] 5 5.2 5.3 5.5 5.8 6 6.3 6.7 7.1 7.5 ...</span>
<span class="co">#&gt;   ..$ CPROB: num [1:40] 0.23 0.241 0.247 0.259 0.277 0.292 0.308 0.328 0.351 0.37 ...</span>
<span class="co">#&gt;   ..$ COLOR: chr [1:40] &quot;#001998&quot; &quot;#0025A4&quot; &quot;#0031B1&quot; &quot;#003EBD&quot; ...</span>
<span class="co">#&gt;  $ SNDPPT  :&#39;data.frame&#39;:    40 obs. of  4 variables:</span>
<span class="co">#&gt;   ..$ MIN  : num [1:40] 0 1 3 4 6 8 10 12 14 16 ...</span>
<span class="co">#&gt;   ..$ MAX  : num [1:40] 1 3 4 6 8 10 12 14 16 19 ...</span>
<span class="co">#&gt;   ..$ CPROB: num [1:40] 0.0125 0.0375 0.0625 0.0875 0.1125 ...</span>
<span class="co">#&gt;   ..$ COLOR: chr [1:40] &quot;#FFFF00&quot; &quot;#F8F806&quot; &quot;#F1F10C&quot; &quot;#EBEB13&quot; ...</span>
<span class="co">#&gt;  $ SLTPPT  :&#39;data.frame&#39;:    40 obs. of  4 variables:</span>
<span class="co">#&gt;   ..$ MIN  : num [1:40] 0 2 3 4 5 6.7 8 9 10 12 ...</span>
<span class="co">#&gt;   ..$ MAX  : num [1:40] 2 3 4 5 6.7 8 9 10 12 13 ...</span>
<span class="co">#&gt;   ..$ CPROB: num [1:40] 0.0125 0.0375 0.0625 0.0875 0.1125 ...</span>
<span class="co">#&gt;   ..$ COLOR: chr [1:40] &quot;#FFFF00&quot; &quot;#F8F806&quot; &quot;#F1F10C&quot; &quot;#EBEB13&quot; ...</span>
<span class="co">#&gt;  $ CLYPPT  :&#39;data.frame&#39;:    40 obs. of  4 variables:</span>
<span class="co">#&gt;   ..$ MIN  : num [1:40] 0 2 3 4 5 6 7 8 9.3 10 ...</span>
<span class="co">#&gt;   ..$ MAX  : num [1:40] 2 3 4 5 6 7 8 9.3 10 12 ...</span>
<span class="co">#&gt;   ..$ CPROB: num [1:40] 0.0125 0.0375 0.0625 0.0875 0.1125 ...</span>
<span class="co">#&gt;   ..$ COLOR: chr [1:40] &quot;#FFFF00&quot; &quot;#F8F806&quot; &quot;#F1F10C&quot; &quot;#EBEB13&quot; ...</span>
<span class="co">#&gt;  $ CRFVOL  :&#39;data.frame&#39;:    40 obs. of  4 variables:</span>
<span class="co">#&gt;   ..$ MIN  : num [1:40] 0 0.1 0.3 0.4 0.6 0.8 1 1.2 1.5 1.8 ...</span>
<span class="co">#&gt;   ..$ MAX  : num [1:40] 0.1 0.3 0.4 0.6 0.8 1 1.2 1.5 1.8 2.2 ...</span>
<span class="co">#&gt;   ..$ CPROB: num [1:40] 0.408 0.41 0.411 0.416 0.418 0.504 0.506 0.513 0.514 0.558 ...</span>
<span class="co">#&gt;   ..$ COLOR: chr [1:40] &quot;#FFFF00&quot; &quot;#FDF800&quot; &quot;#FBF100&quot; &quot;#F9EB00&quot; ...</span>
<span class="co">#&gt;  $ TAXOUSDA:&#39;data.frame&#39;:    74 obs. of  4 variables:</span>
<span class="co">#&gt;   ..$ Number : int [1:74] 0 1 2 3 5 6 7 10 11 12 ...</span>
<span class="co">#&gt;   ..$ Group  : Factor w/ 75 levels &quot;&quot;,&quot;Albolls&quot;,&quot;Anthrepts&quot;,..: 39 50 47 38 35 54 41 28 26 34 ...</span>
<span class="co">#&gt;   ..$ Generic: Factor w/ 17 levels &quot;&quot;,&quot;Alfisols&quot;,..: 11 14 13 8 6 6 6 7 7 7 ...</span>
<span class="co">#&gt;   ..$ COLOR  : chr [1:74] &quot;#1414FF&quot; &quot;#D2D2D2&quot; &quot;#FFB9B9&quot; &quot;#F5F5F5&quot; ...</span>
<span class="co">#&gt;  $ TAXGWRB :&#39;data.frame&#39;:    32 obs. of  4 variables:</span>
<span class="co">#&gt;   ..$ Number: int [1:32] 1 2 3 4 5 6 7 8 9 10 ...</span>
<span class="co">#&gt;   ..$ Code  : Factor w/ 32 levels &quot;AB&quot;,&quot;AC&quot;,&quot;AL&quot;,..: 2 1 3 4 6 5 8 9 7 10 ...</span>
<span class="co">#&gt;   ..$ Group : Factor w/ 32 levels &quot;Acrisols&quot;,&quot;Albeluvisols&quot;,..: 1 2 3 4 5 6 7 8 9 10 ...</span>
<span class="co">#&gt;   ..$ COLOR : chr [1:32] &quot;#FDA463&quot; &quot;#FFEBBE&quot; &quot;#FFFFCC&quot; &quot;#FC6B5D&quot; ...</span>
<span class="co">#&gt;  $ TAXNWRB :&#39;data.frame&#39;:    118 obs. of  5 variables:</span>
<span class="co">#&gt;   ..$ Number        : int [1:118] 1 2 3 4 5 6 7 8 9 10 ...</span>
<span class="co">#&gt;   ..$ Group         : Factor w/ 118 levels &quot;Acric Ferralsols&quot;,..: 28 29 30 31 104 116 32 84 111 18 ...</span>
<span class="co">#&gt;   ..$ Shortened_name: Factor w/ 118 levels &quot;Acric.Ferralsols&quot;,..: 28 29 30 31 104 116 32 84 111 18 ...</span>
<span class="co">#&gt;   ..$ Generic       : Factor w/ 30 levels &quot;Acrisols&quot;,&quot;Albeluvisols&quot;,..: 1 1 1 1 1 1 2 2 2 3 ...</span>
<span class="co">#&gt;   ..$ COLOR         : chr [1:118] &quot;#FE813E&quot; &quot;#FD9F39&quot; &quot;#FDAE6B&quot; &quot;#FD8D3C&quot; ...</span></code></pre></div>
<p>which illustrates the referent cumulative probabilities (<code>CPROB</code>) and appropriate color legend (<code>COLOR</code>; coded as a six-digit, three-byte hexadecimal number) for the values of the target soil variables. The cumulative probabilities were derived using the collection of records in the World Soil Profiles repository and can be considered as an estimate of global prior probabilities for soil pH (see further for example Fig. ).</p>
<p>A general intention is to maintain a <em>Global Soil Data Registry</em> so that a short variable name (in further text <em>“GSIF code”</em>) can be linked to a unique set of metadata which should include:</p>
<ul>
<li><p>Full description;</p></li>
<li><p>Variable type (numeric, quantity, binary, factor etc)</p></li>
<li><p>Measurement unit;</p></li>
<li><p>Biblio reference (URL or DOI);</p></li>
<li><p>ISO code (if available);</p></li>
<li><p>Physical limits (lower / upper);</p></li>
<li><p>Detection limit i.e. numeric resolution;</p></li>
<li><p>Priority level (required, suggested or optional);</p></li>
</ul>
<p>Note that MySQL has some restrictions considering column names: special characters, those outside the set of alphanumeric characters from the current character set, can not be used in the column names. Proposed abbreviations for standard method names are <span class="math inline">\(\mathtt{VOL}\)</span> — volume fraction, <span class="math inline">\(\mathtt{ABU}\)</span> — abundance or relative area cover, <span class="math inline">\(\mathtt{PCT}\)</span> — mass percentage, <span class="math inline">\(\mathtt{ICM}\)</span> — thickness in cm, <span class="math inline">\(\mathtt{MHT}\)</span> — texture by-hand or manual hand texture and <span class="math inline">\(\mathtt{MNS}\)</span> — Munsell color codes, horizon sequence is coded with the capital ASCII letters — <span class="math inline">\(\mathtt{A}\)</span>, <span class="math inline">\(\mathtt{B}\)</span>, <span class="math inline">\(\mathtt{C}\)</span>,<span class="math inline">\(\ldots\)</span> <span class="math inline">\(\mathtt{Z}\)</span>. Another option is to simply use the US Goverment National Cooperative Soil Characterization Database column names (<a href="http://ncsslabdatamart.sc.egov.usda.gov/" class="uri">http://ncsslabdatamart.sc.egov.usda.gov/</a>).</p>
<p>Also note that the metadata can be easily separated from the code so that the short GSIF code (variable name) could be used as a shorthand (replacement) for the long description of the complete metadata. Using short GSIF codes is also important for programming because unique code names are used consistently in all scripts / functions.</p>
</div>
</div>
<div id="descriptive-soil-profile-observations" class="section level2">
<h2><span class="header-section-number">3.2</span> Descriptive soil profile observations</h2>
<div id="depth-to-bedrock" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Depth to bedrock</h3>
<p>Soil depth (specifically depth to bedrock) is predicted because it is an important consideration for a wide variety of engineering, hydrological and agronomic interpretations. Shallow and lithic soils are of particular interest as they impose restrictions for foundations and structures in engineering, limit infiltration and storage of moisture and produce more rapid runoff and erosion and limit growth of many crops by restricting rooting depth and limiting available moisture storage. Most soil legacy profile data do not provide any information about the soil below depths of 1 m <span class="citation">(Richter and Markewitz <a href="#ref-Richter1995">1995</a>)</span>. This characteristic of legacy soil data limits its usefulness for predicting soil depths greater than 2 m.</p>
<p>Soil depth is measured from the soil surface downwards and expressed in positive values increasing with depth. Google Earth and the KML data standard (via the <code>altitudeMode</code> tag) allow one to specify if the vertical dimension refers to actual altitude (vertical distance from the land surface) or to distance from the sea level (<code>absolute</code>). In this case soil depths can be represented using <code>clampToGround</code> and negative values. For example, depth of 30 cm can be expressed as <span class="citation">(Wilson <a href="#ref-OGCKML2008">2008</a>)</span>:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="op">&lt;</span><span class="ex">Placemark</span><span class="op">&gt;</span> <span class="op">&lt;</span>Point<span class="op">&gt;</span>
<span class="op">&lt;</span><span class="ex">altitudeMode</span><span class="op">&gt;</span>clampToGround<span class="op">&lt;</span>/altitudeMode<span class="op">&gt;</span>
<span class="op">&lt;</span><span class="ex">coordinates</span><span class="op">&gt;</span>17.2057,45.8851,-0.<span class="op">3&lt;</span>/coordinates<span class="op">&gt;</span>
<span class="op">&lt;</span>/<span class="ex">Point</span><span class="op">&gt;</span> <span class="op">&lt;</span>/Placemark<span class="op">&gt;</span></code></pre></div>
<p>Soil surface (depth = 0 cm) is the top of the mineral soil; or, for soils with a litter layer (<code>O</code> horizon), the soil surface is the top of the part of the <code>O</code> horizon that is at least slightly decomposed <span class="citation">(FAO <a href="#ref-FAO2006">2006</a>)</span>. Fresh leaf or needle fall that has not undergone observable decomposition is not considered to be part of the soil and may be described separately. For organic soils, the top of any surface horizon identified as an <code>O</code> horizon is considered the soil surface.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_depth_2_bedrock.png" alt="Depth to bedrock for censured and uncensured observations. Image source: Shangguan et al. (2017) doi: 10.1002/2016MS000686." width="100%" />
<p class="caption">
(#fig:scheme-depth-to-bedrock)Depth to bedrock for censured and uncensured observations. Image source: Shangguan et al. (2017) doi: 10.1002/2016MS000686.
</p>
</div>
<p>The <em>depth to bedrock</em> i.e. depth to the <code>R</code> horizon is measured from the soil surface downwards and is expressed in cm with a precision of ±1 cm. Depth to bedrock deeper than e.g. 2–3 m is most often not recorded. Bedrock is consolidated hard rock, with only a few cracks, underlying the soil. It is not necessarily parent material. We imagine it often as something distinct and easy to recognize in the field. In practice, depth to bedrock can be difficult to determine, and is often confused with stoniness or depth to parent material (which can be unconsolidated material). Another issue is that, for most of the soils in the world, hard bedrock is &gt;2 m deep so that we actually don’t know the correct depth to enter, other than &gt;2 m. Rootability is physically restricted by the bedrock, whether hard or soft (see Fig. @ref(fig:scheme-depth-to-bedrock)).</p>

<div class="rmdnote">
Depth to bedrock is the mean distance to <code>R</code> horizon which is the layer impenetrable by roots or agricultural machinery. Depth to bedrock deeper than 2 m is most often not recorded in field survey descriptions.
</div>

<p>In traditional soil characterisation, the total depth of the <code>O</code>, <code>A</code>,<code>E</code>, and <code>B</code> horizons is referred to as the <em>solum</em> <span class="citation">(Harpstead, Sauer, and Bennett <a href="#ref-harpstead2001soil">2001</a>)</span>, while the underlaying layer is referred to as parent material or substratum <span class="citation">(Soil survey Division staff <a href="#ref-SSDS1993">1993</a>)</span>. Parent material can be coarse or fine unconsolidated deposits of e.g. alluvial, colluvial or windblown origin (<code>C</code> horizon) or consolidated residual hard bedrock (<code>R</code> horizon).</p>
</div>
<div id="effective-soil-depth-and-rooting-depth" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Effective soil depth and rooting depth</h3>
<p><em>Effective soil depth</em> is of interest for soil mapping because it is a key indicator of the capability of the soil to store moisture, support crop growth and sustain beneficial land uses. It is often an essential indicator of soil health. The effective soil depth is the depth to which micro-organisms are active in the soil, where roots can develop and where soil moisture can be stored <span class="citation">(FAO <a href="#ref-FAO2006">2006</a>)</span>.</p>
<table>
<caption>(#tab:rootingdepths)Summary of maximum rooting depth by biome (after Canadell et al. (1996)). MMRD = Mean maximum rooting depth in m; HVRD = Highest value for rooting depth in m.</caption>
<thead>
<tr class="header">
<th align="left">Biome</th>
<th align="right">N</th>
<th align="left">MMRD</th>
<th align="right">HVRD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Boreal Forest</td>
<td align="right">6</td>
<td align="left">2.0 ± 0.3</td>
<td align="right">3.3</td>
</tr>
<tr class="even">
<td align="left">Cropland</td>
<td align="right">17</td>
<td align="left">2.1 ± 0.2</td>
<td align="right">3.7</td>
</tr>
<tr class="odd">
<td align="left">Desert</td>
<td align="right">22</td>
<td align="left">9.5 ± 2.4</td>
<td align="right">53.0</td>
</tr>
<tr class="even">
<td align="left">Sclerophyllous shrubland and forest</td>
<td align="right">57</td>
<td align="left">5.2 ± 0.8</td>
<td align="right">40.0</td>
</tr>
<tr class="odd">
<td align="left">Temperate coniferous forest</td>
<td align="right">17</td>
<td align="left">3.9 ± 0.4</td>
<td align="right">7.5</td>
</tr>
<tr class="even">
<td align="left">Temperate deciduous forest</td>
<td align="right">19</td>
<td align="left">2.9 ± 0.2</td>
<td align="right">4.4</td>
</tr>
<tr class="odd">
<td align="left">Temperate grassland</td>
<td align="right">82</td>
<td align="left">2.6 ± 0.2</td>
<td align="right">6.3</td>
</tr>
<tr class="even">
<td align="left">Tropical deciduous forest</td>
<td align="right">5</td>
<td align="left">3.7 ± 0.5</td>
<td align="right">4.7</td>
</tr>
<tr class="odd">
<td align="left">Tropical evergreen forest</td>
<td align="right">5</td>
<td align="left">7.3 ± 2.8</td>
<td align="right">18.0</td>
</tr>
<tr class="even">
<td align="left">Tropical savanna</td>
<td align="right">15</td>
<td align="left">15.0 ± 5.4</td>
<td align="right">68.0</td>
</tr>
</tbody>
</table>
<p>There are many thoughts on how to define effective soil depth. Effective soil depth is closely related to, but not necessarily equivalent to, the <em>rooting depth</em>. Rooting depth is measured and reported relative to a specific prevailing land cover and land use category, while effective soil depth is supposedly the maximum possible depth of soil that can be used by any growing plant (see Tbl. @ref(tab:rootingdepths)).</p>
<p>In some cases soil ends with an abrupt change of material which is either solid, compacted or distinctly impenetrable for plants and organisms living in soil. The root restricting i.e. plant accessible depth, is the depth at which root penetration is strongly inhibited because of physical (including soil temperature), chemical or hydrological characteristics <span class="citation">(Soil survey Division staff <a href="#ref-SSDS1993">1993</a>, Handbook 18:60)</span>. Restriction means the inability to support more than a very few fine or few very fine roots if depth from the soil surface and water state, other than the occurrence of frozen water, are not limiting. For some crops like cotton plants or soybeans, and possibly other crops with less abundant roots than the grasses, the very few class is used instead of the few class. The restriction may be below where plant roots normally occur because of limitations in water state, temperatures, or depth from the surface. This evaluation can be based on the specific plants that are important to the use of the soil, as indicated in Tbl. @ref(tab:rootingdepths); see also <span class="citation">Soil survey Division staff (<a href="#ref-SSDS1993">1993</a>, Handbook 18:60)</span>.</p>
<p>Root restriction can be also influenced by certain pedogenic horizons, such as <em>fragipans</em>. A change in particle size distribution alone, as for example loamy sand over gravel, is not always a basis for physical root restriction. A common indication of physical root restriction is a combination of structure and consistence which together suggest that the resistance of the soil fabric to root entry is high and that vertical cracks and planes of weakness for root entry are absent or widely spaced. Root restriction is inferred for a continuously cemented zone of any thickness; or a zone &gt;10 cm thick that when very moist or wet is massive, platy, or has weak structure of any type for a vertical repeat distance of &gt;10 cm and while very moist or wet is very firm (firm, if sandy), extremely firm, or has a large penetration resistance. Chemical restrictions, such as high extractable aluminum, manganese and/or low extractable calcium, can also be considered but are plant-specific. Root-depth observations preferably should be used to make the generalization. If these are not available then inferences may be made from morphology.</p>
<p>As a general recommendation, it is advisable to focus first on mapping soil properties that limit rooting, including content of coarse fragments and the depth to bedrock, and then define effective soil depth <em>a posteriori</em> using distinct analytical rules. A similar approach has also been promoted by <span class="citation">Rijsberman and Wolman (<a href="#ref-rijsberman1985effect">1985</a>)</span> and <span class="citation">Driessen and Konijn (<a href="#ref-driessen1992land">1992</a>)</span> who refer to it as the <em>Soil-productivity Index</em> — a product of soil-water sufficiency, soil pH sufficiency and soil bulk density sufficiency. Here we consider somewhat wider range of soil properties that can affect rooting depth, such as:</p>
<ul>
<li><p>coarse fragments,</p></li>
<li><p>compaction / porosity (possibly derived from structure and consistence),</p></li>
<li><p>drainage i.e. soil oxygen availability,</p></li>
<li><p>toxicity e.g. Al content,</p></li>
<li><p>acidity, salinity and similar.</p></li>
</ul>
<p>In-field expert interpretation explicitly summarising observations into a single expression for rooting depth is likely the most effective and reliable source of information. The genetically determined maximum rooting depth of vegetation isn’t always a reliable indicator of actual observed effective rooting depth of a given soil at a given site (Fig. @ref(fig:lri-scheme)). Possibly a more robust way to determine the effective rooting depth is to map all limiting soil properties with high accuracy, and then derive rooting index per layer.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_LRI_scheme.png" alt="Derivation of the Limiting Rooting Index: (left) soil pH values and corresponding LRI, (right) coarse fragments and corresponding LRI. Based on Leenaars et al. (2018) doi: 10.1016/j.geoderma.2018.02.046." width="90%" />
<p class="caption">
(#fig:lri-scheme)Derivation of the Limiting Rooting Index: (left) soil pH values and corresponding LRI, (right) coarse fragments and corresponding LRI. Based on Leenaars et al. (2018) doi: 10.1016/j.geoderma.2018.02.046.
</p>
</div>
<p>By using the GSIF package, one can determine Limiting Rooting Index, which can be a good indicator of the effective rooting depth. Consider the following soil profile from Nigeria <span class="citation">(Leenaars <a href="#ref-Leenaars2012">2014</a>)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## sample profile from Nigeria (ISRIC:NG0017):
UHDICM =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">18</span>, <span class="dv">36</span>, <span class="dv">65</span>, <span class="dv">87</span>, <span class="dv">127</span>)
LHDICM =<span class="st"> </span><span class="kw">c</span>(<span class="dv">18</span>, <span class="dv">36</span>, <span class="dv">65</span>, <span class="dv">87</span>, <span class="dv">127</span>, <span class="dv">181</span>)
SNDPPT =<span class="st"> </span><span class="kw">c</span>(<span class="dv">66</span>, <span class="dv">70</span>, <span class="dv">54</span>, <span class="dv">43</span>, <span class="dv">35</span>, <span class="dv">47</span>)
SLTPPT =<span class="st"> </span><span class="kw">c</span>(<span class="dv">13</span>, <span class="dv">11</span>, <span class="dv">14</span>, <span class="dv">14</span>, <span class="dv">18</span>, <span class="dv">23</span>)
CLYPPT =<span class="st"> </span><span class="kw">c</span>(<span class="dv">21</span>, <span class="dv">19</span>, <span class="dv">32</span>, <span class="dv">43</span>, <span class="dv">47</span>, <span class="dv">30</span>)
CRFVOL =<span class="st"> </span><span class="kw">c</span>(<span class="dv">17</span>, <span class="dv">72</span>, <span class="dv">73</span>, <span class="dv">54</span>, <span class="dv">19</span>, <span class="dv">17</span>)
BLD =<span class="st"> </span><span class="kw">c</span>(<span class="fl">1.57</span>, <span class="fl">1.60</span>, <span class="fl">1.52</span>, <span class="fl">1.50</span>, <span class="fl">1.40</span>, <span class="fl">1.42</span>)<span class="op">*</span><span class="dv">1000</span>
PHIHOX =<span class="st"> </span><span class="kw">c</span>(<span class="fl">6.5</span>, <span class="fl">6.9</span>, <span class="fl">6.5</span>, <span class="fl">6.2</span>, <span class="fl">6.2</span>, <span class="fl">6.0</span>)
CEC =<span class="st"> </span><span class="kw">c</span>(<span class="fl">9.3</span>, <span class="fl">4.5</span>, <span class="fl">6.0</span>, <span class="fl">8.0</span>, <span class="fl">9.4</span>, <span class="fl">10.9</span>)
ENA =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>)
EACKCL =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="ot">NA</span>, <span class="ot">NA</span>, <span class="fl">0.5</span>)
EXB =<span class="st"> </span><span class="kw">c</span>(<span class="fl">8.9</span>, <span class="fl">4.0</span>, <span class="fl">5.7</span>, <span class="fl">7.4</span>, <span class="fl">8.9</span>, <span class="fl">10.4</span>)
ORCDRC =<span class="st"> </span><span class="kw">c</span>(<span class="fl">18.4</span>, <span class="fl">4.4</span>, <span class="fl">3.6</span>, <span class="fl">3.6</span>, <span class="fl">3.2</span>, <span class="fl">1.2</span>)
x &lt;-<span class="st"> </span><span class="kw">LRI</span>(<span class="dt">UHDICM=</span>UHDICM, <span class="dt">LHDICM=</span>LHDICM, <span class="dt">SNDPPT=</span>SNDPPT, 
   <span class="dt">SLTPPT=</span>SLTPPT, <span class="dt">CLYPPT=</span>CLYPPT, <span class="dt">CRFVOL=</span>CRFVOL, 
   <span class="dt">BLD=</span>BLD, <span class="dt">ORCDRC=</span>ORCDRC, <span class="dt">CEC=</span>CEC, <span class="dt">ENA=</span>ENA, <span class="dt">EACKCL=</span>EACKCL, 
   <span class="dt">EXB=</span>EXB, <span class="dt">PHIHOX=</span>PHIHOX, <span class="dt">print.thresholds=</span><span class="ot">TRUE</span>)
x
<span class="co">#&gt; [1] TRUE TRUE TRUE TRUE TRUE TRUE</span>
<span class="co">#&gt; attr(,&quot;minimum.LRI&quot;)</span>
<span class="co">#&gt; [1] 35.0 29.5 47.0 54.5 73.0 61.5</span>
<span class="co">#&gt; attr(,&quot;most.limiting.factor&quot;)</span>
<span class="co">#&gt; [1] &quot;tetaS&quot; &quot;tetaS&quot; &quot;tetaS&quot; &quot;tetaS&quot; &quot;tetaS&quot; &quot;tetaS&quot;</span>
<span class="co">#&gt; attr(,&quot;thresholds&quot;)</span>
<span class="co">#&gt; attr(,&quot;thresholds&quot;)$ERscore1</span>
<span class="co">#&gt;  [1] 100.0  80.0  50.0   0.0  95.0  40.0  40.0   5.5   7.8   1.5  10.0</span>
<span class="co">#&gt; [12]   1.0  35.0   2.5 150.0 150.0</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; attr(,&quot;thresholds&quot;)$ERscore2</span>
<span class="co">#&gt;  [1]   0.00  90.00  30.00   0.35 100.00  60.00  60.00   3.62   9.05   6.75</span>
<span class="co">#&gt; [11]  25.00   5.00  85.00   6.50 750.00 750.00</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; attr(,&quot;thresholds&quot;)$Trend</span>
<span class="co">#&gt;  [1]  0 -1  1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; attr(,&quot;thresholds&quot;)$Score</span>
<span class="co">#&gt;  [1] 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; attr(,&quot;thresholds.names&quot;)</span>
<span class="co">#&gt; attr(,&quot;thresholds.names&quot;)$variable</span>
<span class="co">#&gt;  [1] &quot;range&quot;    &quot;CRFVOL&quot;   &quot;tetaS&quot;    &quot;BLD.f&quot;    &quot;SNDPPT&quot;   &quot;CLY.d&quot;   </span>
<span class="co">#&gt;  [7] &quot;SND.d&quot;    &quot;PHIHOX.L&quot; &quot;PHIHOX.H&quot; &quot;ECN&quot;      &quot;ENA.f&quot;    &quot;ENA&quot;     </span>
<span class="co">#&gt; [13] &quot;EACKCL.f&quot; &quot;EACKCL&quot;   &quot;CRB&quot;      &quot;GYP&quot;</span>
## Most limiting: BLD.f and CRFVOL, but nothing &lt; 20</code></pre></div>
<p>where <code>UHDICM</code> and <code>LHDICM</code> are the upper and lower horizon depth in cm, <code>SNDPPT</code>, <code>SLTPPT</code> and <code>CLYPPT</code> are the sand, silt and clay content in percent, <code>CRFVOL</code> is the volume percentage of coarse fragments (&gt;2 mm), <code>BLD</code> is the bulk density in t/m<span class="math inline">\(^3\)</span>, <code>ORCDRC</code> is the soil organic carbon concentration in permille or g/kg, <code>ECN</code> is the electrical conductivity in dS/m, <code>CEC</code> is the Cation Exchange Capacity in cmol/kg (centi-mol per kilogram), <code>ENA</code> is the exchangable Na in cmol/kg, <code>EACKCL</code> is the exchangeable acidity in cmol/kg, <code>EXB</code> is the exchangeable bases in cmol/kg, <code>PHIHOX</code> is the soil pH in water suspension, <code>CRB</code> is the CaCO<span class="math inline">\(_3\)</span> (carbonates) in g/kg, <code>GYP</code> is the CaSO<span class="math inline">\(_4\)</span> (gypsum) in and <code>tetaS</code> is the volumetric percentage of water.</p>
<p>For this specific profile, the most limiting soil property is <code>tetaS</code>, but because none of the soil properties got &lt;20 points, we can conclude that the maximum rooting depth is &gt;180 cm. Note that the threshold values in the <code>LRI</code> function used to derive Limiting Rootability scores are set based on common soil agricultural productivity tresholds (e.g. for maize; see also Fig. @ref(fig:lri-scheme)), and can be adjusted via the <code>thresholds</code> argument. The computation is done per list of soil layers (minimum three) to account for textural changes i.e. sudden changes in sand and clay content and for the limiting layers such as layer saturated with water. To determine futher the effective rooting depth we can run:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sel &lt;-<span class="st"> </span>x<span class="op">==</span><span class="ot">FALSE</span>
<span class="cf">if</span>(<span class="op">!</span><span class="kw">all</span>(sel<span class="op">==</span><span class="ot">FALSE</span>)){ 
  UHDICM[<span class="kw">which</span>(sel<span class="op">==</span><span class="ot">TRUE</span>)[<span class="dv">1</span>]] 
} <span class="cf">else</span> {
  <span class="kw">max</span>(LHDICM)
}
<span class="co">#&gt; [1] 181</span>

xI &lt;-<span class="st"> </span><span class="kw">attr</span>(x, <span class="st">&quot;minimum.LRI&quot;</span>)
## derive Effective rooting depth:
<span class="kw">ERDICM</span>(<span class="dt">UHDICM=</span>UHDICM, <span class="dt">LHDICM=</span>LHDICM, <span class="dt">minimum.LRI=</span>xI, <span class="dt">DRAINFAO=</span><span class="st">&quot;M&quot;</span>)
<span class="co">#&gt; [1] 100</span></code></pre></div>
</div>
</div>
<div id="chemical-soil-properties" class="section level2">
<h2><span class="header-section-number">3.3</span> Chemical soil properties</h2>
<div id="soil-organic-carbon" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Soil organic carbon</h3>
<p>Organic carbon is a soil property of great current global interest <span class="citation">(Smith, Falloon, and Kutsch <a href="#ref-Smith2004SUM">2004</a>; Pete Smith and Kutsch <a href="#ref-Smith2010CUP">2010</a>; Panagos et al. <a href="#ref-Panagos2013439">2013</a>)</span>. It is commonly recognized and used as a key indicator of soil health. The amount of carbon present in the soil, and particularly in topsoil horizons, is grossly indicative of potential productivity for crops. Amounts of organic carbon throughout the profile influence soil structure, permeability, porosity, bulk density, water holding capacity, nutrient retention and availability and, consequently, overall soil health. The ability of soils to sequester significant quantities of atmospheric carbon is of considerable interest as a potential mechanism for mitigating the adverse effects of increases in green house gasses in the atmosphere <span class="citation">(Smith, Falloon, and Kutsch <a href="#ref-Smith2004SUM">2004</a>; Conant et al. <a href="#ref-Conant2010">2010</a>; Scharlemann et al. <a href="#ref-Scharlemann2014CM">2014</a>)</span>. Consequently, soil organic carbon is probably the soil property of greatest current interest and utility from the point of view of global mapping, and interpretation, of soil properties.</p>

<div class="rmdnote">
Soil Organic Carbon is one the key measures of soil health. The standard reference method for assessing and reporting soil organic carbon content of the fine earth fraction is by dry combustion to at least (ISO 10694). Values of organic carbon content are typically reported in (permilles) with integer precision over a range of 0–1000.
</div>

<p>The <em>dry combustion method</em> (Leco at 1000°C) is based on thermal oxidation of both mineral carbon (IC) and organic carbon by means of a furnace. It is a reliable method for the determination of the soil organic carbon when IC is removed through combustion at low temperature prior to combustion at high temperature. Dry combustion is considered to ensure oxidation of all ORC and is considered an accurate method which has been used in many studies as a reference method against which to calibrate other methods <span class="citation">(Grewal, Buchan, and Sherlock <a href="#ref-Grewal1991JSS">1991</a>; Meersmans, Van Wesemael, and Van Molle <a href="#ref-Meersmans2009SUM">2009</a>; Bisutti, Hilke, and Raessler <a href="#ref-Bisutti2004TAC">2004</a>)</span>. A global estimate of the distribution of soil organic carbon is shown in Fig. @ref(fig:sprofs-soil-carbon).</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_sprofs_ORCDRC.png" alt="Histogram and soil-depth density distribution for a global compilation of measurements of soil organic carbon content in permilles. Based on the records from WOSIS (http://www.earth-syst-sci-data.net/9/1/2017/). The log-transformation is used to ensure close-to-normal distribution in the histogram." width="100%" />
<p class="caption">
(#fig:sprofs-soil-carbon)Histogram and soil-depth density distribution for a global compilation of measurements of soil organic carbon content in permilles. Based on the records from WOSIS (<a href="http://www.earth-syst-sci-data.net/9/1/2017/" class="uri">http://www.earth-syst-sci-data.net/9/1/2017/</a>). The log-transformation is used to ensure close-to-normal distribution in the histogram.
</p>
</div>

<div class="rmdnote">
Soil organic carbon content is most commonly expressed in weight percentage and for GSIF in grams per kilogram fine earth fraction or permilles. The standard method of determining the soil organic carbon content is by dry combustion method (Leco at 1000°C).
</div>

<p>Total organic carbon can be determined directly or indirectly. Direct determination consists of previous removal of any carbonates present by treating the soil with hydrochloric acid. Indirect determination consists of applying an empirical correction to the total carbon content to account for for the inorganic carbonates present.</p>
<p>Examples of studies that have used dry combustion for calibrating other methods of analyzing organic carbon include <span class="citation">Kalembasa and Jenkinson (<a href="#ref-Kalembasa1973JSFA">1973</a>; Grewal, Buchan, and Sherlock <a href="#ref-Grewal1991JSS">1991</a>; Soon and Abboud <a href="#ref-Soon1991CSSPA">1991</a>; Wang, Smethurst, and Herbed <a href="#ref-Wang1996AJSR">1996</a>; Konen et al. <a href="#ref-Konen2002SSSAJ">2002</a>; Brye and Slaton <a href="#ref-Brye2003CSSPA">2003</a>; Mikhailova, Noble, and Post <a href="#ref-Mikhailova2003CSSPA">2003</a>; Bisutti, Hilke, and Raessler <a href="#ref-Bisutti2004TAC">2004</a>; Jankauskas et al. <a href="#ref-Jankauskas2006CSSPA">2006</a>; De Vos et al. <a href="#ref-DeVos2007SUM">2007</a>)</span> and <span class="citation">Meersmans, Van Wesemael, and Van Molle (<a href="#ref-Meersmans2009SUM">2009</a>)</span>. It is possible to produce regression equations to permit conversion of results for organic carbon produced by one method into equivalent values in a specified reference method (generally dry combustion). However, local calibration equations that reflect differences in soils on a regional basis are usually needed. It is not possible to provide a single universal equation suitable for use everywhere to convert organic carbon values produced using other methods of analysis to equivalent values in the reference method of dry combustion.</p>
</div>
<div id="soil-ph" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Soil pH</h3>
<p>PH is of interest for global soil mapping because it is one of the more widely available and easily interpreted chemical measures of the health and productivity of the soil. pH provides an indication of base status of the soil which influences nutrient availability, mobility of both beneficial and detrimental ions and the ecology of micro-organisms within the soil. For most crops and agricultural uses, a pH in the range of 5.5 to 7.5 is optimum (considering the agricultural productivity of soil). Low pH is associated with acidic conditions and with increased mobility of toxic ions such as aluminum iron and even acid sulphates. High pH is associated with reduced availability of phosphorus and at higher levels with alkaline conditions that impede water uptake by plants. A global estimate of the distribution of the soil pH is shown in Figs. @ref(fig:sprops-phiho5) and @ref(fig:sprops-phikcl).</p>
<p>PH index approximates concentration of dissolved hydrogen ions (H<span class="math inline">\(_3\)</span>O<span class="math inline">\(^+\)</span>) in a soil suspension. It is estimated as the negative decimal logarithm of the hydrogen ion activity in a soil suspension. As a single measurement, pH describes more than relative acidity or alkalinity. It also provides information on nutrient availability, metal dissolution chemistry, and the activity of microorganisms <span class="citation">(Miller and Kissel <a href="#ref-Miller2010SSSAJ">2010</a>)</span>.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_sprofs_PHIHO5.png" alt="Histogram and soil-depth density distribution for a global compilation of measurements of soil pH (suspension of soil in H2O). Based on the records from WOSIS (http://www.earth-syst-sci-data.net/9/1/2017/)." width="100%" />
<p class="caption">
(#fig:sprops-phiho5)Histogram and soil-depth density distribution for a global compilation of measurements of soil pH (suspension of soil in H2O). Based on the records from WOSIS (<a href="http://www.earth-syst-sci-data.net/9/1/2017/" class="uri">http://www.earth-syst-sci-data.net/9/1/2017/</a>).
</p>
</div>
<div class="figure" style="text-align: center">
<img src="figures/Fig_sprofs_PHIKCL.png" alt="Histogram and soil-depth density distribution for a global compilation of measurements of soil pH (suspension of soil in KCl). Based on the records from WOSIS (http://www.earth-syst-sci-data.net/9/1/2017/)." width="100%" />
<p class="caption">
(#fig:sprops-phikcl)Histogram and soil-depth density distribution for a global compilation of measurements of soil pH (suspension of soil in KCl). Based on the records from WOSIS (<a href="http://www.earth-syst-sci-data.net/9/1/2017/" class="uri">http://www.earth-syst-sci-data.net/9/1/2017/</a>).
</p>
</div>
<p>The standard reference method for reporting pH is ISO 10390:2005. This standard specifies an instrumental method for the routine determination of pH using a glass electrode in a 1:5 (volume fraction) suspension of soil in water (pH in H<span class="math inline">\(_2\)</span>O), in potassium chloride solution (pH in KCl) or in calcium chloride solution (pH in CaCl<span class="math inline">\(_2\)</span>).</p>
<p>The most common method for analyzing pH in North America is a 1:1 soil/water suspension <span class="citation">(Miller and Kissel <a href="#ref-Miller2010SSSAJ">2010</a>)</span>. Adopting ISO 10390:2005 as a standard with its specification of pH measured in a 1:5 suspension of soil in water requires US values to be converted from 1:1 soil/water to 1:5 soil/water equivalent values.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_color_legend_PHI.png" alt="Histogram for soil pH and connected color legend available via the GSIF package. Color breaks in the legend have been selected using histogram equalization (i.e. by using constant quantiles) to ensure maximum contrast in the output maps." width="85%" />
<p class="caption">
(#fig:color-legend-phi)Histogram for soil pH and connected color legend available via the GSIF package. Color breaks in the legend have been selected using histogram equalization (i.e. by using constant quantiles) to ensure maximum contrast in the output maps.
</p>
</div>
<p>The ratio of soil to water in a suspension has a net effect of increasing the pH with a decrease in the soil/water ratio. <span class="citation">Davis (<a href="#ref-Davis1943SS">1943</a>)</span> has shown that decreasing the soil/water ratio from 10:1 to 1:10 resulted in an increase of 0.40 pH units. Values for pH computed using methods with a lower ratio of soil to water (e.g. 1:1 or 1:2.5) will generally be lower than equivalent values for pH in 1:5 water and will need to be adjusted higher. Several authors have demonstrated that fitting quadratic or curvilinear functions to soil pH data produces regression equations with higher coefficients of determination that those obtained from a linear fit <span class="citation">(Aitken and Moody <a href="#ref-Aitken1991AJSR">1991</a>; Miller and Kissel <a href="#ref-Miller2010SSSAJ">2010</a>)</span>. For example, <span class="citation">Brennan and Bolland (<a href="#ref-Brennan1998">1998</a>)</span> have estimated that (at least in Southwestern Australia) pH in CaCl<span class="math inline">\(_2\)</span> can be estimated from the pH 1:5 water by using a simple conversion:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ph_h2o =<span class="st"> </span><span class="fl">7.2</span>
<span class="fl">0.918</span><span class="op">*</span>ph_h2o<span class="fl">-0.3556</span>
<span class="co">#&gt; [1] 6.25</span></code></pre></div>
<p>This model fitted explains 94% of variation in the values of pH CaCl<span class="math inline">\(_2\)</span> (R-square=0.9401).</p>

<div class="rmdnote">
Soil pH is negative decimal logarithm of the hydrogen ion activity in a soil suspension. Soil pH values are usually in the range 3–11 and are recorded with a precision of ±0.1 pH in the range of 5.5 to 7.5 is optimal for growing crops.
</div>

<p>Soil pH varies with season and soil moisture content, with higher pH values associated with wetter soils and winter conditions and lower pH values with drier soils and summer conditions <span class="citation">(Miller and Kissel <a href="#ref-Miller2010SSSAJ">2010</a>)</span>. The effects of both temporal variation in pH and variation due to different analytical methods means that differences in pH of less than some specified range (e.g. ±0.3 units) may not be meaningful in the context of predictions made using noisy legacy soils data analyzed using a variety of different analytical methods. Consequently, it is not necessary or beneficial to report pH with a precision greater than ±0.1 unit. Natural variation of pH in soils is over a range of 2 to 11 with a standard deviation of 1.4. Note also that pH follows a close-to-normal distribution, although it is often argued that, locally, it can show bimodal or even trimodal peaks (Fig. @ref(fig:color-legend-phi)).</p>
</div>
<div id="soil-nutrients" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Soil nutrients</h3>
<p>Nutrients are chemical elements or substances essential for the growth of plants. The most essential elements important for the growth of plants are, in fact, carbon, hydrogen and oxygen. Other essential elements can be separated into macro-nutrients (&gt;100 <span class="math inline">\(\mu\)</span>g or &gt;100 ppm) and micro-nutrients (&lt;100 ppm), although there is no strict border between the two <span class="citation">(Harpstead, Sauer, and Bennett <a href="#ref-harpstead2001soil">2001</a>; Hengl, Leenaars, et al. <a href="#ref-hengl2017soil">2017</a>)</span>. Some macro-nutrients of global importance for soil management and protection are (<a href="http://en.wikipedia.org/wiki/Plant_nutrition" class="uri">http://en.wikipedia.org/wiki/Plant_nutrition</a>):</p>
<ul>
<li><p><em>Nitrogen</em> (N) — It is often considered synonymous with soil fertility. Controls leafy growth. Occurs in soil as nitrates (e.g. NO<span class="math inline">\(_3\)</span>, NO<span class="math inline">\(_2\)</span>).</p></li>
<li><p><em>Phosphorus</em> (P) — High phosphorus deficiency may result in the leaves becoming denatured and showing signs of necrosis. Occurs in the form of phosphates.</p></li>
<li><p><em>Potassium</em> (K) — Potassium deficiency may result in higher risk of pathogens, wilting, chlorosis, brown spotting, and higher chances of damage from frost and heat.</p></li>
<li><p><em>Sulfur</em> (S) — Symptoms of deficiency include yellowing of leaves and stunted growth. Occurs in soil in the form of sulfate salts (SO<span class="math inline">\(_4\)</span>).</p></li>
<li><p><em>Calcium</em> (Ca) — Calcium is involved in photosynthesis and plant structure. Calcium deficiency results in stunting. Occurs in the form of calcium carbonates (CaCO<span class="math inline">\(_3\)</span>).</p></li>
<li><p><em>Magnesium</em> (Mg) — Magnesium is also an important part of chlorophyll. Magnesium deficiency can result in interveinal chlorosis.</p></li>
</ul>

<div class="rmdnote">
Nitrogen, Phosphorus and Potassium are the three relatively mobile and dynamic nutrients in soil that are most often lacking and hence have been identified of primary interest for the fertilizer industry. Other micro-nutrients of interest for global soil mapping would be: Iron (Fe), Zinc (Zn), Manganese (Mn), Copper (Cu), Boron (B), Chlorine (Cl), Molybdenum (Mo), Nickel (Ni) and Sodium (Na).
</div>

<p>Apart from macro- and micro-nutrients important for plant growth, there is an increasing interest in the distribution of heavy metals in soils, especially ones that are considered toxic or dangerous for human health. Some common heavy metals of interest for soil management and soil protection in developed industrial and / or developing countries are Lead (Pb), Arsenic (As), Zinc (Zn), Cadmium (Cd), Nickel (Ni), Copper (Cu), and Aluminium (Al) <span class="citation">(Markus and McBratney <a href="#ref-Markus2001399">2001</a>; Reimann et al. <a href="#ref-reimann2011statistical">2011</a>; Morel et al. <a href="#ref-Morel2005202">2005</a>; Rodríguez-Lado et al. <a href="#ref-Rodriguez-Lado23082013">2013</a>; Hengl, Leenaars, et al. <a href="#ref-hengl2017soil">2017</a>)</span>.</p>
<p>Macro- and micro-nutrients and heavy metals are measured and mapped in parts per million or <span class="math inline">\(\mu\)</span>g per kg of soil. The AfSIS project, provides a good example of mapping macro- and micro-nutrients over a large area <span class="citation">(Hengl, Leenaars, et al. <a href="#ref-hengl2017soil">2017</a>)</span>. The problem with mapping such chemical soil properties, however, is that they are highly dynamic. For example, nitrogen, phosphorus, and potassium are highly mobile nutrients. Their concentration changes from month to month, even from day to day so that space-time models (2D-T or 3D-T) need to be developed and the amount of analysis / storage needed can easily escalate.</p>
</div>
</div>
<div id="physical-and-hydrological-soil-properties" class="section level2">
<h2><span class="header-section-number">3.4</span> Physical and hydrological soil properties</h2>
<div id="coarse-fragments" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Coarse fragments</h3>
<p>Soil texture is connected with soil granulometry or the composition of the particle sizes, typically measured as volume percentages. The most common subdivision of soil granulometry is <span class="citation">(Shirazi, Boersma, and Johnson <a href="#ref-Shirazi2001SSSAJ">2001</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Fine earth (&lt;2 m)</p>
<ol style="list-style-type: decimal">
<li><p>sand (coarser particles in the fine earth),</p></li>
<li><p>silt (medium size particles),</p></li>
<li><p>clay (fine particles &lt;2 <span class="math inline">\(\mu\)</span>m),</p></li>
</ol></li>
<li><p>Coarse fragments (&gt;2 mm)</p>
<ol style="list-style-type: decimal">
<li><p>gravel (2 mm to 8 cm)</p></li>
<li><p>stones or boulders (&gt;8 cm)</p></li>
</ol></li>
</ol>
<p>Coarse fragments occupy volume in the soil matrix, reducing water and nutrient availability as well as influencing rooting depth and workability. We elect to produce maps of coarse fragment content because many assessments, such as total stocks of carbon or available water, are volumetric based and require knowledge of the volume of non-soil materials throughout the profile. This information is required to support calculations of the total volume of the fine earth fraction that is available to hold water or retain organic carbon. Without some estimate of the volume of the soil occupied by solid particles larger than 2 mm, it would not be possible to compute volumetric estimates of stocks of soil carbon or available moisture for fine earth soil.</p>
<p>Coarse fragments include stones as well as gravel (hard and soft nodules) and the attribute is defined as consisting of all mineral particles with a size exceeding 2 mm. Coarse fragment content is most commonly expressed in volume fraction (volume percentage) of the horizon, layer or sample considered. Laboratory analyses tend to be applied to the fine earth fraction of the soil only and commonly omit consideration of the coarse fragment content. Data for coarse fragment content are generally derived from descriptive field observations on soil layer morphology. Those descriptions generally express the content of coarse fragments by class values or categories as for example <em>‘frequent stones’</em> indicating an estimated volumetric content of 15–40% according to the FAO guidelines of 1977 (similar to <em>‘many stones’</em> according to SOTER conventions and the FAO guidelines of 2006). Because coarse fragment content is most frequently based on generalized visual field estimates, and is often lacking in legacy soil descriptions, it is not reasonable to predict or present estimates of coarse fragment content with a precision greater than 1–5%.</p>
<p>Note that the uncertainty associated with coarse fragment content, propagated from the field observed class values, has significant impact on estimations of the volumetric totals of attributes assessed and mapped for the fine earth fraction (see also section @ref(SOC-chapter)). Whilst a 1 meter deep soil, with a bulk density of 1.5 tonne per cubic-metre and an organic carbon content of 10 g per kg, contains 150 tonnes organic carbon. A similar soil with bulk density adjusted for the presence of <em>‘frequent stones’</em> contains 90–127.5 tonnes organic carbon. Despite the inaccuracy of the data for field observed coarse fragments content, it is strongly recommended to collect and compile these data as completely as possible because of their relevance for estimating whole soil bulk density, total volume and volume of the fine earth fraction alone.</p>
<p>The possible nature (and size) of coarse fragments is highly variable (quartz, carbonate, iron, basalt) with consequent variable manageability and variable characteristics such as breakability, solubility, bulk density, etc. Where the coarse fragment content is dominant (&gt;80%), approaching 100%, rootability is near nil which is a determinant for the rooting or effective soil depth and generally also for depth to bedrock. An estimated global distribution of coarse fragments and soil textures is given in Fig. @ref(fig:sprofs-crfvol).</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_sprofs_CRFVOL.png" alt="Histogram and soil-depth density distribution for a global compilation of measurements of coarse fragments in percent. Based on the records from WOSIS (http://www.earth-syst-sci-data.net/9/1/2017/). This variable in principle follows a zero inflated distribution." width="100%" />
<p class="caption">
(#fig:sprofs-crfvol)Histogram and soil-depth density distribution for a global compilation of measurements of coarse fragments in percent. Based on the records from WOSIS (<a href="http://www.earth-syst-sci-data.net/9/1/2017/" class="uri">http://www.earth-syst-sci-data.net/9/1/2017/</a>). This variable in principle follows a zero inflated distribution.
</p>
</div>
</div>
<div id="particle-size-class-distribution-sand-silt-and-clay" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Particle size class distribution: sand, silt and clay</h3>
<p>The majority of global soil mapping initiatives elect to predict the spatial distribution of particle size classes (soil texture) because texture controls or influences many mechanical, hydrological and engineering aspects of use of the soil. Soil texture affects how a soil responds to engineering uses such as construction of roads, buildings, dams and other structures, how water infiltrates into the soil and is stored or transmitted through it, how nutrients, chemicals and dissolved substances adhere to surfaces and are retained or transformed and how energy and matter enter into the soil and are stored or transmitted through it. Texture is the fundamental physical and mechanical property of soils and, as such, it is one of the most widely analysed and widely reported soil properties.</p>
<p>The size of particles in the soil varies greatly from less than a 2 <span class="math inline">\(\mu\)</span>m to several cm’s and occasionally even meters (boulders). This represents a range from to 1 <span class="math inline">\(\mu\)</span>m to 1 million <span class="math inline">\(\mu\)</span>m. Generally, particle size distribution has been simplified through aggregation or classification. The fine earth fraction (&lt;2 mm) is the soil considered for laboratory analyses. This fine earth is further subdivided into particle size classes including, depending on the guidelines or laboratory concerned, fine and coarse clay, fine and coarse silt and very fine, fine, medium, coarse and very coarse sand. The three major particle size classes of the fine earth fraction though are sand, silt and clay. They are generally reported in units of percent by weight with a precision of ±1%.</p>
<p>Soil texture represents the relative composition of sand, silt, and clay in soil. The <em>particle-size class distribution</em> is usually represented in a texture diagram, relating the percentages of sand, silt, and clay (mass percentage of fine earth) to a <em>texture class</em> <span class="citation">(Minasny and McBratney <a href="#ref-Minasny2001AJSR">2001</a>)</span>. Particle size distribution has been defined using a number of systems. One of the most widely used systems is the USDA Soil Survey Laboratory Methods Manual <span class="citation">(Natural Resources Conservation Service <a href="#ref-Burt2004SSIR">2004</a>)</span>. The USDA definition of particle size classes has also been recommended by FAO for use in the Soil Map of the World (Fig. @ref(fig:texture-limits)). The standard reference method adopted by GSIF for reporting particle size classes of sand, silt and clay, is as per the USDA Soil Survey Laboratory Methods Manual <span class="citation">(Natural Resources Conservation Service <a href="#ref-Burt2004SSIR">2004</a>, 347)</span>. An estimated global distribution of sand, silt, and clay is given in Figs. @ref(fig:sprofs-snd), @ref(fig:sprofs-slt) and @ref(fig:sprofs-cly).</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_texture_limits_Minasny2001.jpg" alt="Particle size limits used in European countries, Australia and America. Image source: Minasny and McBratney (2001) doi: 10.1071/SR00065." width="100%" />
<p class="caption">
(#fig:texture-limits)Particle size limits used in European countries, Australia and America. Image source: Minasny and McBratney (2001) doi: 10.1071/SR00065.
</p>
</div>
<div class="figure" style="text-align: center">
<img src="figures/Fig_sprofs_SNDPPT.png" alt="Histogram and soil-depth density distribution for a global compilation of measurements of sand content in percent. Based on the records from WOSIS (http://www.earth-syst-sci-data.net/9/1/2017/)." width="100%" />
<p class="caption">
(#fig:sprofs-snd)Histogram and soil-depth density distribution for a global compilation of measurements of sand content in percent. Based on the records from WOSIS (<a href="http://www.earth-syst-sci-data.net/9/1/2017/" class="uri">http://www.earth-syst-sci-data.net/9/1/2017/</a>).
</p>
</div>
<div class="figure" style="text-align: center">
<img src="figures/Fig_sprofs_SLTPPT.png" alt="Histogram and soil-depth density distribution for a global compilation of measurements of silt content in percent. Based on the records from WOSIS (http://www.earth-syst-sci-data.net/9/1/2017/)." width="100%" />
<p class="caption">
(#fig:sprofs-slt)Histogram and soil-depth density distribution for a global compilation of measurements of silt content in percent. Based on the records from WOSIS (<a href="http://www.earth-syst-sci-data.net/9/1/2017/" class="uri">http://www.earth-syst-sci-data.net/9/1/2017/</a>).
</p>
</div>
<div class="figure" style="text-align: center">
<img src="figures/Fig_sprofs_CLYPPT.png" alt="Histogram and soil-depth density distribution for a global compilation of measurements of clay content in percent. Based on the records from WOSIS (http://www.earth-syst-sci-data.net/9/1/2017/)." width="100%" />
<p class="caption">
(#fig:sprofs-cly)Histogram and soil-depth density distribution for a global compilation of measurements of clay content in percent. Based on the records from WOSIS (<a href="http://www.earth-syst-sci-data.net/9/1/2017/" class="uri">http://www.earth-syst-sci-data.net/9/1/2017/</a>).
</p>
</div>
<p>The current standard for particle size classes adopted by FAO for use in the Harmonized World Soil Database is ISO 10390:2005. This standard differs from the USDA definition in defining the size range for silt as 2–63 <span class="math inline">\(\mu\)</span>m instead of 2–50 <span class="math inline">\(\mu\)</span>m and sand as 63–2000 <span class="math inline">\(\mu\)</span>m instead of 50–2000 <span class="math inline">\(\mu\)</span>m. This is a relatively new standard for FAO which previously adopted the USDA definitions for the digital soil map of the world <span class="citation">(Nachtergaele, Van Engelen, and Batjes <a href="#ref-Nachtergaele2010press">2010</a>)</span>. These differences in attribute definition cause differences in values reported for soil particle size classes. Differences in values can also arise because of differences in method of analysis (e.g. hygrometer, pipette, laser diffraction, dispersion etc). Most literature on harmonization of soil texture data deals with harmonizing differences in attribute definitions or the reported particle size classes (Fig. @ref(fig:texture-limits)).</p>

<div class="rmdnote">
The most commonly used standard for designation of fine earth texture fractions, used by the <em>GlobalSoilMap</em> project, is the USDA system (sand: 50–2000 <span class="math inline">\(\mu\)</span>m, silt: 2–50 <span class="math inline">\(\mu\)</span>m, clay: &lt;2 <span class="math inline">\(\mu\)</span>m).
</div>

<p><span class="citation">Minasny and McBratney (<a href="#ref-Minasny2001AJSR">2001</a>)</span> identified two major textural classifications in the world as the International and USDA/FAO systems (Tbl. @ref(tab:usdafaotexture)). The significant difference between these two was the choice of a threshold value for differentiating silt from sand of 20 <span class="math inline">\(\mu\)</span>m for the International and 50 <span class="math inline">\(\mu\)</span>m for the USDA/FAO systems. The new ISO/FAO standard adds an additional difference by changing the threshold value between silt and sand from 50 <span class="math inline">\(\mu\)</span>m to 63 <span class="math inline">\(\mu\)</span>m. Another very important difference in attribute definition concerns the Russian system which defines the clay fraction as &lt;1 <span class="math inline">\(\mu\)</span>m and the fine earth fraction, or the upper limit of the sand fraction, at 1 cm instead of 2 cm.</p>
<table>
<caption>(#tab:usdafaotexture)Differences between the International, USDA and ISO/FAO particle size classifications.</caption>
<thead>
<tr class="header">
<th align="left">Size.Fraction</th>
<th align="left">International</th>
<th align="left">USDA</th>
<th align="left">ISO.FAO</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">clay</td>
<td align="left"><span class="math inline">\(&lt;\)</span> 2 <span class="math inline">\(\mu\)</span>m</td>
<td align="left"><span class="math inline">\(&lt;\)</span> 2 <span class="math inline">\(\mu\)</span>m</td>
<td align="left"><span class="math inline">\(&lt;\)</span> 2 <span class="math inline">\(\mu\)</span>m</td>
</tr>
<tr class="even">
<td align="left">silt</td>
<td align="left">2–20 <span class="math inline">\(\mu\)</span>m</td>
<td align="left">2–50 <span class="math inline">\(\mu\)</span>m</td>
<td align="left">2–63 <span class="math inline">\(\mu\)</span>m</td>
</tr>
<tr class="odd">
<td align="left">sand</td>
<td align="left">20–2000 <span class="math inline">\(\mu\)</span>m</td>
<td align="left">50–2000 <span class="math inline">\(\mu\)</span>m</td>
<td align="left">63–2000 <span class="math inline">\(\mu\)</span>m</td>
</tr>
</tbody>
</table>
<p>Both <span class="citation">Nemes et al. (<a href="#ref-Nemes1999G">1999</a>)</span> and <span class="citation">Minasny and McBratney (<a href="#ref-Minasny2001AJSR">2001</a>)</span> investigated options for harmonizing values for sand, silt and clay reported using different systems for classifying particle size fractions. Using a compilation of four large databases consisting of a total of 1620 samples, <span class="citation">Minasny and McBratney (<a href="#ref-Minasny2001AJSR">2001</a>)</span> developed a single multiple linear regression model for converting between silt fraction based on the international standard of 2–20 <span class="math inline">\(\mu\)</span>m (<span class="math inline">\(P_{\mathtt{2-20}}\)</span>) to the 2–50 <span class="math inline">\(\mu\)</span>m range of the USDA standard (<span class="math inline">\(P_{\mathtt{2-50}}\)</span>) and vice versa:</p>
<span class="math display">\[\begin{equation}
\begin{cases}
\begin{matrix} \hat P_{\mathtt{2-50}} = &amp; -18.3914 + 2.0971 \cdot P_{\mathtt{2-20}} + 0.6726 \cdot P_{\mathtt{20-2000}}   \\
   &amp; - 0.0142 \cdot P_{\mathtt{2-20}}^2  - 0.0049 \cdot P_{\mathtt{20-2000}}^2
\end{matrix}   &amp; \text{ if } \hat P_{\mathtt{2-50}} &gt; 0 \\ \begin{matrix} \hat P_{\mathtt{2-50}} = &amp; 0.8289 \cdot P_{\mathtt{2-20}} + 0.0198 \cdot P_{\mathtt{20-2000}} \end{matrix} &amp; \text{ if } \hat P_{\mathtt{2-50}} &lt; 0
\end{cases}
(\#P2_50)
\end{equation}\]</span>
<p>where <span class="math inline">\(P_{\mathtt{20-2000}}\)</span> is the international sand fraction. This conversion is fairly accurate since the model explains most of the observed variability in the original values (<span class="math inline">\(R^2\)</span>=0.823). Together with the conversion of the silt fraction is the conversion of the sand fraction.</p>
<p><span class="citation">Minasny and McBratney (<a href="#ref-Minasny2001AJSR">2001</a>)</span> argued that most countries should consider adopting the particle size limits and texture classes of the USDA system. They noted that the 2–50 <span class="math inline">\(\mu\)</span>m particle size range is usually more useful than the 2–20 <span class="math inline">\(\mu\)</span>m range for estimating water retention in pedo-transfer functions and observed that translations from one system into another were relatively easy, given improved computing power and algorithms.</p>
<p><span class="citation">Nemes, Schaap, and Leij (<a href="#ref-Nemes1999">1999</a>; Nemes et al. <a href="#ref-Nemes1999G">1999</a>)</span> evaluated four different interpolation methods (log-linear interpolation, fitting a Gompertz curve, spline interpolation, and similarity method) in order to achieve compatibility of particle-size distributions within the <em>European soil hydraulic database HYPRES</em> (<a href="http://www.macaulay.ac.uk/hypres/" class="uri">http://www.macaulay.ac.uk/hypres/</a>). They introduced a new similarity procedure, which uses an external reference data set that contains a wide variety of reference soil materials, each with 7 or 8 measured particle-size fractions. The procedure involves searching for soil samples in the external reference data set that match the particle-size distribution of the soil to be interpolated. From each search. 10 similar soils are selected that have fractions at the particle size limits similar to the soil under investigation. The arithmetic mean of the fractions of these 10 soils at the specified particle size limit is calculated and assigned as the estimate of the fraction for the soil under investigation.</p>
<p>The HYPRES reference database and the similarity procedures applied to it are appropriate for harmonizing a wide range of soils from a variety of countries and could be used as one of the main methods in a global Soil Reference Library. The generic nature of this conversion approach, and the fact that it does not rely on multiple, locally developed, regression equations, makes it an attractive option for use in harmonization of global particle size data.</p>
</div>
<div id="bulk-density" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Bulk density</h3>
<p>Measurement of soil Bulk Density (BLD) is often time consuming and relatively costly. For this reason, it is not analysed and reported for legacy soil profiles as frequently or consistently as many other, more common, soil properties. Consequently, predicting bulk density globally using digital soil mapping methods is fraught with difficulties and uncertainties. However, it is critical to at least attempt to make some kind of estimate of how bulk density varies spatially because we need to know the bulk density of the soil in order to make any estimates of volumetric concentrations of materials such as organic carbon, water or nutrients.</p>
<p>In practice, we need to be able to make estimates of two different types of bulk density, namely the bulk density of the whole soil and the <em>bulk density of the fine earth fraction</em> (particles &lt;2 mm) only. Calculations such as those for total stocks of carbon are first applied using the bulk density of the fine earth fraction only but this value is then reduced in accordance with the volume proportion of the soil that is occupied by coarse fragments greater than 2 mm in size. Bulk density is also of interest for global soil mapping applications because it influences infiltration and movement of water in the soil, penetration of the soil by plant roots and mechanical workability of the soil using farm implements.</p>
<p>Bulk density is the over-dry mass of soil material divided by the total volume. The standard reference method for reporting bulk density for GSIF is the core method (ISO 11272). The dry bulk density (BD) is the ratio between the mass of oven dry soil material and the volume of the undisturbed fresh sample. The ISO standard defines dry bulk density as the ratio of the oven-dry mass of the solids to the volume (the bulk volume includes the volume of the solids and of the pore space) of the soil. The recommended ISO method (core method) uses steel cylinders of known volume (100 cubic cm, 400 cubic cm) that are driven into the soil vertically or horizontally by percussion. Sampling large volumes results in smaller relative errors but requires heavy equipment. The method cannot be used if stones or large roots are present or when the soil is too dry or too hard.</p>
<p>For soils with a high stone or root content or when the soil is too dry or too hard, methods based on the excavation technique are used as an alternative to the core method. In the excavation method a hole on a horizontal surface is dug and then filled with a material with a known density (e.g. sand which packs to a calibrated volume or water separated from the soil material by an elastic membrane) to assess the volume of the hole or the sample taken. The soil obtained from the hole, is oven-dried to remove the water and the oven-dry mass of the total sample is weighed. The volumetric percentage of the coarse fragments needs to be determined and the weight of the coarse fragments assessed, in order to be able to calculate the oven-dry bulk density of the fine earth separately.</p>
<p>The USDA handbook for analytical procedures describes various methods for assessing various types of bulk density. USDA soil data report values for bulk density of the fine earth as well as of the whole earth (including gravel), with the weight assessed oven-dry as well as at field capacity e.g. including water. The latter method relates the weight of moist soil to the volume of moist or oven-dry soil. Experience has shown that organic carbon and texture or clay content predominately influence soil bulk density, even though the nature of the clay (mineralogy) is as important as the percentage content of the clay. Organic carbon and texture information is often available in soil survey reports, while bulk density is often not as frequently reported.</p>
<p>Many attempts have therefore been made to estimate soil bulk densities through pedo-transfer functions (PTFs) based on soil organic carbon and texture data <span class="citation">(Curtis and Post <a href="#ref-Curtis1964SSSAP">1964</a>; Adams <a href="#ref-Adams1973JSS">1973</a>; Alexander <a href="#ref-Alexander1980SSSAJ">1980</a>; Federer, Turcotte, and Smith <a href="#ref-Federer1993CJFR">1993</a>; Rawls <a href="#ref-Rawls1983SS">1983</a>; Manrique and Jones <a href="#ref-Manrique1991SSSAJ">1991</a>; Bernoux et al. <a href="#ref-Bernoux1998SSSAJ">1998</a>)</span>. <span class="citation">Heuscher, Brandt, and Jardine (<a href="#ref-Heuscher2005SSSAJ">2005</a>)</span> applied a stepwise multiple regression procedure to predict oven-dried bulk density from soil properties using the NRCS National Soil Survey Characterization Data. The database included both subsoil and topsoil samples. An overall regression equation for predicting oven-dried bulk density from soil properties (<span class="math inline">\(R^2=0.45\)</span>, <span class="math inline">\(P&lt;0.001\)</span>) was developed using almost 47,000 soil samples. Partitioning the database by soil suborders improved regression relationships (<span class="math inline">\(R^2=0.62\)</span>, <span class="math inline">\(P&lt;0.001\)</span>). Of the soil properties considered, the stepwise multiple regression indicated that organic C content was the strongest contributor to bulk density prediction. Other significant variables included clay content, water content and to a lesser extent, silt content, and depth.</p>

<div class="rmdnote">
Bulk density is the oven-dry mass of soil material divided by the total volume and typically ranges from 0.7 to 1.8 t/kg<span class="math inline">\(^3\)</span>. The average bulk density of fine earth fraction of soil is about 1.3 t/kg<span class="math inline">\(^3\)</span>; soils with a bulk density higher than tend to restrict root growth. Different values for bulk density typically apply for different soils with different soil genesis as reflected by different materials and mineralogy, e.g. Histosols (organic), Arenosols (sandy), Andosols (allophanic clay), Acrisols (low activity clays) and Vertisols (high activity clays).
</div>

<p>Bulk density tends to be measured and reported less frequently in legacy data bases and reports than most other commonly measured soil analytical properties. Such values as are reported are most often based on field measurements of in-situ bulk density using the core method. Bulk density of the fine earth fraction alone is measured and reported even less frequently than bulk density for the whole soil (Fig. @ref(fig:sprofs-bld)).</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_sprofs_BLD.png" alt="Histogram and soil-depth density distribution for a global compilation of measurements of bulk density (tonnes per cubic metre). Based on the records from WOSIS (http://www.earth-syst-sci-data.net/9/1/2017/)." width="100%" />
<p class="caption">
(#fig:sprofs-bld)Histogram and soil-depth density distribution for a global compilation of measurements of bulk density (tonnes per cubic metre). Based on the records from WOSIS (<a href="http://www.earth-syst-sci-data.net/9/1/2017/" class="uri">http://www.earth-syst-sci-data.net/9/1/2017/</a>).
</p>
</div>
<p>Given that there are more values reported for the bulk density of the whole soil than for the fine earth fraction, we propose to first estimate the bulk density of the whole soil (using appropriate pedo-transfer functions) and then apply corrections to estimate the bulk density of the fine earth fraction, correcting for the effect of course fragments. Correction involves subtracting the volume of coarse fragments from the total volume of soil sampled for assessing bulk density in-situ in the field and then also subtracting the (estimated) weight of coarse fragments from the measured oven-dry weight of the sampled soil.</p>
<p>The revised weight of the fine-earth fraction alone (minus the weight of the coarse fragments) is divided by the adjusted volume of the sample (reduced by the volume of coarse fragments) to obtain an estimate of bulk density for the fine earth fraction alone. This value of density of the fine-earth fraction alone is the one needed to compute estimates of volumetric soil properties, such as total carbon stocks. It is therefore the critical measure of bulk density for reporting concentrations of soil chemical properties. Conversely, bulk density of the whole soil, in situ, is generally of greater use and interest for assessing hydrological behaviours and properties, such as hydraulic conductivity and moisture holding capacity.</p>
<p><span class="citation">Tranter et al. (<a href="#ref-Tranter2007SUM">2007</a>)</span> proposed a conceptual model that incorporated a priori knowledge for predicting soil bulk density from other, more regularly measured, properties. The model considers soil bulk density to be a function of soil mineral packing structures (<span class="math inline">\(\rho_m\)</span>) and soil structure (<span class="math inline">\(\Delta \rho\)</span>). Bulk densities were also observed to increase with depth, suggesting the influence of over-burden pressure. Residuals from the <span class="math inline">\(\rho_m\)</span> model, referred to as <span class="math inline">\(\Delta \rho\)</span>, correlated with organic carbon.</p>
<p><span class="citation">Torri et al. (<a href="#ref-Torri1994C">1994</a>)</span> developed a nomogram for transforming rock fragment content from a by-mass to a by-volume basis and vice versa based on bulk density data. This nomogram facilitates conversion of data on rock fragment content expressed in different units. Most PTFs for predicting bulk density, except those developed by <span class="citation">Rawls (<a href="#ref-Rawls1983SS">1983</a>)</span> and <span class="citation">Bernoux et al. (<a href="#ref-Bernoux1998SSSAJ">1998</a>)</span>, are a function only of organic matter i.e. organic carbon content. Although studies conducted by <span class="citation">Saini (<a href="#ref-Saini1966N">1966</a>)</span> and <span class="citation">Jeffrey (<a href="#ref-Jeffrey1970JE">1970</a>)</span> have shown that organic matter has a dominating effect on soil bulk density and that it can be used alone as a good predictor of soil bulk density, it has been observed (e.g. <span class="citation">Alexander (<a href="#ref-Alexander1980SSSAJ">1980</a>)</span> and <span class="citation">Manrique and Jones (<a href="#ref-Manrique1991SSSAJ">1991</a>)</span>) that, where organic matter is a minor component, soil texture plays a major role in controlling bulk density .</p>
</div>
<div id="soil-organic-carbon-stock" class="section level3">
<h3><span class="header-section-number">3.4.4</span> Soil organic carbon stock</h3>
<p>Primary soil properties such as organic carbon content, bulk density and coarse fragments can be further used as inputs for estimation of secondary soil properties which are typically not measured directly in the field and need to be derived from primary soil properties. For instance, consider estimation of the global carbon stock (in permille). This secondary soil property can be derived from a number of primary soil properties <span class="citation">(Nelson and Sommers <a href="#ref-Nelson1982">1982</a>; Sanderman, Hengl, and Fiske <a href="#ref-sanderman2018soil">2018</a>)</span> (see Fig. @ref(fig:ocs-calculus-scheme)):</p>
<span class="math display">\[\begin{equation}
    \mathtt{OCS} \; [\mathrm{kg \; m^{-2}}] = \frac{{\mathtt{ORC}}}{{1000}} \; [\mathrm{kg \; kg^{-1}}] \cdot \frac{{\mathtt{HOT}}}{{100}} \; [\mathrm{m}] \cdot \mathtt{BLD} \; [\mathrm{kg \; m^{-3}}] \cdot \frac{{100-\mathtt{CRF} \; [\mathrm{\%}]}}{{100}}
(\#eq:ocs-calc)
\end{equation}\]</span>
<p>where <code>OCS</code> is soil organic carbon stock, <code>ORC</code> is soil organic carbon mass fraction in permilles, <code>HOT</code> is horizon thickness in , <code>BLD</code> is soil bulk density in and <code>CRF</code> is volumetric fraction of coarse fragments (<span class="math inline">\(&gt;\)</span> 2 mm) in percent.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_OCS_calculus_scheme.png" alt="Soil organic carbon stock calculus scheme. Example of how total soil organic carbon stock (OCS), and its propagated error, can be estimated for a given volume of soil using organic carbon content (ORC), bulk density (BLD), thickness of horizon (HOT), and percentage of coarse fragments (CRF). Image source: Hengl et al. (2014) doi: 10.1371/journal.pone.0169748. OCSKGM function also available via the GSIF package." width="100%" />
<p class="caption">
(#fig:ocs-calculus-scheme)Soil organic carbon stock calculus scheme. Example of how total soil organic carbon stock (OCS), and its propagated error, can be estimated for a given volume of soil using organic carbon content (ORC), bulk density (BLD), thickness of horizon (HOT), and percentage of coarse fragments (CRF). Image source: Hengl et al. (2014) doi: 10.1371/journal.pone.0169748. OCSKGM function also available via the GSIF package.
</p>
</div>
<p>The propagated error of the soil organic carbon stock (Eq.@ref(eq:ocs-calc)) can be estimated using the Taylor series method <span class="citation">(Heuvelink <a href="#ref-Heuvelink1998a">1998</a>)</span> i.e. by using the standard deviations of the predicted soil organic carbon content, bulk density and coarse fragments, respectively (Fig. @ref(fig:ocs-calculus-scheme)). <code>OCS</code> values can be derived for all depths / horizons, then aggregated to estimate the total stock for the whole profile (e.g. 0–2 m).</p>
<p>The formulas to derive soil organic carbon stock and the propagated uncertainty are implemented in the GSIF package e.g.:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Area &lt;-<span class="st"> </span><span class="fl">1E4</span>  ## 1 ha
HSIZE &lt;-<span class="st"> </span><span class="dv">30</span> ## 0--30 cm
ORCDRC &lt;-<span class="st"> </span><span class="dv">50</span>  ## 5%
ORCDRC.sd &lt;-<span class="st"> </span><span class="dv">10</span>  ## +/-1%
BLD &lt;-<span class="st"> </span><span class="dv">1500</span>  ## 1.5 tonnes per cubic meter
BLD.sd &lt;-<span class="st"> </span><span class="dv">100</span>  ## +/-0.1 tonnes per cubic meter
CRFVOL &lt;-<span class="st"> </span><span class="dv">10</span>  ## 10%
CRFVOL.sd &lt;-<span class="st"> </span><span class="dv">5</span>  ## +/-5%         
x &lt;-<span class="st"> </span><span class="kw">OCSKGM</span>(ORCDRC, BLD, CRFVOL, HSIZE, ORCDRC.sd, BLD.sd, CRFVOL.sd)
x  ## 20.25 +/-4.41 kg/m^2
<span class="co">#&gt; [1] 20.2</span>
<span class="co">#&gt; attr(,&quot;measurementError&quot;)</span>
<span class="co">#&gt; [1] 4.41</span>
<span class="co">#&gt; attr(,&quot;units&quot;)</span>
<span class="co">#&gt; [1] &quot;kilograms per square-meter&quot;</span>
x[[<span class="dv">1</span>]] <span class="op">*</span><span class="st"> </span>Area <span class="op">/</span><span class="st"> </span><span class="dv">1000</span> ## in tonnes per ha:
<span class="co">#&gt; [1] 202</span></code></pre></div>
<p>A more robust way to estimate the propagated uncertainty of deriving <code>OCS</code> would be to use geostatistical simulations e.g. derive standard error from a large number of realizations (e.g. &gt;100) that incorporate spatial and vertical correlations. Because, in the case of soil mapping, we are often dealing with massive data sets, running geostatistical simulations for millions of pixels is current not yet a feasible option.</p>
</div>
<div id="available-water-capacity" class="section level3">
<h3><span class="header-section-number">3.4.5</span> Available Water Capacity</h3>
<p>The available water holding capacity (AWC) is a complex soil property. It is basically a soil or land quality <span class="citation">(Food, Agriculture Organization of the United Nations. Soil Resources, and Service <a href="#ref-food1977framework">1977</a>)</span>, that provides valuable information about the capacity of the soil to hold water, particularly water that is potentially available for root uptake by plants and vegetative transpiration. In practice, AWC is land cover specific. The water available for root uptake depends on the soil properties that determine rootability or rooting depth as genetically required by the vegetative land cover.</p>
<p>The water available for root uptake also depends on the pressure head that the vegetative land cover can generate or bridge between the pressure in the atmosphere and the pressure in the soil matrix. E.g. cotton can still extract water at -2500 kPa (pF 4.4) while pepper wilts at -350 kPa (pF 3.5). The ability of a soil to accept and store water has implications beyond simply the capacity to support plant growth. It also affects how a soil responds to hydrological events such as rainfall, snowmelt and runoff. Soils that can rapidly absorb and retain significant amounts of rainfall act as a buffer reducing rapid runoff and flooding. Soils that have a limited ability to accept and store rainfall contribute to rapid runoff with increased chances of erosion and flooding. Models of crop growth, runoff, erosion and flooding all have requirements for location-specific information about available water capacity.</p>
<p>The AWC is expressed in mm (which equals mm water/cm soil depth, or water/ soil). This volume of water depends on the volume of soil (influenced by depth interval and by volumetric gravel content) and the volumetric fraction of water that is contained by the soil between field capacity and wilting point. GSIF reports AWC with a precision of 1 mm and a maximum range of 0–2000 mm.</p>
<p>Values for AWC are preferably assessed for the fine earth fraction per depth interval and expressed as volumetric fraction. This value can be corrected for the gravel content of the depth interval and summed up over the interval. Preferably, the values for volumetric AWC of the fine earth fraction per depth interval are derived from values for water content at specific water tensions (e.g. at pF 0.1, 2, 2.5, 3, 4.2, 4.5). For pragmatic reasons though the permanent wilting point is set at -1500 kPa (or 15,000cm, 15 bar, 15 atmosphere or pF 4.2).</p>
<p>The standard reference method adopted by GSIF for reporting available water capacity is as per the USDA Soil Survey Laboratory Methods Manual <span class="citation">(Natural Resources Conservation Service <a href="#ref-Burt2004SSIR">2004</a>, 137)</span>. Calculation of the <em>Water Retention Difference</em> (WRD) is considered the initial step in the approximation of the available water capacity (AWC). WRD is a calculated value that denotes the volume fraction for water in the whole soil that is retained between -1500 kPa suction and an upper limit of usually -33 or -10 kPa suction (pF 2.5 or pF 2) <span class="citation">(Natural Resources Conservation Service <a href="#ref-Burt2004SSIR">2004</a>, 137)</span>. The upper limit (lower suction) is selected so that the volume of water retained approximates the volume of water held at field capacity. The -33 and -1500 kPa gravimetric water contents are then converted to a whole soil volume basis by multiplying by the oven dry bulk density of the fine earth fraction (<code>Db33</code>) and adjusting downward for the volume fraction of rock fragments, if present, in the soil.</p>

<div class="rmdnote">
Available water capacity (expressed in mm of water for the effective soil depth) can be estimated based on the Water Retention Difference (WRD) which denotes the volume fraction for water in the whole soil, including gravel, that is retained between -1500 kPa suction and an upper limit of 33 kPa suction.
</div>

<p><em>“The development of hydraulic PTFs has become a boom industry, mostly in the US and Europe”</em> <span class="citation">(Minasny <a href="#ref-Minasny2007JITL">2007</a>)</span>. Results of such research have been reported widely, including in the USA <span class="citation">(Rawls, Gish, and Brakensiek <a href="#ref-Rawls1991AA">1991</a>)</span>, UK, the Netherlands <span class="citation">(Wösten, Fi, and Jansen <a href="#ref-Wosten1995G">1995</a>)</span>, and Germany. Research has attempted to correlate particle size distribution, bulk density and organic matter content with water content at field capacity (FC, <span class="math inline">\(\theta\)</span> at -33 kPa), permanent wilting point (PWP, <span class="math inline">\(\theta\)</span> at -1500 kPa), and available water content (AWC = FC - PWP) <span class="citation">(Minasny <a href="#ref-Minasny2007JITL">2007</a>)</span>. <span class="citation">Gijsman, Thornton, and Hoogenboom (<a href="#ref-Gijsman2007CEA">2007</a>)</span> reported that <em>“many PTFs for estimating soil hydraulic properties have been published already”</em> (see overviews by <span class="citation">Rawls, Gish, and Brakensiek (<a href="#ref-Rawls1991AA">1991</a>)</span>, <span class="citation">Timlin et al. (<a href="#ref-Timlin1996AS">1996</a>)</span> and <span class="citation">Wösten, Pachepsky, and Rawls (<a href="#ref-Wosten2001JH">2001</a>)</span>). <span class="citation">Timlin et al. (<a href="#ref-Timlin1996AS">1996</a>)</span> reported 49 methods and estimated that this covers only about 30% of the total. <span class="citation">Gijsman, Thornton, and Hoogenboom (<a href="#ref-Gijsman2007CEA">2007</a>)</span> compared eight methods for all the soil classes that make up the texture triangle. They went through the triangle in steps of sand, silt and clay and determined the estimated values of wilting point or lower limit of <em>plant extractable water</em> (LL), <em>field capacity</em> or the drained upper limit (DUL), and soil saturation (SAT). They finally concluded that none of the methods were universally good. The best method in the comparison of <span class="citation">Gijsman, Thornton, and Hoogenboom (<a href="#ref-Gijsman2007CEA">2007</a>)</span> was <span class="citation">Saxton et al. (<a href="#ref-Saxton1986SSSAJ">1986</a>)</span>, closely followed by <span class="citation">Rawls and Brakensiek (<a href="#ref-Rawls1982JIDDASCE">1982</a>)</span>.</p>
<p>Alterra institute in collaboration with ISRIC validated the PTF developed by <span class="citation">Hodnett and Tomasella (<a href="#ref-hodnett2002marked">2002</a>)</span> on the basis of the data present in the Africa Soil Profiles database <span class="citation">(Leenaars <a href="#ref-Leenaars2012">2014</a>)</span> to predict tension specific volumetric water content <span class="citation">(Wösten et al. <a href="#ref-wosten2013soil">2013</a>)</span> to assess WRD. <span class="citation">Jagtap et al. (<a href="#ref-Jagtap2004TASAE">2004</a>)</span> developed an approach that does not fit a mathematical equation through the data, but rather compares the soil layer for which the key soil water contents of lower limit (LL), drained upper limit (DUL), and soil saturation (SAT), have to be estimated with all layers in a database of field-measured soil-water-retention data. The layer that is most similar in texture and organic carbon concentration is considered to be the <em>‘nearest neighbor’</em> among all the layers in the database and its soil-water-retention values are assumed to be similar to those that need to be estimated. To avoid making estimated soil-water-retention values dependent on only one soil in the database, the six <em>‘nearest neighbors’</em> are used and weighted according to their degree of similarity <span class="citation">(Jagtap et al. <a href="#ref-Jagtap2004TASAE">2004</a>)</span>. This is a non-parametric procedure, in the sense that it does not assume a fixed mathematical relationship between the physical properties and the water holding properties of soils. The similarity method to convert soil particle size fraction data proposed by <span class="citation">Nemes et al. (<a href="#ref-Nemes1999G">1999</a>; Nemes, Schaap, and Leij <a href="#ref-Nemes1999">1999</a>)</span> is a direct analogue of this similarity method of <span class="citation">Jagtap et al. (<a href="#ref-Jagtap2004TASAE">2004</a>)</span> for soil hydraulic properties.</p>
<p><span class="citation">Zacharias and Wessolek (<a href="#ref-Zacharias2007SSSAJ">2007</a>)</span> identified three different approaches for deriving the WRD from more easily available parameters as:</p>
<ol style="list-style-type: decimal">
<li><p><em>Point-based estimation methods</em>: estimating the water content of selected matric potentials from predictors such as the percentage of sand, silt, or clay, the amount of organic matter, or the bulk density (e.g. <span class="citation">Rawls and Brakensiek (<a href="#ref-Rawls1982JIDDASCE">1982</a>)</span>).</p></li>
<li><p><em>Semi-physical approach</em>: deriving the WRD from information on the cumulative particle size distribution <span class="citation">(Arya and Paris <a href="#ref-Arya1981SSSAJ">1981</a>)</span>; theoretically, this approach is based on the similarity between cumulative particle size distribution and water retention curves. The water contents are derived from the soil’s predicted pore volume and the hydraulic potentials are derived from capillarity relationships.</p></li>
<li><p><em>Parameter estimation methods</em>: using multiple regression to derive the parameters of an analytical closed-form equation for describing the WRD, using predictors such as the percentage of sand, silt, and clay, the amount of organic matter, or the bulk density (e.g. <span class="citation">Van Genuchten (<a href="#ref-van1980closed">1980</a>; Wösten et al. <a href="#ref-Wosten1999G">1999</a>; Wösten et al. <a href="#ref-wosten2013soil">2013</a>)</span>).</p></li>
</ol>
<p><span class="citation">Zacharias and Wessolek (<a href="#ref-Zacharias2007SSSAJ">2007</a>)</span> concluded that approach (1) has the disadvantage that it uses a large number of regression parameters depending on the number of WRD sampling points, which makes its use in the mathematical modeling more difficult; while for approach 2 very detailed information about the particle size distribution is required. They therefore preferred use of (3) the parameter estimation methods.</p>
<p><span class="citation">Zacharias and Wessolek (<a href="#ref-Zacharias2007SSSAJ">2007</a>)</span> also observed that pedo-transfer functions that do not consider soil organic matter are rare and gave the following examples. <span class="citation">Hall et al. (<a href="#ref-Hall1977">1977</a>)</span> developed point-based regression equations using soil texture and bulk density (only for subsoils) for British soils. <span class="citation">Oosterveld and Chang (<a href="#ref-Oosterveld1980CAE">1980</a>)</span> developed an exponential regression equation for Canadian soils for fitting the relationship between clay and sand content, depth of soil, and moisture content. Equations to estimate the WRC from mean particle diameter and bulk density have been proposed by <span class="citation">Campbell and Shiozawa (<a href="#ref-Campbell1989">1989</a>)</span>. <span class="citation">Williams, Ross, and Bristow (<a href="#ref-Williams1992">1992</a>)</span> analyzed Australian data sets and developed regression equations for the relationship between soil moisture and soil texture, structure information, and bulk density including variants for both the case where there is available information on soil organic matter and where the soil organic matter is unknown. <span class="citation">Rawls and Brakensiek (<a href="#ref-Rawls1989">1989</a>)</span> reported regression equations to estimate soil water retention as a function of soil texture and bulk density. <span class="citation">Canarache (<a href="#ref-Canarache1993ST">1993</a>)</span> developed point based regression equations using clay content and bulk density for Romanian soils. More recently, <span class="citation">Nemes, Schaap, and Wösten (<a href="#ref-Nemes2003SSSAJ">2003</a>)</span> developed different PTFs derived from different scales of soil data (Hungary, Europe, and international data) using artificial neural network modeling including a PTF that uses soil texture and bulk density only.</p>
<p><span class="citation">Zacharias and Wessolek (<a href="#ref-Zacharias2007SSSAJ">2007</a>)</span> developed two different regression equations depending upon the percentage of sand in a soil as follows:</p>
<span class="math display">\[\begin{equation}
\begin{cases}
 \begin{matrix}
 \theta_r  = 0 \\
 \theta_s  = 0.788 + 0.001 \cdot {\mathtt{clay}} -0.263 \cdot D_b \\
 \ln(\alpha) = -0.648 + 0.023 \cdot {\mathtt{sand}} + 0.044 \cdot {\mathtt{clay}} -3.168 \cdot D_b \\
 n = 1.392- 0.418\cdot {\mathtt{sand}}^{-0.024} + 1.212\cdot {\mathtt{clay}}^{-0.704}
\end{matrix} &amp; \text{ if } {\mathtt{sand}} &lt; 66.5\% \\ \hline \begin{matrix}
 \theta_r  = 0 \\
 \theta_s  = 0.890 + 0.001 \cdot {\mathtt{clay}} -0.332 \cdot D_b \\
 \ln(\alpha) = -4.197 + 0.013 \cdot {\mathtt{sand}} + 0.076 \cdot {\mathtt{clay}} -0.276 \cdot D_b \\
 n = 2.562 - 7 \cdot 10^{-9} \cdot {\mathtt{sand}} + 3.750\cdot {\mathtt{clay}}^{-0.016}
\end{matrix} &amp; \text{ if } {\mathtt{sand}} &gt; 66.5\%
\end{cases}
(\#eq:thetas)
\end{equation}\]</span>
<p>The regression coefficients from these models were almost identical to those reported by <span class="citation">Vereecken, Maes, and Feyen (<a href="#ref-Vereecken1989SS">1989</a>)</span> (i.e. <span class="math inline">\(\theta_s = 0.81 + 0.001 \cdot {\mathtt{clay}} - 0.283 \cdot D_b\)</span>) for a different data set, adding further credibility to their general applicability. <span class="citation">Zacharias and Wessolek (<a href="#ref-Zacharias2007SSSAJ">2007</a>)</span> recommended using the PTFs of <span class="citation">Vereecken, Maes, and Feyen (<a href="#ref-Vereecken1989SS">1989</a>)</span> if data on soil organic matter were available but felt that their proposed equations were suitable for use where soil organic matter data were not available.</p>
<p>Empirical equations developed by <span class="citation">Williams, Ross, and Bristow (<a href="#ref-Williams1992">1992</a>)</span> for the prediction of the constants <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> in the Campbell function have been widely used in Australia and elsewhere. These regression equations require particle size distribution, field texture and bulk density inputs as follows:</p>
<span class="math display">\[\begin{equation}
\begin{split}   
A =&amp; 1.996 + 0.136 \cdot \ln({\mathtt{clay}}) - 0.00007 \cdot {\mathtt{fsand}} + \\
  &amp; + 0.145\cdot \ln({\mathtt{silt}}) + 0.382 \cdot \ln({\mathtt{TEXMHT}})
\end{split}
\end{equation}\]</span>
<span class="math display">\[\begin{equation}
B = -0.192 + 0.0946\cdot \ln({\mathtt{TEXMHT}}) - 0.00151\cdot \mathtt{fsand}
\end{equation}\]</span>
<p>where <span class="math inline">\(\mathtt{clay}\)</span> (&lt;0.002 mm), <span class="math inline">\(\mathtt{fsilt}\)</span> (0.02–0.20 mm), and <span class="math inline">\(\mathtt{sand}\)</span> (0.002–0.02 mm) are expressed in %; <span class="math inline">\(\mathtt{TEXMHT}\)</span> is texture group from 1–6 as defined by Northcote in <span class="citation">Peverill, Sparrow, and Reuter (<a href="#ref-peverill1999soil">1999</a>)</span>.</p>
<p><span class="citation">Cresswell et al. (<a href="#ref-Cresswell2006SUM">2006</a>)</span> demonstrated applicability of the <span class="citation">Williams, Ross, and Bristow (<a href="#ref-Williams1992">1992</a>)</span> method for French soils and confirmed that the approach of assuming a Campbell SWC model and empirically predicting the slope and air entry potential has merit. They concluded that the empirical regression equations of Campbell appeared transferable to different data sets from very different geographical locations. They provided regression equations for all samples and stratified by horizon type that had R-square values ranging from 0.81 to 0.91.</p>
<p><span class="citation">Cresswell et al. (<a href="#ref-Cresswell2006SUM">2006</a>)</span> further suggested a strategy for achieving adequate coverage of soil hydraulic property data for France that included an efficient sampling strategy based on the use of functional horizons <span class="citation">(Bouma <a href="#ref-Bouma1989S">1989</a>)</span>, and a series of reference sites where soil hydraulic properties could be measured comprehensively. They argued that the functional horizon method recognizes the soil texture class of the horizon rather than the profile as the individual or building block for prediction <span class="citation">(Wösten, Bouma, and Stoffelsen <a href="#ref-Wosten1985SSSAJ">1985</a>; Wösten and Bouma <a href="#ref-Wosten1992">1992</a>)</span>. A significant feature of this approach is the capacity to create a complex range of different hydrologic soil classes from simple combinations of horizon type, sequence, and thickness.</p>
<p>Pedo-transfer functions for available water capacity typically have a general form of:</p>
<span class="math display">\[\begin{equation}
{\mathtt{AWAIMM}} = f ({\rm organic} \; {\rm carbon}, {\rm sand}, {\rm silt}, {\rm clay},  {\rm bulk} \; {\rm density})
\end{equation}\]</span>
<p>where the total profile available water (<span class="math inline">\(\mathtt{AWAIMM}\)</span>) can be summed over the effective depth.</p>
<p>By using the GSIF package, one can estimate <span class="math inline">\(\mathtt{AWAIMM}\)</span> using the pedo-transfer function described by <span class="citation">Hodnett and Tomasella (<a href="#ref-hodnett2002marked">2002</a>)</span> and <span class="citation">Wösten et al. (<a href="#ref-wosten2013soil">2013</a>)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">SNDPPT =<span class="st"> </span><span class="dv">30</span> 
SLTPPT =<span class="st"> </span><span class="dv">25</span> 
CLYPPT =<span class="st"> </span><span class="dv">48</span> 
ORCDRC =<span class="st"> </span><span class="dv">23</span> 
BLD =<span class="st"> </span><span class="dv">1200</span> 
CEC =<span class="st"> </span><span class="dv">12</span> 
PHIHOX =<span class="st"> </span><span class="fl">6.4</span>
x &lt;-<span class="st"> </span><span class="kw">AWCPTF</span>(SNDPPT, SLTPPT, CLYPPT, ORCDRC, BLD, CEC, PHIHOX)
<span class="kw">str</span>(x)
<span class="co">#&gt; &#39;data.frame&#39;:    1 obs. of  5 variables:</span>
<span class="co">#&gt;  $ AWCh1: num 0.16</span>
<span class="co">#&gt;  $ AWCh2: num 0.122</span>
<span class="co">#&gt;  $ AWCh3: num 0.0999</span>
<span class="co">#&gt;  $ WWP  : num 0.259</span>
<span class="co">#&gt;  $ tetaS: num 0.511</span>
<span class="co">#&gt;  - attr(*, &quot;coef&quot;)=List of 4</span>
<span class="co">#&gt;   ..$ lnAlfa: num  -2.29 0 -3.53 0 2.44 ...</span>
<span class="co">#&gt;   ..$ lnN   : num  62.986 0 0 -0.833 -0.529 ...</span>
<span class="co">#&gt;   ..$ tetaS : num  81.799 0 0 0.099 0 ...</span>
<span class="co">#&gt;   ..$ tetaR : num  22.733 -0.164 0 0 0 ...</span>
<span class="co">#&gt;  - attr(*, &quot;PTF.names&quot;)=List of 1</span>
<span class="co">#&gt;   ..$ variable: chr  &quot;ai1&quot; &quot;sand&quot; &quot;silt&quot; &quot;clay&quot; ...</span>
<span class="kw">attr</span>(x, <span class="st">&quot;coef&quot;</span>)
<span class="co">#&gt; $lnAlfa</span>
<span class="co">#&gt;  [1]  -2.294   0.000  -3.526   0.000   2.440   0.000  -0.076 -11.331</span>
<span class="co">#&gt;  [9]   0.019   0.000   0.000   0.000</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $lnN</span>
<span class="co">#&gt;  [1] 62.986  0.000  0.000 -0.833 -0.529  0.000  0.000  0.593  0.000  0.007</span>
<span class="co">#&gt; [11] -0.014  0.000</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $tetaS</span>
<span class="co">#&gt;  [1]  81.7990   0.0000   0.0000   0.0990   0.0000 -31.4200   0.0180</span>
<span class="co">#&gt;  [8]   0.4510   0.0000   0.0000   0.0000  -0.0005</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $tetaR</span>
<span class="co">#&gt;  [1] 22.7330 -0.1640  0.0000  0.0000  0.0000  0.0000  0.2350 -0.8310</span>
<span class="co">#&gt;  [9]  0.0000  0.0018  0.0000  0.0026</span></code></pre></div>
<p>where <code>SNDPPT</code>, <code>SLTPPT</code> and <code>CLYPPT</code> are the measured sand, silt and clay content in percent, <code>ORCDRC</code> is the soil organic carbon concentration in permille or , <code>BLD</code> is the bulk density in , <code>CEC</code> is the Cation Exchange Capacity in , and <code>PHIHOX</code> is the soil pH in water suspension. The output <code>AWCh1</code>, <code>AWCh2</code>, <code>AWCh3</code> are the available soil water capacity (volumetric fraction) for pF 2.0, 2.3 and 2.5, <code>WWP</code> is the soil water capacity (volumetric fraction) until wilting point, and <code>tetaS</code> is the saturated water content, respectively.</p>
</div>
</div>
<div id="harmonisation-of-soil-data-and-pedo-transfer-functions" class="section level2">
<h2><span class="header-section-number">3.5</span> Harmonisation of soil data and pedo-transfer functions</h2>
<div id="basic-concepts-of-harmonisation-of-soil-property-values" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Basic concepts of harmonisation of soil property values</h3>
<p>A well known issue with legacy soils data is the use of different methods for analyzing soils in the laboratory or describing them in the field. These different methods yield different values that are not exactly equivalent or comparable. This creates a need to assess the significance of the differences in values arising from different methods or method-groups, and possibly the need to harmonize values produced using different methods in order to make them roughly equivalent and comparable. The process of conversion of values measured according to an original method to values roughly equivalent to those measured according to an agreed-upon standard reference method is referred to as <em>data harmonization</em>.</p>
<p>Note that differences in methods are not necessarily reflected in different values for a given attribute. The value reported is fundamentally related to the particular method used for analysis, which we correctly or incorrectly label similarly regardless of the analytical method used.</p>
<p>When using legacy soils data for global soil mapping and analysis projects, it is important to first decide whether it is necessary and important to convert measurements made using various different laboratory methods into equivalent values in the specified standard reference method. This assessment can be made for each soil property individually. Decisions as to whether harmonization is necessary may be influenced by the resolution of the mapping and the desired precision and accuracy of the output predictions.</p>
<p>The process of conversion of values measured by an original method to values roughly equivalent to those measured by an agreed-upon standard reference method is referred to as data harmonization. Examples of harmonisation would be converting values assessed by e.g. pH in 1:2 water to values as if assessed by pH in 1:5 water, or organic carbon by <em>Walkley-Black</em> into organic carbon by <em>dry combustion</em>.</p>
</div>
<div id="example-of-harmonization-using-texture-by-hand-classes" class="section level3">
<h3><span class="header-section-number">3.5.2</span> Example of harmonization using texture-by-hand classes</h3>
<p>Harmonization of values reported for sand, silt and clay computed using methods of textural analysis that use definitions for particle size fractions different from the reference method will also have to be converted to the standard particle size definitions adopted for some international specifications. For example, classes in the texture triangle represent fractions for sand, silt and clay which can be assessed using the gravity point for the class (Tbl. @ref(tab:usdatexturec); see also further Fig. @ref(fig:plot-tt-triangle)).</p>
<table>
<caption>(#tab:usdatexturec)Simple conversion of the USDA texture-by-hand classes to texture fractions (sd indicates estimated standard deviation).</caption>
<thead>
<tr class="header">
<th align="right">Number</th>
<th align="left">Texture.class</th>
<th align="right">Sand</th>
<th align="right">Silt</th>
<th align="right">Clay</th>
<th align="right">Sand_sd</th>
<th align="right">Silt_sd</th>
<th align="right">Clay_sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="left">clay (C)</td>
<td align="right">22</td>
<td align="right">22</td>
<td align="right">56</td>
<td align="right">11.8</td>
<td align="right">9.8</td>
<td align="right">11.1</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">clay loam (CL)</td>
<td align="right">32</td>
<td align="right">34</td>
<td align="right">33</td>
<td align="right">7.0</td>
<td align="right">7.7</td>
<td align="right">3.5</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="left">loam (L)</td>
<td align="right">41</td>
<td align="right">39</td>
<td align="right">20</td>
<td align="right">6.8</td>
<td align="right">6.0</td>
<td align="right">5.1</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="left">loamy sand (LS)</td>
<td align="right">83</td>
<td align="right">11</td>
<td align="right">7</td>
<td align="right">3.8</td>
<td align="right">5.8</td>
<td align="right">3.2</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="left">sand (S)</td>
<td align="right">92</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="right">3.0</td>
<td align="right">3.0</td>
<td align="right">2.2</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="left">sandy clay (SC)</td>
<td align="right">51</td>
<td align="right">9</td>
<td align="right">40</td>
<td align="right">4.3</td>
<td align="right">4.4</td>
<td align="right">3.9</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="left">sandy clay loam (SCL)</td>
<td align="right">60</td>
<td align="right">14</td>
<td align="right">26</td>
<td align="right">7.9</td>
<td align="right">7.3</td>
<td align="right">4.2</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="left">silt (Si)</td>
<td align="right">7</td>
<td align="right">85</td>
<td align="right">9</td>
<td align="right">3.9</td>
<td align="right">3.2</td>
<td align="right">3.1</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="left">silty clay (SiC)</td>
<td align="right">7</td>
<td align="right">47</td>
<td align="right">46</td>
<td align="right">4.5</td>
<td align="right">4.7</td>
<td align="right">4.4</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="left">silty clay loam (SiCL)</td>
<td align="right">9</td>
<td align="right">58</td>
<td align="right">33</td>
<td align="right">5.7</td>
<td align="right">6.8</td>
<td align="right">3.5</td>
</tr>
<tr class="odd">
<td align="right">11</td>
<td align="left">silty loam (SiL)</td>
<td align="right">18</td>
<td align="right">64</td>
<td align="right">18</td>
<td align="right">10.9</td>
<td align="right">8.8</td>
<td align="right">6.5</td>
</tr>
<tr class="even">
<td align="right">12</td>
<td align="left">sandy loam (SL)</td>
<td align="right">67</td>
<td align="right">22</td>
<td align="right">12</td>
<td align="right">8.5</td>
<td align="right">10.2</td>
<td align="right">4.7</td>
</tr>
</tbody>
</table>
<p>Neither the <em>GlobalSoilMap</em> project nor GSIF has yet identified and selected specific functions to use to harmonize data produced using different analytical methods for any of the soil properties that are to be predicted and mapped. It is possible that a single globally-applicable default harmonisation function could potentially be identified for each of the methods of analysis for each of the soil properties selected for global analysis. However, this would require the current multitude of method definitions to be unambiguously defined and uniquely identified (IDx), and possibly grouped into aggregate classes, for subsequent conversion from IDx to IDy.</p>

<div class="rmdnote">
Soil observations, such as observation of texture by hand class, are often inexpensive, but rely on good expert knowledge skills. Statistical frameworks are needed that can use both highly precise and quick-and-inaccurate observations to generate better soil maps.
</div>

<p>We have previously noted that locally-specific harmonisation functions have consistently proven to be more effective than global ones and there is widespread agreement that there is generally no universal equation for converting from one method to another in all instances <span class="citation">(Konen et al. <a href="#ref-Konen2002SSSAJ">2002</a>; Meersmans, Van Wesemael, and Van Molle <a href="#ref-Meersmans2009SUM">2009</a>; Jankauskas et al. <a href="#ref-Jankauskas2006CSSPA">2006</a>; Jolivet, Arrouays, and Bernoux <a href="#ref-Jolivet1998CSSPA">1998</a>; De Vos et al. <a href="#ref-DeVos2007SUM">2007</a>)</span>. Consequently, there will likely be a need to develop locally relevant harmonisation functions at the continental or regional level that apply to restricted soil-landscape domains.</p>
<p><span class="citation">McBratney et al. (<a href="#ref-McBratney2002Geoderma">2002</a>)</span> proposed the concept of a <em>soil inference system</em> (SINFERS) that incorporated both expert soil knowledge and statistical prediction equations. The proposed system was intended to implement two major functions, namely:</p>
<ol style="list-style-type: decimal">
<li><p>Predict all soil properties using all possible (known) combinations of inputs and harmonisation functions.</p></li>
<li><p>Select the combination that leads to a prediction with the minimum variance.</p></li>
</ol>
</div>
</div>
<div id="soil-class-data" class="section level2">
<h2><span class="header-section-number">3.6</span> Soil class data</h2>
<div id="soil-types" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Soil types</h3>
<p>Soil types or soil classes are categories of soil bodies with similar soil properties and/or genesis and functions. There are three main approaches to soil classification <span class="citation">(Eswaran et al. <a href="#ref-eswaran2010soil">2010</a>; Buol et al. <a href="#ref-buol2011soil">2011</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><em>Classification of soils for the purpose of engineering</em> — Here the focus is put on predicting soil engineering properties and behaviors i.e. on physical and hydrological soil properties.</p></li>
<li><p><em>Descriptive classification of soil for the purpose of explaining the soil genesis</em> — Here the focus is put on soil morphology and pedogenesis i.e. functioning of the soil as part of an ecosystem. The representative soil types derived through morphological classification are often visualized as soil profiles or by using soil-depth functions.</p></li>
<li><p><em>Numerical or statistical classification of soils</em> — This is purely data-driven soil classification which can result in significant groupings of soil properties, but that then do not have any cognitive name and are difficult to visualize.</p></li>
</ol>
<p>Soil classification or soil taxonomy supports the transfer of soil information from one place, or individual, to another. Classifying soils can also often be very cost effective — if we identify the soil class correctly it is highly likely that we will be able to predict multiple additional soil properties that co-vary by soil type that would otherwise require significant resources to measure in the lab.</p>
<p>There are two major international soil taxonomic systems of primary interest for globla soil mapping: The USDA’s <em>Soil Taxonomy</em> <span class="citation">(U.S. Department of Agriculture <a href="#ref-agriculture2010keys">2014</a>)</span>, and the FAO’s <em>World Reference Base</em> <span class="citation">(IUSS Working Group WRB <a href="#ref-FAO2006WRB">2006</a>)</span>. Both KST and WRB are hierarchial, key-based morphological classification systems, but with increasingly more analytical data required to reach a specific, more refined, class. Mapping soil types, using WRB or KST or both, has been of interest for global soil mapping projects since the first development of the global classification systems. As a matter of interest, the term <em>“World soil map”</em> has been used exclusively for cartographic presentation of the global distribution of KST soil orders (12) and/or FAO WRB soil groups (32).</p>

<div class="rmdnote">
USDA’s Soil Taxonomy is probably the most developed soil classification system in the world. Using it for producing soil data is highly recommended also because all documents, databases and guidelines are publicly available.
</div>

<div class="figure" style="text-align: center">
<img src="figures/Fig_worldmap_suborders.png" alt="The USDA-NRCS map of the Keys to Soil Taxonomy soil suborders of the world at 20 km. The map shows the distribution of 12 soil orders. The original map contains assumed distributions also for suborders e.g. Histels, Udolls, Calcids, and similar. Projected in the Robinson projection commonly used to display world maps." width="100%" />
<p class="caption">
(#fig:worldmap-suborders)The USDA-NRCS map of the Keys to Soil Taxonomy soil suborders of the world at 20 km. The map shows the distribution of 12 soil orders. The original map contains assumed distributions also for suborders e.g. Histels, Udolls, Calcids, and similar. Projected in the Robinson projection commonly used to display world maps.
</p>
</div>
<p>(ref:barplot-suborders) Distribution of the USDA suborders shown in Fig. @ref(fig:worldmap-suborders).</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_barplot_suborders.png" alt="(ref:barplot-suborders)" width="100%" />
<p class="caption">
(#fig:barplot-suborders)(ref:barplot-suborders)
</p>
</div>
<p>Soil types can be mapped from field observations using statistically robust methods such as multinomial logistic regression as implemented in the nnet package for R <span class="citation">(Venables and Ripley <a href="#ref-Venables2002Springer">2002</a>)</span>. Theoretically, given a sufficient number and an adequate spatial distribution of field observed classes, multinomial logistic regression could even be used to map soil taxa at lower taxonomic levels with hundreds of unique taxonomic entities.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_USDA_categories.png" alt="USDA classification system and approximate minimum required number of observations to fit a global multinomial regression model." width="60%" angle=0 />
<p class="caption">
(#fig:usda-categories)USDA classification system and approximate minimum required number of observations to fit a global multinomial regression model.
</p>
</div>
<p>The map in Fig. @ref(fig:worldmap-suborders) shows the global distribution of Soil Taxonomy soil suborders according to <a href="https://www.nrcs.usda.gov/wps/portal/nrcs/detail/soils/use/worldsoils/?cid=nrcs142p2_054010">USDA-NRCS World Soil Index</a>. Assuming a rule of thumb that we need at least 5 and, if possible, 10 observations of a specific soil taxonomic entity per unique combination of predictor variables and observations, <span class="citation">(Harrell <a href="#ref-harrell2001regression">2001</a>)</span>, it is possible to estimate that the optimum number of field observations required to e.g. predict the global distribution of USDA soil series would be in the order of few millions of soil profiles (Fig. @ref(fig:usda-categories)).</p>
</div>
<div id="other-factor-type-variables" class="section level3">
<h3><span class="header-section-number">3.6.2</span> Other factor-type variables</h3>
<p>Pedometric / geostatistical methods can be used not only to predict the spatial distribution of soil types but also any other categorical soil variables. There are many soil categorical variables for which maps would be extremely useful for soil management and modelling. We list here some of the most well known / most widely used soil categorical variables:</p>
<ul>
<li><p><em>Diagnostic soil horizons</em> — Diagnostic soil horizons (e.g. Mollic or Gypsic horizon in the WRB system) are soil layers with specific soil properties commonly developed as a result of soil formation processes. They are much easier to detect in the field than soil types but are rarely mapped over entire areas. Diagnostic soil horizons can theoretically be mapped as 3D soil polygons or probabilities (rasters) attached to specific depths.</p></li>
<li><p><em>Soil material classes</em> — Soil horizons or whole profiles can be dominated by minerals or their combinations e.g. organic material in the soil, calcaric material, tephric material etc.</p></li>
<li><p><em>Munsell colour classes</em> — Soil in dry and/or wet condition can be described using some 1–2 thousand Munsell colour classes. Each Munsell colour class carries a lot of information about the soil <span class="citation">(Fernandez et al. <a href="#ref-fernandez1988color">1988</a>)</span>, so that a map of Munsell soil colour classes could be very useful for soil management.</p></li>
<li><p><em>Soil management zones</em> — Each unique combination of soil properties or types and management zones can be further expanded into a mixed classification system.</p></li>
<li><p><em>Land degradation classes</em> — Land degradation classes contain information about soil, but also about land cover and land use.</p></li>
</ul>
<p>As with any map, categorical, factor-type soil variables can be mapped globally (together with the uncertainty) as long as there is sufficient training field data to properly support application of the prediction algorithm. The other technical problem is the amount of storage required to save and share all the produced predictions. Each category of a soil categorical variable can be mapped separately, which can lead to hundreds of grids. The global land cover map for example contains only some 35 categories, so that it is relatively easy to distribute and use that GIS layer.</p>
</div>
</div>
<div id="importing-and-formatting-soil-data-in-r" class="section level2">
<h2><span class="header-section-number">3.7</span> Importing and formatting soil data in R</h2>
<div id="converting-texture-by-hand-classes-to-fractions" class="section level3">
<h3><span class="header-section-number">3.7.1</span> Converting texture-by-hand classes to fractions</h3>
<p>In the following example we look at how to convert texture-by-hand estimated classes to texture fractions i.e. sand, silt and clay content in %. We focus on the USDA texture-by-hand classes. There classes are embedded in the <a href="http://cran.r-project.org/web/packages/soiltexture/">soiltexture package</a> kindly contributed by Julien Moeys. The USDA texture triangle can be accessed by:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(soiltexture)
<span class="kw">TT.plot</span>(<span class="dt">class.sys =</span> <span class="st">&quot;USDA.TT&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Soil_variables_files/figure-html/plot-tt-triangle-1.png" alt="Soil texture triangle based on the USDA system. Generated using the soiltexture package (http://cran.r-project.org/web/packages/soiltexture/)." width="100%" />
<p class="caption">
(#fig:plot-tt-triangle)Soil texture triangle based on the USDA system. Generated using the soiltexture package (<a href="http://cran.r-project.org/web/packages/soiltexture/" class="uri">http://cran.r-project.org/web/packages/soiltexture/</a>).
</p>
</div>
<p>We can also print out a table with all class names and vertices numbers that defines each class:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">TT.classes.tbl</span>(<span class="dt">class.sys=</span><span class="st">&quot;USDA.TT&quot;</span>, <span class="dt">collapse=</span><span class="st">&quot;, &quot;</span>)
<span class="co">#&gt;       abbr     name              points                          </span>
<span class="co">#&gt;  [1,] &quot;Cl&quot;     &quot;clay&quot;            &quot;24, 1, 5, 6, 2&quot;                </span>
<span class="co">#&gt;  [2,] &quot;SiCl&quot;   &quot;silty clay&quot;      &quot;2, 6, 7&quot;                       </span>
<span class="co">#&gt;  [3,] &quot;SaCl&quot;   &quot;sandy clay&quot;      &quot;1, 3, 4, 5&quot;                    </span>
<span class="co">#&gt;  [4,] &quot;ClLo&quot;   &quot;clay loam&quot;       &quot;5, 4, 10, 11, 12, 6&quot;           </span>
<span class="co">#&gt;  [5,] &quot;SiClLo&quot; &quot;silty clay loam&quot; &quot;6, 12, 13, 7&quot;                  </span>
<span class="co">#&gt;  [6,] &quot;SaClLo&quot; &quot;sandy clay loam&quot; &quot;3, 8, 9, 10, 4&quot;                </span>
<span class="co">#&gt;  [7,] &quot;Lo&quot;     &quot;loam&quot;            &quot;10, 9, 16, 17, 11&quot;             </span>
<span class="co">#&gt;  [8,] &quot;SiLo&quot;   &quot;silty loam&quot;      &quot;11, 17, 22, 23, 18, 19, 13, 12&quot;</span>
<span class="co">#&gt;  [9,] &quot;SaLo&quot;   &quot;sandy loam&quot;      &quot;8, 14, 21, 22, 17, 16, 9&quot;      </span>
<span class="co">#&gt; [10,] &quot;Si&quot;     &quot;silt&quot;            &quot;18, 23, 26, 19&quot;                </span>
<span class="co">#&gt; [11,] &quot;LoSa&quot;   &quot;loamy sand&quot;      &quot;14, 15, 20, 21&quot;                </span>
<span class="co">#&gt; [12,] &quot;Sa&quot;     &quot;sand&quot;            &quot;15, 25, 20&quot;</span></code></pre></div>
<p>So knowing that the soil texture classes are defined geometrically, a logical estimate of the texture fractions from a class is to take the geometric centre of each polygon in the texture triangle. To estimate where the geometric centre is, we can for example use the functionality in the sp package. We start by creating a ‘’SpatialPolygons’’ object, for which we have to calculate coordinates in the xy space and bind polygons one by one:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">vert &lt;-<span class="st"> </span><span class="kw">TT.vertices.tbl</span>(<span class="dt">class.sys =</span> <span class="st">&quot;USDA.TT&quot;</span>)
vert<span class="op">$</span>x &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">-</span>vert<span class="op">$</span>SAND<span class="op">+</span>(vert<span class="op">$</span>SAND<span class="op">-</span>(<span class="dv">1</span><span class="op">-</span>vert<span class="op">$</span>SILT))<span class="op">*</span><span class="fl">0.5</span>
vert<span class="op">$</span>y &lt;-<span class="st"> </span>vert<span class="op">$</span>CLAY<span class="op">*</span><span class="kw">sin</span>(pi<span class="op">/</span><span class="dv">3</span>)
USDA.TT &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">TT.classes.tbl</span>(<span class="dt">class.sys =</span> <span class="st">&quot;USDA.TT&quot;</span>, <span class="dt">collapse =</span> <span class="st">&quot;, &quot;</span>))
TT.pnt &lt;-<span class="st"> </span><span class="kw">as.list</span>(<span class="kw">rep</span>(<span class="ot">NA</span>, <span class="kw">length</span>(USDA.TT<span class="op">$</span>name)))
poly.lst &lt;-<span class="st"> </span><span class="kw">as.list</span>(<span class="kw">rep</span>(<span class="ot">NA</span>, <span class="kw">length</span>(USDA.TT<span class="op">$</span>name)))</code></pre></div>
<p>next we strip the vertices and create a list of polygons:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sp)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(USDA.TT<span class="op">$</span>name)){
  TT.pnt[[i]] &lt;-<span class="st"> </span><span class="kw">as.integer</span>(<span class="kw">strsplit</span>(<span class="kw">unclass</span>(<span class="kw">paste</span>(USDA.TT[i, <span class="st">&quot;points&quot;</span>])), <span class="st">&quot;, &quot;</span>)[[<span class="dv">1</span>]])
  poly.lst[[i]] &lt;-<span class="st"> </span>vert[TT.pnt[[i]],<span class="kw">c</span>(<span class="st">&quot;x&quot;</span>,<span class="st">&quot;y&quot;</span>)]
  ## add extra point:
  pp &lt;-<span class="st"> </span><span class="kw">Polygon</span>(<span class="kw">rbind</span>(poly.lst[[i]], poly.lst[[i]][<span class="dv">1</span>,]))
  poly.lst[[i]] &lt;-<span class="st"> </span>sp<span class="op">::</span><span class="kw">Polygons</span>(<span class="kw">list</span>(pp), <span class="dt">ID=</span>i)
}</code></pre></div>
<p>and convert texture triangle to a spatial object:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">poly.sp &lt;-<span class="st"> </span><span class="kw">SpatialPolygons</span>(poly.lst, <span class="dt">proj4string=</span><span class="kw">CRS</span>(<span class="kw">as.character</span>(<span class="ot">NA</span>)))
poly.USDA.TT &lt;-<span class="st"> </span><span class="kw">SpatialPolygonsDataFrame</span>(poly.sp, 
                      <span class="kw">data.frame</span>(<span class="dt">ID=</span>USDA.TT<span class="op">$</span>name), <span class="dt">match.ID=</span><span class="ot">FALSE</span>)</code></pre></div>
<p>The resulting object now contains also slots of type ‘’labpt’‘which is exactly the geometric gravity point of the first polygon automatically derived by the’‘SpatialPolygons’’ function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">slot</span>(<span class="kw">slot</span>(poly.USDA.TT, <span class="st">&quot;polygons&quot;</span>)[[<span class="dv">1</span>]], <span class="st">&quot;labpt&quot;</span>)
<span class="co">#&gt; [1] 0.490 0.545</span></code></pre></div>
<p>Next we need to create a function that converts the xy coordinates (columns) in a texture triangle to texture fraction values. Let’s call this function <code>get.TF.from.XY</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">get.TF.from.XY &lt;-<span class="st"> </span><span class="cf">function</span>(df, xcoord, ycoord) {
  df<span class="op">$</span>CLAY &lt;-<span class="st"> </span>df[,ycoord]<span class="op">/</span><span class="kw">sin</span>(pi<span class="op">/</span><span class="dv">3</span>)
  df<span class="op">$</span>SAND &lt;-<span class="st"> </span>(<span class="dv">2</span> <span class="op">-</span><span class="st"> </span>df<span class="op">$</span>CLAY <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>df[,xcoord]) <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span>
  df<span class="op">$</span>SILT &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(df<span class="op">$</span>SAND <span class="op">+</span><span class="st"> </span>df<span class="op">$</span>CLAY)
  <span class="kw">return</span>(df)
}</code></pre></div>
<p>and now everything is ready to estimate the soil fractions based on a system of classes. For the case of the USDA classifications system we get:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">USDA.TT.cnt &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">t</span>(<span class="kw">sapply</span>(<span class="kw">slot</span>(poly.USDA.TT, <span class="st">&quot;polygons&quot;</span>), slot, <span class="st">&quot;labpt&quot;</span>)))
USDA.TT.cnt<span class="op">$</span>name &lt;-<span class="st"> </span>poly.USDA.TT<span class="op">$</span>ID
USDA.TT.cnt &lt;-<span class="st"> </span><span class="kw">get.TF.from.XY</span>(USDA.TT.cnt, <span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>)
USDA.TT.cnt[,<span class="kw">c</span>(<span class="st">&quot;SAND&quot;</span>,<span class="st">&quot;SILT&quot;</span>,<span class="st">&quot;CLAY&quot;</span>)] &lt;-<span class="st"> </span><span class="kw">signif</span>(USDA.TT.cnt[,<span class="kw">c</span>(<span class="st">&quot;SAND&quot;</span>,<span class="st">&quot;SILT&quot;</span>,<span class="st">&quot;CLAY&quot;</span>)], <span class="dv">2</span>)
USDA.TT.cnt[,<span class="kw">c</span>(<span class="st">&quot;name&quot;</span>,<span class="st">&quot;SAND&quot;</span>,<span class="st">&quot;SILT&quot;</span>,<span class="st">&quot;CLAY&quot;</span>)]
<span class="co">#&gt;               name  SAND  SILT  CLAY</span>
<span class="co">#&gt; 1             clay 0.200 0.180 0.630</span>
<span class="co">#&gt; 2       silty clay 0.067 0.470 0.470</span>
<span class="co">#&gt; 3       sandy clay 0.520 0.067 0.420</span>
<span class="co">#&gt; 4        clay loam 0.320 0.340 0.340</span>
<span class="co">#&gt; 5  silty clay loam 0.100 0.560 0.340</span>
<span class="co">#&gt; 6  sandy clay loam 0.600 0.130 0.270</span>
<span class="co">#&gt; 7             loam 0.410 0.400 0.190</span>
<span class="co">#&gt; 8       silty loam 0.210 0.650 0.130</span>
<span class="co">#&gt; 9       sandy loam 0.650 0.250 0.100</span>
<span class="co">#&gt; 10            silt 0.073 0.870 0.053</span>
<span class="co">#&gt; 11      loamy sand 0.820 0.120 0.058</span>
<span class="co">#&gt; 12            sand 0.920 0.050 0.033</span></code></pre></div>
<p>Now that we have created a function that converts values in the texture triangle to texture fractions, we can go further and even estimate the uncertainty of estimating each texture fraction based on the class. For this we can use simulations i.e. randomly sample 100 points within some texture class and then derive standard deviations for each texture fraction. Note that, although this sounds like a complicated operation, we can run this in two lines of code. For example to estimate uncertainty of converting the class ‘’Cl’‘(clay) to texture fractions we can simulate 100 random points the class polygon using the’‘spsample’’ function from the sp package <span class="citation">(Bivand, Pebesma, and Rubio <a href="#ref-Bivand2013Springer">2013</a>)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim.Cl &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">spsample</span>(poly.USDA.TT[poly.USDA.TT<span class="op">$</span>ID<span class="op">==</span><span class="st">&quot;clay&quot;</span>,], 
                              <span class="dt">type=</span><span class="st">&quot;random&quot;</span>, <span class="dt">n=</span><span class="dv">100</span>))
sim.Cl &lt;-<span class="st"> </span><span class="kw">get.TF.from.XY</span>(sim.Cl, <span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>)
<span class="kw">sd</span>(sim.Cl<span class="op">$</span>SAND); <span class="kw">sd</span>(sim.Cl<span class="op">$</span>SILT); <span class="kw">sd</span>(sim.Cl<span class="op">$</span>CLAY)
<span class="co">#&gt; [1] 0.123</span>
<span class="co">#&gt; [1] 0.113</span>
<span class="co">#&gt; [1] 0.142</span></code></pre></div>
<p>which means that we should not expect better precision of estimating the clay content based on class <code>Cl</code> than ±15%.</p>
<p>For some real soil profile data set we could also plot all texture fractions in the texture triangle to see how frequently one should expect some soil classes to appear:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(GSIF)
<span class="kw">data</span>(afsp)
tdf &lt;-<span class="st"> </span>afsp<span class="op">$</span>horizons[,<span class="kw">c</span>(<span class="st">&quot;CLYPPT&quot;</span>, <span class="st">&quot;SLTPPT&quot;</span>, <span class="st">&quot;SNDPPT&quot;</span>)]
tdf &lt;-<span class="st"> </span>tdf[<span class="op">!</span><span class="kw">is.na</span>(tdf<span class="op">$</span>SNDPPT)<span class="op">&amp;!</span><span class="kw">is.na</span>(tdf<span class="op">$</span>SLTPPT)<span class="op">&amp;!</span><span class="kw">is.na</span>(tdf<span class="op">$</span>CLYPPT),]
tdf &lt;-<span class="st"> </span>tdf[<span class="kw">runif</span>(<span class="kw">nrow</span>(tdf))<span class="op">&lt;</span>.<span class="dv">15</span>,]
tdf<span class="op">$</span>Sum &lt;-<span class="st"> </span><span class="kw">rowSums</span>(tdf)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">c</span>(<span class="st">&quot;CLYPPT&quot;</span>, <span class="st">&quot;SLTPPT&quot;</span>, <span class="st">&quot;SNDPPT&quot;</span>)) { tdf[,i] &lt;-<span class="st"> </span>tdf[,i]<span class="op">/</span>tdf<span class="op">$</span>Sum <span class="op">*</span><span class="st"> </span><span class="dv">100</span> }
<span class="kw">names</span>(tdf)[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;CLAY&quot;</span>, <span class="st">&quot;SILT&quot;</span>, <span class="st">&quot;SAND&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">TT.plot</span>(<span class="dt">class.sys =</span> <span class="st">&quot;USDA.TT&quot;</span>, <span class="dt">tri.data =</span> tdf, 
        <span class="dt">grid.show =</span> <span class="ot">FALSE</span>, <span class="dt">pch=</span><span class="st">&quot;+&quot;</span>, <span class="dt">cex=</span>.<span class="dv">4</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Soil_variables_files/figure-html/plot-tt-afsis-1.png" alt="Distribution of observed soil textures for the Africa Soil Profiles (http://gsif.r-forge.r-project.org/afsp.html)." width="100%" />
<p class="caption">
(#fig:plot-tt-afsis)Distribution of observed soil textures for the Africa Soil Profiles (<a href="http://gsif.r-forge.r-project.org/afsp.html" class="uri">http://gsif.r-forge.r-project.org/afsp.html</a>).
</p>
</div>
<p>This shows that not all positions in the triangle have the same prior probability. So probably a more sensitive way to estimate uncertainty of converting soil texture classes to fractions would be to run simulations using a density image showing the actual distribution of classes and then, by using the <code>rpoint</code> function in the <a href="http://spatstat.org">spatstat package</a>, we could also derive even more realistic conversions from texture-by-hand classes to texture fractions.</p>
</div>
</div>
<div id="converting-munsell-color-codes-to-other-color-systems" class="section level2">
<h2><span class="header-section-number">3.8</span> Converting Munsell color codes to other color systems</h2>
<p>In the next example we look at the Munsell color codes and conversion algorithms from a code to RGB and other color spaces. Munsell color codes can be matched with RGB values via the <a href="http://www.cis.rit.edu/mcsl/online/munsell.php">Munsell color codes conversion table</a>. You can load a table with 2350 entries from the GSIF dokuwiki:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;extdata/munsell_rgb.rdata&quot;</span>)
<span class="kw">library</span>(colorspace)
munsell.rgb[<span class="kw">round</span>(<span class="kw">runif</span>(<span class="dv">1</span>)<span class="op">*</span><span class="dv">2350</span>, <span class="dv">0</span>),]
<span class="co">#&gt;       Munsell  R  G  B</span>
<span class="co">#&gt; 2254 7.5Y_1_2 37 34 17</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">as</span>(colorspace<span class="op">::</span><span class="kw">RGB</span>(<span class="dt">R=</span>munsell.rgb[<span class="dv">1007</span>,<span class="st">&quot;R&quot;</span>]<span class="op">/</span><span class="dv">255</span>, 
                   <span class="dt">G=</span>munsell.rgb[<span class="dv">1007</span>,<span class="st">&quot;G&quot;</span>]<span class="op">/</span><span class="dv">255</span>, 
                   <span class="dt">B=</span>munsell.rgb[<span class="dv">1007</span>,<span class="st">&quot;B&quot;</span>]<span class="op">/</span><span class="dv">255</span>), <span class="st">&quot;HSV&quot;</span>)
<span class="co">#&gt;         H      S     V</span>
<span class="co">#&gt; [1,] 3.53 0.0798 0.835</span></code></pre></div>
<p>This shows that, for any given Munsell color code, it is relatively easy to convert them to any other color system available in R.</p>
<p>Within the R <a href="http://casoilresource.lawr.ucdavis.edu/drupal/node/201">package aqp</a> one can directly transform Munsell color codes to some color classes in R <span class="citation">(Beaudette, Roudier, and O’Geen <a href="#ref-Beaudette2013CompGeo">2013</a>)</span>. For example, to convert the Munsell color code to RGB values from the example above we would run:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">aqp<span class="op">::</span><span class="kw">munsell2rgb</span>(<span class="dt">the_hue =</span> <span class="st">&quot;10B&quot;</span>, <span class="dt">the_value =</span> <span class="dv">2</span>, <span class="dt">the_chroma =</span> <span class="dv">12</span>)
<span class="co">#&gt; [1] &quot;#003A7CFF&quot;</span></code></pre></div>
<p>Now the colors are coded in the <a href="http://en.wikipedia.org/wiki/Web_colors#Hex_triplet">hexadecimal format</a>, which is quite abstract but can be easily browsed via some web color table. To get the actual RGB values we would run:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">grDevices<span class="op">::</span><span class="kw">col2rgb</span>(<span class="st">&quot;#003A7CFF&quot;</span>)
<span class="co">#&gt;       [,1]</span>
<span class="co">#&gt; red      0</span>
<span class="co">#&gt; green   58</span>
<span class="co">#&gt; blue   124</span></code></pre></div>
<p>The hex triplet format is also very similar to the color format used in the <a href="https://developers.google.com/kml/documentation/kmlreference">KML reference</a>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">plotKML<span class="op">::</span><span class="kw">col2kml</span>(<span class="st">&quot;#003A7CFF&quot;</span>)
<span class="co">#&gt; [1] &quot;#ff7c3a00&quot;</span></code></pre></div>
<p>To plot the actual colors based on an actual soil profile data base we often need to prepare the color codes before we can run the conversion <span class="citation">(Rossel et al. <a href="#ref-VISCARRAROSSEL2006320">2006</a>)</span>. In the case of the <a href="http://gsif.r-forge.r-project.org/afsp.html">Africa Soil Profile Database</a>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(afsp)
<span class="kw">head</span>(afsp<span class="op">$</span>horizons[<span class="op">!</span><span class="kw">is.na</span>(afsp<span class="op">$</span>horizons<span class="op">$</span>MCOMNS),<span class="st">&quot;MCOMNS&quot;</span>])
<span class="co">#&gt; [1] 10YR3/3 10YR3/3 10YR3/3 10YR3/3 10YR3/3 10YR3/3</span>
<span class="co">#&gt; 289 Levels: 10BG4/1 10R2.5/1 10R2/1 10R2/2 10R3/2 10R3/3 10R3/4 ... N7/0</span></code></pre></div>
<p>the Munsell color codes have been prepared as text. Hence we need to spend some effort to separate hue from saturation and intensity before we can derive and plot actual colors. We start by merging the tables of interest so both coordinates and Munsell color codes are available in the same table:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mcol &lt;-<span class="st"> </span>plyr<span class="op">::</span><span class="kw">join</span>(afsp<span class="op">$</span>horizons[,<span class="kw">c</span>(<span class="st">&quot;SOURCEID&quot;</span>,<span class="st">&quot;MCOMNS&quot;</span>,<span class="st">&quot;UHDICM&quot;</span>,<span class="st">&quot;LHDICM&quot;</span>)],
                   afsp<span class="op">$</span>sites[,<span class="kw">c</span>(<span class="st">&quot;SOURCEID&quot;</span>,<span class="st">&quot;LONWGS84&quot;</span>,<span class="st">&quot;LATWGS84&quot;</span>)])
<span class="co">#&gt; Joining by: SOURCEID</span>
mcol &lt;-<span class="st"> </span>mcol[<span class="op">!</span><span class="kw">is.na</span>(mcol<span class="op">$</span>MCOMNS),]
<span class="kw">str</span>(mcol)
<span class="co">#&gt; &#39;data.frame&#39;:    31502 obs. of  6 variables:</span>
<span class="co">#&gt;  $ SOURCEID: Factor w/ 26270 levels &quot;100902&quot;,&quot;100903&quot;,..: 974 974 974 974 974 974 975 975 975 975 ...</span>
<span class="co">#&gt;  $ MCOMNS  : Factor w/ 289 levels &quot;10BG4/1&quot;,&quot;10R2.5/1&quot;,..: 40 40 40 40 40 40 23 23 23 23 ...</span>
<span class="co">#&gt;  $ UHDICM  : num  0 8 25 50 81 133 0 8 19 30 ...</span>
<span class="co">#&gt;  $ LHDICM  : num  8 25 50 81 133 160 8 19 30 50 ...</span>
<span class="co">#&gt;  $ LONWGS84: num  17.6 17.6 17.6 17.6 17.6 ...</span>
<span class="co">#&gt;  $ LATWGS84: num  -11 -11 -11 -11 -11 ...</span></code></pre></div>
<p>Next we need to format all Munsell color codes to ‘’Hue_Saturation_Intensity’’ format. We can incrementally replace the existing codes until all codes can be matched with the RGB table:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mcol<span class="op">$</span>Munsell &lt;-<span class="st"> </span><span class="kw">sub</span>(<span class="st">&quot; &quot;</span>, <span class="st">&quot;&quot;</span>, <span class="kw">sub</span>(<span class="st">&quot;/&quot;</span>, <span class="st">&quot;_&quot;</span>, mcol<span class="op">$</span>MCOMNS))
hue.lst &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="kw">c</span>(<span class="st">&quot;2.5&quot;</span>, <span class="st">&quot;5&quot;</span>, <span class="st">&quot;7.5&quot;</span>, <span class="st">&quot;10&quot;</span>),
                       <span class="kw">c</span>(<span class="st">&quot;YR&quot;</span>,<span class="st">&quot;GY&quot;</span>,<span class="st">&quot;BG&quot;</span>,<span class="st">&quot;YE&quot;</span>,<span class="st">&quot;YN&quot;</span>,<span class="st">&quot;YY&quot;</span>,<span class="st">&quot;R&quot;</span>,<span class="st">&quot;Y&quot;</span>,<span class="st">&quot;B&quot;</span>,<span class="st">&quot;G&quot;</span>))
hue.lst<span class="op">$</span>mhue &lt;-<span class="st"> </span><span class="kw">paste</span>(hue.lst<span class="op">$</span>Var1, hue.lst<span class="op">$</span>Var2, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)
<span class="cf">for</span>(j <span class="cf">in</span> hue.lst<span class="op">$</span>mhue[<span class="dv">1</span><span class="op">:</span><span class="dv">28</span>]){
  mcol<span class="op">$</span>Munsell &lt;-<span class="st"> </span><span class="kw">sub</span>(j, <span class="kw">paste</span>(j, <span class="st">&quot;_&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>), mcol<span class="op">$</span>Munsell, <span class="dt">fixed=</span><span class="ot">TRUE</span>)
}
mcol<span class="op">$</span>depth &lt;-<span class="st"> </span>mcol<span class="op">$</span>UHDICM <span class="op">+</span><span class="st"> </span>(mcol<span class="op">$</span>LHDICM<span class="op">-</span>mcol<span class="op">$</span>UHDICM)<span class="op">/</span><span class="dv">2</span>
mcol.RGB &lt;-<span class="st"> </span><span class="kw">merge</span>(mcol, munsell.rgb, <span class="dt">by=</span><span class="st">&quot;Munsell&quot;</span>)
<span class="kw">str</span>(mcol.RGB)
<span class="co">#&gt; &#39;data.frame&#39;:    11806 obs. of  11 variables:</span>
<span class="co">#&gt;  $ Munsell : chr  &quot;10R_2_2&quot; &quot;10R_2_2&quot; &quot;10R_2_2&quot; &quot;10R_2_2&quot; ...</span>
<span class="co">#&gt;  $ SOURCEID: Factor w/ 26270 levels &quot;100902&quot;,&quot;100903&quot;,..: 18724 18724 20331 18724 20331 20331 18724 9089 4859 23688 ...</span>
<span class="co">#&gt;  $ MCOMNS  : Factor w/ 289 levels &quot;10BG4/1&quot;,&quot;10R2.5/1&quot;,..: 4 4 4 4 4 4 4 5 5 5 ...</span>
<span class="co">#&gt;  $ UHDICM  : num  90 35 30 10 53 0 0 18 0 0 ...</span>
<span class="co">#&gt;  $ LHDICM  : num  135 90 53 35 98 30 10 24 15 5 ...</span>
<span class="co">#&gt;  $ LONWGS84: num  32.23 32.23 4.76 32.23 4.76 ...</span>
<span class="co">#&gt;  $ LATWGS84: num  -26.15 -26.15 8.79 -26.15 8.79 ...</span>
<span class="co">#&gt;  $ depth   : num  112.5 62.5 41.5 22.5 75.5 ...</span>
<span class="co">#&gt;  $ R       : int  67 67 67 67 67 67 67 91 91 91 ...</span>
<span class="co">#&gt;  $ G       : int  48 48 48 48 48 48 48 68 68 68 ...</span>
<span class="co">#&gt;  $ B       : int  45 45 45 45 45 45 45 63 63 63 ...</span></code></pre></div>
<p>Which allows us to plot the actual observed colors of the top soil (0–30 cm) for the whole of Africa:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mcol.RGB &lt;-<span class="st"> </span>mcol.RGB[<span class="op">!</span><span class="kw">is.na</span>(mcol.RGB<span class="op">$</span>R),]
mcol.RGB<span class="op">$</span>Rc &lt;-<span class="st"> </span><span class="kw">round</span>(mcol.RGB<span class="op">$</span>R<span class="op">/</span><span class="dv">255</span>, <span class="dv">3</span>)
mcol.RGB<span class="op">$</span>Gc &lt;-<span class="st"> </span><span class="kw">round</span>(mcol.RGB<span class="op">$</span>G<span class="op">/</span><span class="dv">255</span>, <span class="dv">3</span>)
mcol.RGB<span class="op">$</span>Bc &lt;-<span class="st"> </span><span class="kw">round</span>(mcol.RGB<span class="op">$</span>B<span class="op">/</span><span class="dv">255</span>, <span class="dv">3</span>)
mcol.RGB<span class="op">$</span>col &lt;-<span class="st"> </span><span class="kw">rgb</span>(mcol.RGB<span class="op">$</span>Rc, mcol.RGB<span class="op">$</span>Gc, mcol.RGB<span class="op">$</span>Bc)
mcol.RGB &lt;-<span class="st"> </span>mcol.RGB[mcol.RGB<span class="op">$</span>depth<span class="op">&gt;</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>mcol.RGB<span class="op">$</span>depth<span class="op">&lt;</span><span class="dv">30</span> <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span><span class="kw">is.na</span>(mcol.RGB<span class="op">$</span>col),]
<span class="kw">coordinates</span>(mcol.RGB) &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>LONWGS84<span class="op">+</span>LATWGS84</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;extdata/admin.af.rda&quot;</span>)
<span class="kw">proj4string</span>(admin.af) &lt;-<span class="st"> &quot;+proj=longlat +datum=WGS84&quot;</span>
<span class="co">#&gt; Warning in ReplProj4string(obj, CRS(value)): A new CRS was assigned to an object with an existing CRS:</span>
<span class="co">#&gt; +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0</span>
<span class="co">#&gt; without reprojecting.</span>
<span class="co">#&gt; For reprojection, use function spTransform</span>
country &lt;-<span class="st"> </span><span class="kw">as</span>(admin.af, <span class="st">&quot;SpatialLines&quot;</span>)
<span class="kw">par</span>(<span class="dt">mar=</span><span class="kw">c</span>(.<span class="dv">0</span>,.<span class="dv">0</span>,.<span class="dv">0</span>,.<span class="dv">0</span>), <span class="dt">mai=</span><span class="kw">c</span>(.<span class="dv">0</span>,.<span class="dv">0</span>,.<span class="dv">0</span>,.<span class="dv">0</span>))
<span class="kw">plot</span>(country, <span class="dt">col=</span><span class="st">&quot;darkgrey&quot;</span>, <span class="dt">asp=</span><span class="dv">1</span>)
<span class="kw">points</span>(mcol.RGB, <span class="dt">pch=</span><span class="dv">21</span>, <span class="dt">bg=</span>mcol.RGB<span class="op">$</span>col, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Soil_variables_files/figure-html/plot-af-soil-cols-1.png" alt="Actual observed soil colors (moist) for the top soil based on the Africa Soil Profiles Database (http://gsif.r-forge.r-project.org/afsp.html)." width="100%" />
<p class="caption">
(#fig:plot-af-soil-cols)Actual observed soil colors (moist) for the top soil based on the Africa Soil Profiles Database (<a href="http://gsif.r-forge.r-project.org/afsp.html" class="uri">http://gsif.r-forge.r-project.org/afsp.html</a>).
</p>
</div>
<p>Finally, via the plotKML package you can also plot the actual colors of horizons by converting tables to SoilProfileCollection class in the <a href="http://cran.r-project.org/web/packages/aqp/">aqp package</a>. Consider this soil profile from Nigeria:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(plyr)
<span class="kw">library</span>(aqp)
<span class="co">#&gt; This is aqp 1.16-3</span>
lon =<span class="st"> </span><span class="fl">3.90</span>; lat =<span class="st"> </span><span class="fl">7.50</span>; id =<span class="st"> &quot;ISRIC:NG0017&quot;</span>; FAO1988 =<span class="st"> &quot;LXp&quot;</span>
top =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">18</span>, <span class="dv">36</span>, <span class="dv">65</span>, <span class="dv">87</span>, <span class="dv">127</span>)
bottom =<span class="st"> </span><span class="kw">c</span>(<span class="dv">18</span>, <span class="dv">36</span>, <span class="dv">65</span>, <span class="dv">87</span>, <span class="dv">127</span>, <span class="dv">181</span>)
ORCDRC =<span class="st"> </span><span class="kw">c</span>(<span class="fl">18.4</span>, <span class="fl">4.4</span>, <span class="fl">3.6</span>, <span class="fl">3.6</span>, <span class="fl">3.2</span>, <span class="fl">1.2</span>)
hue =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;7.5YR&quot;</span>, <span class="st">&quot;7.5YR&quot;</span>, <span class="st">&quot;2.5YR&quot;</span>, <span class="st">&quot;5YR&quot;</span>, <span class="st">&quot;5YR&quot;</span>, <span class="st">&quot;10YR&quot;</span>)
value =<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">7</span>); chroma =<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">3</span>)
## prepare a SoilProfileCollection:
prof1 &lt;-<span class="st"> </span>plyr<span class="op">::</span><span class="kw">join</span>(<span class="kw">data.frame</span>(id, top, bottom, ORCDRC, hue, value, chroma),  
              <span class="kw">data.frame</span>(id, lon, lat, FAO1988), <span class="dt">type=</span><span class="st">&#39;inner&#39;</span>)
<span class="co">#&gt; Joining by: id</span>
prof1<span class="op">$</span>soil_color &lt;-<span class="st"> </span><span class="kw">with</span>(prof1, aqp<span class="op">::</span><span class="kw">munsell2rgb</span>(hue, value, chroma))
<span class="co">#&gt; Notice: converting hue to character</span>
<span class="kw">depths</span>(prof1) &lt;-<span class="st"> </span>id <span class="op">~</span><span class="st"> </span>top <span class="op">+</span><span class="st"> </span>bottom
<span class="co">#&gt; Warning: converting IDs from factor to character</span>
<span class="kw">site</span>(prof1) &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>lon <span class="op">+</span><span class="st"> </span>lat <span class="op">+</span><span class="st"> </span>FAO1988
<span class="kw">coordinates</span>(prof1) &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>lon <span class="op">+</span><span class="st"> </span>lat
<span class="kw">proj4string</span>(prof1) &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="st">&quot;+proj=longlat +datum=WGS84&quot;</span>)
prof1
<span class="co">#&gt; Object of class SoilProfileCollection</span>
<span class="co">#&gt; Number of profiles: 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Horizon attributes:</span>
<span class="co">#&gt;             id top bottom ORCDRC   hue value chroma soil_color</span>
<span class="co">#&gt; 1 ISRIC:NG0017   0     18   18.4 7.5YR     3      2  #584537FF</span>
<span class="co">#&gt; 2 ISRIC:NG0017  18     36    4.4 7.5YR     4      4  #7E5A3BFF</span>
<span class="co">#&gt; 3 ISRIC:NG0017  36     65    3.6 2.5YR     5      6  #A96C4FFF</span>
<span class="co">#&gt; 4 ISRIC:NG0017  65     87    3.6   5YR     5      8  #B06A32FF</span>
<span class="co">#&gt; 5 ISRIC:NG0017  87    127    3.2   5YR     5      4  #9A7359FF</span>
<span class="co">#&gt; 6 ISRIC:NG0017 127    181    1.2  10YR     7      3  #C4AC8CFF</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Sampling site attributes:</span>
<span class="co">#&gt;             id FAO1988</span>
<span class="co">#&gt; 1 ISRIC:NG0017     LXp</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Spatial Data:</span>
<span class="co">#&gt;     min max</span>
<span class="co">#&gt; lon 3.9 3.9</span>
<span class="co">#&gt; lat 7.5 7.5</span>
<span class="co">#&gt; [1] &quot;+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0&quot;</span></code></pre></div>
<p>Once an object is in the format of ‘’SoilProfileCollection’’ it can be directly plotted in Google Earth via the generic plotKML command:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plotKML</span>(prof1, <span class="dt">var.name=</span><span class="st">&quot;ORCDRC&quot;</span>, <span class="dt">color.name=</span><span class="st">&quot;soil_color&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/soil_profile_plot.png" alt="Soil profile from Nigeria plotted in Google Earth with actual observed colors." width="70%" />
<p class="caption">
(#fig:soil-profile-plot)Soil profile from Nigeria plotted in Google Earth with actual observed colors.
</p>
</div>
</div>
<div id="mla-ptfs" class="section level2">
<h2><span class="header-section-number">3.9</span> Using Machine Learning to build Pedo-Transfer-Functions</h2>
<div id="ptf-for-bulk-density" class="section level3">
<h3><span class="header-section-number">3.9.1</span> PTF for Bulk Density</h3>
<p>In the following examples we look at possibilities of using <a href="wiki/soilmapping_using_mla">Machine Learning</a> to predict soil properties and classes from other soil properties and classes. In the first example, we try to build a Pedo-Transfer-Function (PTF) to predict bulk density using soil properties such as organic carbon content, soil texture and coarse fragments. Bulk density is often available only for a part of soil profiles, so if we could use a PTF to fill in all gaps in bulk density, then most likely we would not need to omit BD from further analysis. For testing PTFs to predict bulk density from other soil properties we will use a subset of the ISRIC WISE soil profile data set <span class="citation">(Batjes <a href="#ref-Batjes2009SUM">2009</a>)</span>, which can be loaded from:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(randomForestSRC)
<span class="co">#&gt; </span>
<span class="co">#&gt;  randomForestSRC 2.7.0 </span>
<span class="co">#&gt;  </span>
<span class="co">#&gt;  Type rfsrc.news() to see new features, changes, and bug fixes. </span>
<span class="co">#&gt; </span>
<span class="kw">library</span>(ggRandomForests)
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: &#39;ggRandomForests&#39;</span>
<span class="co">#&gt; The following object is masked from &#39;package:randomForestSRC&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     partial.rfsrc</span>
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(scales)
<span class="kw">load</span>(<span class="st">&quot;extdata/sprops.wise.rda&quot;</span>)
<span class="kw">str</span>(SPROPS.WISE)
<span class="co">#&gt; &#39;data.frame&#39;:    47833 obs. of  17 variables:</span>
<span class="co">#&gt;  $ SOURCEID: Factor w/ 10253 levels &quot;AF0001&quot;,&quot;AF0002&quot;,..: 1 1 1 2 2 2 2 3 3 3 ...</span>
<span class="co">#&gt;  $ SAMPLEID: chr  &quot;AF0001_1&quot; &quot;AF0001_2&quot; &quot;AF0001_3&quot; &quot;AF0002_1&quot; ...</span>
<span class="co">#&gt;  $ UHDICM  : int  0 15 60 0 20 60 110 0 20 50 ...</span>
<span class="co">#&gt;  $ LHDICM  : int  15 60 150 20 60 110 170 20 50 110 ...</span>
<span class="co">#&gt;  $ DEPTH   : num  7.5 37.5 105 10 40 85 140 10 35 80 ...</span>
<span class="co">#&gt;  $ CRFVOL  : int  20 NA NA NA NA NA NA NA NA NA ...</span>
<span class="co">#&gt;  $ CECSUM  : num  NA NA NA NA NA NA NA NA NA NA ...</span>
<span class="co">#&gt;  $ SNDPPT  : int  40 10 10 40 15 10 40 40 65 60 ...</span>
<span class="co">#&gt;  $ CLYPPT  : int  20 35 35 20 20 35 20 20 10 25 ...</span>
<span class="co">#&gt;  $ BLD     : num  NA NA NA NA NA NA NA NA NA NA ...</span>
<span class="co">#&gt;  $ SLTPPT  : int  40 55 55 40 65 55 40 40 25 15 ...</span>
<span class="co">#&gt;  $ PHIHOX  : num  7.9 7.9 7.9 8.5 8.6 8.5 8.8 8.8 9.2 8.9 ...</span>
<span class="co">#&gt;  $ PHIKCL  : num  NA NA NA NA NA NA NA NA NA NA ...</span>
<span class="co">#&gt;  $ ORCDRC  : num  7.6 2.3 0.9 12.8 6 3.9 2.7 5.9 2.4 NA ...</span>
<span class="co">#&gt;  $ LONWGS84: num  69.2 69.2 69.2 69.2 69.2 ...</span>
<span class="co">#&gt;  $ LATWGS84: num  34.5 34.5 34.5 34.5 34.5 34.5 34.5 34.5 34.5 34.5 ...</span>
<span class="co">#&gt;  $ SOURCEDB: chr  &quot;WISE&quot; &quot;WISE&quot; &quot;WISE&quot; &quot;WISE&quot; ...</span></code></pre></div>
<p>For model fitting we will use the <a href="https://cran.r-project.org/package=randomForestSRC">randomForestSRC</a> package, which is a robust implementation of random forest algorithm with options for parallelization and visualization of model outputs:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bd.fm =<span class="st"> </span><span class="kw">as.formula</span>(<span class="st">&quot;BLD ~ ORCDRC + PHIHOX + SNDPPT + CLYPPT + CRFVOL + DEPTH&quot;</span>)
rfsrc_BD &lt;-<span class="st"> </span><span class="kw">rfsrc</span>(bd.fm, <span class="dt">data=</span>SPROPS.WISE)
rfsrc_BD
<span class="co">#&gt;                          Sample size: 3330</span>
<span class="co">#&gt;                      Number of trees: 1000</span>
<span class="co">#&gt;            Forest terminal node size: 5</span>
<span class="co">#&gt;        Average no. of terminal nodes: 685</span>
<span class="co">#&gt; No. of variables tried at each split: 2</span>
<span class="co">#&gt;               Total no. of variables: 6</span>
<span class="co">#&gt;        Resampling used to grow trees: swr</span>
<span class="co">#&gt;     Resample size used to grow trees: 3330</span>
<span class="co">#&gt;                             Analysis: RF-R</span>
<span class="co">#&gt;                               Family: regr</span>
<span class="co">#&gt;                       Splitting rule: mse *random*</span>
<span class="co">#&gt;        Number of random split points: 10</span>
<span class="co">#&gt;                 % variance explained: 39.6</span>
<span class="co">#&gt;                           Error rate: 46370</span></code></pre></div>
<p>which shows that the model explains about 40% with an RMSE of ±200 kg/m<span class="math inline">\(^3\)</span>. Although the MSE is relatively high, it least can be used to fill in the missing values for BD which can be significant. We can plot the partial plots between the target variable and all covariates by using:</p>
<div class="figure" style="text-align: center">
<img src="figures/bulk_density_ptf_plots.png" alt="Bulk density as a function of organic carbon, pH, sand and clay content coarse fragments and depth." width="100%" />
<p class="caption">
(#fig:bulk-density-ptf)Bulk density as a function of organic carbon, pH, sand and clay content coarse fragments and depth.
</p>
</div>
<p>Obviously the key to explaining bulk density is soil organic carbon, while depth and pH are the 2nd and 3rd most important covariates. Using this MLA-based model we can predict bulk density for various combinations of soil properties:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(rfsrc_BD, <span class="kw">data.frame</span>(<span class="dt">ORCDRC=</span><span class="fl">1.2</span>, <span class="dt">PHIHOX=</span><span class="fl">7.6</span>, 
                  <span class="dt">SNDPPT=</span><span class="dv">45</span>, <span class="dt">CLYPPT=</span><span class="dv">12</span>, <span class="dt">CRFVOL=</span><span class="dv">0</span>, <span class="dt">DEPTH=</span><span class="dv">20</span>))<span class="op">$</span>predicted
<span class="co">#&gt; [1] 1548</span></code></pre></div>
<p>and a soil with higher organic carbon content:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(rfsrc_BD, <span class="kw">data.frame</span>(<span class="dt">ORCDRC=</span><span class="dv">150</span>, <span class="dt">PHIHOX=</span><span class="fl">4.6</span>, 
                  <span class="dt">SNDPPT=</span><span class="dv">25</span>, <span class="dt">CLYPPT=</span><span class="dv">35</span>, <span class="dt">CRFVOL=</span><span class="dv">0</span>, <span class="dt">DEPTH=</span><span class="dv">20</span>))<span class="op">$</span>predicted
<span class="co">#&gt; [1] 906</span></code></pre></div>
</div>
<div id="ptf-for-correlating-classification-systems" class="section level3">
<h3><span class="header-section-number">3.9.2</span> PTF for correlating classification systems</h3>
<p>In the second example we use ISRIC WISE to to build a correlation function to translate soil classes from one classification system to the other. The training data can be loaded from:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;extdata/wise_tax.rda&quot;</span>)
<span class="kw">str</span>(WISE_tax)
<span class="co">#&gt; &#39;data.frame&#39;:    8189 obs. of  7 variables:</span>
<span class="co">#&gt;  $ SOURCEID: Factor w/ 8189 levels &quot;AF0001&quot;,&quot;AF0002&quot;,..: 1 2 3 4 5 6 7 8 9 10 ...</span>
<span class="co">#&gt;  $ LATWGS84: num  34.5 34.5 34.5 34.3 32.4 ...</span>
<span class="co">#&gt;  $ LONWGS84: num  69.2 69.2 69.2 61.4 62.1 ...</span>
<span class="co">#&gt;  $ TAXNWRB : Factor w/ 146 levels &quot;#N/A&quot;,&quot;Albic Arenosol&quot;,..: 104 9 9 72 17 16 122 49 8 9 ...</span>
<span class="co">#&gt;  $ TAXOUSDA: Factor w/ 1728 levels &quot;&quot;,&quot; Calciorthid&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</span>
<span class="co">#&gt;  $ LFORM   : chr  &quot;LV&quot; &quot;LV&quot; &quot;LV&quot; &quot;LV&quot; ...</span>
<span class="co">#&gt;  $ LANDUS  : chr  &quot;AA4&quot; &quot;AA6&quot; &quot;AA6&quot; &quot;AA4&quot; ...</span></code></pre></div>
<p>we also need to get the <em>cleaned</em> legend for USDA classification:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">leg &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;extdata/taxousda_greatgroups.csv&quot;</span>)
<span class="kw">str</span>(leg)
<span class="co">#&gt; &#39;data.frame&#39;:    434 obs. of  4 variables:</span>
<span class="co">#&gt;  $ Great_Group: Factor w/ 434 levels &quot;Acraquox&quot;,&quot;Acrohumox&quot;,..: 9 57 77 112 121 145 170 259 286 301 ...</span>
<span class="co">#&gt;  $ Suborder   : Factor w/ 79 levels &quot;Albolls&quot;,&quot;Andepts&quot;,..: 4 4 4 4 4 4 4 4 4 4 ...</span>
<span class="co">#&gt;  $ Order      : Factor w/ 12 levels &quot;Alfisols&quot;,&quot;Andisols&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</span>
<span class="co">#&gt;  $ TAX        : Factor w/ 434 levels &quot;Alfisols_Aqualfs_Albaqualfs&quot;,..: 1 2 3 4 5 6 7 8 9 10 ...</span></code></pre></div>
<p>Our objective is to develop a function to translate WRB classes into USDA classes with help of some soil properties. We can try to add soil pH and clay content to increase the accuracy of the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x.PHIHOX &lt;-<span class="st"> </span><span class="kw">aggregate</span>(SPROPS.WISE<span class="op">$</span>PHIHOX, 
                      <span class="dt">by=</span><span class="kw">list</span>(SPROPS.WISE<span class="op">$</span>SOURCEID), 
                      <span class="dt">FUN=</span>mean, <span class="dt">na.rm=</span><span class="ot">TRUE</span>); <span class="kw">names</span>(x.PHIHOX)[<span class="dv">1</span>] =<span class="st"> &quot;SOURCEID&quot;</span>
x.CLYPPT &lt;-<span class="st"> </span><span class="kw">aggregate</span>(SPROPS.WISE<span class="op">$</span>CLYPPT, 
                      <span class="dt">by=</span><span class="kw">list</span>(SPROPS.WISE<span class="op">$</span>SOURCEID), 
                      <span class="dt">FUN=</span>mean, <span class="dt">na.rm=</span><span class="ot">TRUE</span>); <span class="kw">names</span>(x.CLYPPT)[<span class="dv">1</span>] =<span class="st"> &quot;SOURCEID&quot;</span>
WISE_tax<span class="op">$</span>PHIHOX &lt;-<span class="st"> </span>plyr<span class="op">::</span><span class="kw">join</span>(WISE_tax, x.PHIHOX, <span class="dt">type=</span><span class="st">&quot;left&quot;</span>)<span class="op">$</span>x
<span class="co">#&gt; Joining by: SOURCEID</span>
WISE_tax<span class="op">$</span>CLYPPT &lt;-<span class="st"> </span>plyr<span class="op">::</span><span class="kw">join</span>(WISE_tax, x.CLYPPT, <span class="dt">type=</span><span class="st">&quot;left&quot;</span>)<span class="op">$</span>x
<span class="co">#&gt; Joining by: SOURCEID</span></code></pre></div>
<p>After that we need to cleanup the classes so that we can focus on USDA suborders only:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sel.tax =<span class="st"> </span><span class="kw">complete.cases</span>(WISE_tax[,<span class="kw">c</span>(<span class="st">&quot;TAXNWRB&quot;</span>,<span class="st">&quot;PHIHOX&quot;</span>,<span class="st">&quot;CLYPPT&quot;</span>,<span class="st">&quot;TAXOUSDA&quot;</span>)])
WISE_tax.sites &lt;-<span class="st"> </span>WISE_tax[sel.tax,]
WISE_tax.sites<span class="op">$</span>TAXOUSDA.f &lt;-<span class="st"> </span><span class="ot">NA</span>
<span class="cf">for</span>(j <span class="cf">in</span> leg<span class="op">$</span>Suborder){
  sel &lt;-<span class="st"> </span><span class="kw">grep</span>(j, WISE_tax.sites<span class="op">$</span>TAXOUSDA, <span class="dt">ignore.case=</span><span class="ot">TRUE</span>)
  WISE_tax.sites<span class="op">$</span>TAXOUSDA.f[sel] =<span class="st"> </span>j
}
WISE_tax.sites<span class="op">$</span>TAXOUSDA.f &lt;-<span class="st"> </span><span class="kw">as.factor</span>(WISE_tax.sites<span class="op">$</span>TAXOUSDA.f)
WISE_tax.sites<span class="op">$</span>TAXNWRB &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">paste</span>(WISE_tax.sites<span class="op">$</span>TAXNWRB))</code></pre></div>
<p>and finally we can fit a model to translate WRB profiles to USDA suborders:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TAXNUSDA.rf &lt;-<span class="st"> </span><span class="kw">rfsrc</span>(TAXOUSDA.f <span class="op">~</span><span class="st"> </span>TAXNWRB <span class="op">+</span><span class="st"> </span>PHIHOX <span class="op">+</span><span class="st"> </span>CLYPPT, <span class="dt">data=</span>WISE_tax.sites)
<span class="co">#TAXNUSDA.rf</span></code></pre></div>
<p>which shows that the average accuracy is about 45%. We can test converting some classes with the help of additional soil properties:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newdata =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">TAXNWRB=</span><span class="kw">factor</span>(<span class="st">&quot;Calcaric Cambisol&quot;</span>, 
                  <span class="dt">levels=</span><span class="kw">levels</span>(WISE_tax.sites<span class="op">$</span>TAXNWRB)), 
                  <span class="dt">PHIHOX=</span><span class="fl">7.8</span>, <span class="dt">CLYPPT=</span><span class="dv">12</span>)
x &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">predict</span>(TAXNUSDA.rf, newdata, <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)<span class="op">$</span>predicted)
x[,<span class="kw">order</span>(<span class="dv">1</span><span class="op">/</span>x)[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]]
<span class="co">#&gt;   Ochrepts Orthids</span>
<span class="co">#&gt; 1    0.288   0.154</span></code></pre></div>
<p>so for example, the two most likely classes to equate to Calcaric Cambisols seem to be Ochrepts and Orthids, which is not that much different from correlation classes reported in <span class="citation">Krasilnikov et al. (<a href="#ref-Krasilnikov2009handbook">2009</a>)</span> in fact.</p>
</div>
</div>
<div id="summary-points" class="section level2">
<h2><span class="header-section-number">3.10</span> Summary points</h2>
<p>In this chapter, we have endeavoured to provide precise and explicit descriptions of the soil properties and soil classes of interest to current PSM activities. For each soil property (or class) we have provided an explanation for why that property (or class) is of interest to users and why it has been selected to be mapped. In many cases, the most obvious reason is that the soil property is widely recorded and reported in legacy soil profile data bases and is therefore available. But these soil properties are widely reported for good reasons, mainly because they have been found to be important to consider when managing land or making decisions about land capability or use. We have defined the spatial attributes of the soil properties mapped at various scales, defined a standard reference (analysis) method for each soil property, provided information on the units, precision and range of values used to describe each mapped soil property and reviewed problems and opportunities related to harmonization of soil property values contained in legacy soil profile databases that have been analysed using different methods of analysis.</p>
<p>It should be noted that, in this chapter, we have emphasized the use of existing, or legacy, soil profile data to provide the evidence used to construct predicton models for PSM. This emphasis reflects the reality that, for most of the world, legacy soil profile data is all that we have to work with, at the present time, and all that we can reasonably expect to obtain for the foreseeable future. Many of the issues and challenges related to use and harmonization of legacy soil profile data discussed in this chapter will hopefully be of less importance as newer, contemporary data are collected in the field and analysed in the laboratory using more robust, statistically valid and reproduceable methods (e.g. spectroscopy). In the meantime, standardization and harmonization of legacy soil profile data will continue to present a challenge for global to regional PSM.</p>
<p>One attractive option for harmonizing soil analytical data following the SINFER concept would be to create and maintain a Global Soil Reference Library (GSRL). This concept is further discussed in the final chapter. Such a library would need to include data for a significant number of soils from each continent or region. Each soil would be analysed for all properties of interest using all commonly used methods of analysis. Values for a soil property for any soil analysed by a given method could be converted into equivalent values in any other analytical method (as long as data analysed by both methods were included in the GSRL) by developing pedo-transfer (or conversion) functions using the fully analysed samples in the conversion library. In particular, some variation of the similarity approach described by <span class="citation">Jagtap et al. (<a href="#ref-Jagtap2004TASAE">2004</a>)</span> for avaialble water capacity and <span class="citation">Nemes et al. (<a href="#ref-Nemes1999G">1999</a>)</span> for harmonization of particle size data could be implemented to harmonize all types of soil property values anywhere in the world. The value of the soil property in the desired reference method could be estimated by finding the soil or soils in the reference library that were most similar to the soil for which harmonization was required and then using the value of the soil (or soils) in the desired reference method as the predicted harmonized value. If several similar soils were identified, as is done by <span class="citation">Nemes et al. (<a href="#ref-Nemes1999G">1999</a>)</span>, then the predicted harmonized value would be computed as the weighted mean, in the appropriate reference method, of all similar soils; with weights selected according to the similarity of the soils in the conversion library to the soil being harmonized.</p>
<p>What a GSRL would do, in effect, is to provide a single, centralized framework for harmonization and conversion of soil property values. It would do this using a database of reference soils analysed for all soil properties of interest by all major analytical methods. These fully analysed reference soils would provide a framework for computing individualized, locally relevant conversion or pedo-transfer functions in a consistent and standardized manner. Consequently, global soil mapping would benefit from having the best of both worlds, namely locally-specific harmonization functions (which are known to be most effective) and also ones that were computed everywhere in a single standardized manner using data in a single comprehensive reference database (which is desirable in terms of simplifying harmonization and maintaining a record of how any value was harmonized).</p>
<p>Over time, we expect to see progress made in developing, applying and documenting harmonization methods so that the values for any given soil property used to create predictive models for global soil property mapping are fully harmonized and roughly equivalent for all input data sets. In the shorter term, it is likely that the accuracy of global predictions will be reduced because of weak, inconsistent or completely absent efforts to harmonize soil property values produced using different analytical methods. In the longer term, we hope, and expect, that data collected in the future, as we move forward, will benefit from adoption of methods of data collection and analysis that are more systematic, more reproduceable, more accurate and more interchangeable. These improvements should reduce the need for harmonization and standardization and should make use of soil observation and measurement data easier and more consistent.</p>
<!--chapter:end:Soil_variables.Rmd-->
</div>
</div>
<div id="soil-covs-chapter" class="section level1">
<h1><span class="header-section-number">4</span> Preparation of soil covariates for soil mapping</h1>
<p><em>Edited by: T. Hengl</em></p>
<div id="soil-covariate-data-sources" class="section level2">
<h2><span class="header-section-number">4.1</span> Soil covariate data sources</h2>
<div id="types-of-soil-covariates" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Types of soil covariates</h3>
<p>Soils (and vegetation + ecosystem) form under complex interactions between climate, living organism and anthropogenic influences, modified by relief and hydrological processes and run through long periods of time. This has been clearly identified first by <span class="citation">Jenny (<a href="#ref-jenny1994factors">1994</a>)</span> with his CLORPT factors of soil formation and later on by <span class="citation">McBratney, Mendonça Santos, and Minasny (<a href="#ref-MCBRATNEY20033">2003</a>)</span> with the SCORPAN formulation (see <a href="#soil-mapping-theory">Introduction chapter</a>).</p>
<p>In general, the following list of covariates are commonly used in Predictive Soil Mapping:</p>
<ol style="list-style-type: decimal">
<li>Climate related covariates include:</li>
</ol>
<ul>
<li>temperature maps,</li>
<li>precipitation maps,</li>
<li>snow cover maps,</li>
<li>potential evapotranspiration,</li>
<li>cloud fraction and other atmospheric images,</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Vegetation and Living Organisms include:</li>
</ol>
<ul>
<li>vegetation indices e.g. FAPAR (mean, median), NDVI, EVI,</li>
<li>biomass, Leaf Area Index,</li>
<li>land cover type maps,</li>
<li>vegetation types and communities (if mapped at high accuracy),</li>
<li>land cover,</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Relief and Topography covariates include:</li>
</ol>
<ul>
<li>standard window calculations e.g. slope, curvatures, standard deviation,</li>
<li>standard flow model outputs,</li>
<li>landform classes / landform class likelihoods,</li>
<li>hydrological / soil accumulation and deposition indices — MRVBFI, MRRTFI, Wetness index, height above channel, height below ridge, horizontal distance to channel, horizontal distance to ridge,</li>
<li>climatic and micro-climatic indices determined by relief — incoming solar insolation and similar,</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Parent material / geologic Material covariates include:</li>
</ol>
<ul>
<li>bedrock type and age,</li>
<li>bedrock mineralogy (acid, basic),</li>
<li>surface material type, texture, age, mineralogy, thickness,</li>
<li>volcanic activity, historic earthquake density,</li>
<li>seismic activity level,</li>
<li>gamma Ray Spectroscopy grids,</li>
<li>gravity measurements,</li>
<li>electrical conductivity/resistance,</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>Estimated geological age of surface:</li>
</ol>
<ul>
<li>bedrock age / surface material age,</li>
<li>recent disturbance age,</li>
</ul>
<ol start="6" style="list-style-type: decimal">
<li>Spatial position or spatial context</li>
</ol>
<ul>
<li>Latitude and Longitude,</li>
<li>distance to nearest large ocean</li>
<li>Northing — distance to north pole,</li>
<li>Southing — distance to south pole,</li>
<li>Easting — distance to east,</li>
<li>Westing — distance to west,</li>
<li>shortest distance in any direction,</li>
<li>distance to nearest high mountain,</li>
<li>distance to nearest moderate hill,</li>
<li>distance to nearest major river,</li>
</ul>
<ol start="7" style="list-style-type: decimal">
<li>Human or Anthropogenic Influences</li>
</ol>
<ul>
<li>land use / land management maps,</li>
<li>probability / intensity of agricultural land use,</li>
<li>probability / intensity of pasture or grazing use,</li>
<li>probability / intensity of forest land management,</li>
<li>probability / intensity of urbanization,</li>
<li>soil dredging, surface sealing,</li>
<li>night time illumination (nightlights) images,</li>
<li>probability of gullying or human-induced erosion,</li>
<li>soil nutrient fertilization, liming and similar maps,</li>
</ul>
<p>In the following sections we provide some practical tips (with links to the most important data sources) on how to prepare soil covariates for PSM.</p>
</div>
<div id="soil-covs-30m" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Soil covariate data sources (30–100 m resolution)</h3>
<p>Adding relevant covariates that can help explain the distribution of soil properties increases the accuracy of spatial predictions. Hence, prior to generating predictions of soil properties, it is a good idea to invest in preparing a list of Remote Sensing (RS), geomorphological/lithologic and DEM-based covariates that can potentially help explain the spatial distribution of soil properties and classes. There are now many finer resolution (30–250 m) covariates with global coverage that are publicly available without restrictions. The spatial detail, accessibility and accuracy of RS-based products has been growing exponentially and there is no evidence that this trend is going to slow down in the coming decades <span class="citation">(Herold et al. <a href="#ref-Herold2016">2016</a>)</span>.</p>
<p>The most relevant (global) publicly available remote sensing-based covariates that can be downloaded and used to improve predictive soil mapping at high spatial resolutions are, for example:</p>
<ul>
<li><p><a href="https://lta.cr.usgs.gov/SRTM1Arc">SRTM</a> and/or <a href="http://www.eorc.jaxa.jp/ALOS/en/aw3d/index_e.htm">ALOS W3D</a> Digital Elevation Model (DEM) at 30 m and <a href="http://hydro.iis.u-tokyo.ac.jp/~yamadai/MERIT_DEM/">MERIT DEM</a> at 100 m (these can be used to derive at least 8–12 DEM derivatives of which some generally prove to be beneficial for mapping of soil chemical and hydrological properties);</p></li>
<li><p>Landsat 7, 8 satellite images, either available from USGS’s <a href="http://glovis.usgs.gov/">GloVis</a> / <a href="http://earthexplorer.usgs.gov/">EarthExplorer</a>, or from the <a href="https://earthenginepartners.appspot.com/science-2013-global-forest/download_v1.2.html">GlobalForestChange project</a> repository <span class="citation">(Hansen et al. <a href="#ref-hansen2013high">2013</a>)</span>;</p></li>
<li><p><a href="https://global-surface-water.appspot.com/download">Landsat-based Global Surface Water (GSW) dynamics images</a> at 30 m resolution for the period 1985–2016 <span class="citation">(Pekel et al. <a href="#ref-pekel2016high">2016</a>)</span>;</p></li>
<li><p>Global Land Cover (GLC) maps based on the <a href="http://www.globallandcover.com">GLC30 project</a> at 30 m resolution for 2000 and 2010 <span class="citation">(Chen et al. <a href="#ref-Chen2014">2015</a>)</span> and similar land cover projects <span class="citation">(Herold et al. <a href="#ref-Herold2016">2016</a>)</span>;</p></li>
<li><p>USGS’s <a href="https://landcover.usgs.gov/glc/">global bare surface images</a> at 30 m resolution;</p></li>
<li><p><a href="http://www.eorc.jaxa.jp/ALOS/en/dataset/dataset_index.htm">JAXA’s ALOS</a> (PALSAR/PALSAR-2) radar images at 20 m resolution <span class="citation">(Shimada et al. <a href="#ref-shimada2014new">2014</a>)</span>; radar images, bands HH: -27.7 (5.3) dB and HV: -35.8 (3.0) dB, from the JAXA’s ALOS project are especially interesting for mapping rock outcrops and exposed bedrock but are also to distinguish between bare soil and dense vegetation;</p></li>
</ul>
<p>Note that the download time for 30 m global RS data can be significant if the data are needed for a larger area (hence you might consider using some RS data processing hub such as <a href="http://www.sentinel-hub.com/">Sentinel hub</a>, <a href="https://earthengine.google.com/">Google Earth Engine</a> and/or <a href="https://aws.amazon.com/public-datasets/">Amazon Web Services</a> instead of trying to download large mosaics yourself).</p>
</div>
<div id="soil-covs-250m" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Soil covariate data sources (250 m resolution or coarser)</h3>
<p><span class="citation">Hengl, Mendes de Jesus, et al. (<a href="#ref-Hengl2017SoilGrids250m">2017</a>)</span> used a large stack of coarser resolution covariate layers for producing SoilGrids250m predictions, most of which were based on remote sensing data:</p>
<ul>
<li><p>DEM-derived surfaces — slope, profile curvature, Multiresolution Index of Valley Bottom Flatness (VBF), deviation from Mean Value, valley depth, negative and positive Topographic Openness and SAGA Wetness Index — all based on a global merge of SRTMGL3 DEM and GMTED2010 <span class="citation">(Danielson and Gesch <a href="#ref-Danielson2011GMTED">2011</a>)</span>. All DEM derivatives were computed using SAGA GIS <span class="citation">(Conrad et al. <a href="#ref-gmd-8-1991-2015">2015</a>)</span>,</p></li>
<li><p>Long-term averaged monthly mean and standard deviation of the MODIS Enhanced Vegetation Index (EVI). Derived using a stack of MOD13Q1 EVI images <span class="citation">(Savtchenko et al. <a href="#ref-Savtchenko2004ASR">2004</a>)</span>,</p></li>
<li><p>Long-term averaged mean monthly surface reflectances for MODIS bands 4 (NIR) and 7 (MIR). Derived using a stack of MCD43A4 images <span class="citation">(Mira et al. <a href="#ref-mira2015modis">2015</a>)</span>,</p></li>
<li><p>Long-term averaged monthly mean and standard deviation of the MODIS land surface temperature (daytime and nighttime). Derived using a stack of MOD11A2 LST images <span class="citation">(Wan <a href="#ref-wan2006modis">2006</a>)</span>,</p></li>
<li><p>Long-term averaged mean monthly hours under snow cover based on a stack of MOD10A2 8-day snow occurrence images <span class="citation">(Hall and Riggs <a href="#ref-hall2007accuracy">2007</a>)</span>,</p></li>
<li><p>Land cover classes (cultivated land, forests, grasslands, shrublands, wetlands, tundra, artificial surfaces and bareland cover) for the year 2010 based on the GlobCover30 product by the National Geomatics Center of China <span class="citation">(Chen et al. <a href="#ref-Chen2014">2015</a>)</span>. Upscaled to 250 m resolution and expressed as percent of pixel coverage,</p></li>
<li><p>Monthly precipitation images derived as the weighted average between the WorldClim monthly precipitation <span class="citation">(Hijmans et al. <a href="#ref-Hijmans2005IJC">2005</a>)</span> and GPCP Version 2.2 <span class="citation">(Huffman and Bolvin <a href="#ref-huffman2009gpcp">2009</a>)</span>,</p></li>
<li><p>Long-term averaged mean monthly hours under snow cover. Derived using a stack of MOD10A2 8-day snow occurrence images,</p></li>
<li><p>Lithologic units (acid plutonics, acid volcanic, basic plutonics, basic volcanics, carbonate sedimentary rocks, evaporite, ice and glaciers, intermediate plutonics, intermediate volcanics, metamorphics, mixed sedimentary rocks, pyroclastics, siliciclastic sedimentary rocks, unconsolidated sediment) based on a Global Lithological Map GLiM <span class="citation">(Hartmann and Moosdorf <a href="#ref-GGGE:GGGE2352">2012</a>)</span>,</p></li>
<li><p>Landform classes (breaks/foothills, flat plains, high mountains/deep canyons, hills, low hills, low mountains, smooth plains) based on the USGS’s Map of Global Ecological Land Units <span class="citation">(Sayre et al. <a href="#ref-sayre2014new">2014</a>)</span>.</p></li>
<li><p>Global Water Table Depth in meters <span class="citation">(Fan, Li, and Miguez-Macho <a href="#ref-fan2013global">2013</a>)</span>,</p></li>
<li><p>Landsat-based estimated distribution of Mangroves <span class="citation">(Giri et al. <a href="#ref-giri2011status">2011</a>)</span>,</p></li>
<li><p>Average soil and sedimentary-deposit thickness in meters <span class="citation">(Pelletier et al. <a href="#ref-Pelletier2016">2016</a>)</span>.</p></li>
</ul>
<p>These covariates were selected to represent factors of soil formation according to <span class="citation">Jenny (<a href="#ref-jenny1994factors">1994</a>)</span>: climate, relief, living organisms, water dynamics and parent material. Out of the five main factors, water dynamics and living organisms (especially vegetation dynamics) are not trivial to represent as these operate over long periods of time and often exhibit chaotic behavior. Using reflectance bands such as the mid-infrared MODIS bands from a single day, would be of little use for soil mapping for areas with dynamic vegetation, i.e. with strong seasonal changes in vegetation cover. To account for seasonal fluctuation and for inter-annual variations in surface reflectance, long-term temporal signatures of the soil surface derived as monthly averages from long-term MODIS imagery (15 years of data) can be used <span class="citation">(Hengl, Mendes de Jesus, et al. <a href="#ref-Hengl2017SoilGrids250m">2017</a>)</span>. Long-term average seasonal signatures of surface reflectance or vegetation index provide a better indication of soil characteristics than only a single snapshot of surface reflectance. Computing temporal signatures of the land surface requires a considerable investment of time (comparable to the generation of climatic images vs temporary weather maps), but it is possibly the only way to represent the cumulative influence of living organisms on soil formation.</p>
<p><span class="citation">Behrens et al. (<a href="#ref-Behrens2018128">2018</a>)</span> recently reported that, for example, DEM derivatives derived at coarser resolutions correlated better with some targeted soil properties than the derivatives derived at finer resolutions. In this case, resolution (or scale) was represented through various DEM aggregation levels and filter sizes. Some physical and chemical processes of soil formation or vegetation distribution might not be effective or obvious at finer aggregation levels, but these can become very visible at coarser aggregation levels. In fact, it seems that spatial dependencies and interactions of the covariates can often be explained better simply by a aggregating DEM and its derivatives.</p>
</div>
</div>
<div id="preparing-soil-covariate-layers" class="section level2">
<h2><span class="header-section-number">4.2</span> Preparing soil covariate layers</h2>
<p>Before we are able to fit spatial prediction models and generate soil maps, a significant amount of effort is also spent on preparing covariate “layers” that can be used as independent variables (i.e. “predictor variables”) in the statistical modelling. Typical operations used to generate soil covariate layers include:</p>
<ul>
<li><p>Converting polygon maps to rasters,</p></li>
<li><p>Downscaling or upscaling (aggregating) rasters to match the target resolution (i.e. preparing a <em>stack</em>),</p></li>
<li><p>Filtering out missing pixels / reducing noise and multicolinearity (data overlap) problems,</p></li>
<li><p>Overlaying and subsetting raster stacks and points,</p></li>
</ul>
<p>The following examples should provide some ideas about how to program these steps using the shortest possible syntax running the fastest and most robust algorithms. Raster data can often be very large (e.g. millions of pixels) so processing large stacks of remote sensing scenes in R needs to be planned carefully. The complete R tutorial you can download from the <strong><a href="https://github.com/envirometrix/PredictiveSoilMapping">github repository</a></strong>. Instructions on how to install and set-up all software used in this example can be found in the software installation chapter @ref(software).</p>
<div id="converting-polygon-maps-to-rasters" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Converting polygon maps to rasters</h3>
<p>Before we can attach a polygon map to other stacks of covariates, it needs to be rasterized i.e. converted to a raster layer defined with its bounding box (extent) and spatial resolution. Consider for example the <a href="http://plotkml.r-forge.r-project.org/eberg.html">Ebergötzen data set</a> polygon map from the plotKML package (Fig. @ref(fig:eberg-zones-spplot)):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rgdal)
<span class="co">#&gt; Loading required package: sp</span>
<span class="co">#&gt; rgdal: version: 1.3-6, (SVN revision 773)</span>
<span class="co">#&gt;  Geospatial Data Abstraction Library extensions to R successfully loaded</span>
<span class="co">#&gt;  Loaded GDAL runtime: GDAL 2.3.2, released 2018/09/21</span>
<span class="co">#&gt;  Path to GDAL shared files: /usr/share/gdal</span>
<span class="co">#&gt;  GDAL binary built with GEOS: TRUE </span>
<span class="co">#&gt;  Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493]</span>
<span class="co">#&gt;  Path to PROJ.4 shared files: (autodetected)</span>
<span class="co">#&gt;  Linking to sp version: 1.3-1</span>
<span class="kw">library</span>(raster)
<span class="kw">library</span>(plotKML)
<span class="co">#&gt; plotKML version 0.5-8 (2017-05-12)</span>
<span class="co">#&gt; URL: http://plotkml.r-forge.r-project.org/</span>
<span class="kw">data</span>(eberg_zones)
<span class="kw">spplot</span>(eberg_zones[<span class="dv">1</span>])</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Soil_covariates_files/figure-html/eberg-zones-spplot-1.png" alt="Ebergotzen parent material polygon map with legend." width="70%" />
<p class="caption">
(#fig:eberg-zones-spplot)Ebergotzen parent material polygon map with legend.
</p>
</div>
<p>We can convert this object to a raster by using the <a href="https://cran.r-project.org/web/packages/raster/">raster package</a>. Note that before we can run the operation, we need to know the target grid system i.e. the extent of the grid and its spatial resolution. We can use this from an existing layer:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(plotKML)
<span class="kw">data</span>(<span class="st">&quot;eberg_grid25&quot;</span>)
<span class="kw">gridded</span>(eberg_grid25) &lt;-<span class="st"> </span><span class="er">~</span>x<span class="op">+</span>y
<span class="kw">proj4string</span>(eberg_grid25) &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="st">&quot;+init=epsg:31467&quot;</span>)
r &lt;-<span class="st"> </span><span class="kw">raster</span>(eberg_grid25)
r
<span class="co">#&gt; class       : RasterLayer </span>
<span class="co">#&gt; dimensions  : 400, 400, 160000  (nrow, ncol, ncell)</span>
<span class="co">#&gt; resolution  : 25, 25  (x, y)</span>
<span class="co">#&gt; extent      : 3570000, 3580000, 5708000, 5718000  (xmin, xmax, ymin, ymax)</span>
<span class="co">#&gt; coord. ref. : +init=epsg:31467 +proj=tmerc +lat_0=0 +lon_0=9 +k=1 +x_0=3500000 +y_0=0 +datum=potsdam +units=m +no_defs +ellps=bessel +towgs84=598.1,73.7,418.2,0.202,0.045,-2.455,6.7 </span>
<span class="co">#&gt; data source : in memory</span>
<span class="co">#&gt; names       : DEMTOPx </span>
<span class="co">#&gt; values      : 159, 428  (min, max)</span></code></pre></div>
<p>The <code>eberg_grids25</code> object is a <code>SpatialPixelsDataFrame</code>, which is a spatial gridded data structure of the <a href="https://cran.r-project.org/web/packages/sp/">sp package</a> package. The raster package also offers data structures for spatial (gridded) data, and stores such data as <code>RasterLayer</code> class. Gridded data can be converted from class <code>SpatialPixelsDataFrame</code> to Raster layer with the <a href="http://www.rdocumentation.org/packages/raster/functions/raster"><code>raster</code></a> command. The <a href="http://www.inside-r.org/packages/cran/sp/docs/CRS"><code>CRS</code></a> command of the sp package can be used to set a spatial projection. <a href="http://spatialreference.org/ref/epsg/31467/">EPSG projection 31467</a> is the German coordinate system (each coordinate system has an associated EPSG number that can be obtained from <a href="http://spatialreference.org/" class="uri">http://spatialreference.org/</a>.</p>
<p>Conversion from polygon to raster is now possible via the <a href="http://www.rdocumentation.org/packages/raster/functions/rasterize"><code>rasterize</code></a> command:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(eberg_zones)
<span class="co">#&gt; [1] &quot;ZONES&quot;</span>
eberg_zones_r &lt;-<span class="st"> </span><span class="kw">rasterize</span>(eberg_zones, r, <span class="dt">field=</span><span class="st">&quot;ZONES&quot;</span>)
<span class="kw">plot</span>(eberg_zones_r)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Soil_covariates_files/figure-html/eberg-zones-grid-1.png" alt="Ebergotzen parent material polygon map rasterized." width="70%" />
<p class="caption">
(#fig:eberg-zones-grid)Ebergotzen parent material polygon map rasterized.
</p>
</div>
<p>Converting large polygons in R using the raster package can be very time-consuming. To speed up the rasterization of polygons we highly recommend using instead the <code>fasterize</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sf)
<span class="co">#&gt; Linking to GEOS 3.6.1, GDAL 2.3.2, PROJ 4.9.3</span>
<span class="kw">library</span>(fasterize)
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: &#39;fasterize&#39;</span>
<span class="co">#&gt; The following object is masked from &#39;package:graphics&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     plot</span>
eberg_zones_sf &lt;-<span class="st"> </span><span class="kw">as</span>(eberg_zones, <span class="st">&quot;sf&quot;</span>)
eberg_zones_r &lt;-<span class="st"> </span><span class="kw">fasterize</span>(eberg_zones_sf, r, <span class="dt">field=</span><span class="st">&quot;ZONES&quot;</span>)</code></pre></div>
<p><code>fasterize</code> function is an order of magnitude faster and hence more suitable for operational work. Note also that it only works with Simple Feature (sf) objects.</p>
<p>Another efficient approach to rasterize polygons is to use SAGA GIS as this can handle large data and is easy to run in parallel. First, you need to export the polygon map to shapefile format which can be done with commands of the <a href="https://cran.r-project.org/web/packages/rgdal/">rgdal package</a> package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">eberg_zones<span class="op">$</span>ZONES_int &lt;-<span class="st"> </span><span class="kw">as.integer</span>(eberg_zones<span class="op">$</span>ZONES)
<span class="kw">writeOGR</span>(eberg_zones[<span class="st">&quot;ZONES_int&quot;</span>], <span class="st">&quot;extdata/eberg_zones.shp&quot;</span>, <span class="st">&quot;.&quot;</span>, <span class="st">&quot;ESRI Shapefile&quot;</span>)</code></pre></div>
<p>The <code>writeOGR()</code> command writes a SpatialPolygonsDataFrame (the data structure for polygon data in R) to an ESRI shapefile. Here we only write the attribute <code>&quot;ZONES_int&quot;</code> to the shapefile. It is, however, also possible to write all attributes of the SpatialPolygonsDataFrame to a shapefile.</p>
<p>Next, you can locate the (previously installed) SAGA GIS command line program (on Microsoft Windows OS or Linux system):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">if</span>(.Platform<span class="op">$</span>OS.type<span class="op">==</span><span class="st">&quot;unix&quot;</span>){
  saga_cmd =<span class="st"> &quot;saga_cmd&quot;</span>
}
<span class="cf">if</span>(.Platform<span class="op">$</span>OS.type<span class="op">==</span><span class="st">&quot;windows&quot;</span>){
  saga_cmd =<span class="st"> &quot;C:/Progra~1/SAGA-GIS/saga_cmd.exe&quot;</span>
}
saga_cmd
<span class="co">#&gt; [1] &quot;saga_cmd&quot;</span></code></pre></div>
<p>and finally use the module <a href="http://saga-gis.org/saga_module_doc/2.2.7/grid_gridding_0.html">‘’grid_gridding’’</a> to convert the shapefile to a grid:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pix =<span class="st"> </span><span class="dv">25</span>
<span class="kw">system</span>(<span class="kw">paste0</span>(saga_cmd, <span class="st">&#39; grid_gridding 0 -INPUT </span><span class="ch">\&quot;</span><span class="st">extdata/eberg_zones.shp</span><span class="ch">\&quot;</span><span class="st"> &#39;</span>,
      <span class="st">&#39;-FIELD </span><span class="ch">\&quot;</span><span class="st">ZONES_int</span><span class="ch">\&quot;</span><span class="st"> -GRID </span><span class="ch">\&quot;</span><span class="st">extdata/eberg_zones.sgrd</span><span class="ch">\&quot;</span><span class="st"> -GRID_TYPE 0 &#39;</span>,
      <span class="st">&#39;-TARGET_DEFINITION 0 -TARGET_USER_SIZE &#39;</span>, pix, <span class="st">&#39; -TARGET_USER_XMIN &#39;</span>, 
      <span class="kw">extent</span>(r)[<span class="dv">1</span>]<span class="op">+</span>pix<span class="op">/</span><span class="dv">2</span>,<span class="st">&#39; -TARGET_USER_XMAX &#39;</span>, <span class="kw">extent</span>(r)[<span class="dv">2</span>]<span class="op">-</span>pix<span class="op">/</span><span class="dv">2</span>, 
      <span class="st">&#39; -TARGET_USER_YMIN &#39;</span>, <span class="kw">extent</span>(r)[<span class="dv">3</span>]<span class="op">+</span>pix<span class="op">/</span><span class="dv">2</span>,<span class="st">&#39; -TARGET_USER_YMAX &#39;</span>, 
      <span class="kw">extent</span>(r)[<span class="dv">4</span>]<span class="op">-</span>pix<span class="op">/</span><span class="dv">2</span>))
<span class="co">#&gt; Warning in system(paste0(saga_cmd, &quot; grid_gridding 0 -INPUT \&quot;extdata/</span>
<span class="co">#&gt; eberg_zones.shp\&quot; &quot;, : error in running command</span>
eberg_zones_r2 &lt;-<span class="st"> </span><span class="kw">readGDAL</span>(<span class="st">&quot;extdata/eberg_zones.sdat&quot;</span>)
<span class="co">#&gt; extdata/eberg_zones.sdat has GDAL driver SAGA </span>
<span class="co">#&gt; and has 400 rows and 400 columns</span></code></pre></div>
<p>With the <code>system()</code> command we can invoke an operating system (OS) command, here we use it to run the <code>saga_cmd.exe</code> file from R. The paste0 function is used to paste together a string that is passed to the <code>system()</code> command. The string starts with the OS command we would like to invoke (here <code>saga_cmd.exe</code>) followed by input required for the running the OS command.</p>
<p>Note that the bounding box (in SAGA GIS) needs to be defined using the center of the corner pixel and not the corners, hence we take half of the pixel size for extent coordinates from raster package. Also note that the class names have been lost during rasterization (we work with integers in SAGA GIS), but we can attach them back by using e.g.:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">levels</span>(eberg_zones<span class="op">$</span>ZONES)
<span class="co">#&gt; [1] &quot;Clay_and_loess&quot;  &quot;Clayey_derivats&quot; &quot;Sandy_material&quot;  &quot;Silt_and_sand&quot;</span>
eberg_zones_r2<span class="op">$</span>ZONES &lt;-<span class="st"> </span><span class="kw">as.factor</span>(eberg_zones_r2<span class="op">$</span>band1)
<span class="kw">levels</span>(eberg_zones_r2<span class="op">$</span>ZONES) &lt;-<span class="st"> </span><span class="kw">levels</span>(eberg_zones<span class="op">$</span>ZONES)
<span class="kw">summary</span>(eberg_zones_r2<span class="op">$</span>ZONES)
<span class="co">#&gt;  Clay_and_loess Clayey_derivats  Sandy_material   Silt_and_sand </span>
<span class="co">#&gt;           28667           35992           21971           73370</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/eberg_zones_rasterized.png" alt="Eberg zones rasterized to 25 m resolution." width="70%" />
<p class="caption">
(#fig:eberg-zones-rasterized)Eberg zones rasterized to 25 m resolution.
</p>
</div>
</div>
<div id="downscaling-upscaling" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Downscaling or upscaling (aggregating) rasters</h3>
<p>In order for all covariates to perfectly <em>stack</em>, we also need to adjust the resolution of some covariates that have either too coarse or too fine a resolution compared to the target resolution. The process of bringing raster layers to a target grid resolution is also known as <strong>resampling</strong>. Consider the following example from the Ebergotzen case study:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(eberg_grid)
<span class="kw">gridded</span>(eberg_grid) &lt;-<span class="st"> </span><span class="er">~</span>x<span class="op">+</span>y
<span class="kw">proj4string</span>(eberg_grid) &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="st">&quot;+init=epsg:31467&quot;</span>)
<span class="kw">names</span>(eberg_grid)
<span class="co">#&gt; [1] &quot;PRMGEO6&quot; &quot;DEMSRT6&quot; &quot;TWISRT6&quot; &quot;TIRAST6&quot; &quot;LNCCOR6&quot;</span></code></pre></div>
<p>In this case we have a few layers that we would like to use for spatial prediction in combination with the maps produced in the previous sections, but their resolution is 100 m i.e. about 16 times coarser. Probably the most robust way to resample rasters is to use the <a href="http://www.gdal.org/gdalwarp.html"><code>gdalwarp</code></a> function from the GDAL software. Assuming that you have already installed GDAL, you only need to locate the program on your system, and then you can again run <a href="http://www.gdal.org/gdalwarp.html"><code>gdalwarp</code></a> via the system command:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">writeGDAL</span>(eberg_grid[<span class="st">&quot;TWISRT6&quot;</span>], <span class="st">&quot;extdata/eberg_grid_TWISRT6.tif&quot;</span>)
<span class="kw">system</span>(<span class="kw">paste0</span>(<span class="st">&#39;gdalwarp extdata/eberg_grid_TWISRT6.tif&#39;</span>,
              <span class="st">&#39; extdata/eberg_grid_TWISRT6_25m.tif -r </span><span class="ch">\&quot;</span><span class="st">cubicspline</span><span class="ch">\&quot;</span><span class="st"> -te &#39;</span>, 
              <span class="kw">paste</span>(<span class="kw">as.vector</span>(<span class="kw">extent</span>(r))[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>)], <span class="dt">collapse=</span><span class="st">&quot; &quot;</span>),
              <span class="st">&#39; -tr &#39;</span>, pix, <span class="st">&#39; &#39;</span>, pix, <span class="st">&#39; -overwrite&#39;</span>))</code></pre></div>
<p>The writeGDAL command writes the TWISRT6 grid, that is stored in the eberg_grid grid stack, to a TIFF file. This TIFF is subsequently read by the <code>gdalwarp</code> function and resampled to a 25 m TIFF file using <code>cubicspline</code>, which will fill in values between original grid nodes using smooth surfaces. Note that the paste0 function in the <code>system()</code> command pastes together the following string:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="st">&quot;C:/Progra~1/GDAL/gdalwarp.exe eberg_grid_TWISRT6.tif </span>
<span class="st">eberg_grid_TWISRT6_25m.tif -r </span><span class="dt">\&quot;</span><span class="st">cubicspline</span><span class="dt">\&quot;</span><span class="st"> </span>
<span class="st">-te 3570000 5708000 3580000 5718000 -tr 25 25 -overwrite&quot;</span></code></pre></div>
<p>We can compare the two maps (the original and the downscaled) next to each other by using:</p>
<div class="figure" style="text-align: center">
<img src="figures/eberg_original_vs_downscaled.png" alt="Original TWI vs downscaled map from 100 m to 25 m." width="100%" />
<p class="caption">
(#fig:eberg-original-vs-downscaled)Original TWI vs downscaled map from 100 m to 25 m.
</p>
</div>
<p>The map on the right looks much smoother of course (assuming that this variable varies continuously in space, this could very well be an accurate picture), but it is important to realize that downscaling can only be implemented up to certain target resolution i.e. only for certain features. For example, downscaling TWI from 100 to 25 m is not much of problem, but to go beyond 10 m would probably result in large differences from a TWI calculated at 10 m resolution (in other words: be careful with downscaling because it is often not trivial).</p>
<p>The opposite process to downscaling is upscaling or aggregation. Although this one can also potentially be tricky, it is a much more straightforward process than downscaling. We recommend using the <code>average</code> method in GDAL for aggregating values e.g.:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system</span>(<span class="kw">paste0</span>(<span class="st">&#39;gdalwarp extdata/eberg_grid_TWISRT6.tif&#39;</span>,
              <span class="st">&#39; extdata/eberg_grid_TWISRT6_250m.tif -r </span><span class="ch">\&quot;</span><span class="st">average</span><span class="ch">\&quot;</span><span class="st"> -te &#39;</span>, 
              <span class="kw">paste</span>(<span class="kw">as.vector</span>(<span class="kw">extent</span>(r))[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>)], <span class="dt">collapse=</span><span class="st">&quot; &quot;</span>),
              <span class="st">&#39; -tr 250 250 -overwrite&#39;</span>))</code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/eberg_original_vs_aggregated.png" alt="Original TWI vs aggregated map from 100 m to 250 m." width="100%" />
<p class="caption">
(#fig:eberg-original-vs-aggregated)Original TWI vs aggregated map from 100 m to 250 m.
</p>
</div>
</div>
<div id="deriving-dem-parameters-using-saga-gis" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Deriving DEM parameters using SAGA GIS</h3>
<p>Now that we have established a connection between R and SAGA GIS, we can also use SAGA GIS to derive some standard DEM parameters of interest to soil mapping. To automate further processing, we make the following function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">saga_DEM_derivatives &lt;-<span class="st"> </span><span class="cf">function</span>(INPUT, <span class="dt">MASK=</span><span class="ot">NULL</span>, 
                                 <span class="dt">sel=</span><span class="kw">c</span>(<span class="st">&quot;SLP&quot;</span>,<span class="st">&quot;TWI&quot;</span>,<span class="st">&quot;CRV&quot;</span>,<span class="st">&quot;VBF&quot;</span>,<span class="st">&quot;VDP&quot;</span>,<span class="st">&quot;OPN&quot;</span>,<span class="st">&quot;DVM&quot;</span>)){
  <span class="cf">if</span>(<span class="op">!</span><span class="kw">is.null</span>(MASK)){
    ## Fill in missing DEM pixels:
    <span class="kw">suppressWarnings</span>( <span class="kw">system</span>(<span class="kw">paste0</span>(saga_cmd, 
                                    <span class="st">&#39; grid_tools 25 -GRID=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, INPUT, 
                                    <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st"> -MASK=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, MASK, <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st"> -CLOSED=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
                                    INPUT, <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st">&#39;</span>)) )
  }
  ## Slope:
  <span class="cf">if</span>(<span class="kw">any</span>(sel <span class="op">%in%</span><span class="st"> &quot;SLP&quot;</span>)){
    <span class="kw">try</span>( <span class="kw">suppressWarnings</span>( <span class="kw">system</span>(<span class="kw">paste0</span>(saga_cmd, 
                                         <span class="st">&#39; ta_morphometry 0 -ELEVATION=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
                                         INPUT, <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st"> -SLOPE=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
                                         <span class="kw">gsub</span>(<span class="st">&quot;.sgrd&quot;</span>, <span class="st">&quot;_slope.sgrd&quot;</span>, INPUT), 
                                         <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st"> -C_PROF=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
                                         <span class="kw">gsub</span>(<span class="st">&quot;.sgrd&quot;</span>, <span class="st">&quot;_cprof.sgrd&quot;</span>, INPUT), <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st">&#39;</span>) ) ) )
  }
  ## TWI:
  <span class="cf">if</span>(<span class="kw">any</span>(sel <span class="op">%in%</span><span class="st"> &quot;TWI&quot;</span>)){
    <span class="kw">try</span>( <span class="kw">suppressWarnings</span>( <span class="kw">system</span>(<span class="kw">paste0</span>(saga_cmd, 
                                         <span class="st">&#39; ta_hydrology 15 -DEM=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
                                         INPUT, <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st"> -TWI=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
                                         <span class="kw">gsub</span>(<span class="st">&quot;.sgrd&quot;</span>, <span class="st">&quot;_twi.sgrd&quot;</span>, INPUT), <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st">&#39;</span>) ) ) )
  }
  ## MrVBF:
  <span class="cf">if</span>(<span class="kw">any</span>(sel <span class="op">%in%</span><span class="st"> &quot;VBF&quot;</span>)){
    <span class="kw">try</span>( <span class="kw">suppressWarnings</span>( <span class="kw">system</span>(<span class="kw">paste0</span>(saga_cmd, 
                                         <span class="st">&#39; ta_morphometry 8 -DEM=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
                                         INPUT, <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st"> -MRVBF=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>,
                                         <span class="kw">gsub</span>(<span class="st">&quot;.sgrd&quot;</span>, <span class="st">&quot;_vbf.sgrd&quot;</span>, INPUT),
                                         <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st"> -T_SLOPE=10 -P_SLOPE=3&#39;</span>) ) ) )
  }
  ## Valley depth:
  <span class="cf">if</span>(<span class="kw">any</span>(sel <span class="op">%in%</span><span class="st"> &quot;VDP&quot;</span>)){
    <span class="kw">try</span>( <span class="kw">suppressWarnings</span>( <span class="kw">system</span>(<span class="kw">paste0</span>(saga_cmd, 
                                         <span class="st">&#39; ta_channels 7 -ELEVATION=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
                                         INPUT, <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st"> -VALLEY_DEPTH=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
                                         <span class="kw">gsub</span>(<span class="st">&quot;.sgrd&quot;</span>, <span class="st">&quot;_vdepth.sgrd&quot;</span>, 
                                              INPUT), <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st">&#39;</span>) ) ) )
  }
  ## Openess:
  <span class="cf">if</span>(<span class="kw">any</span>(sel <span class="op">%in%</span><span class="st"> &quot;OPN&quot;</span>)){
    <span class="kw">try</span>( <span class="kw">suppressWarnings</span>( <span class="kw">system</span>(<span class="kw">paste0</span>(saga_cmd, 
                                         <span class="st">&#39; ta_lighting 5 -DEM=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
                                         INPUT, <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st"> -POS=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
                                         <span class="kw">gsub</span>(<span class="st">&quot;.sgrd&quot;</span>, <span class="st">&quot;_openp.sgrd&quot;</span>, INPUT), 
                                         <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st"> -NEG=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
                                         <span class="kw">gsub</span>(<span class="st">&quot;.sgrd&quot;</span>, <span class="st">&quot;_openn.sgrd&quot;</span>, INPUT), 
                                         <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st"> -METHOD=0&#39;</span> ) ) ) )
  }
  ## Deviation from Mean Value:
  <span class="cf">if</span>(<span class="kw">any</span>(sel <span class="op">%in%</span><span class="st"> &quot;DVM&quot;</span>)){
    <span class="kw">suppressWarnings</span>( <span class="kw">system</span>(<span class="kw">paste0</span>(saga_cmd, 
                                    <span class="st">&#39; statistics_grid 1 -GRID=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
                                    INPUT, <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st"> -DEVMEAN=</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
                                    <span class="kw">gsub</span>(<span class="st">&quot;.sgrd&quot;</span>, <span class="st">&quot;_devmean.sgrd&quot;</span>, INPUT), 
                                    <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st"> -RADIUS=11&#39;</span> ) ) )
  }
}</code></pre></div>
<p>To run this function we only need DEM as input:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">writeGDAL</span>(eberg_grid[<span class="st">&quot;DEMSRT6&quot;</span>], <span class="st">&quot;extdata/DEMSRT6.sdat&quot;</span>, <span class="st">&quot;SAGA&quot;</span>)
<span class="kw">saga_DEM_derivatives</span>(<span class="st">&quot;DEMSRT6.sgrd&quot;</span>)</code></pre></div>
<p>which derives all DEM derivatives in a single operation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dem.lst &lt;-<span class="st"> </span><span class="kw">list.files</span>(<span class="dt">pattern=</span><span class="kw">glob2rx</span>(<span class="st">&quot;^DEMSRT6_*.sdat&quot;</span>))
<span class="kw">plot</span>(<span class="kw">stack</span>(dem.lst), <span class="dt">col=</span>SAGA_pal[[<span class="dv">1</span>]])</code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/dem_derivatives_plot.png" alt="Some standard DEM derivatives calculated using SAGA GIS." width="90%" />
<p class="caption">
(#fig:dem-derivatives-plot)Some standard DEM derivatives calculated using SAGA GIS.
</p>
</div>
<p>This function can now be used with any DEM to derive a standard set of 7-8 DEM parameters consisting of slope and curvature, TWI and MrVBF, positive and negative openess, valley depth and deviation from mean value. You could easily add more parameters to this function and then test if some of the other DEM derivatives can help improve mapping soil properties and classes. Note that SAGA GIS will by default optimize computing of DEM derivatives by using most of the available cores to compute (parallelization is turned on automatically).</p>
<!-- 
### Deriving DEM parameters using LITAP

To derive unique hydrological parameters that can potentially help soil mapping we can also use the LITAP (Landscape Integrated Terrain Analysis Package) package originally implemented as FlowMapR program [@macmillan2003landmapr], and compiled for R users by Stefanie LaZerte and Li Sheng from the Agriculture and Agri-Food Canada. We can install this package from github by using:


```r
if(!require("LITAP")){ devtools::install_github("steffilazerte/LITAP") }
```

A suite of hydrological flow DEM derivatives can be derived by using the generic command:


```r
library(LITAP)
library(rgdal)
writeGDAL(eberg_grid["DEMSRT6"], "extdata/DEMSRT6.tif", type="Int16", mvFlag=-32768)
LITAP::complete_run(file = "extdata/DEMSRT6.tif", folder_out = "extdata/", 
                    report = FALSE)
```

which will unblock a DEM and then derive a number of hydrological parameters 
and that are maybe not available in other packages and that can be used 
to quantify soil deposition / accumulation processes. To plot for example 
watersheds by flow paths we can run:

<div class="figure" style="text-align: center">
<img src="Soil_covariates_files/figure-html/watersheds-by-flow-paths-1.png" alt="Watersheds by flow paths dervied using the [LITAP](https://steffilazerte.github.io/LITAP/) package." width="100%" />
<p class="caption">(\#fig:watersheds-by-flow-paths)Watersheds by flow paths dervied using the [LITAP](https://steffilazerte.github.io/LITAP/) package.</p>
</div>

Which shows all watersheds and individual streams derived using the unblocked DEM (after pit removal).
 -->
</div>
<div id="filtering-out-missing-pixels-and-artifacts" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Filtering out missing pixels and artifacts</h3>
<p>After bringing all covariates to the same grid definition, remaining problems for using covariates in spatial modelling may include:</p>
<ul>
<li><p>Missing pixels,</p></li>
<li><p>Artifacts and noise,</p></li>
<li><p>Multicolinearity (i.e. data overlap),</p></li>
</ul>
<p>In a stack with tens of rasters, the <em>weakest layer</em> (i.e. the layer with greatest number of missing pixels or largest number of artifacts) could cause serious problems for producing soil maps as the missing pixels and artifacts would propagate to predictions: if only one layer in the raster stack misses values then predictive models might drop whole rows in the predictions even though data is available for 95% of rows. Missing pixels occur for various reasons: in the case of remote sensing, missing pixels can be due to clouds or similar; noise is often due to atmospheric conditions. Missing pixels (as long as we are dealing with only a few patches of missing pixels) can be efficiently filtered by using for example the <a href="http://saga-gis.org/saga_module_doc/2.2.7/grid_tools_7.html">gap filling functionality</a> available in the SAGA GIS e.g.:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">image</span>(<span class="kw">raster</span>(eberg_grid[<span class="st">&quot;test&quot;</span>]), <span class="dt">col=</span>SAGA_pal[[<span class="dv">1</span>]], <span class="dt">zlim=</span>zlim, <span class="dt">main=</span><span class="st">&quot;Original&quot;</span>, <span class="dt">asp=</span><span class="dv">1</span>)
<span class="kw">image</span>(<span class="kw">raster</span>(<span class="st">&quot;test.sdat&quot;</span>), <span class="dt">col=</span>SAGA_pal[[<span class="dv">1</span>]], <span class="dt">zlim=</span>zlim, <span class="dt">main=</span><span class="st">&quot;Filtered&quot;</span>, <span class="dt">asp=</span><span class="dv">1</span>)</code></pre></div>
<p>In this example we use the same input and output file for filling in gaps. There are several other gap filling possibilities in SAGA GIS including Close Gaps with Spline, Close Gaps with Stepwise Resampling and Close One Cell Gaps. Note all of these are equally applicable to all missing pixel problems, but having &lt;10% of missing pixels is often not much of a problem for soil mapping.</p>
<p>Another elegant way to filter the missing pixels, to reduce noise and to reduce data overlap is to use <a href="http://www.rdocumentation.org/packages/stats/functions/prcomp">Principal Components</a> transformation of original data. This is available also via the GSIF function <a href="http://www.rdocumentation.org/packages/GSIF/functions/spc">‘’spc’’</a>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(eberg_grid)
<span class="kw">gridded</span>(eberg_grid) &lt;-<span class="st"> </span><span class="er">~</span>x<span class="op">+</span>y
<span class="kw">proj4string</span>(eberg_grid) &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="st">&quot;+init=epsg:31467&quot;</span>)
formulaString &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>PRMGEO6<span class="op">+</span>DEMSRT6<span class="op">+</span>TWISRT6<span class="op">+</span>TIRAST6
eberg_spc &lt;-<span class="st"> </span>GSIF<span class="op">::</span><span class="kw">spc</span>(eberg_grid, formulaString)
<span class="kw">names</span>(eberg_spc<span class="op">@</span>predicted) <span class="co"># 11 components on the end;</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/eberg_spc_11_plot.png" alt="11 PCs derived using eberg covariates." width="100%" />
<p class="caption">
(#fig:eberg-spc-11-plot)11 PCs derived using eberg covariates.
</p>
</div>
<p>The advantages of using the <a href="http://www.rdocumentation.org/packages/GSIF/functions/spc">‘’spc’’</a> function are:</p>
<ul>
<li><p>All output soil covariates are numeric (and not a mixture of factors and numeric),</p></li>
<li><p>The last 1-2 PCs often contain signal noise and could be excluded from modelling,</p></li>
<li><p>In subsequent analysis it becomes easier to remove covariates that do not help in modelling (e.g. by using step-wise selection and similar),</p></li>
</ul>
<p>A disadvantage of using SPCs (spatial predictive components) is that these components are often abstract so that interpretation of correlations can become difficult. Also, if one of the layers contains many factor levels, then the number of output covariates might explode, which becomes impractical as we should then have at least 10 observations per covariate to avoid overfitting.</p>
</div>
<div id="overlaying-and-subsetting-raster-stacks-and-points" class="section level3">
<h3><span class="header-section-number">4.2.5</span> Overlaying and subsetting raster stacks and points</h3>
<p>Now that we have prepared all covariates (resampled them to the same grid and filtered out all problems), we can proceed with running overlays and fitting statistical models. Assuming that we deal with a large number of files, an elegant way to read all those into R is by using the raster package, especially the <a href="http://www.rdocumentation.org/packages/raster/functions/stack">‘’stack’’</a> and <a href="http://www.rdocumentation.org/packages/raster/functions/stack">‘’raster’’</a> commands. In the following example we can list all files of interest, and then read them all at once:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(raster)
grd.lst &lt;-<span class="st"> </span><span class="kw">list.files</span>(<span class="dt">pattern=</span><span class="st">&quot;25m&quot;</span>)
grd.lst
grid25m &lt;-<span class="st"> </span><span class="kw">stack</span>(grd.lst)
grid25m &lt;-<span class="st"> </span><span class="kw">as</span>(grid25m, <span class="st">&quot;SpatialGridDataFrame&quot;</span>)
<span class="kw">str</span>(grid25m)</code></pre></div>
<p>One could now save all the prepared covariates stored in SpatialGridDataFrame as an RDS data object for future use.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">saveRDS</span>(grid25m, <span class="dt">file =</span> <span class="st">&quot;extdata/covariates25m.rds&quot;</span>)</code></pre></div>
<p>To overlay rasters and points and prepare a regression matrix, we can either use the <a href="http://www.rdocumentation.org/packages/sp/functions/over"><code>over</code></a> function from the sp package, or <a href="http://www.rdocumentation.org/packages/raster/functions/extract"><code>extract</code></a> function from the raster package. By using the raster package, one can run overlay even without reading the rasters into memory:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sp)
<span class="kw">data</span>(eberg)
<span class="kw">coordinates</span>(eberg) &lt;-<span class="st"> </span><span class="er">~</span>X<span class="op">+</span>Y
<span class="kw">proj4string</span>(eberg) &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="st">&quot;+init=epsg:31467&quot;</span>)
ov =<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">extract</span>(<span class="kw">stack</span>(grd.lst), eberg))
<span class="kw">str</span>(ov[<span class="kw">complete.cases</span>(ov),])</code></pre></div>
<p>If the raster layers can not be stacked and if each layer is available in a different projection system, you can also create a function that reprojects points to the target raster layer projection system:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">overlay.fun =<span class="st"> </span><span class="cf">function</span>(i, y){
  raster<span class="op">::</span><span class="kw">extract</span>(<span class="kw">raster</span>(i), <span class="dt">na.rm=</span><span class="ot">FALSE</span>, 
      <span class="kw">spTransform</span>(y, <span class="kw">proj4string</span>(<span class="kw">raster</span>(i))))}</code></pre></div>
<p>which can also be run in parallel for example by using the parallel package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ov =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">mclapply</span>(grd.lst, <span class="dt">FUN=</span>overlay.fun, <span class="dt">y=</span>eberg))
<span class="kw">names</span>(ov) =<span class="st"> </span><span class="kw">basename</span>(grd.lst)</code></pre></div>
<p>In a similar way, one could also make wrapper functions that downscale/upscale grids, then filter missing values and stack all data together so that it becomes available in the working memory (sp grid or pixels object). Overlay and model fitting is also implemented directly in the GSIF package, so any attempt to fit models will automatically perform overlay.</p>
</div>
<div id="working-with-larger-rasters" class="section level3">
<h3><span class="header-section-number">4.2.6</span> Working with large(r) rasters</h3>
<p>As R is often inefficient in handling large objects in memory (such as large raster images), a good strategy to run raster processing in R is to consider using for example the <code>clusterR</code> function from the <a href="https://cran.r-project.org/package=raster">raster</a> package, which automatically parallelizes use of raster functions. To have full control over parallelization, you can alternatively tile large rasters using the <code>getSpatialTiles</code> function from the GSIF package and process them as separate objects in parallel. The following examples shows how to run a simple function in parallel on tiles and then mosaic these tiles after all processing has been completed. Consider for example the GeoTiff from the rgdal package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fn =<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;pictures/SP27GTIF.TIF&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;rgdal&quot;</span>)
obj &lt;-<span class="st"> </span>rgdal<span class="op">::</span><span class="kw">GDALinfo</span>(fn)
<span class="co">#&gt; Warning in rgdal::GDALinfo(fn): statistics not supported by this driver</span></code></pre></div>
<p>We can split that object in 35 tiles, each of 5 x 5 km in size by running:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tiles &lt;-<span class="st"> </span>GSIF<span class="op">::</span><span class="kw">getSpatialTiles</span>(obj, <span class="dt">block.x=</span><span class="dv">5000</span>, <span class="dt">return.SpatialPolygons =</span> <span class="ot">FALSE</span>)
tiles.pol &lt;-<span class="st"> </span>GSIF<span class="op">::</span><span class="kw">getSpatialTiles</span>(obj, <span class="dt">block.x=</span><span class="dv">5000</span>, <span class="dt">return.SpatialPolygons =</span> <span class="ot">TRUE</span>)
tile.pol  &lt;-<span class="st"> </span><span class="kw">SpatialPolygonsDataFrame</span>(tiles.pol, tiles)
<span class="kw">plot</span>(<span class="kw">raster</span>(fn), <span class="dt">col=</span><span class="kw">bpy.colors</span>(<span class="dv">20</span>))
<span class="kw">lines</span>(tile.pol, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/rplot_large_raster_tiles.png" alt="Example of a tiling system derived using the `GSIF::getSpatialTiles` function." width="80%" />
<p class="caption">
(#fig:rplot-large-raster-tiles)Example of a tiling system derived using the <code>GSIF::getSpatialTiles</code> function.
</p>
</div>
<p>rgdal further allows us to read only a single tile of the GeoTiff by using the <code>offset</code> and <code>region.dim</code> arguments:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x =<span class="st"> </span><span class="kw">readGDAL</span>(fn, <span class="dt">offset=</span><span class="kw">unlist</span>(tiles[<span class="dv">1</span>,<span class="kw">c</span>(<span class="st">&quot;offset.y&quot;</span>,<span class="st">&quot;offset.x&quot;</span>)]),
             <span class="dt">region.dim=</span><span class="kw">unlist</span>(tiles[<span class="dv">1</span>,<span class="kw">c</span>(<span class="st">&quot;region.dim.y&quot;</span>,<span class="st">&quot;region.dim.x&quot;</span>)]),
             <span class="dt">output.dim=</span><span class="kw">unlist</span>(tiles[<span class="dv">1</span>,<span class="kw">c</span>(<span class="st">&quot;region.dim.y&quot;</span>,<span class="st">&quot;region.dim.x&quot;</span>)]), <span class="dt">silent =</span> <span class="ot">TRUE</span>)
<span class="kw">spplot</span>(x)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/sp27gtif_tile.png" alt="A tile produced for a satellite image in the example above." width="60%" />
<p class="caption">
(#fig:sp27gtif-tile)A tile produced for a satellite image in the example above.
</p>
</div>
<p>We would like to run a function on this raster in parallel, for example a simple function that converts values to 0/1 values based on a threshold:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fun_mask &lt;-<span class="st"> </span><span class="cf">function</span>(i, tiles, <span class="dt">dir=</span><span class="st">&quot;./tiled/&quot;</span>, <span class="dt">threshold=</span><span class="dv">190</span>){
  out.tif =<span class="st"> </span><span class="kw">paste0</span>(dir, <span class="st">&quot;T&quot;</span>, i, <span class="st">&quot;.tif&quot;</span>)
  <span class="cf">if</span>(<span class="op">!</span><span class="kw">file.exists</span>(out.tif)){
    x =<span class="st"> </span><span class="kw">readGDAL</span>(fn, <span class="dt">offset=</span><span class="kw">unlist</span>(tiles[i,<span class="kw">c</span>(<span class="st">&quot;offset.y&quot;</span>,<span class="st">&quot;offset.x&quot;</span>)]),
                 <span class="dt">region.dim=</span><span class="kw">unlist</span>(tiles[i,<span class="kw">c</span>(<span class="st">&quot;region.dim.y&quot;</span>,<span class="st">&quot;region.dim.x&quot;</span>)]), 
                 <span class="dt">output.dim=</span><span class="kw">unlist</span>(tiles[i,<span class="kw">c</span>(<span class="st">&quot;region.dim.y&quot;</span>,<span class="st">&quot;region.dim.x&quot;</span>)]),
                 <span class="dt">silent =</span> <span class="ot">TRUE</span>)
    x<span class="op">$</span>mask =<span class="st"> </span><span class="kw">ifelse</span>(x<span class="op">$</span>band1<span class="op">&gt;</span>threshold, <span class="dv">1</span>, <span class="dv">0</span>)
    <span class="kw">writeGDAL</span>(x[<span class="st">&quot;mask&quot;</span>], <span class="dt">type=</span><span class="st">&quot;Byte&quot;</span>, <span class="dt">mvFlag =</span> <span class="dv">255</span>, out.tif, <span class="dt">options=</span><span class="kw">c</span>(<span class="st">&quot;COMPRESS=DEFLATE&quot;</span>))
  }
}</code></pre></div>
<p>This can now be run through <code>mclapply</code> function from the parallel package (which automatically employs all available cores):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x0 &lt;-<span class="st"> </span><span class="kw">mclapply</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(tiles), <span class="dt">FUN=</span>fun_mask, <span class="dt">tiles=</span>tiles)</code></pre></div>
<p>We can look in the the tiles folder, and this should show 35 produced GeoTiffs. These can be further used to construct a virtual mosaic by using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t.lst &lt;-<span class="st"> </span><span class="kw">list.files</span>(<span class="dt">path=</span><span class="st">&quot;extdata/tiled&quot;</span>, <span class="dt">pattern=</span><span class="kw">glob2rx</span>(<span class="st">&quot;^T*.tif$&quot;</span>), 
                    <span class="dt">full.names=</span><span class="ot">TRUE</span>, <span class="dt">recursive=</span><span class="ot">TRUE</span>)
<span class="kw">cat</span>(t.lst, <span class="dt">sep=</span><span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="dt">file=</span><span class="st">&quot;SP27GTIF_tiles.txt&quot;</span>)
<span class="kw">system</span>(<span class="st">&#39;gdalbuildvrt -input_file_list SP27GTIF_tiles.txt SP27GTIF.vrt&#39;</span>)
<span class="kw">system</span>(<span class="st">&#39;gdalwarp SP27GTIF.vrt SP27GTIF_mask.tif -ot </span><span class="ch">\&quot;</span><span class="st">Byte</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
  <span class="st">&#39; -dstnodata 255 -co </span><span class="ch">\&quot;</span><span class="st">BIGTIFF=YES</span><span class="ch">\&quot;</span><span class="st"> -r </span><span class="ch">\&quot;</span><span class="st">near</span><span class="ch">\&quot;</span><span class="st"> -overwrite -co </span><span class="ch">\&quot;</span><span class="st">COMPRESS=DEFLATE</span><span class="ch">\&quot;</span><span class="st">&#39;</span>)</code></pre></div>
<p>Note we use a few important settings here for GDAL e.g. <code>-overwrite -co &quot;COMPRESS=DEFLATE&quot;</code> to overwrite the GeoTiff and internally compress it to save space and <code>-r &quot;near&quot;</code> basically no resampling just binding tiles together. Also, if the output GeoTiff is HUGE, you will most likely have to turn on <code>-co &quot;BIGTIFF=YES&quot;</code> otherwise <code>gdalwarp</code> would not run through. The output mosaic looks like this:</p>
<div class="figure" style="text-align: center">
<img src="figures/sp27gtif_mask.png" alt="Final processed output." width="80%" />
<p class="caption">
(#fig:sp27gtif-mask)Final processed output.
</p>
</div>
<p>This demonstrates that R can be used to compute with large rasters provided that these operations can be parallelized. Suggested best practice for this is to: (1) design a tiling system that optimizes use of RAM and read/write speed of a disk, (2) prepare and test a function that can be then run in parallel, and (3) stitch back all tiles to a large raster using <code>gdalwarp</code>.</p>
<p>Note that Tiling and and stitching can not be applied universally to all problems e.g. functions that require global geographical search or all data in the raster, in such cases tiling should be applied with overlap (to minimize boundary effects) or to irregular tiling systems (e.g. per watershed). Once an optimal tiling system and function is prepared, R is no longer limited to running efficient computing, but only dependent on how much RAM and cores you have available i.e. it becomes more a hardware than a software problem.</p>
</div>
</div>
<div id="summary-points-1" class="section level2">
<h2><span class="header-section-number">4.3</span> Summary points</h2>
<p>Soil covariate layers are one of the key inputs to predictive soil mapping. Before any spatial layer can be used for modeling, it typically needs to be preprocessed to remove artifacts, resample to a standard resolution, fill in the missing values etc. All these operations can be successfully run by combining R and Open Source GIS software and by careful programming and optimization.</p>
<p>Preparing soil covariates can often be time and resources consuming so careful preparation and prioritization of processing is highly recommended. <span class="citation">Hengl, Mendes de Jesus, et al. (<a href="#ref-Hengl2017SoilGrids250m">2017</a>)</span> show that, for soil types and soil textures, DEM-parameters, i.e. soil forming factors of relief, especially flow-based DEM-indices, emerge as the second-most dominant covariates. These results largely correspond with conventional soil survey knowledge (surveyors have been using relief as a key guideline to delineate soil bodies for decades).</p>
<p>Although lithology is not in the list of the top 15 most important predictors, spatial patterns of lithologic classes can often be distinctly recognized in the output predictions. This is especially true for soil texture fractions and coarse fragments. In general, for predicting soil chemical properties, climatic variables (especially precipitation) and surface reflectance seem to be the most important, while for soil classes and soil physical properties it is a combination of relief, vegetation dynamics and parent material. Investing extra time in preparing a better map of soil parent material is often a good idea.</p>
<p>Other potentially useful covariates for predicting soil properties and classes could be maps of paleo i.e. pre-historic climatic conditions of soil formation, e.g. glacial landscapes and processes, past climate conditions and similar. These could likely become significant predictors of many current soil characteristics. Information on pre-historic climatic conditions and land use is unfortunately often not available, especially not at detailed cartographic scales, although there are now several global products that represent, for example, dynamics of land use / changes of land cover (see e.g. HYDE data set <span class="citation">(Klein Goldewijk et al. <a href="#ref-klein2011hyde">2011</a>)</span>) through the past 1500+ years. As the spatial detail and completeness of such pre-historic maps increases, they will become potentially interesting covariates for global soil modeling.</p>
<p>USA’s NASA and USGS, with its MODIS, Landsat and similar civil-applications missions will likely remain the main source of spatial covariate data to support global and local soil mapping initiatives.</p>
<!--chapter:end:Soil_covariates.Rmd-->
</div>
</div>
<div id="statistical-theory" class="section level1">
<h1><span class="header-section-number">5</span> Statistical theory for predictive soil mapping</h1>
<p><em>Edited by: Hengl T., Heuvelink G.B.M and MacMillan R. A.</em></p>
<div id="aspects-variability" class="section level2">
<h2><span class="header-section-number">5.1</span> Aspects of spatial variability of soil variables</h2>
<p>In this chapter we review the statistical theory for soil mapping. We focus on models considered most suitable for practical implementation and use with soil profile data and gridded covariates, and we provide the mathematical-statistical details of the selected models. We start by revisiting some basic statistical aspects of soil mapping, and conclude by illustrating a proposed framework for reproducible, semi-automated mapping of soil variables using simple, real-world examples.</p>
<p>The code and examples are provided only for illustration. More complex predictive modeling is described in section @ref(soilmapping-using-mla). To install and optimize all packages used in this chapter please refer to section @ref(Rstudio).</p>
<div id="modelling-soil-variability" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Modelling soil variability</h3>
<p>Soils vary spatially in a way that is often only partially understood. The main (deterministic) causes of soil spatial variation are the well-known causal factors — climate, organisms, relief, parent material and time — but how these factors jointly shape the soil over time is a very complex process that is (still) extremely difficult to model mechanistically. Moreover, mechanistic modelling approaches require large sets of input data that are realistically not available in practice. Some initial steps have been made, notably for mechanistic modelling of vertical soil variation (see e.g. <span class="citation">Finke and Hutson (<a href="#ref-Finke2008462">2008</a>)</span>, <span class="citation">Sommer, Gerke, and Deumlich (<a href="#ref-Sommer2008480">2008</a>)</span>, <span class="citation">Minasny, McBratney, and Salvador-Blanes (<a href="#ref-Minasny2008140">2008</a>)</span>, and <span class="citation">Vanwalleghem et al. (<a href="#ref-vanwalleghem2010spatial">2010</a>)</span>), but existing approaches are still rudimentary and cannot be used for operational soil mapping. Mainstream soil mapping therefore takes an empirical approach in which the relationship between the soil variable of interest and causal factors (or their proxies) is modelled statistically, using various types of regression models. The explanatory variables used in regression are also known as <em>covariates</em> (a list of common covariates used in soil mapping is provided in chapter @ref(soil-covs-chapter)).</p>
<p>Regression models explain only part of the variation (i.e. variance) of the soil variable of interest, because:</p>
<ul>
<li><p><em>The structure of the regression model does not represent the true mechanistic relationship between the soil and its causal factors</em>.</p></li>
<li><p><em>The regression model includes only a few of the many causal factors that formed the soil</em>.</p></li>
<li><p><em>The covariates used in regression are often only incomplete proxies of the true soil forming factors</em>.</p></li>
<li><p><em>The covariates often contain measurement errors and/or are measured at a much coarser scale (i.e. support) than that of the soil that needs to be mapped</em>.</p></li>
</ul>
<p>As a result, soil spatial regression models will often display a substantial amount of residual variance, which may well be larger than the amount of variance explained by the regression itself. The residual variation can subsequently be analysed on spatial structure through a variogram analysis. If there is spatial structure, then kriging the residual and incorporating the result of this in mapping can improve the accuracy of soil predictions <span class="citation">(Hengl, Heuvelink, and Rossiter <a href="#ref-hengl2007regression">2007</a>)</span>.</p>
</div>
<div id="umsv" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Universal model of soil variation</h3>
<p>From a statistical point of view, it is convenient to distinguish between three major components of soil variation: (1) deterministic component (trend), (2) spatially correlated component and (3) pure noise. This is the basis of the <em>universal model of soil variation</em> [<span class="citation">Burrough and McDonnell (<a href="#ref-Burrough1998OUP">1998</a>)</span>;Webster2001Wiley p.133]:</p>
<span class="math display">\[\begin{equation}
Z({s}) = m({s}) + \varepsilon &#39;({s}) + \varepsilon &#39;&#39;({s})
(\#eq:ukm)
\end{equation}\]</span>
<p>where <span class="math inline">\(s\)</span> is two-dimensional location, <span class="math inline">\(m({s})\)</span> is the deterministic component, <span class="math inline">\(\varepsilon &#39;({s})\)</span> is the spatially correlated stochastic component and <span class="math inline">\(\varepsilon &#39;&#39;({s})\)</span> is the pure noise (micro-scale variation and measurement error). This model was probably first introduced by <span class="citation">Matheron (<a href="#ref-Matheron1969PhD">1969</a>)</span>, and has been used as a general framework for spatial prediction of quantities in a variety of environmental research disciplines.</p>

<div class="rmdnote">
<p>The <em>universal model of soil variation</em> assumes that there are three major components of soil variation: (1) the deterministic component (function of covariates), (2) spatially correlated component (treated as stochastic) and (3) pure noise.</p>
</div>

<p>The universal model of soil variation model (Eq.@ref(eq:ukm)) can be further generalised to three-dimensional space and the spatio-temporal domain (3D+T) by letting the variables also depend on depth and time:</p>
<span class="math display">\[\begin{equation}
Z({s}, d, t) = m({s}, d, t) + \varepsilon &#39;({s}, d, t) + \varepsilon &#39;&#39;({s}, d, t)
(\#eq:ukm3DT)
\end{equation}\]</span>
<p>where <span class="math inline">\(d\)</span> is depth expressed in meters downward from the land surface and <span class="math inline">\(t\)</span> is time. The deterministic component <span class="math inline">\(m\)</span> may be further decomposed into parts that are purely spatial, purely temporal, purely depth-related or mixtures of all three. Space-time statistical soil models are discussed by <span class="citation">Grunwald (<a href="#ref-Grunwald2005CRCPress">2005</a><a href="#ref-Grunwald2005CRCPress">b</a>)</span>, but this area of soil mapping is still rather experimental.</p>
<p>In this chapter, we mainly focus on purely 2D models but also present some theory for 3D models, while 2D+T and 3D+T models of soil variation are significantly more complex (Fig. @ref(fig:scheme-2D-3D-maps)).</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_2D_3DT_maps.png" alt="Number of variogram parameters assuming an exponential model, minimum number of samples and corresponding increase in number of prediction locations for 2D, 3D, 2D+T and 3D+T models of soil variation. Here “altitude” refers to vertical distance from the land surface, which is in case of soil mapping often expressed as negative vertical distance from the land surface." width="60%" />
<p class="caption">
(#fig:scheme-2D-3D-maps)Number of variogram parameters assuming an exponential model, minimum number of samples and corresponding increase in number of prediction locations for 2D, 3D, 2D+T and 3D+T models of soil variation. Here “altitude” refers to vertical distance from the land surface, which is in case of soil mapping often expressed as negative vertical distance from the land surface.
</p>
</div>
<p>One of the reasons why 2D+T and 3D+T models of soil variations are rare is because there are very few point data sets that satisfy the requirements for analysis. One national soil data set that could be analyzed using space-time geostatistics is, for example, the Swiss soil-monitoring network (NABO) data set <span class="citation">(Desaules, Ammann, and Schwab <a href="#ref-JPLN:JPLN200900269">2010</a>)</span>, but even this data set does not contain complete profile descriptions following international standards. At regional and global scales it would be even more difficult to find enough data to fit space-time models (and to fit 3D+T variogram models could be even more difficult). For catchments and plots, space-time datasets of soil moisture have been recorded and used in space-time geostatistical modelling (see e.g. <span class="citation">Snepvangers, Heuvelink, and Huisman (<a href="#ref-snepvangers2003soil">2003</a>)</span> and <span class="citation">Jost, Heuvelink, and Papritz (<a href="#ref-jost2005analysing">2005</a>)</span>).</p>
<p>Statistical modelling of the spatial distribution of soils requires field observations because most statistical methods are data-driven. The minimum recommended number of points required to fit 2D geostatistical models, for example, is in the range 50–100 points, but this number increases with any increase in spatial or temporal dimension (Fig. @ref(fig:scheme-2D-3D-maps)). The Cookfarm data set for example contains hundreds of thousands of observations, although the study area is relatively small and there are only ca. 50 station locations <span class="citation">(Gasch et al. <a href="#ref-Gasch2015SPASTA">2015</a>)</span>.</p>
<p>The deterministic and stochastic components of soil spatial variation are separately described in more detail in subsequent sections, but before we do this, we first address soil vertical variability and how it can be modelled statistically.</p>
</div>
<div id="soil-depth-models" class="section level3">
<h3><span class="header-section-number">5.1.3</span> Modelling the variation of soil with depth</h3>
<p>Soil properties vary with depth, in some cases much more than in the horizontal direction. There is an increasing awareness that the vertical dimension is important and needs to be incorporated in soil mapping. For example, many spatial prediction models are built using ambiguous vertical reference frames such as predicted soil property for <em>“top-soil”</em> or <em>“A-horizon”</em>. Top-soil can refer to different depths / thicknesses and so can the A-horizon range from a few centimeters to over one meter. Hence before fitting a 2D spatial model to soil profile data, it is a good idea to standardize values to standard depths, otherwise soil observation depth becomes an additional source of uncertainty. For example soil organic carbon content is strongly controlled by soil depth, so combining values from two A horizons one thick and the other thin, would increase the complexity of 2D soil mapping because a fraction of the variance is controlled by the depth, which is ignored.</p>
<p>The concept of perfectly homogeneous soil horizons is often too restrictive and can be better replaced with continuous representations of soil vertical variation i.e. <em>soil-depth functions</em> or curves. Variation of soil properties with depth is typically modelled using one of two approaches (Fig. @ref(fig:soil-depth-examples)):</p>
<ol style="list-style-type: decimal">
<li><p><em>Continuous vertical variation</em> — This assumes that soil variables change continuously with depth. The soil-depth relationship is modelled using either:</p>
<ol style="list-style-type: decimal">
<li><p><em>Parametric model</em> — The relationship is modelled using mathematical functions such as logarithmic or exponential decay functions.</p></li>
<li><p><em>Non-parametric model</em> — The soil property changes continuously but without obvious regularity with depth. Changes in values are modelled using locally fitted functions such as piecewise linear functions or splines.</p></li>
</ol></li>
<li><p><em>Abrupt or stratified vertical variation</em> — This assumes that soil horizons are distinct and homogeneous bodies of soil material and that soil properties are constant within horizons and change abruptly at boundaries between horizons.</p></li>
</ol>
<p>Combinations of the two approaches are also possible, such as the use of exponential decay functions per soil horizon <span class="citation">(Kempen, Brus, and Stoorvogel <a href="#ref-Kempen2011Geoderma">2011</a>)</span>.</p>
<p>Parametric continuous models are chosen to reflect pedological knowledge e.g. knowledge of soil forming processes. For example, organic carbon usually originates from plant production i.e. litter or roots. Generally, the upper layers of the soil tend to have greater organic carbon content, which decreases continuously with depth, so that the soil-depth relationship can be modelled with a negative-exponential function:</p>
<span class="math display">\[\begin{equation}
{\texttt{ORC}} (d) = {\texttt{ORC}} (d_0) \cdot \exp(-\tau \cdot d)
(\#eq:SOMdepth)
\end{equation}\]</span>
<p>where <span class="math inline">\(\texttt{ORC}(d)\)</span> is the soil organic carbon content at depth (<span class="math inline">\(d\)</span>), <span class="math inline">\({\texttt{ORC}} (d_0)\)</span> is the organic carbon content at the soil surface and <span class="math inline">\(\tau\)</span> is the rate of decrease with depth. This model has only two parameters that must be chosen such that model averages over sampling horizons match those of the observations as closely as possible. Once the model parameters have been estimated, we can easily predict concentrations for any depth interval.</p>
<p>Consider for example this sample profile from Nigeria:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lon =<span class="st"> </span><span class="fl">3.90</span>; lat =<span class="st"> </span><span class="fl">7.50</span>; id =<span class="st"> &quot;ISRIC:NG0017&quot;</span>; FAO1988 =<span class="st"> &quot;LXp&quot;</span> 
top =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">18</span>, <span class="dv">36</span>, <span class="dv">65</span>, <span class="dv">87</span>, <span class="dv">127</span>) 
bottom =<span class="st"> </span><span class="kw">c</span>(<span class="dv">18</span>, <span class="dv">36</span>, <span class="dv">65</span>, <span class="dv">87</span>, <span class="dv">127</span>, <span class="dv">181</span>)
ORCDRC =<span class="st"> </span><span class="kw">c</span>(<span class="fl">18.4</span>, <span class="fl">4.4</span>, <span class="fl">3.6</span>, <span class="fl">3.6</span>, <span class="fl">3.2</span>, <span class="fl">1.2</span>)
munsell =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;7.5YR3/2&quot;</span>, <span class="st">&quot;7.5YR4/4&quot;</span>, <span class="st">&quot;2.5YR5/6&quot;</span>, <span class="st">&quot;5YR5/8&quot;</span>, <span class="st">&quot;5YR5/4&quot;</span>, <span class="st">&quot;10YR7/3&quot;</span>)
## prepare a SoilProfileCollection:
prof1 &lt;-<span class="st"> </span>plyr<span class="op">::</span><span class="kw">join</span>(<span class="kw">data.frame</span>(id, top, bottom, ORCDRC, munsell), 
         <span class="kw">data.frame</span>(id, lon, lat, FAO1988), <span class="dt">type=</span><span class="st">&#39;inner&#39;</span>) 
<span class="co">#&gt; Joining by: id</span>
prof1<span class="op">$</span>mdepth &lt;-<span class="st"> </span>prof1<span class="op">$</span>top<span class="op">+</span>(prof1<span class="op">$</span>bottom<span class="op">-</span>prof1<span class="op">$</span>top)<span class="op">/</span><span class="dv">2</span></code></pre></div>
<p>we can fit a log-log model by using e.g.:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d.lm &lt;-<span class="st"> </span><span class="kw">glm</span>(ORCDRC <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(mdepth), <span class="dt">data=</span>prof1, <span class="dt">family=</span><span class="kw">gaussian</span>(log))
<span class="kw">options</span>(<span class="kw">list</span>(<span class="dt">scipen=</span><span class="dv">3</span>, <span class="dt">digits=</span><span class="dv">2</span>))
d.lm<span class="op">$</span>fitted.values
<span class="co">#&gt;    1    2    3    4    5    6 </span>
<span class="co">#&gt; 18.1  6.3  3.5  2.4  1.7  1.2</span></code></pre></div>
<p>which shows that the log-log fit comes relatively close to the actual values. Another possibility would be to fit a power-law model:</p>
<span class="math display">\[\begin{equation}
{\texttt{ORC}} (d) = a \cdot d^b
(\#eq:loglog)
\end{equation}\]</span>
<p>A disadvantage of a single parametric soil property-depth model along the entire soil profile is that these completely ignore stratigraphy and abrupt changes at the boundaries between soil horizons. For example, <span class="citation">Kempen, Brus, and Stoorvogel (<a href="#ref-Kempen2011Geoderma">2011</a>)</span> show that there are many cases where highly contrasting layers of peat can be found buried below the surface due to cultivation practices or holocene drift sand. The model given by Eq.@ref(eq:loglog) illustrated in Fig. @ref(fig:soil-depth-examples) (left) will not be able to represent such abrupt changes.</p>

<div class="rmdnote">
Before fitting a 2D spatial prediction model to soil profile data, it is important to standardize values to standard depths, otherwise soil observation depth can be an additional source of uncertainty.
</div>

<p>Non-parametric soil-depth functions are more flexible and can represent observations of soil property averages for sampling layers or horizons more accurately. One such technique that is particularly interesting is <em>equal-area or mass-preserving splines</em> <span class="citation">(Bishop, McBratney, and Laslett <a href="#ref-Bishop1999Geoderma">1999</a>; Malone et al. <a href="#ref-Malone2009Geoderma">2009</a>)</span> because it ensures that, for each sampling layer (usually a soil horizon), the average of the spline function equals the measured value for the horizon. Disadvantages of the spline model are that it may not fit well if there are few observations along the soil profile and that it may create unrealistic values (through overshoots or extrapolation) in some instances, for example near the surface. Also, mass-preserving splines cannot accommodate discontinuities unless, of course, separate spline functions are fitted above and below the discontinuity.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_soil_depth_examples.png" alt="Vertical variation in soil carbon modelled using a logarithmic function (left) and a mass-preserving spline (right) with abrupt changes by horizon ilustrated with solid lines." width="100%" />
<p class="caption">
(#fig:soil-depth-examples)Vertical variation in soil carbon modelled using a logarithmic function (left) and a mass-preserving spline (right) with abrupt changes by horizon ilustrated with solid lines.
</p>
</div>
<p>To fit mass preserving splines we can use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(aqp)
<span class="co">#&gt; This is aqp 1.16-3</span>
<span class="kw">library</span>(rgdal)
<span class="co">#&gt; Loading required package: sp</span>
<span class="co">#&gt; rgdal: version: 1.3-6, (SVN revision 773)</span>
<span class="co">#&gt;  Geospatial Data Abstraction Library extensions to R successfully loaded</span>
<span class="co">#&gt;  Loaded GDAL runtime: GDAL 2.3.2, released 2018/09/21</span>
<span class="co">#&gt;  Path to GDAL shared files: /usr/share/gdal</span>
<span class="co">#&gt;  GDAL binary built with GEOS: TRUE </span>
<span class="co">#&gt;  Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493]</span>
<span class="co">#&gt;  Path to PROJ.4 shared files: (autodetected)</span>
<span class="co">#&gt;  Linking to sp version: 1.3-1</span>
<span class="kw">library</span>(GSIF)
<span class="co">#&gt; GSIF version 0.5-4 (2017-04-25)</span>
<span class="co">#&gt; URL: http://gsif.r-forge.r-project.org/</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: &#39;GSIF&#39;</span>
<span class="co">#&gt; The following object is masked _by_ &#39;.GlobalEnv&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     munsell</span>
prof1.spc &lt;-<span class="st"> </span>prof1
<span class="kw">depths</span>(prof1.spc) &lt;-<span class="st"> </span>id <span class="op">~</span><span class="st"> </span>top <span class="op">+</span><span class="st"> </span>bottom
<span class="co">#&gt; Warning: converting IDs from factor to character</span>
<span class="kw">site</span>(prof1.spc) &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>lon <span class="op">+</span><span class="st"> </span>lat <span class="op">+</span><span class="st"> </span>FAO1988 
<span class="kw">coordinates</span>(prof1.spc) &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>lon <span class="op">+</span><span class="st"> </span>lat
<span class="kw">proj4string</span>(prof1.spc) &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="st">&quot;+proj=longlat +datum=WGS84&quot;</span>)
## fit a spline:
ORCDRC.s &lt;-<span class="st"> </span><span class="kw">mpspline</span>(prof1.spc, <span class="dt">var.name=</span><span class="st">&quot;ORCDRC&quot;</span>, <span class="dt">show.progress=</span><span class="ot">FALSE</span>)
<span class="co">#&gt; Fitting mass preserving splines per profile...</span>
ORCDRC.s<span class="op">$</span>var.std
<span class="co">#&gt;   0-5 cm 5-15 cm 15-30 cm 30-60 cm 60-100 cm 100-200 cm soil depth</span>
<span class="co">#&gt; 1     21      17      7.3      3.3       3.6        1.8        181</span></code></pre></div>
<p>where <code>var.std</code> shows average fitted values for standard depth intervals (i.e. those given in the <em>GlobalSoilMap</em> specifications), and <code>var.1cm</code> are the values fitted at 1–cm increments (Fig. @ref(fig:soil-depth-examples)).</p>
<p>A disadvantage of using mathematical functions to convert soil observations at specific depth intervals to continuous values along the whole profile is that these values are only estimates with associated estimation errors. If estimates are treated as if these were observations then an important source of error is ignored, which may jeopardize the quality of the final soil predictions and in particular the associated uncertainty (see further Section @ref(accuracy-assessment)). This problem can be avoided by taking, for example, a 3D modelling approach <span class="citation">(Poggio and Gimona <a href="#ref-poggio2014national">2014</a>; Hengl <a href="#ref-Hengl2015AfSoilGrids250m">2015</a>)</span>, in which model calibration and spatial interpolation are based on the original soil observations directly (although proper use of this requires that the differences in vertical support between measurements are taken into account also). We will address this also in later sections of this chapter, among others in Section @ref(prediction-3D).</p>

<div class="rmdnote">
Soil property-depth relationships are commonly modelled using various types of mathematical functions. Mass-preserving splines, which ensure that the average of the spline function equals the measured value for each sampling layer or horizon, can be used to convert measurements per layer to point values along the profile. Because soils can show both abrupt and continuous transitions within the same profile, no simple spline model is universally valid and case-dependent adjustments often need to be made.
</div>

</div>
<div id="vertical-aggregation" class="section level3">
<h3><span class="header-section-number">5.1.4</span> Vertical aggregation of soil properties</h3>
<p>As mentioned previously, soil variables refer to aggregate values over specific depth intervals (see Fig. @ref(fig:soil-depth-examples)). For example, the organic carbon content is typically observed per soil horizon with values in e.g. g/kg or permilles <span class="citation">(Conant et al. <a href="#ref-Conant2010">2010</a>; Baritz et al. <a href="#ref-Rainer2010">2010</a>; Panagos et al. <a href="#ref-Panagos2013439">2013</a>)</span>. The <em>Soil Organic Carbon Storage</em> (or <em>Soil Organic Carbon Stock</em>) in the whole profile can be calculated by using Eq @ref(eq:ocs). Once we have determined soil organic carbon storage (<span class="math inline">\(\mathtt{OCS}\)</span>) per horizon, we can derive the total organic carbon in the soil by summing over all (<span class="math inline">\(H\)</span>) horizons:</p>
<span class="math display">\[\begin{equation}
\mathtt{OCS} = \sum\limits_{h = 1}^H { \mathtt{OCS}_h }
(\#eq:ORGCsum)
\end{equation}\]</span>
<p>Obviously, the horizon-specific soil organic carbon content (<span class="math inline">\(\mathtt{ORC}_h\)</span>) and total soil organic carbon content (<span class="math inline">\(\mathtt{OCS}\)</span>) are NOT the same variables and need to be analysed and mapped separately.</p>
<p>In the case of pH (<span class="math inline">\(\mathtt{PHI}\)</span>) we usually do not aim at estimating the actual mass or quantity of hydrogen ions. To represent a soil profile with a single number, we may take a weighted mean of the measured pH values per horizon:</p>
<span class="math display">\[\begin{equation}
\mathtt{PHI} = \sum\limits_{h = 1}^H { w_h \cdot \mathtt{PHI}_h }; \qquad \sum\limits_{h = 1}^H{w_h} = 1
(\#eq:pHmean)
\end{equation}\]</span>
<p>where the weights can be chosen proportional to the horizon thickness:</p>
<span class="math display">\[\begin{equation}
w _h  = \frac{{\mathtt{HSIZE}_h}}{\sum\limits_{h = 1}^H {{\mathtt{HSIZE}}_h}}
\end{equation}\]</span>
<p>Thus, it is important to be aware that all soil variables: (A) can be expressed as relative (percentages) or absolute (mass / quantities) values, and (B) refer to specific horizons or depth intervals or to the whole soil profile.</p>
<p>Similar <em>support</em>-effects show up in the horizontal, because soil observations at <em>point</em> locations are not the same as average or <em>bulk soil samples</em> taken by averaging a large number of point observations on a site or plot <span class="citation">(Webster and Oliver <a href="#ref-Webster2001Wiley">2001</a>)</span>.</p>

<div class="rmdnote">
Soil variables can refer to a specific depth interval or to the whole profile. The differences in spatial patterns between variables representing fundamentally the same feature (e.g. soil organic carbon in of a specific soil horizon or soil layer and total organic carbon stock in of the whole profile), but at different spatial and vertical support, can be significant.
</div>

<p>In order to avoid misinterpretation of the results of mapping, we recommend that any delivered map of soil properties should specify the support size in the vertical and lateral directions, the analysis method (detection limit) and measurement units. Such information can be included in the metadata and/or in any key visualization or plot. Likewise, any end-user of soil data should specify whether estimates of the relative or total organic carbon, aggregated or at 2D/3D point support are required.</p>
</div>
</div>
<div id="spatial-prediction-of-soil-variables" class="section level2">
<h2><span class="header-section-number">5.2</span> Spatial prediction of soil variables</h2>
<div id="main-principles" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Main principles</h3>
<p><em>“Pragmatically, the goal of a model is to predict, and at the same time scientists want to incorporate their understanding of how the world works into their models”</em> <span class="citation">(Cressie and Wikle <a href="#ref-cressie2011statistics">2011</a>)</span>. In general terms, spatial prediction consists of the following seven steps (Fig. @ref(fig:general-sp-process)):</p>
<ol style="list-style-type: decimal">
<li><p><em>Select the target variable, scale (spatial resolution) and associated geographical region of interest</em>;</p></li>
<li><p><em>Define a model of spatial variation for the target variable</em>;</p></li>
<li><p><em>Prepare a sampling plan and collect samples and relevant explanatory variables</em>;</p></li>
<li><p><em>Estimate the model parameters using the collected data</em>;</p></li>
<li><p><em>Derive and apply the spatial prediction method associated with the selected model</em>;</p></li>
<li><p><em>Evaluate the spatial prediction outputs and collect new data / run alternative models if necessary</em>;</p></li>
<li><p><em>Use the outputs of the spatial prediction process for decision making and scenario testing</em>.</p></li>
</ol>
<div class="figure" style="text-align: center">
<img src="figures/Fig_general_SP_process.png" alt="From data to knowledge and back: the general spatial prediction scheme applicable to many environmental sciences." width="85%" />
<p class="caption">
(#fig:general-sp-process)From data to knowledge and back: the general spatial prediction scheme applicable to many environmental sciences.
</p>
</div>
<p>The spatial prediction process is repeated at all nodes of a grid covering <span class="math inline">\(D\)</span> (or a space-time domain in case of spatiotemporal prediction) and produces three main outputs:</p>
<ol style="list-style-type: decimal">
<li><p>Estimates of the model parameters (e.g., regression coefficients and variogram parameters), i.e. the <strong>model</strong>;</p></li>
<li><p>Predictions at new locations, i.e. a <strong>prediction map</strong>;</p></li>
<li><p>Estimate of uncertainty associated with the predictions, i.e. a <strong>prediction error variance map</strong>.</p></li>
</ol>
<p>It is clear from Fig. @ref(fig:general-sp-process) that the key steps in the mapping procedure are: (a) <em>choice of the sampling scheme</em> (e.g. <span class="citation">Ng et al. (<a href="#ref-Ng2018">2018</a>)</span>), (b) <em>choice of the model of spatial variation</em> (e.g. <span class="citation">Diggle and Ribeiro Jr (<a href="#ref-Diggle2007Springer">2007</a>)</span>), and (c) <em>choice of the parameter estimation technique</em> (e.g. <span class="citation">Lark, Cullis, and Welham (<a href="#ref-lark2006spatial">2006</a>)</span>). When the sampling scheme is given and cannot be changed, the focus of optimization of the spatial prediction process is then on selecting and fine-tuning the best performing spatial prediction method.</p>
<p>In a geostatistical framework, spatial prediction is estimation of values of some target variable <span class="math inline">\(Z\)</span> at a new location (<span class="math inline">\({s}_0\)</span>) given the input data:</p>
<span class="math display">\[\begin{equation}
\hat Z({s}_0) = E\left\{ Z({s}_0)|z({s}_i), \; {{X}}({s}_0), \; i=1,...,n \right\}
(\#eq:sp)
\end{equation}\]</span>
<p>where the <span class="math inline">\(z({s}_i)\)</span> are the input set of observations of the target variable, <span class="math inline">\({s}_i\)</span> is a geographical location, <span class="math inline">\(n\)</span> is the number of observations and <span class="math inline">\({{X}}({s}_0)\)</span> is a list of <em>covariates</em> or explanatory variables, available at all prediction locations within the study area of interest (<span class="math inline">\({s} \in \mathbb{A}\)</span>). To emphasise that the model parameters also influence the outcome of the prediction process, this can be made explicit by writing <span class="citation">(Cressie and Wikle <a href="#ref-cressie2011statistics">2011</a>)</span>:</p>
<span class="math display">\[\begin{equation}
[Z|Y,{{\theta}} ]
(\#eq:datamodel)
\end{equation}\]</span>
<p>where <span class="math inline">\(Z\)</span> is the data, <span class="math inline">\(Y\)</span> is the (hidden) process that we are predicting, and <span class="math inline">\({{\theta}}\)</span> is a list of model parameters (e.g. trend coefficients and variogram parameters).</p>
<p>There are many spatial prediction methods for generating spatial predictions from soil samples and covariate information. All differ in the underlying statistical model of spatial variation, although this model is not always made explicit and different methods may use the same statistical model. A review of currently used digital soil mapping methods is given, for example, in <span class="citation">McBratney et al. (<a href="#ref-McBratney2011HSS">2011</a>)</span>, while the most extensive review can be found in <span class="citation">McBratney, Mendoça Santos, and Minasny (<a href="#ref-McBratney2003Geoderma">2003</a>)</span> and <span class="citation">McBratney, Minasny, and Stockmann (<a href="#ref-mcbratney2018pedometrics">2018</a>)</span>. <span class="citation">Li and Heap (<a href="#ref-LiHeap2010EI">2010</a>)</span> list 40+ spatial prediction / spatial interpolation techniques. Many spatial prediction methods are often just different names for essentially the same thing.<br />
What is often known under a single name, in the statistical, or mathematical literature, can be implemented through different computational frameworks, and lead to different outputs (mainly because many models are not written out in the finest detail and leave flexibility for actual implementation).</p>
</div>
<div id="soil-sampling" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Soil sampling</h3>
<p>A <em>soil sample</em> is a collection of field observations, usually represented as points. Statistical aspects of sampling methods and approaches are discussed in detail by <span class="citation">Schabenberger and Gotway (<a href="#ref-schabenberger2005statistical">2005</a>)</span> and <span class="citation">de Gruijter et al. (<a href="#ref-deGruijter2006sampling">2006</a>)</span>, while some more practical suggestions for soil sampling can be found in <span class="citation">Pansu, Gautheyrou, and Loyer (<a href="#ref-pansu2001soil">2001</a>)</span> <span class="citation">Webster and Oliver (<a href="#ref-Webster2001Wiley">2001</a>)</span>, <span class="citation">Tan (<a href="#ref-tan2005soil">2005</a>)</span>, and <span class="citation">Legros (<a href="#ref-Legros2006SP">2006</a>)</span>. Some general recommendations for soil sampling are:</p>
<ol style="list-style-type: decimal">
<li><p><em>Points need to cover the entire geographical area of interest and not overrepresent specific subareas that have much different characteristics than the main area.</em></p></li>
<li><p><em>Soil observations at point locations should be made using consistent measurement methods. Replicates should ideally be taken to quantify the measurement error.</em></p></li>
<li><p><em>Bulk sampling is recommended when short-distance spatial variation is expected to be large and not of interest to the map user.</em></p></li>
<li><p><em>If a variogram is to be estimated then the sample size should be &gt;50 and there should be sufficient point pairs with small separation distances.</em></p></li>
<li><p><em>If trend coefficients are to be estimated then the covariates at sampling points should cover the entire feature space of each covariate.</em></p></li>
</ol>
<p>The sampling design or rationale used to decide where to locate soil profile observations, or sampling points, is often not clear and may vary from case to case. Therefore, there is no guarantee that available legacy point data used as input to geostatistical modelling will satisfy the recommendations listed above. Many of the legacy profile data locations in the world were selected using convenience sampling. In fact, many points in traditional soil surveys may have been selected and sampled to capture information about unusual conditions or to locate boundaries at points of transition and maximum confusion about soil properties <span class="citation">(Legros <a href="#ref-Legros2006SP">2006</a>)</span>. Once a soil becomes recognized as being widely distributed and dominant in the landscape, field surveyors often choose not to record observations when that soil is encountered, preferring to focus instead on recording unusual sites or areas where soil transition occurs. Thus the population of available soil point observations may not be representative of the true population of soils, with some soils being either over or under-represented.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_eberg_sampling_locs.png" alt="Occurrence probabilities derived for the actual sampling locations (left), and for a purely random sample design with exactly the same number of points (right). Probabilities derived using the `spsample.prob` function from the package. The shaded area on the left indicates which areas (in the environmental space) have been systematically represented, while the white colour indicates areas which have been systematically omitted (and which is not by chance)." width="100%" />
<p class="caption">
(#fig:eberg-sampling-locs)Occurrence probabilities derived for the actual sampling locations (left), and for a purely random sample design with exactly the same number of points (right). Probabilities derived using the <code>spsample.prob</code> function from the package. The shaded area on the left indicates which areas (in the environmental space) have been systematically represented, while the white colour indicates areas which have been systematically omitted (and which is not by chance).
</p>
</div>
<p>Fig. @ref(fig:eberg-sampling-locs) (the Ebergötzen study area) illustrates a problem of dealing with clustered samples and omission of environmental features. Using the actual samples shown in the plot on the left of Fig. @ref(fig:eberg-sampling-locs) we would like to map the whole area inside the rectangle. This is technically possible, but the user should be aware that the actual Ebergötzen points systematically miss sampling some environmental features: in this case natural forests / rolling hills that were not of interest to the survey project. This does not mean that the Ebergötzen point data are not applicable for geostatistical analyses. It simply means that the sampling bias and under-representation of specific environmental conditions will lead to spatial predictions that may be biased and highly uncertain under these conditions <span class="citation">(Brus and Heuvelink <a href="#ref-Brus2007Geoderma">2007</a>)</span>.</p>
</div>
<div id="sec:expertsystems" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Knowledge-driven soil mapping</h3>
<p>As mentioned previously in section @ref(tacit-knowledge), knowledge-driven mapping is often based on unstated and unformalized rules and understanding that exists mainly in the minds and memories of the individual soil surveyors who conducted field studies and mapping. Expert, or knowledge-based, information can be converted to mapping algorithms by applying conceptual rules to decision trees and/or statistical models <span class="citation">(MacMillan, Pettapiece, and Brierley <a href="#ref-MacMillan2005CJSS">2005</a>; Walter, Lagacherie, and Follain <a href="#ref-Walter2006DSS">2006</a>; Liu and Zhu <a href="#ref-Liu2009">2009</a>)</span>. For example, a surveyor can define the classification rules subjectively, i.e. based on his/her knowledge of the area, then iteratively adjust the model until the output maps fit his/her expectation of the distribution of soils.</p>
<p>In areas where few, or no, field observations of soil properties are available, the most common way to produce estimates is to rely on expert knowledge, or to base estimates on data from other, similar areas. This is a kind of <em>‘knowledge transfer’</em> system. The best example of a knowledge transfer system is the concept of <em>soil series</em> in the USA <span class="citation">(Simonson <a href="#ref-Simonson1968AA">1968</a>)</span>. Soil series (+phases) are the lowest (most detailed) level classes of soil types typically mapped. Each soil series should consist of pedons having soil horizons that are similar in colour, texture, structure, pH, consistence, mineral and chemical composition, and arrangement in the soil profile.</p>
<p>If one finds the same type of soil series repeatedly at similar locations, then there is little need to sample the soil again at additional, similar, locations and, consequently, soil survey field costs can be reduced. This sounds like an attractive approach because one can minimize the survey costs by focusing on delineating the distribution of soil series only. The problem is that there are &gt;15,000 soil series in the USA <span class="citation">(Smith <a href="#ref-Smith1986SMSS">1986</a>)</span>, which obviously means that it is not easy to recognize the same soil series just by doing rapid field observations. In addition, the accuracy with which one can consistently recognize a soil series may well fail on standard kappa statistics tests, indicating that there may be substantial confusion between soil series (e.g. large measurement error).</p>
<p>Large parts of the world basically contain very few (sparce) field records and hence one will need to <em>improvise</em> to be able to produce soil predictions. One idea to map such areas is to build attribute tables for representative soil types, then map the distribution of these soil types in areas without using local field samples. <span class="citation">Mallavan, Minasny, and McBratney (<a href="#ref-Mallavan2010PSS">2010</a>)</span> refer to soil classes that can be predicted far away from the actual sampling locations as <em>homosoils</em>. The homosoils concept is based on the assumption that locations that share similar environments (e.g. soil-forming factors) are likely to exhibit similar soils and soil properties also.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_cross_section_catena.png" alt="Landform positions and location of a prediction point for the Maungawhau data set." width="100%" />
<p class="caption">
(#fig:cross-section-catena)Landform positions and location of a prediction point for the Maungawhau data set.
</p>
</div>
<p>Expert-based systems also rely on using standard mapping paradigms such as the concept of relating soil series occurrance to landscape position along a toposequence, or catena . Fig. @ref(fig:cross-section-catena), for example, shows a cross-section derived using the elevation data in Fig. @ref(fig:catena-maungawhau-3d). An experienced soil surveyor would visit the area and attempt to produce a diagram showing a sequence of soil types positioned along this cross-section. This expert knowledge can be subsequently utilized as manual mapping rules, provided that it is representative of the area, that it can be formalized through repeatable procedures and that it can be tested using real observations.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_catena_Maungawhau_A.jpg" alt="A cross-section for the Maungawhau volcano dataset commonly used in R to illustrate DEM and image analysis techniques." width="80%" />
<p class="caption">
(#fig:catena-maungawhau-3d)A cross-section for the Maungawhau volcano dataset commonly used in R to illustrate DEM and image analysis techniques.
</p>
</div>
<div class="figure" style="text-align: center">
<img src="figures/Fig_catena_Maungawhau_B.png" alt="Associated values of DEM-based covariates: TWI — Topographic Wetness Index and Valley depth for the cross-section from the previous figure." width="100%" />
<p class="caption">
(#fig:catena-maungawhau)Associated values of DEM-based covariates: TWI — Topographic Wetness Index and Valley depth for the cross-section from the previous figure.
</p>
</div>
<p>If relevant auxiliary information, such as a Digital Elevation Model (DEM), is available for the study area, one can derive a number of DEM parameters that can help to quantify landforms and geomorphological processes. Landforms can also automatically be classified by computing various DEM parameters per pixel, or by using knowledge from, Fig. @ref(fig:catena-maungawhau) (a sample of the study area) to objectively extract landforms and associated soils in an area. Such auxiliary landform information can be informative about the spatial distribution of the soil, which is the key principle of, for example, the SOTER methodology <span class="citation">(Van Engelen and Dijkshoorn <a href="#ref-VanEngelen2012">2012</a>)</span>.</p>
<p>The mapping process of knowledge-driven soil mapping can be summarized as follows <span class="citation">(MacMillan, Pettapiece, and Brierley <a href="#ref-MacMillan2005CJSS">2005</a>; MacMillan et al. <a href="#ref-MacMillan2010DSM">2010</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><em>Sample the study area using transects oriented along topographic cross-sections</em>;</p></li>
<li><p><em>Assign soil types to each landform position and at each sample location</em>;</p></li>
<li><p><em>Derive DEM parameters and other auxiliary data sets</em>;</p></li>
<li><p><em>Develop (fuzzy) rules relating the distribution of soil classes to the auxiliary (mainly topographic) variables</em>;</p></li>
<li><p><em>Implement (fuzzy) rules to allocate soil classes (or compute class probabi;ities) for each grid location</em>;</p></li>
<li><p><em>Generate soil property values for each soil class using representative observations (class centers)</em>;</p></li>
<li><p><em>Estimate values of the target soil variable at each grid location using a weighted average of allocated soil class or membership values and central soil property values for each soil class</em>;</p></li>
</ol>
<p>In mathematical terms, soil property prediction based on fuzzy soil classification values using the SOLIM approach <span class="citation">Zhu et al. (<a href="#ref-Zhu2001">2001</a>; Zhu et al. <a href="#ref-Zhu2010Geoderma">2010</a>)</span> works as follows:</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
 \hat z({s}_0) = \sum\limits_{c_j = 1}^{c_p} {\nu _{c_j} ({s}_0) \cdot z_{c_j} }; &amp; \hspace{.6cm}
 \sum\limits_{c_j = 1}^{c_p} {\nu _j ({s}_0)}  = 1\end{aligned}
(\#eq:solim)
\end{equation}\]</span>
<p>where <span class="math inline">\(\hat z({s}_0)\)</span> is the predicted soil attribute at <span class="math inline">\({s}_0\)</span>, <span class="math inline">\(\nu _{c_j} ({s}_0)\)</span> is the membership value of class <span class="math inline">\(c_j\)</span> at location <span class="math inline">\({s}_0\)</span>, and <span class="math inline">\(z_{c_j}\)</span> is the modal (or best representative) value of the inferred soil attribute of the <span class="math inline">\(c_j\)</span>-th category. The predicted soil attribute is mapped directly from membership maps using a linear additive weighing function. Consider the example of six soil classes <code>A</code>, <code>B</code>, <code>C</code>, <code>D</code>, <code>E</code> and <code>F</code>. The attribute table indicates that soil type <code>A</code> has 10%, <code>B</code> 10%, <code>C</code> 30%, <code>D</code> 40%, <code>E</code> 25%, and <code>F</code> 35% of clay. If the membership values at a grid position are 0.6, 0.2, 0.1, 0.05, 0.00 and 0.00, then Eq.@ref(eq:solim) predicts the clay content as 13.5%.</p>
<p>It is obvious from this work flow that the critical aspects that determine the accuracy of the final predictions are the selection of where we locate the cross-sections and the <em>representative soil profiles</em> and the strength of the relationship between the resulting soil classes and target soil properties. <span class="citation">Qi et al. (<a href="#ref-Qi2006Geoderma">2006</a>)</span>, for example, recommended that the most representative values for soil classes can be identified, if many soil profiles are available, by finding the sampling location that occurs at the grid cell with highest similarity value for a particular soil class. Soil mappers are now increasingly looking for ways to combine expert systems with statistical data mining and regression modelling techniques.</p>
<p>One problem of using a supervised mapping system, as described above, is that it is difficult to get an objective estimate of the prediction error (or at least a robust statistical theory for this has not yet been developed). The only possibility to assess the accuracy of such maps would be to collect independent validation samples and estimate the mapping accuracy following the methods described in section @ref(accuracy-assessment). So, in fact, expert-based systems also depend on statistical sampling and inference for evaluation of the accuracy of the resulting map.</p>
</div>
<div id="regression-kriging" class="section level3">
<h3><span class="header-section-number">5.2.4</span> Geostatistics-driven soil mapping (pedometric mapping)</h3>
<p>Pedometric mapping is based on using statistical models to predict soil properties, which leads us to the field of geostatistics. Geostatistics treats the soil as a realization of a <em>random process</em> <span class="citation">(Webster and Oliver <a href="#ref-Webster2001Wiley">2001</a>)</span>. It uses the point observations and gridded covariates to predict the random process at unobserved locations, which yields conditional probability distributions, whose spread (i.e. standard deviation, width of prediction intervals) explicitly characterizes the uncertainty associated with the predictions. As mentioned previously in section @ref(pedometric-mapping), geostatistics is a data-driven approach to soil mapping in which georeferenced point samples are the key input to map production.</p>
<p>Traditional geostatistics has basically been identified with various ways of variogram modeling and kriging <span class="citation">(Haining, Kerry, and Oliver <a href="#ref-Haining2010GEAN780">2010</a>)</span>. Contemporary geostatistics extends linear models and plain kriging techniques to non-linear and hybrid models; it also extends purely spatial models (2D) to 3D and space-time models <span class="citation">(Schabenberger and Gotway <a href="#ref-schabenberger2005statistical">2005</a>; Bivand, Pebesma, and Rubio <a href="#ref-Bivand2008Springer">2008</a>; Diggle and Ribeiro Jr <a href="#ref-Diggle2007Springer">2007</a>; Cressie and Wikle <a href="#ref-cressie2011statistics">2011</a>)</span>. Implementation of more sophisticated geostatistical models for soil mapping is an ongoing activity and is quite challenging (computationally), especially in the case of fine-resolution mapping of large areas <span class="citation">(Hengl, Mendes de Jesus, et al. <a href="#ref-Hengl2017SoilGrids250m">2017</a>)</span>.</p>
<p>Note also that geostatistical mapping is often restricted to quantitative soil properties. Soil prediction models that predict categorical soil variables such as soil type or soil colour class are often quite complex (see e.g. <span class="citation">Hengl et al. (<a href="#ref-Hengl2007Geoderma">2007</a>)</span> and <span class="citation">Kempen et al. (<a href="#ref-Kempen2009Geoderma">2009</a>)</span> for a discussion). Most large scale soil mapping projects also require predictions in 3D, or at least 2D predictions (layers) for several depth intervals. This can be done by treating each layer separately in a 2D analysis, possibly by taking vertical correlations into account, but also by direct 3D geostatistical modelling. Both approaches are reviewed in the following sections.</p>
<p>Over the last decade statisticians have recommended using <em>model-based geostatistics</em> as the most reliable framework for spatial predictions. The essence of model-based statistics is that <em>“the statistical methods are derived by applying general principles of statistical inference based on an explicitly declared stochastic model of the data generating mechanism”</em> <span class="citation">(Diggle and Ribeiro Jr <a href="#ref-Diggle2007Springer">2007</a>; Brown <a href="#ref-Brown2014JSS">2015</a>)</span>. This avoids <em>ad hoc</em>, heuristic solution methods and has the advantage that it yields generic and portable solutions. Some examples of diverse geostatistical models are given in <span class="citation">Brown (<a href="#ref-Brown2014JSS">2015</a>)</span>.</p>
<p>The basic geostatistical model treats the soil property of interest as the sum of a deterministic trend and a stochastic residual (Eq.@ref(eq:ukm)):</p>
<span class="math display">\[\begin{equation}
Z({s}) = m({s}) + \varepsilon({s})
(\#eq:ukm-gstat)
\end{equation}\]</span>
<p>where <span class="math inline">\(\varepsilon\)</span> and hence <span class="math inline">\(Z\)</span> are normally distributed stochastic processes. This is the same model as that presented in Eq.@ref(eq:ukm), with in this case <span class="math inline">\(\varepsilon = \varepsilon &#39; + \varepsilon &#39;&#39;\)</span> being the sum of the spatially correlated and spatially uncorrelated stochastic components. The mean of <span class="math inline">\(\varepsilon\)</span> is taken to be zero. Note that we use capital letter <span class="math inline">\(Z\)</span> because we use a probabilistic model, i.e. we treat the soil property as an outcome of a stochastic process and define a model of that stochastic process. Ideally, the spatial variation of the stochastic residual of Eq.@ref(eq:ukm-gstat) is much less than that of the dependent variable.</p>
<p>When the assumption of normality is not realistic, such as when the frequency distribution of the residuals at observation locations is very skewed, the easiest solution is to take a Transformed Gaussian approach <span class="citation">(Diggle and Ribeiro Jr <a href="#ref-Diggle2007Springer">2007</a> $$3.8)</span> in which the Gaussian geostatistical model is formulated for a transformation of the dependent variable (e.g. logarithmic, logit, square root, Box-Cox transform). A more advanced approach would drop the normal distribution approach entirely and assume a <em>Generalized Linear Geostatistical Model</em> <span class="citation">(Diggle and Ribeiro Jr <a href="#ref-Diggle2007Springer">2007</a>; Brown <a href="#ref-Brown2014JSS">2015</a>)</span> but this complicates the statistical analysis and prediction process dramatically. The Transformed Gaussian approach is nearly as simple as the Gaussian approach although the back-transformation requires attention, especially when the spatial prediction includes a change of support (leading to block kriging). If this is the case, it may be necessary to use a stochastic simulation approach and derive the predictions and associated uncertainty (i.e. the conditional probability distribution) using numerical simulations.</p>
<p>Model-based geostatistics is based on using an explicitly declared stochastic model of the data generating mechanism. One basic geostatistical model of soil variation is to treat the soil property of interest as the sum of a deterministic trend (modelled via some regression function) and a zero-mean stochastic residual.</p>
<p>The trend part of Eq.@ref(eq:ukm-gstat) (i.e. <span class="math inline">\(m\)</span>) can take many forms. In the simplest case it would be a constant but usually it is taken as some function of known, exhaustively available covariates. This is where soil mapping can benefit from other sources of information and can implement Jenny’s <em>State Factor Model of soil formation</em> <span class="citation">(Jenny, Salem, and Wallis <a href="#ref-Jenny1968">1968</a>; Jenny <a href="#ref-jenny1994factors">1994</a>; Heuvelink and Webster <a href="#ref-Heuvelink2001Geoderma">2001</a>; McBratney et al. <a href="#ref-McBratney2011HSS">2011</a>)</span>, which has been known from the time of Dokuchaev <span class="citation">(Florinsky <a href="#ref-Florinsky2012Dokuchaev">2012</a>)</span>. The covariates are often maps of environmental properties that are known to be related to the soil property of interest (e.g. elevation, land cover, geology) but could also be the outcome of a mechanistic soil process model (such as a soil acidification model, a soil nutrient leaching model or a soil genesis model). In the case of the latter one might opt for taking <span class="math inline">\(m\)</span> equal to the output of the deterministic model, but when the covariates are related environmental properties one must define a structure for <span class="math inline">\(m\)</span> and introduce parameters to be estimated from paired observations of the soil property and covariates. One of the simplest approaches is to use <em>multiple linear regression</em> to predict values at some new location <span class="math inline">\({s}_0\)</span> <span class="citation">(Kutner et al. <a href="#ref-kutner2005applied">2005</a>)</span>:</p>
<span class="math display">\[\begin{equation}
m({s}_0 ) = \sum\limits_{j = 0}^p { \beta _j \cdot X_j ({s}_0 )}
(\#eq:MRK2D)
\end{equation}\]</span>
<p>where <span class="math inline">\(\beta _j\)</span> are the regression model coefficients, <span class="math inline">\(\beta _0\)</span> is the intercept, <span class="math inline">\(j=1,\ldots,p\)</span> are <em>covariates</em> or explanatory variables (available at all locations within the study area of interest <span class="math inline">\(\mathbb{A}\)</span>), and <span class="math inline">\(p\)</span> is the number of covariates. Eq.@ref(eq:MRK2D) can also include categorical covariates (e.g. maps of land cover, geology, soil type) by representing these by as many binary dummy variables as there are categories (minus one, to be precise, since an intercept is included in the model). In addition, transformed covariates may also be included or interactions between covariates. The latter is achieved by extending the set of covariates with products or other mixtures of covariates. However, note that this will dramatically increase the number of covariates. The risk of considering a large number of covariates is that it may become difficult to obtain reliable estimates of the regression coefficients. Also one may run the risk of <em>multicollinearity</em> — the property of covariates being mutually strongly correlated (as indicated by <span class="citation">Jenny, Salem, and Wallis (<a href="#ref-Jenny1968">1968</a>)</span> already in <span class="citation">(<a href="#ref-Jenny1968">1968</a>)</span>).</p>
<p>The advantage of Eq.@ref(eq:MRK2D) is that it is linear in the unknown coefficients, which makes their estimation relatively straightforward and also permits derivation of the uncertainty about the regression coefficients (<span class="math inline">\(\beta\)</span>). However, in many practical cases, the linear formulation may be too restrictive and that is why alternative structures have been extensively developed to establish the relationship between the dependent and covariates. Examples of these so-called <em>‘statistical learning’</em> and/or <em>‘machine learning’</em> approaches are:</p>
<ul>
<li><p><em>artificial neural networks</em> <span class="citation">(Yegnanarayana <a href="#ref-yegnanarayana2004artificial">2004</a>)</span>,</p></li>
<li><p><em>classification and regression trees</em> <span class="citation">(Breiman <a href="#ref-breiman1993classification">1993</a>)</span>,</p></li>
<li><p><em>support vector machines</em> <span class="citation">(Hearst et al. <a href="#ref-hearst1998support">1998</a>)</span>,</p></li>
<li><p><em>computer-based expert systems</em>,</p></li>
<li><p><em>random forests</em> <span class="citation">(Breiman <a href="#ref-breiman2001random">2001</a>; Meinshausen <a href="#ref-meinshausen2006quantile">2006</a>)</span>,</p></li>
</ul>
<p>Statistical treatment of many of these methods is given in <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-hastie2009elements">2009</a>)</span>. Care needs to be taken when using machine learning techniques, such as random forest, because such techniques are more sensitive to noise and blunders in the data .</p>
<p>Most methods listed above require appropriate levels of expertise to avoid pitfalls and incorrect use but, when feasible and used properly, these methods should extract maximal information about the target variable from the covariates <span class="citation">(Statnikov, Wang, and Aliferis <a href="#ref-Statnikov2008">2008</a>; Kanevski, Timonin, and Pozdnukhov <a href="#ref-kanevski2009machine">2009</a>)</span>.</p>
<p>The trend (<span class="math inline">\(m\)</span>) relates covariates to soil properties and for this it uses a soil-environment correlation model — the so-called <em>CLORPT model</em>, which was formulated by Jenny in 1941 (a <span class="citation">(<a href="#ref-jenny1994factors">1994</a>)</span> reprint from that book is also available). <span class="citation">McBratney, Mendoça Santos, and Minasny (<a href="#ref-McBratney2003Geoderma">2003</a>)</span> further formulated an extension of the CLORPT model known as the <em>“SCORPAN”</em> model.</p>
<p>The CLORPT model may be written as <span class="citation">(Jenny <a href="#ref-jenny1994factors">1994</a>; Florinsky <a href="#ref-Florinsky2012Dokuchaev">2012</a>)</span>:</p>
<span class="math display">\[\begin{equation}
S = f (cl, o, r, p, t)
(\#eq:clorpt)
\end{equation}\]</span>
<p>where <span class="math inline">\(S\)</span> stands for soil (properties and classes), <span class="math inline">\(cl\)</span> for climate, <span class="math inline">\(o\)</span> for organisms (including humans), <span class="math inline">\(r\)</span> is relief, <span class="math inline">\(p\)</span> is parent material or geology and <span class="math inline">\(t\)</span> is time. In other words, we can assume that the distribution of both soil and vegetation (at least in a natural system) can be at least partially explained by environmental conditions. Eq.@ref(eq:clorpt) suggests that soil is a result of environmental factors, while in reality there are many feedbacks and soil, in turn, influences many of the factors on the right-hand side of Eq.@ref(eq:clorpt), such as <span class="math inline">\(cl\)</span>, <span class="math inline">\(o\)</span> and <span class="math inline">\(r\)</span>.</p>
<p>Uncertainty about the estimation errors of model coefficients can fairly easily be taken into account in the subsequent prediction analysis if the model is linear in the coefficients, such as in Eq.@ref(eq:MRK2D). In this book we therefore restrict ourselves to this case but allow that the <span class="math inline">\(X_j\)</span>’s in Eq.@ref(eq:MRK2D) are derived in various ways.</p>
<p>Since the stochastic residual of Eq.@ref(eq:ukm-gstat) is normally distributed and has zero mean, only its variance-covariance remains to be specified:</p>
<span class="math display">\[\begin{equation}
C\left[Z({s}),Z({s}+{h})\right] = \sigma (s) \cdot \sigma(s+{h}) \cdot \rho ({h})
\end{equation}\]</span>
<p>where <span class="math inline">\({{h}}\)</span> is the separation distance between two locations. Note that here we assumed that the correlation function <span class="math inline">\(\rho\)</span> is invariant to geographic translation (i.e., it only depends on the distance <span class="math inline">\({h}\)</span> between locations and not on the locations themselves). If in addition the standard deviation <span class="math inline">\(\sigma\)</span> would be spatially invariant then <span class="math inline">\(C\)</span> would be <em>second-order stationary</em>. These type of simplifying assumptions are needed to be able to estimate the variance-covariance structure of <span class="math inline">\(C\)</span> from the observations. If the standard deviation is allowed to vary with location, then it could be defined in a similar way as in Eq.@ref(eq:MRK2D). The correlation function <span class="math inline">\(\rho\)</span> would be parameterised to a common form (e.g. exponential, spherical, Matérn), thus ensuring that the model is statistically valid and <em>positive-definite</em>. It is also quite common to assume isotropy, meaning that two-dimensional geographic distance <span class="math inline">\({{h}}\)</span> can be reduced to one-dimensional Euclidean distance <span class="math inline">\(h\)</span>.</p>
<p>Once the model has been defined, its parameters must be estimated from the data. These are the regression coefficients of the trend (when applicable) and the parameters of the variance-covariance structure of the stochastic residual. Commonly used estimation methods are least squares and maximum likelihood. Both methods have been extensively described in the literature (e.g. <span class="citation">Webster and Oliver (<a href="#ref-Webster2001Wiley">2001</a>)</span> and <span class="citation">Diggle and Ribeiro Jr (<a href="#ref-Diggle2007Springer">2007</a>)</span>). More complex trend models may also use the same techniques to estimate their parameters, although they might also need to rely on more complex parameter estimation methods such as genetic algorithms and <em>simulated annealing</em> <span class="citation">(Lark and Papritz <a href="#ref-lark2003fitting">2003</a>)</span>.</p>

<div class="rmdnote">
Spatial prediction under the linear Gaussian model with a trend boils down to <em>regression-kriging</em> when the trend coefficients are determined prior to kriging i.e. to <em>universal kriging</em> or <em>kriging with external drift</em> when they are estimated together with kriging weights. Both computational approaches — regression-kriging, kriging with external drift or universal kriging — yield exactly the same predictions if run using the same inputs and assuming the same (global) geostatistical model <span class="citation">(Hengl, Heuvelink, and Rossiter <a href="#ref-hengl2007regression">2007</a>)</span>.
</div>

<p>The optimal spatial prediction in the case of a model Eq.@ref(eq:ukm-gstat) with a linear trend Eq.@ref(eq:MRK2D) and a normally distributed residual is given by the well-kown <em>Best Linear Unbiased Predictor</em> (BLUP):</p>
<p><span class="math display">\[\label{E:BLUP}
\hat z({{{s}}_0}) = {{X}}_{{0}}^{{T}}\cdot \hat{{\beta}} + \hat{{\lambda}}_{{0}}^{{T}}\cdot({{z}} - {{X}}\cdot \hat{{\beta}} )\]</span></p>
<p>where the regression coefficients and kriging weights are estimated using:</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
\hat{{\beta}}  &amp;= {\left( {{{{X}}^{{T}}}\cdot{{{C}}^{ - {{1}}}}\cdot{{X}}} \right)^{ - {{1}}}}\cdot{{{X}}^{{T}}}\cdot{{{C}}^{ - {{1}}}}\cdot{{z}} \\
\hat{{\lambda}}_{{0}} &amp;= {C}^{ - {{1}}} \cdot {{c}}_{{0}} \notag\end{aligned}
(\#eq:betas)
\end{equation}\]</span>
<p>and where <span class="math inline">\({{X}}\)</span> is the matrix of <span class="math inline">\(p\)</span> predictors at the <span class="math inline">\(n\)</span> sampling locations, <span class="math inline">\(\hat{{\beta}}\)</span> is the vector of estimated regression coefficients, <span class="math inline">\({C}\)</span> is the <span class="math inline">\(n\)</span><span class="math inline">\(n\)</span> variance-covariance matrix of residuals, <span class="math inline">\({c}_{{0}}\)</span> is the vector of <span class="math inline">\(n\)</span><span class="math inline">\(1\)</span> covariances at the prediction location, and <span class="math inline">\({\lambda}_{{0}}\)</span> is the vector of <span class="math inline">\(n\)</span> kriging weights used to interpolate the residuals. Derivation of BLUP for spatial data can be found in many standard statistical books e.g. <span class="citation">Stein (<a href="#ref-Stein1999Springer">1999</a>)</span>, <span class="citation">Christensen (<a href="#ref-Christensen2001Springer">2001</a>, 277)</span>, <span class="citation">Venables and Ripley (<a href="#ref-Venables2002Springer">2002</a>, 425–30)</span> and/or <span class="citation">Schabenberger and Gotway (<a href="#ref-schabenberger2005statistical">2005</a>)</span>.</p>
<p>Any form of kriging computes the conditional distribution of <span class="math inline">\(Z({{s}}_0)\)</span> at an unobserved location <span class="math inline">\({{s}}_0\)</span> from the observations <span class="math inline">\(z({{s}}_1 )\)</span>, <span class="math inline">\(z({{s}}_2 ), \ldots , z({{s}}_n )\)</span> and the covariates <span class="math inline">\({{X}}({{s}}_0)\)</span> (matrix of size <span class="math inline">\(p \times n\)</span>). From a statistical perspective this is straightforward for the case of a linear model and normally distributed residuals. However, solving large matrices and more sophisticated model fitting algorithms such as restricted maximum likelihood can take a significant amount of time if the number of observations is large and/or the prediction grid dense. Pragmatic approaches to addressing constraints imposed by large data sets are to constrain the observation data set to local neighbourhoods or to take a multiscale nested approach.</p>
<p>Kriging not only yields optimal predictions but also quantifies the prediction error with the kriging standard deviation. Prediction intervals can be computed easily because the prediction errors are normally distributed. Alternatively, uncertainty in spatial predictions can also be quantified with spatial stochastic simulation. While kriging yields the <em>‘optimal’</em> prediction of the soil property at any one location, spatial stochastic simulation yields a series of possible values by sampling from the conditional probability distribution. In this way a large number of <em>‘realizations’</em> can be generated, which can be useful when the resulting map needs to be back-transformed or when it is used in a spatial uncertainty propagation analysis. Spatial stochastic simulation of the linear Gaussian model can be done using a technique known as sequential Gaussian simulation <span class="citation">(Goovaerts <a href="#ref-Goovaerts1997Oxford">1997</a>; Yamamoto <a href="#ref-Yamamoto2008">2008</a>)</span>. It is not, in principal, more difficult than kriging but it is certainly numerically more demanding i.e. takes significantly more time to compute.</p>
</div>
<div id="RK-generic" class="section level3">
<h3><span class="header-section-number">5.2.5</span> Regression-kriging (generic model)</h3>
<p>Ignoring the assumptions about the cross-correlation between the trend and residual components, we can extend the regression-kriging model and use any type of (non-linear) regression to predict values (e.g. regression trees, artificial neural networks and other machine learning models), calculate residuals at observation locations, fit a variogram for these residuals, interpolate the residuals using ordinary or simple kriging, and add the result to the predicted regression part. This means that RK can, in general, be formulated as:</p>
<span class="math display">\[\begin{equation}
{\rm prediction} \; = \;
\begin{matrix}
{\rm trend} \; {\rm predicted} \\
{\rm using} \; {\rm regression} \end{matrix} \; + \;
\begin{matrix}
{\rm residual} \; {\rm predicted} \\
{\rm using} \; {\rm kriging} \end{matrix}
(\#eq:RKgeneral)
\end{equation}\]</span>
<p>Again, statistical inference and prediction is relatively simple if the stochastic residual, or a transformation thereof, may be assumed normally distributed. Error of the regression-kriging model is likewise a sum of the regression and the kriging model errors.</p>
</div>
<div id="spatial-prediction-using-multiple-linear-regression" class="section level3">
<h3><span class="header-section-number">5.2.6</span> Spatial Prediction using multiple linear regression</h3>
<p>The predictor <span class="math inline">\(\hat Y({{ s}_0})\)</span> of <span class="math inline">\(Y({{ s}_0})\)</span> is typically taken as a function of covariates and the <span class="math inline">\(Y({ s}_i)\)</span> which, upon substitution of the observations <span class="math inline">\(y({ s}_i)\)</span>, yields a (deterministic) prediction <span class="math inline">\(\hat y({{ s}_0})\)</span>. In the case of multiple linear regression (MLR), model assumptions state that at any location in <span class="math inline">\(D\)</span> the dependent variable is the sum of a linear combination of the covariates at that location and a zero-mean normally distributed residual. Thus, at the <span class="math inline">\(n\)</span> observation locations we have:</p>
<span class="math display">\[\begin{equation}
{ Y} = { X}^{{ T}} \cdot { \beta} + { \varepsilon}
(\#eq:lm)
\end{equation}\]</span>
<p>where <span class="math inline">\({ Y}\)</span> is a vector of the target variable at the <span class="math inline">\(n\)</span> observation locations, <span class="math inline">\({ X}\)</span> is an <span class="math inline">\(n \times p\)</span> matrix of covariates at the same locations and <span class="math inline">\({ \beta}\)</span> is a vector of <span class="math inline">\(p\)</span> regression coefficients. The stochastic residual <span class="math inline">\({ \varepsilon}\)</span> is assumed to be independently and identically distributed. The paired observations of the target variable and covariates (<span class="math inline">\({ y}\)</span> and <span class="math inline">\({ X}\)</span>) are used to estimate the regression coefficients using, e.g., Ordinary Least Squares <span class="citation">(Kutner et al. <a href="#ref-Kutner2004McGraw">2004</a>)</span>:</p>
<span class="math display">\[\begin{equation}
\hat{{ \beta}}  = \left( {{{ X}}^{{ T}} \cdot {{ X}}} \right)^{ - {{ 1}}} \cdot
{{ X}}^{{ T}} \cdot {{ y}}
(\#eq:ols-betas)
\end{equation}\]</span>
<p>once the coefficients are estimated, these can be used to generate a prediction at <span class="math inline">\({ s}_0\)</span>:</p>
<span class="math display">\[\begin{equation}
\hat y({ s}_0) = { x}_0^{ T} \cdot { \hat \beta}
\end{equation}\]</span>
<p>with associated prediction error variance:</p>
<span class="math display">\[\begin{equation}
\sigma ^2 ({ s}_0 ) = var\left[ \varepsilon ({ s}_0) \right] \cdot \left[ {1 +
{\mathbf x}_0^{\rm T}  \cdot \left(
{{\mathbf X}^{\rm T}  \cdot {\mathbf X}} \right)^{ -
{\mathbf 1}}  \cdot {\mathbf x}_0 } \right]
(\#eq:ols-sigma)
\end{equation}\]</span>
<p>here, <span class="math inline">\({\mathbf x}_0\)</span> is a vector with covariates at the prediction location and <span class="math inline">\(var\left[ \varepsilon ({ s}_0) \right]\)</span> is the variance of the stochastic residual. The latter is usually estimated by the mean squared error (MSE):</p>
<span class="math display">\[\begin{equation}
{\mathrm{MSE}} = \frac{\sum\limits_{i = 1}^n {(y_i - \hat y_i)^2}}{n-p}
\end{equation}\]</span>
<p>The prediction error variance given by Eq.@ref(eq:ols-sigma) is smallest at prediction points where the covariate values are in the center of the covariate (<em>‘feature’</em>) space and increases as predictions are made further away from the center. They are particularly large in case of extrapolation in feature space <span class="citation">(Kutner et al. <a href="#ref-Kutner2004McGraw">2004</a>)</span>. Note that the model defined in Eq.@ref(eq:lm) is a non-spatial model because the observation locations and spatial-autocorrelation of the dependent variable are not taken into account.</p>
</div>
<div id="universal-kriging-prediction-error" class="section level3">
<h3><span class="header-section-number">5.2.7</span> Universal kriging prediction error</h3>
<p>In the case of universal kriging, regression-kriging or Kriging with External Drift, the prediction error is computed as <span class="citation">(Christensen <a href="#ref-Christensen2001Springer">2001</a>)</span>:</p>
<span class="math display">\[\begin{equation}
\hat \sigma _{\tt{UK}}^2 ({{s}}_0 )  = (C_0  + C_1 ) - {{c}}_{{0}}^{{T}}  \cdot {{C}}^{ - {{1}}}  \cdot
{{c}}_{{0}} + \left( {{{X}}_{{0}}  -
{{X}}^{{T}} \cdot {{C}}^{ - {{1}}} \cdot
{{c}}_{{0}} } \right)^{{T}}  \cdot \left( {{{X}}^{{T}}
\cdot {{C}}^{ - {{1}}} \cdot {{X}}} \right)^{{{ - 1}}} \cdot \left( {{{X}}_{{0}}  - {{X}}^{{T}}  \cdot
{{C}}^{ - {{1}}} \cdot {{c}}_{{0}} } \right)
(\#eq:UKvar)
\end{equation}\]</span>
<p>where <span class="math inline">\(C_0 + C_1\)</span> is the sill variation (variogram parameters), <span class="math inline">\({C}\)</span> is the covariance matrix of the residuals, and <span class="math inline">\({{c}}_0\)</span> is the vector of covariances of residuals at the unvisited location.</p>
<p>Ignoring the mixed component of the prediction variance in Eq.@ref(eq:UKvar), one can also derive a simplified regression-kriging variance i.e. as a sum of the kriging variance and the standard error of estimating the regression mean:</p>
<span class="math display">\[\begin{equation}
\hat \sigma _{\tt{RK}}^2 ({{s}}_0) = (C_0  + C_1 ) -
{{c}}_{{0}}^{{T}}  \cdot {{C}}^{ - {{1}}}  \cdot
{{c}}_{{0}} + {\it{SEM}}^2
(\#eq:RKvar-simple)
\end{equation}\]</span>
<p>which is the general approach used in the GSIF package.</p>
<p>Note that there will always be a small difference between results of Eq.@ref(eq:UKvar) and Eq.@ref(eq:RKvar-simple), and this is a major disadvantage of using the general regression-kriging framework for spatial prediction. Although the predicted mean derived by using regression-kriging or universal kriging approaches might not differ, the estimate of the prediction variance using Eq.@ref(eq:RKvar-simple) will be suboptimal as it ignores product component. On the other hand, the advantage of running separate regression and kriging predictions is often worth the sacrifice as the computing time is an order of magnitude shorter and we have more flexibility to combine different types of regression models with kriging when regression is run separately from kriging <span class="citation">(Hengl, Heuvelink, and Rossiter <a href="#ref-hengl2007regression">2007</a>)</span>.</p>
</div>
<div id="regression-kriging-examples" class="section level3">
<h3><span class="header-section-number">5.2.8</span> Regression-kriging examples</h3>
<p>The type of regression-kriging model explained in the previous section can be implemented here by combining the (<strong><em>WHAT</em></strong>) and (<strong><em>WHAT</em></strong>) packages. Consider for example the Meuse case study:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gstat)
<span class="kw">demo</span>(meuse, <span class="dt">echo=</span><span class="ot">FALSE</span>)</code></pre></div>
<p>We can overlay the points and grids to create the regression matrix by:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">meuse.ov &lt;-<span class="st"> </span><span class="kw">over</span>(meuse, meuse.grid)
meuse.ov &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">as.data.frame</span>(meuse), meuse.ov)
<span class="kw">head</span>(meuse.ov[,<span class="kw">c</span>(<span class="st">&quot;x&quot;</span>,<span class="st">&quot;y&quot;</span>,<span class="st">&quot;dist&quot;</span>,<span class="st">&quot;soil&quot;</span>,<span class="st">&quot;om&quot;</span>)])
<span class="co">#&gt;        x      y   dist soil   om</span>
<span class="co">#&gt; 1 181072 333611 0.0014    1 13.6</span>
<span class="co">#&gt; 2 181025 333558 0.0122    1 14.0</span>
<span class="co">#&gt; 3 181165 333537 0.1030    1 13.0</span>
<span class="co">#&gt; 4 181298 333484 0.1901    2  8.0</span>
<span class="co">#&gt; 5 181307 333330 0.2771    2  8.7</span>
<span class="co">#&gt; 6 181390 333260 0.3641    2  7.8</span></code></pre></div>
<p>which lets us fit a linear model for organic carbon as a function of distance to river and soil type:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log1p</span>(om)<span class="op">~</span>dist<span class="op">+</span>soil, meuse.ov)
<span class="kw">summary</span>(m)
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = log1p(om) ~ dist + soil, data = meuse.ov)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residuals:</span>
<span class="co">#&gt;     Min      1Q  Median      3Q     Max </span>
<span class="co">#&gt; -1.0831 -0.1504  0.0104  0.2098  0.5913 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;             Estimate Std. Error t value  Pr(&gt;|t|)    </span>
<span class="co">#&gt; (Intercept)   2.3421     0.0425   55.05   &lt; 2e-16 ***</span>
<span class="co">#&gt; dist         -0.8009     0.1787   -4.48 0.0000147 ***</span>
<span class="co">#&gt; soil2        -0.3358     0.0702   -4.78 0.0000041 ***</span>
<span class="co">#&gt; soil3         0.0366     0.1247    0.29      0.77    </span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residual standard error: 0.33 on 149 degrees of freedom</span>
<span class="co">#&gt;   (2 observations deleted due to missingness)</span>
<span class="co">#&gt; Multiple R-squared:  0.384,  Adjusted R-squared:  0.371 </span>
<span class="co">#&gt; F-statistic: 30.9 on 3 and 149 DF,  p-value: 1.32e-15</span></code></pre></div>
<p>Next, we can derive the regression residuals and fit a variogram:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">meuse.s &lt;-<span class="st"> </span>meuse[<span class="op">-</span>m<span class="op">$</span>na.action,]
meuse.s<span class="op">$</span>om.res &lt;-<span class="st"> </span><span class="kw">resid</span>(m)
vr.fit &lt;-<span class="st"> </span><span class="kw">fit.variogram</span>(<span class="kw">variogram</span>(om.res<span class="op">~</span><span class="dv">1</span>, meuse.s), <span class="kw">vgm</span>(<span class="dv">1</span>, <span class="st">&quot;Exp&quot;</span>, <span class="dv">300</span>, <span class="dv">1</span>))
vr.fit
<span class="co">#&gt;   model psill range</span>
<span class="co">#&gt; 1   Nug 0.048     0</span>
<span class="co">#&gt; 2   Exp 0.065   285</span></code></pre></div>
<p>With this, all model parameters (four regression coefficients and three variogram parameters) for regression-kriging have been estimated and the model can be used to generate predictions. Note that the regression model we fitted is significant, and the remaining residuals still show spatial auto-correlation. The nugget variation is about (<strong><em>WHAT</em></strong>) of the sill variation.</p>
<p>Using the gstat package <span class="citation">(Pebesma <a href="#ref-Pebesma2004CG">2004</a>; Bivand, Pebesma, and Rubio <a href="#ref-Bivand2013Springer">2013</a>)</span>, regression and kriging can be combined by running universal kriging or kriging with external drift <span class="citation">(Hengl, Heuvelink, and Rossiter <a href="#ref-hengl2007regression">2007</a>)</span>. First, the variogram of the residuals is calculated:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">v.s &lt;-<span class="st"> </span><span class="kw">variogram</span>(<span class="kw">log1p</span>(om)<span class="op">~</span>dist<span class="op">+</span>soil, meuse.s)
vr.fit &lt;-<span class="st"> </span><span class="kw">fit.variogram</span>(v.s, <span class="kw">vgm</span>(<span class="dv">1</span>, <span class="st">&quot;Exp&quot;</span>, <span class="dv">300</span>, <span class="dv">1</span>))
vr.fit
<span class="co">#&gt;   model psill range</span>
<span class="co">#&gt; 1   Nug 0.048     0</span>
<span class="co">#&gt; 2   Exp 0.065   285</span></code></pre></div>
<p>which gives almost the same model parameter values as the regression-kriging above. Next, the kriging can be executed with a single call to the generic <code>krige</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">om.rk &lt;-<span class="st"> </span><span class="kw">krige</span>(<span class="kw">log1p</span>(om)<span class="op">~</span>dist<span class="op">+</span>soil, meuse.s, meuse.grid, vr.fit)
<span class="co">#&gt; [using universal kriging]</span></code></pre></div>
<p>The package nlme fits the regression model and the variogram of the residuals concurrently <span class="citation">(Pinheiro and Bates <a href="#ref-pinheiro2009mixed">2009</a>)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nlme)
m.gls &lt;-<span class="st"> </span><span class="kw">gls</span>(<span class="kw">log1p</span>(om)<span class="op">~</span>dist<span class="op">+</span>soil, meuse.s, <span class="dt">correlation=</span><span class="kw">corExp</span>(<span class="dt">nugget=</span><span class="ot">TRUE</span>))
m.gls
<span class="co">#&gt; Generalized least squares fit by REML</span>
<span class="co">#&gt;   Model: log1p(om) ~ dist + soil </span>
<span class="co">#&gt;   Data: meuse.s </span>
<span class="co">#&gt;   Log-restricted-likelihood: -26</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt; (Intercept)        dist       soil2       soil3 </span>
<span class="co">#&gt;       2.281      -0.623      -0.244      -0.057 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Correlation Structure: Exponential spatial correlation</span>
<span class="co">#&gt;  Formula: ~1 </span>
<span class="co">#&gt;  Parameter estimate(s):</span>
<span class="co">#&gt;  range nugget </span>
<span class="co">#&gt;   2.00   0.07 </span>
<span class="co">#&gt; Degrees of freedom: 153 total; 149 residual</span>
<span class="co">#&gt; Residual standard error: 0.34</span></code></pre></div>
<p>In this case, the regression coefficients have been estimated using Eq.@ref(eq:betas) i.e. via <em>Restricted maximum likelihood</em> (REML). The advantage of fitting the regression model and spatial autocorrelation structure concurrently is that both fits are adjusted: the estimation of the regression coefficients is adjusted for spatial autocorrelation of the residual and the variogram parameters are adjusted for the adjusted trend estimate. A disadvantage of using the nlme package is that the computational intensity increases with the size of the data set, so for any data set &gt;1000 points the computation time can increase to tens of hours of computing. On the other hand, coefficients fitted by REML methods might not result in significantly better predictions. Getting the most objective estimate of the model parameters is sometimes not worth the effort, as demonstrated by <span class="citation">Minasny and McBratney (<a href="#ref-Minasny2007Geoderma">2007</a>)</span>.</p>
<p>Simultaneous estimation of regression coefficients and variogram parameters and including estimation errors in regression coefficients into account by using universal kriging / kriging with external drift is more elegant from a statistical point of view, but there are computational and other challenges. One of these is that it is difficult to implement global estimation of regression coefficients with local spatial prediction of residuals, which is a requirement in the case of large spatial data sets. Also, the approach does not extend to more complex non-linear trend models. In such cases, we recommend separating trend estimation from kriging of residuals by using the regression-kriging approach discussed above (Eq.@ref(eq:RKgeneral)).</p>
</div>
<div id="regression-kriging-examples-using-the-gsif-package" class="section level3">
<h3><span class="header-section-number">5.2.9</span> Regression-kriging examples using the GSIF package</h3>
<p>In the (<strong><em>GSIF?</em></strong>) package, most of the steps described above (regression modelling and variogram modelling) used to fit regression-kriging models are wrapped into generic functions. A regression-kriging model can be fitted in one step by running:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">omm &lt;-<span class="st"> </span><span class="kw">fit.gstatModel</span>(meuse, <span class="kw">log1p</span>(om)<span class="op">~</span>dist<span class="op">+</span>soil, meuse.grid)
<span class="co">#&gt; Fitting a linear model...</span>
<span class="co">#&gt; Fitting a 2D variogram...</span>
<span class="co">#&gt; Saving an object of class &#39;gstatModel&#39;...</span>
<span class="kw">str</span>(omm, <span class="dt">max.level =</span> <span class="dv">2</span>)
<span class="co">#&gt; Formal class &#39;gstatModel&#39; [package &quot;GSIF&quot;] with 4 slots</span>
<span class="co">#&gt;   ..@ regModel :List of 32</span>
<span class="co">#&gt;   .. ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;glm&quot; &quot;lm&quot;</span>
<span class="co">#&gt;   ..@ vgmModel :&#39;data.frame&#39;:    2 obs. of  9 variables:</span>
<span class="co">#&gt;   .. ..- attr(*, &quot;singular&quot;)= logi FALSE</span>
<span class="co">#&gt;   .. ..- attr(*, &quot;SSErr&quot;)= num 0.00000107</span>
<span class="co">#&gt;   .. ..- attr(*, &quot;call&quot;)= language gstat::fit.variogram(object = svgm, model = ivgm)</span>
<span class="co">#&gt;   ..@ svgmModel:&#39;data.frame&#39;:    15 obs. of  6 variables:</span>
<span class="co">#&gt;   .. ..- attr(*, &quot;direct&quot;)=&#39;data.frame&#39;: 1 obs. of  2 variables:</span>
<span class="co">#&gt;   .. ..- attr(*, &quot;boundaries&quot;)= num [1:16] 0 106 213 319 426 ...</span>
<span class="co">#&gt;   .. ..- attr(*, &quot;pseudo&quot;)= num 0</span>
<span class="co">#&gt;   .. ..- attr(*, &quot;what&quot;)= chr &quot;semivariance&quot;</span>
<span class="co">#&gt;   ..@ sp       :Formal class &#39;SpatialPointsDataFrame&#39; [package &quot;sp&quot;] with 5 slots</span></code></pre></div>
<p>the resulting <code>gstatModel</code> class object consists of a (1) regression component, (2) variogram model for residual, and (3) sample variogram for plotting, (4) spatial locations of observations used to fit the model. To predict values of organic carbon using this model, we can run:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">om.rk &lt;-<span class="st"> </span><span class="kw">predict</span>(omm, meuse.grid)
<span class="co">#&gt; Subsetting observations to fit the prediction domain in 2D...</span>
<span class="co">#&gt; Generating predictions using the trend model (RK method)...</span>
<span class="co">#&gt; [using ordinary kriging]</span>
<span class="co">#&gt; </span>
<span class="dv">100</span>% done
<span class="co">#&gt; Running 5-fold cross validation using &#39;krige.cv&#39;...</span>
<span class="co">#&gt; Creating an object of class &quot;SpatialPredictions&quot;</span>
om.rk
<span class="co">#&gt;   Variable           : om </span>
<span class="co">#&gt;   Minium value       : 1 </span>
<span class="co">#&gt;   Maximum value      : 17 </span>
<span class="co">#&gt;   Size               : 153 </span>
<span class="co">#&gt;   Total area         : 4964800 </span>
<span class="co">#&gt;   Total area (units) : square-m </span>
<span class="co">#&gt;   Resolution (x)     : 40 </span>
<span class="co">#&gt;   Resolution (y)     : 40 </span>
<span class="co">#&gt;   Resolution (units) : m </span>
<span class="co">#&gt;   GLM call formula   : log1p(om) ~ dist + soil </span>
<span class="co">#&gt;   Family             : gaussian </span>
<span class="co">#&gt;   Link function      : identity </span>
<span class="co">#&gt;   Vgm model          : Exp </span>
<span class="co">#&gt;   Nugget (residual)  : 0.048 </span>
<span class="co">#&gt;   Sill (residual)    : 0.065 </span>
<span class="co">#&gt;   Range (residual)   : 285 </span>
<span class="co">#&gt;   RMSE (validation)  : 2.4 </span>
<span class="co">#&gt;   Var explained      : 49.4% </span>
<span class="co">#&gt;   Effective bytes    : 295 </span>
<span class="co">#&gt;   Compression method : gzip</span>
## back-transformation:
meuse.grid<span class="op">$</span>om.rk &lt;-<span class="st"> </span><span class="kw">expm1</span>(om.rk<span class="op">@</span>predicted<span class="op">$</span>om <span class="op">+</span><span class="st"> </span>om.rk<span class="op">@</span>predicted<span class="op">$</span>var1.var<span class="op">/</span><span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/Fig_meuse_om_RK_vs_GLMK.png" alt="Predictions of organic carbon in percent (top soil) for the Meuse data set derived using regression-kriging with transformed values, GLM-kriging, regression tress (rpart) and random forest models combined with kriging. The percentages in brackets indicates amount of variation explained by the models." width="85%" />
<p class="caption">
(#fig:meuse-om-rk-glm)Predictions of organic carbon in percent (top soil) for the Meuse data set derived using regression-kriging with transformed values, GLM-kriging, regression tress (rpart) and random forest models combined with kriging. The percentages in brackets indicates amount of variation explained by the models.
</p>
</div>
<p>We could also have opted for fitting a GLM with a link function, which would look like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">omm2 &lt;-<span class="st"> </span><span class="kw">fit.gstatModel</span>(meuse, om<span class="op">~</span>dist<span class="op">+</span>soil, meuse.grid, <span class="dt">family=</span><span class="kw">gaussian</span>(<span class="dt">link=</span>log))
<span class="co">#&gt; Fitting a linear model...</span>
<span class="co">#&gt; Fitting a 2D variogram...</span>
<span class="co">#&gt; Saving an object of class &#39;gstatModel&#39;...</span>
<span class="kw">summary</span>(omm2<span class="op">@</span>regModel)
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; glm(formula = om ~ dist + soil, family = fit.family, data = rmatrix)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Deviance Residuals: </span>
<span class="co">#&gt;    Min      1Q  Median      3Q     Max  </span>
<span class="co">#&gt; -7.066  -1.492  -0.281   1.635   7.401  </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="co">#&gt; (Intercept)   10.054      0.348   28.88  &lt; 2e-16 ***</span>
<span class="co">#&gt; dist          -8.465      1.461   -5.79    4e-08 ***</span>
<span class="co">#&gt; soil2         -2.079      0.575   -3.62  0.00041 ***</span>
<span class="co">#&gt; soil3          0.708      1.021    0.69  0.48913    </span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; (Dispersion parameter for gaussian family taken to be 7.2)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     Null deviance: 1791.4  on 152  degrees of freedom</span>
<span class="co">#&gt; Residual deviance: 1075.5  on 149  degrees of freedom</span>
<span class="co">#&gt;   (2 observations deleted due to missingness)</span>
<span class="co">#&gt; AIC: 742.6</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of Fisher Scoring iterations: 2</span>
om.rk2 &lt;-<span class="st"> </span><span class="kw">predict</span>(omm2, meuse.grid)
<span class="co">#&gt; Subsetting observations to fit the prediction domain in 2D...</span>
<span class="co">#&gt; Generating predictions using the trend model (RK method)...</span>
<span class="co">#&gt; [using ordinary kriging]</span>
<span class="co">#&gt; </span>
<span class="dv">100</span>% done
<span class="co">#&gt; Running 5-fold cross validation using &#39;krige.cv&#39;...</span>
<span class="co">#&gt; Creating an object of class &quot;SpatialPredictions&quot;</span></code></pre></div>
<p>or fitting a regression tree:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">omm3 &lt;-<span class="st"> </span><span class="kw">fit.gstatModel</span>(meuse, <span class="kw">log1p</span>(om)<span class="op">~</span>dist<span class="op">+</span>soil, meuse.grid, <span class="dt">method=</span><span class="st">&quot;rpart&quot;</span>)
<span class="co">#&gt; Fitting a regression tree model...</span>
<span class="co">#&gt; Estimated Complexity Parameter (for prunning): 0.09396</span>
<span class="co">#&gt; Fitting a 2D variogram...</span>
<span class="co">#&gt; Saving an object of class &#39;gstatModel&#39;...</span></code></pre></div>
<p>or a random forest model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">omm4 &lt;-<span class="st"> </span><span class="kw">fit.gstatModel</span>(meuse, om<span class="op">~</span>dist<span class="op">+</span>soil, meuse.grid, <span class="dt">method=</span><span class="st">&quot;quantregForest&quot;</span>)
<span class="co">#&gt; Fitting a Quantile Regression Forest model...</span>
<span class="co">#&gt; Fitting a 2D variogram...</span>
<span class="co">#&gt; Saving an object of class &#39;gstatModel&#39;...</span></code></pre></div>
<p>All regression-kriging models listed above are valid and the differences between their respective results are not likely to be large (Fig. @ref(fig:meuse-om-rk-glm)). Regression tree combined with kriging (rpart-kriging) seems to produce slightly better results i.e. smallest cross-validation error, although the difference between the four prediction methods is, in fact, not large (±5% of variance explained). It is important to run such comparisons nevertheless, as they allow us to objectively select the most efficient method.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_RK_vs_randomForestK_Meuse.png" alt="Predictions of the organic carbon (log-transformed values) using random forest vs linear regression-kriging. The random forest-kriging variance has been derived using the quantregForest package." width="100%" />
<p class="caption">
(#fig:rk-vs-rf-meuse)Predictions of the organic carbon (log-transformed values) using random forest vs linear regression-kriging. The random forest-kriging variance has been derived using the quantregForest package.
</p>
</div>
<p>Fig. @ref(fig:rk-vs-rf-meuse) shows the RK variance derived for the random forest model using the package <span class="citation">(Meinshausen <a href="#ref-meinshausen2006quantile">2006</a>)</span> and the formula in Eq.@ref(eq:RKvar-simple). Note that the quantregForest package estimates a much larger prediction variance than simple linear RK for large parts of the study area.</p>
</div>
<div id="regression-kriging-and-polygon-averaging" class="section level3">
<h3><span class="header-section-number">5.2.10</span> Regression-kriging and polygon averaging</h3>
<p>Although many soil mappers may not realize it, many simpler regression-based techniques can be viewed as a special case of RK, or its variants. Consider for example a technique commonly used to generate predictions of soil properties from polygon maps: weighted averaging. Here the principal covariate available is a polygon map (showing the distribution of mapping units). In this model it is assumed that the trend is constant within mapping units and that the stochastic residual is spatially uncorrelated. In that case, the Best Linear Unbiased Predictor of the values is simple averaging of soil properties per unit <span class="citation">(Webster and Oliver <a href="#ref-Webster2001Wiley">2001</a>, 43)</span>:</p>
<span class="math display">\[\begin{equation}
\hat z({{s}}_0 ) = \bar \mu _p  = \frac{1}{{n_p }}\sum\limits_{i = 1}^{n_p } {z({{s}}_i )}
(\#eq:regavg)
\end{equation}\]</span>
<p>The output map produced by polygon averaging will exhibit abrupt changes at boundaries between polygon units. The prediction variance of this area-class prediction model is simply the sum of the within-unit variance and the estimation variance of the unit mean:</p>
<span class="math display">\[\begin{equation}
\hat \sigma^2 ({{s}}_0 ) = \left( 1 + \frac{1}{n_p } \right) \cdot \sigma _p^2
(\#eq:polvar)
\end{equation}\]</span>
<p>From Eq.@ref(eq:polvar), it is evident that the accuracy of the prediction under this model depends on the degree of within-unit variation. The approach is advantageous if the within-unit variation is small compared to the between-unit variation. The predictions under this model can also be expressed as:</p>
<span class="math display">\[\begin{equation}
\hat z({{s}}_0 ) = \sum\limits_{i = 1}^n {w_i  \cdot z({{s}}_i)}; \qquad w_i  = \left\{ {\begin{array}{*{20}c}
   {1/n_p } &amp; {{\rm for} \; {{s}}_i \in p}  \\
   0 &amp; {{\rm otherwise}}  \\
 \end{array} } \right.
\end{equation}\]</span>
<p>where <span class="math inline">\(p\)</span> is the unit identifier. So, in fact, weighted averaging per unit is a special version of regression-kriging where spatial autocorrelation is ignored (assumed zero) and all covariates are categorical variables.</p>
<p>Going back to the Meuse data set, we can fit a regression model for organic matter using soil types as predictors, which gives:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">omm &lt;-<span class="st"> </span><span class="kw">fit.gstatModel</span>(meuse, <span class="kw">log1p</span>(om)<span class="op">~</span>soil<span class="dv">-1</span>, meuse.grid)
<span class="co">#&gt; Fitting a linear model...</span>
<span class="co">#&gt; Fitting a 2D variogram...</span>
<span class="co">#&gt; Saving an object of class &#39;gstatModel&#39;...</span>
<span class="kw">summary</span>(omm<span class="op">@</span>regModel)
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; glm(formula = log1p(om) ~ soil - 1, family = fit.family, data = rmatrix)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Deviance Residuals: </span>
<span class="co">#&gt;     Min       1Q   Median       3Q      Max  </span>
<span class="co">#&gt; -1.0297  -0.2087  -0.0044   0.2098   0.6668  </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;       Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="co">#&gt; soil1   2.2236     0.0354    62.9   &lt;2e-16 ***</span>
<span class="co">#&gt; soil2   1.7217     0.0525    32.8   &lt;2e-16 ***</span>
<span class="co">#&gt; soil3   1.9293     0.1006    19.2   &lt;2e-16 ***</span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; (Dispersion parameter for gaussian family taken to be 0.12)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     Null deviance: 672.901  on 153  degrees of freedom</span>
<span class="co">#&gt; Residual deviance:  18.214  on 150  degrees of freedom</span>
<span class="co">#&gt;   (2 observations deleted due to missingness)</span>
<span class="co">#&gt; AIC: 116.6</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of Fisher Scoring iterations: 2</span></code></pre></div>
<p>and these regression coefficients for soil classes <code>1</code>, <code>2</code>, <code>3</code> are equal to the mean values per class:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">aggregate</span>(<span class="kw">log1p</span>(om) <span class="op">~</span><span class="st"> </span>soil, meuse, mean) 
<span class="co">#&gt;   soil log1p(om)</span>
<span class="co">#&gt; 1    1       2.2</span>
<span class="co">#&gt; 2    2       1.7</span>
<span class="co">#&gt; 3    3       1.9</span></code></pre></div>
<p>Note that this equality can be observed only if we remove the intercept from the regression model, hence we use:</p>
<pre><code>log1p(om) ~ soil-1</code></pre>
<p>and NOT:</p>
<pre><code>log1p(om) ~ soil</code></pre>
<p>The RK model can also be extended to fuzzy memberships, in which case <span class="math inline">\({\rm{MU}}\)</span> values are binary variables with continuous values in the range 0–1. Hence also the SOLIM model (Eq.@ref(eq:solim)) is in fact just a special version of regression on mapping units:</p>
<span class="math display">\[\begin{equation}
\hat z({{s}}_0 ) = \sum\limits_{c_j = 1}^{c_p} {\nu _{c_j} ({{s}}_0) \cdot z_{c_j} } = \sum\limits_{j = 1}^p { {\rm{MU}}_j \cdot \hat b_j}  \hspace{.5cm} {\rm {for}}  \hspace{.5cm}  z_{c_j} = \frac{1}{{n_p }}\sum\limits_{i = 1}^{n_p } {z({{s}}_i )}
(\#eq:SOLIMreg)
\end{equation}\]</span>
<p>where <span class="math inline">\({\rm{MU}}\)</span> is the mapping unit or soil type, <span class="math inline">\(z_{c_j}\)</span> is the modal (or most representative) value of some soil property <span class="math inline">\(z\)</span> for the <span class="math inline">\(c_j\)</span> class, and <span class="math inline">\(n_p\)</span> is total number of points in some mapping unit <span class="math inline">\({\rm{MU}}\)</span>.</p>
<p>Ultimately, spatially weighted averaging of values per mapping unit, different types of regression, and regression kriging are all, in principle, different variants of the same statistical method. The differences are related to whether only categorical or both categorical and continuous covariates are used and whether the stochastic residual is spatially correlated or not. Although there are different ways to implement combined deterministic/stochastic predictions, one should not treat these nominally equivalent techniques as highly different.</p>
</div>
<div id="block-support" class="section level3">
<h3><span class="header-section-number">5.2.11</span> Predictions at point vs block support</h3>
<p>The geostatistical model refers to a soil variable that is defined by the type of property and how it is measured (e.g. soil pH (KCl), soil pH (H<span class="math inline">\(_2\)</span>O), clay content, soil organic carbon measured with spectroscopy), but also to the size and orientation of the soil samples that were taken from the field. This is important because the spatial variation of the dependent variable strongly depends on the support size (e.g. due to an averaging out effect, the average organic content of bulked samples taken from 1 ha plots typically has less spatial variation than that of single soil samples taken from squares). This implies that observations at different supports cannot be merged without taking this effect into account <span class="citation">(Webster and Oliver <a href="#ref-Webster2001Wiley">2001</a>)</span>. When making spatial predictions using kriging one can use <em>block-kriging</em> <span class="citation">(Webster and Oliver <a href="#ref-Webster2001Wiley">2001</a>)</span> or <em>area-to-point kriging</em> <span class="citation">(Kyriakidis <a href="#ref-Kyriakidis2004GEAN1135">2004</a>)</span> to make predictions at larger or smaller supports. Both block-kriging and area-to-point kriging are implemented in the gstat package via the generic function <code>krige</code> <span class="citation">(Pebesma <a href="#ref-Pebesma2004CG">2004</a>)</span>.</p>
<p><em>Support</em> can be defined as the integration volume or aggregation level at which an observation is taken or for which an estimate or prediction is given. Support is often used in the literature as a synonym for <em>scale</em> — large support can be related to coarse or general scales and vice versa <span class="citation">(Hengl <a href="#ref-Hengl2006CG">2006</a>)</span>. The notion of support is important to characterize and relate different scales of soil variation <span class="citation">(Schabenberger and Gotway <a href="#ref-schabenberger2005statistical">2005</a>)</span>. Any research of soil properties is made with specific support and spatial spacing, the latter being the distance between sampling locations. If properties are to be used with different support, e.g. when model inputs require a different support than the support of the observations, scaling (aggregation or disaggregation) becomes necessary <span class="citation">(Heuvelink and Pebesma <a href="#ref-Heuvelink1999Geoderma">1999</a>)</span>.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_confidence_limits.png" alt="Scheme with predictions on point (above) and block support (below). In the case of various versions of kriging, both point and block predictions smooth the original measurements proportionally to the nugget variation. After Goovaerts (1997)." width="100%" />
<p class="caption">
(#fig:confidence-limits-block)Scheme with predictions on point (above) and block support (below). In the case of various versions of kriging, both point and block predictions smooth the original measurements proportionally to the nugget variation. After Goovaerts (1997).
</p>
</div>
<p>Depending on how significant the nugget variation is, prediction variance estimated by a model can be significantly reduced by increasing the support from points to blocks. The block kriging variance is smaller than the point kriging variance for an amount approximately equal to the nugget variation. Even if we take a block size of a few meters this decreases the prediction error significantly, if indeed the nugget variation occurs within a few meters. Because, by definition, many kriging-type techniques smooth original sampled values, one can easily notice that for support sizes smaller than half of the average shortest distance between the sampling locations, both point and block predictions might lead to practically the same predictions (see some examples by <span class="citation">Goovaerts (<a href="#ref-Goovaerts1997Oxford">1997</a>, 158)</span>, <span class="citation">Heuvelink and Pebesma (<a href="#ref-Heuvelink1999Geoderma">1999</a>)</span> and/or <span class="citation">Hengl (<a href="#ref-Hengl2006CG">2006</a>)</span>).</p>

<div class="rmdnote">
The spatial support is the integration volume or size of the blocks being sampled and/or predicted. By increasing the support size from point to block support we decrease the prediction error variance. The decrease in the prediction error variance is approximately equal to the nugget variance.
</div>

<p>Consider, for example, point and block predictions and simulations using the estimates of organic matter content in the topsoil (in dg/kg) for the Meuse case study. We first generate predictions and simulations on point support:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">omm &lt;-<span class="st"> </span><span class="kw">fit.gstatModel</span>(meuse, <span class="kw">log1p</span>(om)<span class="op">~</span>dist<span class="op">+</span>soil, meuse.grid)
<span class="co">#&gt; Fitting a linear model...</span>
<span class="co">#&gt; Fitting a 2D variogram...</span>
<span class="co">#&gt; Saving an object of class &#39;gstatModel&#39;...</span>
om.rk.p &lt;-<span class="st"> </span><span class="kw">predict</span>(omm, meuse.grid, <span class="dt">block=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>))
<span class="co">#&gt; Subsetting observations to fit the prediction domain in 2D...</span>
<span class="co">#&gt; Generating predictions using the trend model (RK method)...</span>
<span class="co">#&gt; [using ordinary kriging]</span>
<span class="co">#&gt; </span>
<span class="dv">100</span>% done
<span class="co">#&gt; Running 5-fold cross validation using &#39;krige.cv&#39;...</span>
<span class="co">#&gt; Creating an object of class &quot;SpatialPredictions&quot;</span>
om.rksim.p &lt;-<span class="st"> </span><span class="kw">predict</span>(omm, meuse.grid, <span class="dt">nsim=</span><span class="dv">20</span>, <span class="dt">block=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>))
<span class="co">#&gt; Subsetting observations to fit the prediction domain in 2D...</span>
<span class="co">#&gt; Generating 20 conditional simulations using the trend model (RK method)...</span>
<span class="co">#&gt; drawing 20 GLS realisations of beta...</span>
<span class="co">#&gt; [using conditional Gaussian simulation]</span>
<span class="co">#&gt; </span>
 <span class="dv">17</span>% done
<span class="dv">100</span>% done
<span class="co">#&gt; Creating an object of class &quot;RasterBrickSimulations&quot;</span>
<span class="co">#&gt; Loading required package: raster</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: &#39;raster&#39;</span>
<span class="co">#&gt; The following object is masked from &#39;package:nlme&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     getData</span>
<span class="co">#&gt; The following objects are masked from &#39;package:aqp&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     metadata, metadata&lt;-</span></code></pre></div>
<p>where the argument <code>block</code> defines the support size for the predictions (in this case points). To produce predictions on block support for square blocks of (<strong><em>WHAT</em></strong>) by(<strong><em>WHAT</em></strong>) we run:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">om.rk.b &lt;-<span class="st"> </span><span class="kw">predict</span>(omm, meuse.grid, <span class="dt">block=</span><span class="kw">c</span>(<span class="dv">40</span>,<span class="dv">40</span>), <span class="dt">nfold=</span><span class="dv">0</span>)
<span class="co">#&gt; Subsetting observations to fit the prediction domain in 2D...</span>
<span class="co">#&gt; Generating predictions using the trend model (RK method)...</span>
<span class="co">#&gt; [using ordinary kriging]</span>
<span class="co">#&gt; </span>
<span class="dv">100</span>% done
<span class="co">#&gt; Creating an object of class &quot;SpatialPredictions&quot;</span>
om.rksim.b &lt;-<span class="st"> </span><span class="kw">predict</span>(omm, meuse.grid, <span class="dt">nsim=</span><span class="dv">2</span>, <span class="dt">block=</span><span class="kw">c</span>(<span class="dv">40</span>,<span class="dv">40</span>), <span class="dt">debug.level=</span><span class="dv">0</span>)
<span class="co">#&gt; Subsetting observations to fit the prediction domain in 2D...</span>
<span class="co">#&gt; Generating 2 conditional simulations using the trend model (RK method)...</span>
<span class="co">#&gt; Creating an object of class &quot;RasterBrickSimulations&quot;</span>
## computationally intensive</code></pre></div>
<p>Visual comparison confirms that the point and block kriging prediction maps are quite similar, while the block kriging variance is much smaller than the point kriging variance (Fig. @ref(fig:meuse-block-predictions)).</p>
<p>Even though block kriging variances are smaller than point kriging variances this does not imply that block kriging should always be preferred over point kriging. If the user interest is in point values rather than block averages, point kriging should be used. Block kriging is also computationally more demanding than point kriging. Note also that it is more difficult (read: more expensive) to validate block kriging maps. In the case of point predictions, maps can be validated to some degree using cross-validation, which is inexpensive. For example, via one can estimate the cross-validation error using the <code>krige.cv</code> function. The (<strong><em>WHAT</em></strong>) package reports automatically the cross-validation error <span class="citation">(Hengl, Nikolić, and MacMillan <a href="#ref-Hengl2013JAG">2013</a>)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">om.rk.p
<span class="co">#&gt;   Variable           : om </span>
<span class="co">#&gt;   Minium value       : 1 </span>
<span class="co">#&gt;   Maximum value      : 17 </span>
<span class="co">#&gt;   Size               : 153 </span>
<span class="co">#&gt;   Total area         : 4964800 </span>
<span class="co">#&gt;   Total area (units) : square-m </span>
<span class="co">#&gt;   Resolution (x)     : 40 </span>
<span class="co">#&gt;   Resolution (y)     : 40 </span>
<span class="co">#&gt;   Resolution (units) : m </span>
<span class="co">#&gt;   GLM call formula   : log1p(om) ~ dist + soil </span>
<span class="co">#&gt;   Family             : gaussian </span>
<span class="co">#&gt;   Link function      : identity </span>
<span class="co">#&gt;   Vgm model          : Exp </span>
<span class="co">#&gt;   Nugget (residual)  : 0.048 </span>
<span class="co">#&gt;   Sill (residual)    : 0.065 </span>
<span class="co">#&gt;   Range (residual)   : 285 </span>
<span class="co">#&gt;   RMSE (validation)  : 2.5 </span>
<span class="co">#&gt;   Var explained      : 47.3% </span>
<span class="co">#&gt;   Effective bytes    : 313 </span>
<span class="co">#&gt;   Compression method : gzip</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/Fig_meuse_block_predictions.jpg" alt="Predictions and simulations (2) at point (above) and block (below) support using the Meuse dataset. Note that prediction values produced by point and block methods are quite similar. Simulations on block support produce smoother maps than the point-support simulations." width="100%" />
<p class="caption">
(#fig:meuse-block-predictions)Predictions and simulations (2) at point (above) and block (below) support using the Meuse dataset. Note that prediction values produced by point and block methods are quite similar. Simulations on block support produce smoother maps than the point-support simulations.
</p>
</div>
<p>which shows that the mapping accuracy at point support is ca. 53% of the original variance (see further Eq.@ref(eq:normvar)).</p>
<p>Note also that, cross-validation using block support in is not possible because the input data needed for cross-validation are only available at point support. This basically means that, for the Meuse example, to estimate the mapping accuracy at block support we would have to revisit the study area and collect additional (composite) samples on block support that match the support size of block predictions.</p>
<p>Although prediction at block support is attractive because it leads to more <em>precise</em> predictions, the amount of variation explained by predictions at block versus point support might not differ all that much or even at all. Likewise users might not be interested in block averages and may require point predictions. Geostatistical simulations on block support can also be computationally intensive and extra field effort is almost certain to be necessary to validate these maps.</p>
<p>One can use point samples to produce both point and block predictions, but it is more difficult to produce point predictions from block observations. This can be done using area-to-point kriging <span class="citation">(Kyriakidis <a href="#ref-Kyriakidis2004GEAN1135">2004</a>)</span>, but this technique is computationally intensive, yields large prediction uncertainties, and is hampered by the fact that it requires the point support variogram which cannot uniquely be derived from only block observations.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_meuse_block_support_plots1.png" alt="Correlation plots for predictions and prediction variance: point vs block support." width="100%" />
<p class="caption">
(#fig:meuse-block-support-plots1)Correlation plots for predictions and prediction variance: point vs block support.
</p>
</div>
<div class="figure" style="text-align: center">
<img src="figures/Fig_meuse_block_support_plots2.png" alt="Difference in variograms sampled from the simulated maps: point vs block support." width="100%" />
<p class="caption">
(#fig:meuse-block-support-plots2)Difference in variograms sampled from the simulated maps: point vs block support.
</p>
</div>
<p>What confuses non-geostatisticians is that both point and block predictions are normally visualized using raster GIS models, hence one does not see that the point predictions refer to the centres of the grid cells <span class="citation">(Hengl <a href="#ref-Hengl2006CG">2006</a>)</span>. In the case of soil survey, the available soil profile data most typically refer to point locations (<span class="math inline">\(1\times 1\)</span> meter or smaller horizontal blocks) because soil samples have small support. In some cases surveyors mix soil samples from several different profle locations to produce composite estimates of values. Nevertheless, we can assume that the vast majority of soil profiles that are collected in the world refer to (lateral) point support. Hence the most typical combination of support size that we work with is: point support for soil property observations, block support for covariates and point or block support for soil property predictions. Modelling at full point support (both soil sampled, covariates and outputs at point support) is in fact very rare. Soil covariates are often derived from remote sensing data, which is almost always delivered at block support.</p>
<p>In principle, there is no problem with using covariates at block support to predict the soil at point support, except the strength of the relationship between the covariate and target soil property may be weakened by a mismatch in the support. Ideally, one should always try to collect all input data at the finest support possible, then aggregate based on the project requirements. This is unfortunately not always possible, as most inputs are often <em>bulked</em> already and our knowledge about the short range variation is often very limited.</p>
<p>Figs. @ref(fig:meuse-block-support-plots1) and @ref(fig:meuse-block-support-plots2) (correlation plots for Meuse data set) confirms that: (1) predictions on block and point support show practically no differences and (2) the difference in the prediction error variance for point and block kriging effectively equals the nugget variance.</p>
<p>The targeted support size for the <em>GlobalSoilMap</em> project, for example, is 3–arcsecond (ca. 100 m) horizontal dimensions of the SRTM and other covariate data layers used to support prediction of spatial variation in soil properties. This project probably needs predictions at both point and block support at the target resolution, and then also provide aggregated values at coarser resolution blocks (250, 500, 1000 m etc). In any case, understanding consequences of aggregating spatial data and converting from point to block support is important.</p>

<div class="rmdnote">
In geostatistics, one needs to consider that any input / output spatial layer refers to some support. In soil mapping, there are three main support sizes: support size of the soil samples (sampling support; can refer to point locations or blocks of land), support size of the covariates (often equivalent to the grid cell size), and support size of predictions (again point locations or blocks of land).
</div>

</div>
<div id="gstat-sims" class="section level3">
<h3><span class="header-section-number">5.2.12</span> Geostatistical simulations</h3>
<p>In statistical terms, the assessment of the uncertainty of produced maps is equally important as the prediction of values at all locations. As shown in the previous Section, uncertainty of soil variables can be assessed in several ways. Three aspects, however, appear to be important for any type of spatial prediction model:</p>
<ul>
<li><p>What are the <em>conditional probability distribution functions</em> (PDFs) of the target variable at each location?</p></li>
<li><p>Where does the prediction model exhibit its <em>largest errors</em>?</p></li>
<li><p>What is the <em>accuracy</em> of the spatial predictions for the entire area of interest? And how accurate is the map overall?</p></li>
</ul>
<p>For situations in which PDFs can be estimated <em>‘reliably’</em>, <span class="citation">Heuvelink and Brown (<a href="#ref-Heuvelink2006Elsevier">2006</a>)</span> argued that they confer a number of advantages over non-probabilistic techniques. For example, PDFs include methods for describing interdependence or correlation between uncertainties, methods for propagating uncertainties through environmental models and methods for tracing the sources of uncertainty in environmental data and models <span class="citation">(Heuvelink <a href="#ref-Heuvelink1998a">1998</a>)</span>. By taking a geostatistical approach, kriging not only yields prediction maps, but also automatically produces PDFs at prediction points and quantifies the spatial correlation in the prediction errors. Geostatistical simulation, as already introduced in previous sections, refers to a method where realizations are drawn from the conditional PDF using a pseudo-random number generator. These simulations give a more realistic image of the spatial correlation structure or spatial pattern of the target variable because, unlike kriging, they do not smooth out the values.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_20_sims_cross_section.png" alt="20 simulations (at block support) of the soil organic carbon for the Meuse study area (cross-section from West to East at Y=330348). Bold line indicates the median value and broken lines indicate upper and lower quantiles (95\% probability)." width="100%" />
<p class="caption">
(#fig:sims-cross-section)20 simulations (at block support) of the soil organic carbon for the Meuse study area (cross-section from West to East at Y=330348). Bold line indicates the median value and broken lines indicate upper and lower quantiles (95% probability).
</p>
</div>
<p>Estimates of the model accuracy are also provided by the geostatistical model, i.e. the kriging variance. It is useful to note that the variance of a large number of geostatistical simulations will approximate the kriging variance (and likewise the average of a large number of simulations will approximate the kriging prediction map).</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_hist_om_predicted_vs_simulated.png" alt="Histogram for the target variable (Meuse data set; log of organic matter) based on the actual observations (left), predictions at all grid nodes (middle) and simulations (right). Note that the histogram for predicted values will always show somewhat narrower distribution (smoothed), depending on the strength of the model, while the simulations should be able to reproduce the original range (see also Yamamoto et al. (2008))." width="100%" />
<p class="caption">
(#fig:hist-om-predicted-simulated)Histogram for the target variable (Meuse data set; log of organic matter) based on the actual observations (left), predictions at all grid nodes (middle) and simulations (right). Note that the histogram for predicted values will always show somewhat narrower distribution (smoothed), depending on the strength of the model, while the simulations should be able to reproduce the original range (see also Yamamoto et al. (2008)).
</p>
</div>
<p>The differences among an ensemble of realizations produced using geostatistical simulations capture the uncertainty associated with the prediction map and can be used to communicate uncertainty or used as input in a spatial uncertainty propagation analysis.</p>
<p>Even though the kriging variance and geostatistical simulations are valid and valuable means to quantify the prediction accuracy, it is important to be aware that these assessments of uncertainty are <em>model-based</em>, i.e. are only valid under the assumptions made by the geostatistical model. A truly <em>model-free</em> assessment of the map accuracy can (only) be obtained by probability-based validation <span class="citation">(Brus, Kempen, and Heuvelink <a href="#ref-Brus2011EJSS">2011</a>)</span>. For this we need independent sample i.e. a sample that was not used to build the model and make the predictions, and that, in addition, was selected from the study area using a probabilistic sampling design.</p>
<p>For the regression-kriging model fitted for organic carbon of the Meuse data set, we can produce 20 simulations by switching the <code>nsim</code> argument:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">om.rksim.p &lt;-<span class="st"> </span><span class="kw">predict</span>(omm, meuse.grid, <span class="dt">block=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">nsim=</span><span class="dv">20</span>)
<span class="co">#&gt; Subsetting observations to fit the prediction domain in 2D...</span>
<span class="co">#&gt; Generating 20 conditional simulations using the trend model (RK method)...</span>
<span class="co">#&gt; drawing 20 GLS realisations of beta...</span>
<span class="co">#&gt; [using conditional Gaussian simulation]</span>
<span class="co">#&gt; </span>
<span class="dv">100</span>% done
<span class="co">#&gt; Creating an object of class &quot;RasterBrickSimulations&quot;</span>
<span class="kw">log1p</span>(meuse<span class="op">@</span>data[<span class="dv">1</span>,<span class="st">&quot;om&quot;</span>])
<span class="co">#&gt; [1] 2.7</span>
<span class="kw">extract</span>(<span class="kw">raster</span>(om.rk.p<span class="op">@</span>predicted), meuse[<span class="dv">1</span>,])
<span class="co">#&gt; [1] 2.7</span>
<span class="kw">extract</span>(om.rksim.p<span class="op">@</span>realizations, meuse[<span class="dv">1</span>,])
<span class="co">#&gt;      sim1 sim2 sim3 sim4 sim5 sim6 sim7 sim8 sim9 sim10 sim11 sim12 sim13</span>
<span class="co">#&gt; [1,]  2.3  2.8  2.8  2.9  2.2  2.4  2.8  2.4  2.4     2   2.3   2.9   2.8</span>
<span class="co">#&gt;      sim14 sim15 sim16 sim17 sim18 sim19 sim20</span>
<span class="co">#&gt; [1,]   2.7   2.5   2.9   2.7   2.8   2.4   2.5</span></code></pre></div>
<p>which shows the difference between sampled value (2.681022), predicted value (2.677931) and simulated values for about the same location i.e. a PDF (see also histograms in Fig. @ref(fig:hist-om-predicted-simulated)). If we average the 20 simulations we obtain an alternative estimate of the mean:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">extract</span>(om.rksim.p<span class="op">@</span>realizations, meuse[<span class="dv">1</span>,]))
<span class="co">#&gt; [1] 2.6</span></code></pre></div>
<p>In this case there remains a small difference between the two results, which is probably due to the small number of simulations (20) used.</p>
</div>
<div id="automated-mapping" class="section level3">
<h3><span class="header-section-number">5.2.13</span> Automated mapping</h3>
<p>Applications of geostatistics today suggest that we will be increasingly using <em>automated mapping</em> algorithms for mapping environmental variables. The authors of the <a href="https://cran.r-project.org/package=intamap">intamap</a> package for R, for example, have produced a wrapper function <code>interpolate</code> that automatically generates predictions for any given combiination of input observations and prediction locations <span class="citation">(Pebesma et al. <a href="#ref-Pebesma2011CompGeoSci">2011</a>)</span>. Consider the following example for predicting organic matter content using the Meuse case study:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(intamap)
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: &#39;intamap&#39;</span>
<span class="co">#&gt; The following object is masked from &#39;package:raster&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     interpolate</span>
<span class="kw">demo</span>(meuse, <span class="dt">echo=</span><span class="ot">FALSE</span>)
meuse<span class="op">$</span>value =<span class="st"> </span>meuse<span class="op">$</span>zinc
output &lt;-<span class="st"> </span><span class="kw">interpolate</span>(meuse, meuse.grid, <span class="kw">list</span>(<span class="dt">mean=</span><span class="ot">TRUE</span>, <span class="dt">variance=</span><span class="ot">TRUE</span>))
<span class="co">#&gt; R 2018-12-18 15:21:29 interpolating 155 observations, 3103 prediction locations</span>
<span class="co">#&gt; Warning in predictTime(nObs = dim(observations)[1], nPred = nPred, formulaString = formulaString, : </span>
<span class="co">#&gt;  using standard model for estimating time. For better </span>
<span class="co">#&gt;  platform spesific predictions, please run </span>
<span class="co">#&gt;  timeModels &lt;- generateTimeModels()</span>
<span class="co">#&gt;   and save the workspace</span>
<span class="co">#&gt; [1] &quot;estimated time for  copula 147.539115038528&quot;</span>
<span class="co">#&gt; Checking object ... OK</span></code></pre></div>
<p>which gives the (presumably) best interpolation method for the problem at hand (<code>value</code> column), given the time available set with <code>maximumTime</code> <span class="citation">(Pebesma et al. <a href="#ref-Pebesma2011CompGeoSci">2011</a>)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(output, <span class="dt">max.level =</span> <span class="dv">2</span>)
<span class="co">#&gt; List of 16</span>
<span class="co">#&gt;  $ observations       :Formal class &#39;SpatialPointsDataFrame&#39; [package &quot;sp&quot;] with 5 slots</span>
<span class="co">#&gt;  $ formulaString      :Class &#39;formula&#39;  language value ~ 1</span>
<span class="co">#&gt;   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: 0x55ff0110bdd0&gt; </span>
<span class="co">#&gt;  $ predictionLocations:Formal class &#39;SpatialPixelsDataFrame&#39; [package &quot;sp&quot;] with 7 slots</span>
<span class="co">#&gt;  $ params             :List of 18</span>
<span class="co">#&gt;   ..$ doAnisotropy     : logi TRUE</span>
<span class="co">#&gt;   ..$ testMean         : logi FALSE</span>
<span class="co">#&gt;   ..$ removeBias       : logi NA</span>
<span class="co">#&gt;   ..$ addBias          : logi NA</span>
<span class="co">#&gt;   ..$ biasRemovalMethod: chr &quot;LM&quot;</span>
<span class="co">#&gt;   ..$ nmax             : num 50</span>
<span class="co">#&gt;   ..$ nmin             : num 0</span>
<span class="co">#&gt;   ..$ omax             : num 0</span>
<span class="co">#&gt;   ..$ maxdist          : num Inf</span>
<span class="co">#&gt;   ..$ ngrid            : num 100</span>
<span class="co">#&gt;   ..$ nsim             : num 100</span>
<span class="co">#&gt;   ..$ sMin             : num 4</span>
<span class="co">#&gt;   ..$ block            : num(0) </span>
<span class="co">#&gt;   ..$ processType      : chr &quot;gaussian&quot;</span>
<span class="co">#&gt;   ..$ confProj         : logi TRUE</span>
<span class="co">#&gt;   ..$ debug.level      : num 0</span>
<span class="co">#&gt;   ..$ nclus            : num 1</span>
<span class="co">#&gt;   ..$ significant      : logi TRUE</span>
<span class="co">#&gt;   ..- attr(*, &quot;class&quot;)= chr &quot;IntamapParams&quot;</span>
<span class="co">#&gt;  $ outputWhat         :List of 2</span>
<span class="co">#&gt;   ..$ mean    : logi TRUE</span>
<span class="co">#&gt;   ..$ variance: logi TRUE</span>
<span class="co">#&gt;  $ blockWhat          : chr &quot;none&quot;</span>
<span class="co">#&gt;  $ intCRS             : chr &quot;+init=epsg:28992 +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=46&quot;| __truncated__</span>
<span class="co">#&gt;  $ lambda             : num -0.27</span>
<span class="co">#&gt;  $ anisPar            :List of 4</span>
<span class="co">#&gt;   ..$ ratio     : num 1.48</span>
<span class="co">#&gt;   ..$ direction : num 56.1</span>
<span class="co">#&gt;   ..$ Q         : num [1, 1:3] 3.05e-07 2.29e-07 -9.28e-08</span>
<span class="co">#&gt;   .. ..- attr(*, &quot;dimnames&quot;)=List of 2</span>
<span class="co">#&gt;   ..$ doRotation: logi TRUE</span>
<span class="co">#&gt;  $ variogramModel     :Classes &#39;variogramModel&#39; and &#39;data.frame&#39;:    2 obs. of  9 variables:</span>
<span class="co">#&gt;   ..$ model: Factor w/ 20 levels &quot;Nug&quot;,&quot;Exp&quot;,&quot;Sph&quot;,..: 1 3</span>
<span class="co">#&gt;   ..$ psill: num [1:2] 0.00141 0.02527</span>
<span class="co">#&gt;   ..$ range: num [1:2] 0 1282</span>
<span class="co">#&gt;   ..$ kappa: num [1:2] 0 0</span>
<span class="co">#&gt;   ..$ ang1 : num [1:2] 0 33.9</span>
<span class="co">#&gt;   ..$ ang2 : num [1:2] 0 0</span>
<span class="co">#&gt;   ..$ ang3 : num [1:2] 0 0</span>
<span class="co">#&gt;   ..$ anis1: num [1:2] 1 0.674</span>
<span class="co">#&gt;   ..$ anis2: num [1:2] 1 1</span>
<span class="co">#&gt;   ..- attr(*, &quot;singular&quot;)= logi FALSE</span>
<span class="co">#&gt;   ..- attr(*, &quot;SSErr&quot;)= num 2.84e-08</span>
<span class="co">#&gt;   ..- attr(*, &quot;call&quot;)= language fit.variogram(object = experimental_variogram, model = vgm(psill = psill,      model = model, range = range, nugg| __truncated__ ...</span>
<span class="co">#&gt;  $ sampleVariogram    :Classes &#39;gstatVariogram&#39; and &#39;data.frame&#39;:    11 obs. of  6 variables:</span>
<span class="co">#&gt;   ..$ np     : num [1:11] 7 31 94 132 147 ...</span>
<span class="co">#&gt;   ..$ dist   : num [1:11] 67.2 94.2 142.9 193.5 248.9 ...</span>
<span class="co">#&gt;   ..$ gamma  : num [1:11] 0.000891 0.005635 0.005537 0.006056 0.010289 ...</span>
<span class="co">#&gt;   ..$ dir.hor: num [1:11] 0 0 0 0 0 0 0 0 0 0 ...</span>
<span class="co">#&gt;   ..$ dir.ver: num [1:11] 0 0 0 0 0 0 0 0 0 0 ...</span>
<span class="co">#&gt;   ..$ id     : Factor w/ 1 level &quot;var1&quot;: 1 1 1 1 1 1 1 1 1 1 ...</span>
<span class="co">#&gt;   ..- attr(*, &quot;direct&quot;)=&#39;data.frame&#39;:    1 obs. of  2 variables:</span>
<span class="co">#&gt;   ..- attr(*, &quot;boundaries&quot;)= num [1:12] 36.8 73.5 110.3 165.5 220.6 ...</span>
<span class="co">#&gt;   ..- attr(*, &quot;pseudo&quot;)= num 0</span>
<span class="co">#&gt;   ..- attr(*, &quot;what&quot;)= chr &quot;semivariance&quot;</span>
<span class="co">#&gt;  $ methodParameters   : chr &quot;  vmodel = data.frame(matrix(0,nrow =  2 ,ncol =  9 ))\nnames(vmodel) = c(\&quot;model\&quot;,\&quot;psill\&quot;,\&quot;range\&quot;,\&quot;kappa&quot;| __truncated__</span>
<span class="co">#&gt;  $ predictions        :Formal class &#39;SpatialPixelsDataFrame&#39; [package &quot;sp&quot;] with 7 slots</span>
<span class="co">#&gt;  $ outputTable        : num [1:4, 1:3103] 181180 333740 842 44785 181140 ...</span>
<span class="co">#&gt;   ..- attr(*, &quot;dimnames&quot;)=List of 2</span>
<span class="co">#&gt;   ..- attr(*, &quot;transposed&quot;)= logi TRUE</span>
<span class="co">#&gt;  $ processPlot        : chr &quot;&quot;</span>
<span class="co">#&gt;  $ processDescription : chr &quot;Spatial prediction using the method  transGaussian&quot;</span>
<span class="co">#&gt;  - attr(*, &quot;class&quot;)= chr &quot;transGaussian&quot;</span></code></pre></div>
<p>The interpolate function automatically chooses between: (1) kriging, (2) copula methods, (3) inverse distance interpolation, projected spatial gaussian process methods in the package, (4) transGaussian kriging or Yamamoto interpolation.</p>

<div class="rmdnote">
Automated mapping is the computer-aided generation of (meaningful) maps from measurements. In the context of geostatistical mapping, automated mapping implies that the model fitting, prediction and visualization can be run with little or no human interaction / intervention.
</div>

<p>The same idea of automated model fitting and prediction has been implemented in the package for (<strong><em>WHAT</em></strong>) , which extends simple point-based models to 2D, 3D, 2D+T regression-kriging models. Some examples of automated soil mapping have been already (<strong><em>WHAT</em></strong>) shown previously.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_statmodels.png" alt="A modern workflow of predictive soil mapping. This often includes state-of-the-art Machine Learning Algorithms." width="60%" />
<p class="caption">
(#fig:scheme-statmodels)A modern workflow of predictive soil mapping. This often includes state-of-the-art Machine Learning Algorithms.
</p>
</div>
<p>Automated mapping, as long as it is not a <em>black-box</em> system, is beneficial for soil mapping applications for several reasons: (1) it saves time and effort needed to get initial results, (2) it allows generation of maps using current data (live geostatistics) even via a web-interfaces, (3) it greatly reduces the workload in cases where maps need to be produced repeatedly, such as when regular updates are needed or the same model is applied in different subareas. In practice, automated mapping is typically a three-stage process (Fig. @ref(fig:scheme-statmodels)):</p>
<ol style="list-style-type: decimal">
<li><p><em>Rapidly generate predictions and a report of analysis</em> (analyze why a particular technique was chosen and how well it performs? Are there any outliers or artifacts? Which predictors are most significant? etc).</p></li>
<li><p><em>Review the results of spatial prediction and fine-tune some parameters</em> if necessary / filter and/or adjust the input maps.</p></li>
<li><p><em>Re-run the prediction process and publish the final maps</em>.</p></li>
</ol>
<p>hence geostatisticians are still an essential and active part of the process. In automated mapping they primarily focus their expertise on doing interpretation of the results rather than on manually analyzing the data.</p>
<p>It is unlikely that a simple linear prediction model can be used to fit every type of soil data. It is more likely that some customized models, i.e. models specific for each property, would perform better than if a single model were used for a diversity of soil properties. This is because different soil properties have different distributions, they vary differently at different scales, and are controlled by different processes. On the other hand, the preferred way to ensure that a single model can be used to map a variety of soil properties is to develop a generic framework with multi-thematic, multi-scale predictors that allows for iterative search for optimal model structure and parameters, and then implement this model via an automated mapping system.</p>
</div>
<div id="selecting-spatial-prediction-models" class="section level3">
<h3><span class="header-section-number">5.2.14</span> Selecting spatial prediction models</h3>
<p>The purpose of spatial prediction is to (a) produce a map showing spatial distribution of the variable of interest for the area of interest, and (b) to do this in an unbiased way. A comprehensive path to evaluating spatial predictions is the <a href="http://topepo.github.io/caret/index.html">caret</a> approach <span class="citation">(Kuhn and Johnson <a href="#ref-kuhn2013applied">2013</a>)</span>, which wraps up many of the standard processes such as model training and validation, method comparison and visualization. Consider, for example, organic matter % in the topsoil in the meuse data set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret); <span class="kw">library</span>(rgdal)
<span class="co">#&gt; Loading required package: lattice</span>
<span class="co">#&gt; Loading required package: ggplot2</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: &#39;caret&#39;</span>
<span class="co">#&gt; The following object is masked from &#39;package:intamap&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     preProcess</span>
<span class="kw">demo</span>(meuse, <span class="dt">echo=</span><span class="ot">FALSE</span>)
meuse.ov &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">over</span>(meuse, meuse.grid), meuse<span class="op">@</span>data)
meuse.ov<span class="op">$</span>x0 =<span class="st"> </span><span class="dv">1</span></code></pre></div>
<p>We can quickly compare performance of using GLM vs random forest vs no model for predicting organic matter (om) by using the caret package functionality:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fitControl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;repeatedcv&quot;</span>, <span class="dt">number=</span><span class="dv">2</span>, <span class="dt">repeats=</span><span class="dv">2</span>)
mFit0 &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(om<span class="op">~</span>x0, <span class="dt">data=</span>meuse.ov, <span class="dt">method=</span><span class="st">&quot;glm&quot;</span>, 
               <span class="dt">family=</span><span class="kw">gaussian</span>(<span class="dt">link=</span>log), <span class="dt">trControl=</span>fitControl, 
               <span class="dt">na.action=</span>na.omit)
mFit1 &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(om<span class="op">~</span>soil, <span class="dt">data=</span>meuse.ov, <span class="dt">method=</span><span class="st">&quot;glm&quot;</span>, 
               <span class="dt">family=</span><span class="kw">gaussian</span>(<span class="dt">link=</span>log), <span class="dt">trControl=</span>fitControl, 
               <span class="dt">na.action=</span>na.omit)
mFit2 &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(om<span class="op">~</span>dist<span class="op">+</span>soil<span class="op">+</span>ffreq, <span class="dt">data=</span>meuse.ov, <span class="dt">method=</span><span class="st">&quot;glm&quot;</span>, 
               <span class="dt">family=</span><span class="kw">gaussian</span>(<span class="dt">link=</span>log), <span class="dt">trControl=</span>fitControl, 
               <span class="dt">na.action=</span>na.omit)
mFit3 &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(om<span class="op">~</span>dist<span class="op">+</span>soil<span class="op">+</span>ffreq, <span class="dt">data=</span>meuse.ov, <span class="dt">method=</span><span class="st">&quot;ranger&quot;</span>, 
               <span class="dt">trControl=</span>fitControl, <span class="dt">na.action=</span>na.omit)</code></pre></div>
<p>This will run repeated Cross-validation with 50% : 50% splits training and validation, which means that in each iteration models will be refitted from scratch. Next we can compare performance of the three models by using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">resamps &lt;-<span class="st"> </span><span class="kw">resamples</span>(<span class="kw">list</span>(<span class="dt">Mean=</span>mFit0, <span class="dt">Soilmap=</span>mFit1, <span class="dt">GLM=</span>mFit2, <span class="dt">RF=</span>mFit3))
<span class="kw">bwplot</span>(resamps, <span class="dt">layout =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>), <span class="dt">metric=</span><span class="kw">c</span>(<span class="st">&quot;RMSE&quot;</span>,<span class="st">&quot;Rsquared&quot;</span>), 
       <span class="dt">fill=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">scales =</span> <span class="kw">list</span>(<span class="dt">relation =</span> <span class="st">&quot;free&quot;</span>))</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Statistical_theory_files/figure-html/bwplot-meuse-1.png" alt="Comparison of spatial prediction accuracy (RMSE at cross-validation points) for simple averaging (Mean), GLM with only soil map as covariate (Soilmap), GLM and random forest (RF) models with all possible covariates. Error bars indicate range of RMSE values for repeated CV." width="100%" />
<p class="caption">
(#fig:bwplot-meuse)Comparison of spatial prediction accuracy (RMSE at cross-validation points) for simple averaging (Mean), GLM with only soil map as covariate (Soilmap), GLM and random forest (RF) models with all possible covariates. Error bars indicate range of RMSE values for repeated CV.
</p>
</div>
<p>In the case above, it seems that random forest (<a href="https://github.com/imbs-hl/ranger">ranger package</a>) helps reduce mean RMSE of predicting organic matter by about 32%:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>((<span class="dv">1</span><span class="op">-</span><span class="kw">min</span>(mFit3<span class="op">$</span>results<span class="op">$</span>RMSE)<span class="op">/</span><span class="kw">min</span>(mFit0<span class="op">$</span>results<span class="op">$</span>RMSE))<span class="op">*</span><span class="dv">100</span>)
<span class="co">#&gt; [1] 32</span></code></pre></div>
<p>There is certainly added value in using spatial covariates (in the case above: distance to water and flooding frequency maps) and in using machine learning for spatial prediction, even with smaller data sets.</p>
<p>Note also that the assessment of spatial prediction accuracy for the three models based on the train function above is model-free, i.e. cross-validation of the models is independent of the models used because, at each cross-validation subset, fitting of the model is repeated and validation points are maintained separate from model training. Subsetting point samples is not always trivial however: in order to consider cross-validation as completely reliable, the samples ought to be representative of the study area and preferably collected using objective sampling such as simple random sampling or similar <span class="citation">(Brus, Kempen, and Heuvelink <a href="#ref-Brus2011EJSS">2011</a>)</span>. In the case the sampling locations are clustered in geographical space i.e. if some parts of the study area are completely omitted from sampling, then also the results of cross-validation will reflect that sampling bias / poor representation. In all the following examples we will assume that cross-validation gives a reliable measure of mapping accuracy and we will use it as the basis of accuracy assessment i.e. mapping efficiency. In reality, cross-validation might be tricky to implement and could often lead to somewhat over-optimistic results if either sampling bias exists or/and if there are too few points for model validation. For example, in the case of soil profile data, it is highly recommended that entire profiles are removed from CV because soil horizons are too strongly correlated (as discussed in detail in <span class="citation">Gasch et al. (<a href="#ref-Gasch2015SPASTA">2015</a>)</span> and <span class="citation">Brenning (<a href="#ref-Brenning2012">2012</a>)</span>).</p>
<p>The whole process of spatial prediction of soil properties could be summarized in 5 steps:</p>
<ol style="list-style-type: decimal">
<li>Initial model comparison (comparison of prediction accuracy and computing time).</li>
<li>Selection of applicable model(s) and estimation of model parameters i.e. model fitting.</li>
<li>Predictions i.e. generation of maps for all areas of interest.</li>
<li>Objective accuracy assessment using independent (cross-)validation.</li>
<li>Export and sharing of maps and summary documentation explaining all processing steps.</li>
</ol>
<p>Studying the <a href="http://topepo.github.io/caret/index.html">caret package tutorial</a> and/or the <a href="https://mlr-org.github.io">mlr tutorials</a> is highly recommended for anyone looking for a systematic introduction to predictive modelling.</p>
</div>
<div id="regression-kriging-3D" class="section level3">
<h3><span class="header-section-number">5.2.15</span> 3D regression-kriging</h3>
<p>Measurements of soil properties at point support can be thought of as describing explicit 3D locations (easting, northing and depth), and are amenable to being dealt with using 3D geostatistics (e.g. 3D kriging). Application of 3D kriging to soil measurements is cumbersome for several reasons:</p>
<ol style="list-style-type: decimal">
<li><p>The differences between sampling intervals and spatial correlation in the horizontal and vertical dimensions are very large (&lt;10 in the vertical v.s. 100’s to 1000’s of in the horizontal). The resulting strong anisotropy must be accounted for when the geostatisitcal model is derived. Estimation of the anisotropy may be hampered by the relatively small number of observations along the vertical profile, although under a stationarity assumption it can benefit from the many repetitions of profile data for all profile locations.</p></li>
<li><p>Soil property values refer to vertical block support (usually because they are composite samples, i.e. the average over a soil horizon), hence some of the local variation (in the vertical dimension) has been smoothed out.</p></li>
<li><p>Soil surveyors systematically under-represent lower depths — surveyors tend to systematically take fewer samples as they assume that deeper horizons are of less importance for management or because deeper horizons are more expensive to collect or because deeper horizons are assumed to be more homogeneous and uniform.</p></li>
<li><p>Many soil properties show clear trends in the vertical dimension and, if this is ignored, the result can be a very poor geostatistical model. It may not be that easy to incorporate a vertical trend because such a trend is generally not consistently similar between different soil types. On the other hand, soil variables are auto-correlated in both horizontal and vertical (depth) dimensions, so that it makes sense to treat them using 3D geostatistics whenever we have enough 3D soil observations.</p></li>
</ol>

<div class="rmdnote">
Because soil variables are auto-correlated in both horizontal and vertical (depth) dimensions it makes sense to treat them using 3D geostatistics, as long as there are enough measurements in all spatial dimensions.
</div>

<div class="figure" style="text-align: center">
<img src="figures/Fig_voxel_scheme.png" alt="Spatial 3D prediction locations in a gridded system (voxels). In soil mapping, we often predict for larger blocks of land e.g. 100 to 1000 m, but then for vertical depths of few tens of centimeters, so the output voxels might appear in reality as being somewhat disproportional." width="60%" />
<p class="caption">
(#fig:voxel-scheme)Spatial 3D prediction locations in a gridded system (voxels). In soil mapping, we often predict for larger blocks of land e.g. 100 to 1000 m, but then for vertical depths of few tens of centimeters, so the output voxels might appear in reality as being somewhat disproportional.
</p>
</div>
<p>The fact that there are almost always &lt;10 soil observations over the total depth of a soil profile, so that the estimates of the range in the vertical dimension will be relatively poor, is something that cannot be improved. The fact that soil samples taken by horizon refer to block support is a more serious problem, as part of short range variation has been lost, plus we know that the point values do not refer to the horizon center but to the whole horizon block, which, in addition to everything else, tend to be irregular i.e. do not have constant depth and width.</p>
<p>To predict in 3D space, we extend the regression model from Eq.@ref(eq:ukm-gstat) with a soil depth function:</p>
<span class="math display">\[\begin{equation}
\begin{split}
\hat z({{s}}_0, d_0 ) = \sum\limits_{j = 0}^p {\hat \beta _j \cdot X_j ({{s}}_0, d_0 )} + {{\hat g}}(d_0) + \sum\limits_{i = 1}^n {\hat{\lambda}_i ({{s}}_0, d_0 ) \cdot e({{s}}_i, d_i )}
\end{split}
(\#eq:MRK3D)
\end{equation}\]</span>
<p>where <span class="math inline">\(d\)</span> is the 3rd depth dimension expressed in meters from the land surface, <span class="math inline">\({{\hat g}}(d_0)\)</span> is the predicted soil depth function, typically modelled by a spline function. This allows prediction of soil properties at any depth using observations at other depths but does require 3D modelling of the covariance structure, which is not easy because there may be zonal and geometric anisotropies (i.e. the variance and correlation lengths may differ between vertical and horizontal directions). Also, the vertical support of observations becomes important and it should be realized that observations are the averages over depth intervals and not values at points along the vertical axis (Fig. @ref(fig:voxel-scheme)). Spline functions have been proposed and used as mass-preserving curve fitting methods to derive point and block values along the vertical axis from observations at given depth intervals, but the difficulty is that these yield estimates (with uncertainties) that should not be confused with real observations.</p>
<p>A 3D variogram, e.g. modelled using an exponential model with three standard parameters (nugget <span class="math inline">\(c_0\)</span>, partial sill <span class="math inline">\(c_1\)</span>, range parameter <span class="math inline">\(r\)</span>):</p>
<span class="math display">\[\begin{equation}
\gamma \left( {{h}} \right) = \left\{
{\begin{array}{*{20}c}
   0 &amp; {{\rm{if}}} &amp; {h = 0}  \\
   {c_0  + c_1  \cdot \left[ {1 - e^{ - \left( {\frac{{h}}
{r}} \right)} } \right]} &amp; {{\rm{if}}} &amp; {h &gt; 0}  \\
 \end{array} } \right. \qquad {{h}} =  \left[ {h_x  , h_y  , h_d } \right]
(\#eq:exp)
\end{equation}\]</span>
<p>where the scalar <em>‘distance’</em> <span class="math inline">\(h\)</span> is calculated by scaling horizontal and vertical separation distances using three anisotropy parameters:</p>
<span class="math display">\[\begin{equation}
h = \sqrt {\left( {\frac{{h_x  }}{{a_x  }}} \right)^2  + \left( {\frac{{h_y  }}{{a_y  }}} \right)^2  + \left( {\frac{{h_d }}{{a_d }}} \right)^2 }
(\#eq:anisotropy)
\end{equation}\]</span>
<p>Typically, in the case of soil data, the anisotropy ratio between horizontal and vertical distances is high — spatial variation observed in a few depth changes may correspond with several or more in horizontal space, so that the initial settings of the anisotropy ratio (i.e. the ratio of the horizontal and vertical variogram ranges) are between 3000–8000, for example. Variogram fitting criteria can then be used to optimize the anisotropy parameters. In our case we assumed no horizontal anisotropy and hence assumed <span class="math inline">\(a_x=a_y=1\)</span>, leaving only <span class="math inline">\(a_d\)</span> to be estimated. Once the anisotropy ratio is obtained, 3D variogram modelling does not meaningfully differ from 2D variogram modelling.</p>
<p>The 3D RK framework explained above can be compared to the approach of <span class="citation">Malone et al. (<a href="#ref-Malone2009Geoderma">2009</a>)</span>, who first fit an equal-area spline function to estimate the soil properties at a standard depth, and next fit regression and variogram models at each depth. A drawback of the approach by <span class="citation">Malone et al. (<a href="#ref-Malone2009Geoderma">2009</a>)</span>, however, is that the separate models for each depth ignore all vertical correlations. In addition, the equal-area spline is not used to model soil-depth relationships but only to estimate the values at standard depths for sampling locations i.e. it is implemented for each soil profile (site) separately. In the 3D RK framework explained above, a single model is used to generate predictions at any location and for any depth, and which takes into account both horizontal and vertical relationships simultaneously. The 3D RK approach is both easier to implement, and allows for incorporating all (vertical) soil-depth relationships including the spatial correlations.</p>
</div>
<div id="multiscale" class="section level3">
<h3><span class="header-section-number">5.2.16</span> Predicting with multiscale and multisource data</h3>
<p>Fig. @ref(fig:general-sp-process) indicates that spatial prediction is a linear processes with one line of inputs and one line of outputs. In some cases soil mappers have to use methods that can work with <em>multi-scale</em> and/or <em>multi-source</em> data i.e. data with different extents, resolution and uncertainty. Here by <em>multiscale data</em> we imply covariates used for geostatistical mapping that are available at two or more (distinctly different) resolutions, but that cover the same area of interest (see also: <code>RasterStack</code> class in the package). In the case of the <em>multisource data</em>, covariates can be of any scale, they can have a variable extent, and variable accuracy (Fig. @ref(fig:multiscale-vs-multisource)b). In other words, when referring to multiscale data, we assume that the input covariate layers differ only in their resolution; whereas in referring to multisource data, we consider that all technical aspects of the input data could potentially be different.</p>
<p>Organizing (and using) multiscale and multisource data is something that probably can not be avoided in global soil mapping projects. From the GIS perspective, and assuming a democratic right to independently develop and apply spatial prediction models, merging of the multiscale and multisource data is likely to be inevitable.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_multiscale_vs_multisource.png" alt="A general scheme for generating spatial predictions using multiscale and multisource data." width="90%" />
<p class="caption">
(#fig:multiscale-vs-multisource)A general scheme for generating spatial predictions using multiscale and multisource data.
</p>
</div>
<p>As a general strategy, for multi-scale data, a statistically robust approach is to fit a single model to combined covariates downscaled or upscaled to a single, common resolution (Fig. @ref(fig:multiscale-vs-multisource)a). For the multi-source data data assimilation methods i.e. merging of predictions (Fig. @ref(fig:multiscale-vs-multisource)b) can be used <span class="citation">(Caubet et al. <a href="#ref-CAUBET201999">2019</a>)</span>. Imagine if we have covariate layers for one whole continent at some coarse resolution of e.g. 500 m, but for some specific country have other predictions at a finer resolution of e.g. 100 m. Obviously any model we develop that uses both sources of data is limited in its application to just the extent of that country. To ensure that all covariate and soil data available for that country are used to generate predictions, we can fit two models at seperate scales and independently of each other, and then merge the predictions only for the extent of the country of interest. A statistical framework for merging such predictions is given, for example, in <span class="citation">Caubet et al. (<a href="#ref-CAUBET201999">2019</a>)</span>. In that sense, methods for multisource data merging are more attractive for pan-continental and global projects, because for most of the countries in the world, both soil and covariate data are available at different effective scales.</p>

<div class="rmdnote">
A sensible approach to merging multiple predictions (usually at multiple resolutions) is to derive a weighted average of two or more predictions / use the per-pixel accuracy to assign relative weights, so that more accurate predictions receive more weight <span class="citation">(Heuvelink and Bierkens <a href="#ref-Heuvelink19921">1992</a>)</span>.
</div>

<p>It is important to emphasize, however, that, in order to combine various predictors, we do need to have an estimate of the prediction uncertainty e.g. derived using cross-validation, otherwise we are not able to assign the weights. In principle, a linear combination of statistical techniques using the equation above should be avoided if a theoretical basis exists that incorporates such a combination.</p>
<p>Combined predictions are especially interesting for situations where:</p>
<ul>
<li><p>predictions are produced using different inputs i.e. data with different coverage,</p></li>
<li><p>there are several prediction methods which are equally applicable,</p></li>
<li><p>where no theory exists that describes a combination of spatial prediction methods,</p></li>
<li><p>where fitting and prediction of individual models is faster and less problematic than fitting of a hybrid model.</p></li>
</ul>
<p>Estimation of the prediction variance and confidence interval of combined or merged predictions is more complex than estimation of the mean value.</p>
</div>
</div>
<div id="accuracy-assessment" class="section level2">
<h2><span class="header-section-number">5.3</span> Accuracy assessment and the mapping efficiency</h2>
<div id="mapping-accuracy" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Mapping accuracy and numeric resolution</h3>
<p>Every time a digital soil mapper produces soil maps, soil GIS and soil geographical databases those products can be evaluated using independent validation studies. Unfortunately, much evaluation of soil maps in the world is still done using subjective <em>‘look-good’</em> assessments and the inherent uncertainty of the product is often underreported. In this book, we promote objective assessment of the mapping accuracy, i.e. based on statistical testing using ground truth data.</p>
<p><em>Mapping accuracy</em> can be defined as the difference between an estimated value and the <em>“true”</em> value, i.e. a value of the same target variable arrived at using a significantly more accurate method. In the most simple terms, accuracy is the error component of the perfectly accurate map <span class="citation">(Mowrer and Congalton <a href="#ref-mowrer2000quantifying">2000</a>)</span>. Although we know that soils form under systematic environmental conditions and probably much of the variation is deterministic (Eq.@ref(eq:ukm)), we do not yet have tools that allow us to model soil formation and evolution processes perfectly (see also section @ref(sources-uncertainty)). The best we can do is to calibrate some spatial prediction model using field records, and then generate (the best possible) predictions. The resulting soil property map, i.e. what we know about soils, is then a sum of two <em>signals</em>:</p>
<span class="math display">\[\begin{equation}
z^{\rm{map}}({{s}}) = Z({{s}}) + \varepsilon({{s}})
(\#eq:varparts)
\end{equation}\]</span>
<p>where <span class="math inline">\(Z({{s}})\)</span> is the <em>true</em> variation, and <span class="math inline">\(\varepsilon({{s}})\)</span> is the error component i.e. what we do not know. The error component, also known as the <em>error budget</em>, consists of two parts: (1) <em>the unexplained part of soil variation</em>, and (2) <em>the pure noise</em> (sampling and measurement errors described in section @ref(sources-uncertainty)).</p>
<p>The unexplained part of soil variation is the variation we somehow failed to explain because we are not using all relevant covariates and/or due to the limited sampling intensity. For example, the sampling plan might fail to sample some hot-spots or other important local features. The unexplained part of variation also includes short-range variation, which is possibly deterministic but often not of interest or is simply not feasible to describe at common mapping scales.</p>
<p>The way to determine the error part in Eq.@ref(eq:varparts) is to collect additional samples and then determine the average error or the <em>Root Mean Square Error</em> <span class="citation">(Goovaerts <a href="#ref-goovaerts2001geostatistical">2001</a>; Finke <a href="#ref-Finke2006Elsevier">2006</a>; Li and Heap <a href="#ref-LiHeap2010EI">2010</a>)</span>:</p>
<span class="math display">\[\begin{equation}
{\it RMSE} = \sqrt {\frac{1}{l} \cdot \sum\limits_{i = 1}^l {\left[
{\hat z({{s}}_i ) - z ({{s}}_i )} \right]^2 } }
(\#eq:RMSE)
\end{equation}\]</span>
<p>where <span class="math inline">\(l\)</span> is the number of validation points, and the expected estimate of prediction error at sampling locations is equal to the nugget variation (<span class="math inline">\(E\{ {\it RMSE} \} = \sigma({{h}}=0)\)</span>). In addition to <span class="math inline">\(\it{RMSE}\)</span>, it is often interesting to see also whether the errors are, on average, positive (over-estimation) or negative (under-estimation) i.e. whether there is possibly any clear bias in our predictions:</p>
<span class="math display">\[\begin{equation}
{\rm ME} = \frac{1}{m} \sum_{j=1}^{m} (\hat y ({ s}_j) - y ({ s}_j))
(\#eq:ME)
\end{equation}\]</span>
<p>To see how much of the global variation budget has been explained by the model we can use:</p>
<span class="math display">\[\begin{equation}
 {\Sigma}_{\%} = \left[ 1 - \frac{{\it{SSE}}}{{\it{SSTO}}} \right] = \left[ 1 - \frac{{\it{RMSE}}^2}{\sigma_z^2} \right] [0-100\%]
(\#eq:normvar)
\end{equation}\]</span>
<p>where <span class="math inline">\(\it{SSE}\)</span> is the sum of squares for residuals at cross-validation points (i.e. <span class="math inline">\({\it{MSE}} \cdot n\)</span>), and <span class="math inline">\(\it{SSTO}\)</span> is the total sum of squares. <span class="math inline">\({\Sigma}_{\%}\)</span> is a global estimate of the map accuracy, valid only under the assumption that the validation points are spatially independent from the calibration points, representative and large enough (e.g. <span class="math inline">\(l&gt;50\)</span>), and that the error component is normally distributed around the zero value (<span class="math inline">\(E\left\{ {\hat z({{{s}}_i}) - z({{{s}}_i})} \right\} = 0\)</span>).</p>
<p>Once we have estimated <span class="math inline">\(\it{RMSE}\)</span>, we can also determine the effective <em>numeric resolution</em> for the predictions <span class="citation">(Hengl, Nikolić, and MacMillan <a href="#ref-Hengl2013JAG">2013</a>)</span>. For example, assuming that the original sampling variance is 1.85 and that <span class="math inline">\(\it{RMSE}\)</span>=1 (i.e. <span class="math inline">\({\Sigma}_{\%}\)</span>=47%), the effective numeric resolution for predictions is then 0.5 (as shown previously in Fig. @ref(fig:sigma-rmse-relationship)). There is probably no need to code the values with a better precision than 0.5 units.</p>
</div>
<div id="accuracy-assessment-methods" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Accuracy assessment methods</h3>
<p>There are three possibilities for estimating the <span class="math inline">\(\it{RMSE}\)</span> (Fig. @ref(fig:cross-validation-types)):</p>
<ol style="list-style-type: decimal">
<li><p><em>Run cross-validation using the same input data used for model fitting</em>.</p></li>
<li><p><em>Collect new samples using a correct probability sampling design to ensure an unbiased estimate of accuracy</em>.</p></li>
<li><p><em>Compare predicted values with more detailed maps for small study areas produced at much higher accuracy, usually also at much finer level of detail</em>.</p></li>
</ol>
<div class="figure" style="text-align: center">
<img src="figures/Fig_cross_validation_types.png" alt="General types of validation procedures for evaluating accuracy of spatial prediction models." width="70%" />
<p class="caption">
(#fig:cross-validation-types)General types of validation procedures for evaluating accuracy of spatial prediction models.
</p>
</div>
<p>Although the prediction variance already indicates what the potential accuracy of the maps is, only by independent validation can we determine the true accuracy of the maps. <span class="citation">Brus, Kempen, and Heuvelink (<a href="#ref-Brus2011EJSS">2011</a>)</span> further show that, actually, only if the validation points are selected using some probability-based sampling, like simple random sampling or stratified sampling, can one determine the true accuracy of any produced gridded maps. In practice, we can rarely afford to collect new samples, so that cross-validation is often the only viable option.</p>
</div>
<div id="cross-validation-and-its-limitations" class="section level3">
<h3><span class="header-section-number">5.3.3</span> Cross-validation and its limitations</h3>
<p>Because collecting additional (independent) samples is often impractical and expensive, validation of prediction models is commonly done by using <em>cross-validation</em> i.e. by subsetting the original point set into two data sets — calibration and validation — and then repeating the analysis. There are several types of cross-validation methods <span class="citation">(Bivand, Pebesma, and Rubio <a href="#ref-Bivand2008Springer">2008</a>, 221–26)</span>:</p>
<ul>
<li><p>the <span class="math inline">\(k\)</span>–fold cross-validation — the original sample is split into <span class="math inline">\(k\)</span> equal parts and then each is used for cross-validation;</p></li>
<li><p><em>leave-one-out</em> cross-validation (LOO) — each sampling point is used for cross-validation;</p></li>
<li><p><em>Jackknifing</em> — similar to LOO, but aims at estimating the bias of statistical analysis and not of predictions;</p></li>
</ul>
<div class="figure" style="text-align: center">
<img src="figures/Fig_cross_validation_repetitions.png" alt="Left: confidence limits for the amount of variation explained (0–100\%) for two spatial prediction methods: inverse distance interpolation (IDW) and regression-kriging (RK) for mapping organic carbon content (Meuse data set). Right: the average amount of variation explained for two realizations (5-fold cross-validation) as a function of the number of cross-validation runs (repetitions). In this case, the RK method is distinctly better than method IDW, but the cross-validation score seems to stabilize only after 10 runs." width="85%" />
<p class="caption">
(#fig:cross-validation-repetitions)Left: confidence limits for the amount of variation explained (0–100%) for two spatial prediction methods: inverse distance interpolation (IDW) and regression-kriging (RK) for mapping organic carbon content (Meuse data set). Right: the average amount of variation explained for two realizations (5-fold cross-validation) as a function of the number of cross-validation runs (repetitions). In this case, the RK method is distinctly better than method IDW, but the cross-validation score seems to stabilize only after 10 runs.
</p>
</div>

<div class="rmdnote">
Cross-validation is a cost-efficient way to get an objective estimate of the mapping accuracy. Under an assumption that the input samples are representative of the study area (ideally collected using objective / probability sampling to avoid any kind of bias).
</div>

<p>Both <span class="math inline">\(k\)</span>–fold and the leave-one-out cross validation are implemented in the e.g. package (<code>krige.cv</code> methods), which makes this type of assessment convenient to implement. Note also that cross-validation is not necessarily independent — points used for cross-validation are a subset of the original sampling design, hence if the original design is biased and/or non-representative, then also the cross-validation might not reveal the true accuracy of a technique. However, if the sampling design has been generated using some unbiased design based sampling (e.g. random sampling), randomly seleced subsets will provide unbiased estimators of the true mapping accuracy.</p>
<p><em>“Models can only be evaluated in relative terms, and their predictive value is always open to question. The primary value of models is heuristic.”</em> <span class="citation">(Oreskes, Shrader-Frechette, and Belitz <a href="#ref-Oreskes04021994">1994</a>)</span> Hence, also in soil mapping, accuracy assessment should only be considered in relative terms. Each evaluation of soil mapping accuracy might give somewhat different numbers, so it is often a good idea to repeat the evaluation multiple times. Also cross-validation requires enough repetition (at least &gt;1) otherwise over-positive or over-negative results can be produced by chance (Fig. @ref(fig:cross-validation-repetitions)). Many geostatisticians (see e.g. <code>krige.cv</code> function described in <span class="citation">Bivand, Pebesma, and Rubio (<a href="#ref-Bivand2008Springer">2008</a>, 222–23)</span>) suggest that at least 5 repetitions are needed to produce <em>‘stable’</em> measures of the mapping accuracy. If only one realization of cross-validation is used, this can accidentally lead to over-optimistic or over-pessimistic estimates of the true mapping accuracy.</p>
</div>
<div id="accuracy-of-the-predicted-model-uncertainty" class="section level3">
<h3><span class="header-section-number">5.3.4</span> Accuracy of the predicted model uncertainty</h3>
<p>Recall from Eq.@ref(eq:sp) that the output of the prediction process is typically (1) predicted mean value at some location (<span class="math inline">\(\hat Z({{s}}_0)\)</span>), and (2) predicted prediction variance i.e. regression-kriging error (<span class="math inline">\(\hat{\sigma}({{s}}_0)\)</span>). In the previous section we have shown some common accuracy measures for the prediction of the mean value. It might sound confusing but, in geostatistics, one can also validate the <em>uncertainty of uncertainty</em> i.e. derive the <em>error of the estimation error</em>. In the case of the Meuse data set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">om.rk.cv &lt;-<span class="st"> </span><span class="kw">krige.cv</span>(<span class="kw">log1p</span>(om)<span class="op">~</span>dist<span class="op">+</span>soil, meuse.s, vr.fit)
<span class="kw">hist</span>(om.rk.cv<span class="op">$</span>zscore, <span class="dt">main=</span><span class="st">&quot;Z-scores histogram&quot;</span>, 
       <span class="dt">xlab=</span><span class="st">&quot;z-score value&quot;</span>, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">breaks=</span><span class="dv">25</span>)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Statistical_theory_files/figure-html/z-scores-histogram-1.png" alt="Z-scores for the cross-validation of the soil organic carbon model." width="100%" />
<p class="caption">
(#fig:z-scores-histogram)Z-scores for the cross-validation of the soil organic carbon model.
</p>
</div>
<p>Here, the cross-validation function <code>krige.cv</code> reports errors at validation points (5–fold cross-validation by default), but it also compares the difference between the regression-kriging error estimated by the model and the actual error. The ratio between the actual and expected error is referred to as the <span class="math inline">\(z\)</span>-scores <span class="citation">(Bivand, Pebesma, and Rubio <a href="#ref-Bivand2008Springer">2008</a>, 225)</span>:</p>
<span class="math display">\[\begin{equation}
 \sigma_r ({{s}}_j) = \frac{\hat z({{s}}_j ) - z^* ({{s}}_j )}{\hat{\sigma}({{s}}_j )}; \qquad  E\{var(\sigma_r)\} = 1
(\#eq:z-scores)
\end{equation}\]</span>
<p>Ideally, the mean value of <span class="math inline">\(z\)</span>-scores should be around 0 and the variance of the <span class="math inline">\(z\)</span>-scores should be around 1. If the <span class="math inline">\(z\)</span>-score variance is substantially smaller than <span class="math inline">\(1\)</span>, then the model overestimates the actual prediction uncertainty. If the <span class="math inline">\(z\)</span>-score variance is substantially greater than <span class="math inline">\(1\)</span>, then the model underestimates the prediction uncertainty. The difference between the actual and predicted model error can be also referred to as the <em>model reliability</em>. A model can be accurate but then <em>‘overpessimistic’</em> if the predicted model uncertainty is wider than the actual uncertainty, or accurate but <em>‘overoptimistic’</em> if the reported confidence limits are too narrow (Fig. @ref(fig:difference-accuracy-reliability)).</p>
<p>Ideally, we aim to produce prediction, and prediction error, maps that are both accurate and realistic; or at least realistic. For a review of methods for assessment of uncertainty in soil maps refer to <span class="citation">Goovaerts (<a href="#ref-goovaerts2001geostatistical">2001</a>, 3–26)</span> and/or <span class="citation">Brus, Kempen, and Heuvelink (<a href="#ref-Brus2011EJSS">2011</a>)</span>.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_difference_accuracy_reliability.png" alt="Mapping accuracy and model reliability (accuracy of the prediction intervals vs actual intervals). Although a method can be accurate in predicting the mean values, it could fail in predicting the prediction intervals i.e. the associated uncertainty." width="85%" />
<p class="caption">
(#fig:difference-accuracy-reliability)Mapping accuracy and model reliability (accuracy of the prediction intervals vs actual intervals). Although a method can be accurate in predicting the mean values, it could fail in predicting the prediction intervals i.e. the associated uncertainty.
</p>
</div>
<p>In the case discussed above (Fig. @ref(fig:z-scores-histogram)) it appears that the error estimated by the model is often different from the actual regression-kriging variance: in this case the estimated values are often lower than actual measured values (under-estimation), so that the whole histogram shifts toward 0 value. Because the variance of the <span class="math inline">\(z\)</span>-scores is &lt;1:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(om.rk.cv<span class="op">$</span>zscore, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)
<span class="co">#&gt; [1] 0.95</span></code></pre></div>
<p>we can also say that the regression-kriging variance is slightly over-pessimistic or too conservative about the actual accuracy of the model. On the other hand, Fig. @ref(fig:z-scores-histogram) shows that, at some points, the cross-validation errors are much higher than the error estimated by the model.</p>
</div>
<div id="derivation-and-interpretation-of-prediction-interval" class="section level3">
<h3><span class="header-section-number">5.3.5</span> Derivation and interpretation of prediction interval</h3>
<p>Another important issue for understanding the error budget is derivation of <em>prediction interval</em> i.e. upper and lower values of the target variable for which we assume that our predictions will fall within, with a high probability (e.g. 19 out of 20 times or the 95% probability). Prediction interval or <em>confidence limits</em> are commonly well accepted by users as the easiest way to communicate uncertainty <span class="citation">(Brodlie, Osorio, and Lopes <a href="#ref-brodlie2012review">2012</a>)</span>. For example, organic carbon in Meuse study area (based on 153 samples of organic matter) has a 95% interval of 2–16%:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">signif</span>(<span class="kw">quantile</span>(meuse<span class="op">$</span>om, <span class="kw">c</span>(.<span class="dv">025</span>, <span class="fl">.975</span>), <span class="dt">na.rm=</span><span class="ot">TRUE</span>), <span class="dv">2</span>)
<span class="co">#&gt; 2.5%  98% </span>
<span class="co">#&gt;    2   16</span></code></pre></div>
<p>We have previously fitted a geostatistical model using two covariates, which can now be used to generate predictions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">om.rk &lt;-<span class="st"> </span><span class="kw">predict</span>(omm, meuse.grid)
<span class="co">#&gt; Subsetting observations to fit the prediction domain in 2D...</span>
<span class="co">#&gt; Generating predictions using the trend model (RK method)...</span>
<span class="co">#&gt; [using ordinary kriging]</span>
<span class="co">#&gt; </span>
<span class="dv">100</span>% done
<span class="co">#&gt; Running 5-fold cross validation using &#39;krige.cv&#39;...</span>
<span class="co">#&gt; Creating an object of class &quot;SpatialPredictions&quot;</span></code></pre></div>
<p>and which allows us to estimate the confidence limits for organic matter (assuming normal distribution) at any location within the study area e.g.:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pt1 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span><span class="dv">179390</span>, <span class="dt">y=</span><span class="dv">330820</span>)
<span class="kw">coordinates</span>(pt1) &lt;-<span class="st"> </span><span class="er">~</span>x<span class="op">+</span>y
<span class="kw">proj4string</span>(pt1) =<span class="st"> </span><span class="kw">proj4string</span>(meuse.grid)
pt1.om &lt;-<span class="st"> </span><span class="kw">over</span>(pt1, om.rk<span class="op">@</span>predicted[<span class="st">&quot;om&quot;</span>])
pt1.om.sd &lt;-<span class="st"> </span><span class="kw">over</span>(pt1, om.rk<span class="op">@</span>predicted[<span class="st">&quot;var1.var&quot;</span>])
<span class="kw">signif</span>(<span class="kw">expm1</span>(pt1.om<span class="fl">-1.645</span><span class="op">*</span><span class="kw">sqrt</span>(pt1.om.sd)), <span class="dv">2</span>)
<span class="co">#&gt;    om</span>
<span class="co">#&gt; 1 4.6</span>
<span class="kw">signif</span>(<span class="kw">expm1</span>(pt1.om<span class="fl">+1.645</span><span class="op">*</span><span class="kw">sqrt</span>(pt1.om.sd)), <span class="dv">2</span>)
<span class="co">#&gt;    om</span>
<span class="co">#&gt; 1 8.9</span></code></pre></div>
<p>where 4.6–8.9 are the upper and lower confidence limits. This interval can also be expressed as:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">signif</span>((<span class="kw">expm1</span>(pt1.om<span class="fl">+1.645</span><span class="op">*</span><span class="kw">sqrt</span>(pt1.om.sd)) <span class="op">-</span>
<span class="st">       </span><span class="kw">expm1</span>(pt1.om<span class="fl">-1.645</span><span class="op">*</span><span class="kw">sqrt</span>(pt1.om.sd)))<span class="op">/</span><span class="dv">2</span>, <span class="dv">2</span>)
<span class="co">#&gt;    om</span>
<span class="co">#&gt; 1 2.1</span></code></pre></div>
<p>or 6.3 ± 2.1 where half the error of estimating organic matter at that location is about 1 s.d. Note that these are location specific prediction intervals and need to be computed for each location.</p>
<p>To visualize the range of values within different strata, we can use simulations that we can generate using the geostatistical model (which can be time-consuming to compute!):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">om.rksim &lt;-<span class="st"> </span><span class="kw">predict</span>(omm, meuse.grid, <span class="dt">nsim=</span><span class="dv">5</span>, <span class="dt">debug.level=</span><span class="dv">0</span>)
<span class="co">#&gt; Subsetting observations to fit the prediction domain in 2D...</span>
<span class="co">#&gt; Generating 5 conditional simulations using the trend model (RK method)...</span>
<span class="co">#&gt; Creating an object of class &quot;RasterBrickSimulations&quot;</span>
ov &lt;-<span class="st"> </span><span class="kw">as</span>(om.rksim<span class="op">@</span>realizations, <span class="st">&quot;SpatialGridDataFrame&quot;</span>)
meuse.grid<span class="op">$</span>om.sim1 &lt;-<span class="st"> </span><span class="kw">expm1</span>(ov<span class="op">@</span>data[,<span class="dv">1</span>][meuse.grid<span class="op">@</span>grid.index])
meuse.grid<span class="op">$</span>om.rk &lt;-<span class="st"> </span><span class="kw">expm1</span>(om.rk<span class="op">@</span>predicted<span class="op">$</span>om)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">boxplot</span>(om<span class="op">~</span>ffreq, omm<span class="op">@</span>regModel<span class="op">$</span>data, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>,
    <span class="dt">xlab=</span><span class="st">&quot;Flooding frequency classes&quot;</span>,
    <span class="dt">ylab=</span><span class="st">&quot;Organic matter in %&quot;</span>,
    <span class="dt">main=</span><span class="st">&quot;Sampled (N = 153)&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">20</span>))
<span class="kw">boxplot</span>(om.sim1<span class="op">~</span>ffreq, meuse.grid, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>,
    <span class="dt">xlab=</span><span class="st">&quot;Flooding frequency classes&quot;</span>,
    <span class="dt">ylab=</span><span class="st">&quot;Organic matter in %&quot;</span>,
    <span class="dt">main=</span><span class="st">&quot;Predicted (spatial simulations)&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">20</span>))</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Statistical_theory_files/figure-html/confidence-limits-boxplot-1.png" alt="Prediction intervals for three flooding frequency classes for sampled and predicted soil organic matter. The grey boxes show 1st and 3rd quantiles i.e. range where of data falls." width="90%" />
<p class="caption">
(#fig:confidence-limits-boxplot)Prediction intervals for three flooding frequency classes for sampled and predicted soil organic matter. The grey boxes show 1st and 3rd quantiles i.e. range where of data falls.
</p>
</div>
<p>Fig. @ref(fig:confidence-limits-boxplot) shows that the confidence limits for samples and (<strong><em>WHAT</em></strong>) based on the geostatistical model are about the same width (grey boxes in the plot showing 1st and 3rd quantile), which should be the case because geostatistical simulations are supposed maintain the original variances (see also Fig. @ref(fig:hist-om-predicted-simulated)).</p>
<p>What is also often of interest to soil information users is the error of estimating the mean value i .e. <em>standard error of the mean</em> (<span class="math inline">\({\rm{SE}}_{\bar{x}}\)</span>), which can be derived using samples only <span class="citation">(Kutner et al. <a href="#ref-kutner2005applied">2005</a>)</span>:</p>
<span class="math display">\[\begin{equation}
{\rm{SE}}_{\bar{x}} = \frac{\sigma_x}{\sqrt{n-1}}
(\#eq:mean-pop)
\end{equation}\]</span>
<p>or in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sd.om &lt;-<span class="st"> </span><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="kw">length</span>(meuse<span class="op">$</span>om)<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span>
<span class="st">    </span><span class="kw">sd</span>(meuse<span class="op">$</span>om, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">length</span>(meuse<span class="op">$</span>om))
sd.om
<span class="co">#&gt; [1] 0.54</span></code></pre></div>
<p>Note that this is (only) the error of estimating the population mean, which is much narrower than the actual variation inside the units. This number does not mean that we can estimate organic matter at any location with precision of ±0.54! This number means that, if we would like to estimate (aggregated) mean value for the whole population, then the standard error of that mean would be ±0.54. In other words the population mean for organic matter based on 153 samples is 7.48 ± 0.54, but if we would know the values of organic matter at specific, individual locations, then the confidence limits are about 7.48 ± 3.4 (where 3.4 is the standard error).</p>
<p>The actual variation within the units based on simulations is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lapply</span>(<span class="kw">levels</span>(meuse.grid<span class="op">$</span>ffreq), <span class="cf">function</span>(x){
    <span class="kw">sapply</span>(<span class="kw">subset</span>(meuse.grid<span class="op">@</span>data, ffreq<span class="op">==</span>x,
           <span class="dt">select=</span>om.sim1), sd, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)
})
<span class="co">#&gt; [[1]]</span>
<span class="co">#&gt; om.sim1 </span>
<span class="co">#&gt;       3 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; [[2]]</span>
<span class="co">#&gt; om.sim1 </span>
<span class="co">#&gt;     2.4 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; [[3]]</span>
<span class="co">#&gt; om.sim1 </span>
<span class="co">#&gt;     1.9</span></code></pre></div>
<p>This can be confusing especially if the soil data producer does not clearly report if the confidence limits refer to the population mean, or to individual values. In principle, most users are interested in the confidence limits of measuring some value at an individual location, which are always considerably wider than the confidence limits of estimating the population mean.</p>
<p>Assessment of the confidence limits should be best considered as a regression problem, in fact. It can easily be shown that, by fitting a regression model on strata, we automatically get an estimate of confidence limits for the study area:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">omm0 &lt;-<span class="st"> </span><span class="kw">lm</span>(om<span class="op">~</span>ffreq<span class="dv">-1</span>, omm<span class="op">@</span>regModel<span class="op">$</span>data)
om.r &lt;-<span class="st"> </span><span class="kw">predict</span>(omm0, meuse.grid, <span class="dt">se.fit=</span><span class="ot">TRUE</span>)
meuse.grid<span class="op">$</span>se.fit &lt;-<span class="st"> </span>om.r<span class="op">$</span>se.fit
<span class="kw">signif</span>(<span class="kw">mean</span>(meuse.grid<span class="op">$</span>se.fit, <span class="dt">na.rm=</span><span class="ot">TRUE</span>), <span class="dv">3</span>)
<span class="co">#&gt; [1] 0.48</span></code></pre></div>
<p>This number is similar to 0.54, which we derived directly from the simulations. The difference in the values is because the regression model estimates the prediction intervals for the whole study area based on the covariate data (and not only for the sampling locations). The value is also different than the previously derived 0.54 because we use <code>ffreq</code> stratification as a covariate, so that, as long as the strata is relatively homogenous, the confidence limits get narrower.</p>

<div class="rmdnote">
Prediction intervals (upper and lower ranges of expected values with some high probability) are possibly the most accepted way to communicate uncertainty. Users are commonly interested in what the probability confidence limits are of measuring some value at a specific location, or the high probability prediction range.
</div>

<p>To estimate the actual prediction intervals of estimating individual values (estimation error) we need to add the residual scale value which is a constant number:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">aggregate</span>(<span class="kw">sqrt</span>(meuse.grid<span class="op">$</span>se.fit<span class="op">^</span><span class="dv">2</span><span class="op">+</span>om.r<span class="op">$</span>residual.scale<span class="op">^</span><span class="dv">2</span>),
     <span class="dt">by=</span><span class="kw">list</span>(meuse.grid<span class="op">$</span>ffreq), mean, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)
<span class="co">#&gt;   Group.1   x</span>
<span class="co">#&gt; 1       1 3.3</span>
<span class="co">#&gt; 2       2 3.3</span>
<span class="co">#&gt; 3       3 3.3</span></code></pre></div>
<p>and if we compare these limits to the confidence bands for the values predicted by the geostatistical model fitted above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">aggregate</span>(meuse.grid<span class="op">$</span>om.sim1, <span class="dt">by=</span><span class="kw">list</span>(meuse.grid<span class="op">$</span>ffreq), sd, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)
<span class="co">#&gt;   Group.1   x</span>
<span class="co">#&gt; 1       1 3.0</span>
<span class="co">#&gt; 2       2 2.4</span>
<span class="co">#&gt; 3       3 1.9</span></code></pre></div>
<p>we can clearly see that the geostatistical model has helped us narrow down the confidence limits, especially for class <code>3</code>.</p>
</div>
<div id="universal-measures-of-mapping-accuracy" class="section level3">
<h3><span class="header-section-number">5.3.6</span> Universal measures of mapping accuracy</h3>
<p>In the examples above, we have seen that mapping accuracy can be determined by running cross-validation and determining e.g. <span class="math inline">\(\it{RMSE}\)</span> and R-square. In addition to R–square, a more universal measure of prediction success is the Lin’s Concordance Correlation Coefficient (CCC) <span class="citation">(Steichen and Cox <a href="#ref-steichen2002note">2002</a>)</span>:</p>
<span class="math display">\[\begin{equation}
\rho_c = \frac{2 \cdot \rho \cdot \sigma_{\hat y} \cdot \sigma_y }{ \sigma_{\hat y}^2 + \sigma_y^2 + (\mu_{\hat y} - \mu_y)^2}
(\#eq:CCC)
\end{equation}\]</span>
<p>where <span class="math inline">\(\hat y\)</span> are the predicted values and <span class="math inline">\(y\)</span> are actual values at cross-validation points, <span class="math inline">\(\mu_{\hat y}\)</span> and <span class="math inline">\(\mu_y\)</span> are predicted and observed means and <span class="math inline">\(\rho\)</span> is the correlation coefficient between predicted and observed values. CCC correctly quantifies how far the observed data deviate from the line of perfect concordance (1:1 line in Fig. @ref(fig:validation-scheme)). It is usually equal to or somewhat lower than R–square, depending on the amount of bias in predictions.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_validation_plots.png" alt="Universal plots of predictive performance: (a) 1:1 predicted vs observed plot, (b) CCC vs standard deviation of the z-scores plot, (c) nominal vs coverage probabilities, and (d) variogram of cross-validation residuals. Image source: Hengl et al. (2018) doi: 10.7717/peerj.5518." width="80%" />
<p class="caption">
(#fig:validation-scheme)Universal plots of predictive performance: (a) 1:1 predicted vs observed plot, (b) CCC vs standard deviation of the z-scores plot, (c) nominal vs coverage probabilities, and (d) variogram of cross-validation residuals. Image source: Hengl et al. (2018) doi: 10.7717/peerj.5518.
</p>
</div>
<p>CCC and variance or standard deviation of the z-scores are two universal / scale-free parameters that can be used to assign multiple spatial prediction algorithms to work on multiple soil variables. Two additional measures of the predictive performance of a mapping algoritm are the spatial dependence structure in the cross-validation residuals and so called <em>“accuracy plots”</em> i.e. <span class="citation">(Goovaerts <a href="#ref-goovaerts1999geostatistics">1999</a>)</span> (Fig. @ref(fig:validation-scheme)). Ideally, a variogram of the residuals should show no spatial dependence (i.e. pure nugget effect), which is a proof that there is no spatial bias in predictions. Likewise, nominal vs coverage probabilities in the target variable should also ideally be on a 1:1 line.</p>
<p>So in summary, four universal measures to access predicitive success of any spatial prediction method are <span class="citation">(T. Hengl, Nussbaum, et al. <a href="#ref-Hengl2018RFsp">2018</a>)</span>:</p>
<ul>
<li><strong>Concordance Correlation Coefficient</strong> (0–1): showing predictive success of a method on a 1:1 predictions vs observations plot,</li>
<li><strong>Variance of the z-scores</strong> (0–<span class="math inline">\(\infty\)</span>): showing how reliable the modeled estimate of the prediction errors is,</li>
<li><strong>Variogram of the cross-validation residuals</strong>: showing whether residuals still contain spatial dependence structure,</li>
<li><strong>Accuracy plots</strong>: showing whether the model over- or under-estimates either lower or higher values,</li>
</ul>
</div>
<div id="mapping-accuracy-and-soil-survey-costs" class="section level3">
<h3><span class="header-section-number">5.3.7</span> Mapping accuracy and soil survey costs</h3>
<p>Once the accuracy of some model have been assessed, the next measure of overall mapping success of interest is the soil information production costs. Undoubtedly, producing soil information costs money. <span class="citation">Burrough, Beckett, and Jarvis (<a href="#ref-Burrough1971">1971</a>)</span>, <span class="citation">Bie and Ulph (<a href="#ref-BieUlph1972JAE">1972</a>)</span>, and <span class="citation">Bie, Uph, and Beckett (<a href="#ref-Bie1973JSS">1973</a>)</span> postulated in the early 70s that the survey costs are a direct function of the mapping scale:</p>
<span class="math display">\[\begin{equation}
\log \begin{Bmatrix}
{\rm cost} \; {\rm per} \; {\rm km}^2\\
{\rm or}\\
{\rm man-days} \; {\rm per} \; {\rm km}^2
\end{Bmatrix}
= a + b \cdot \log( {\rm map} \; {\rm scale} )
(\#eq:burrough)
\end{equation}\]</span>
<p>To produce soil information costs money. On the other hand soil information, if used properly, can lead to significant financial benefits: accurate soil information is a tool to improve decision making, increase crop and livestock production and help to reduce investments risk and planning for environmental conservation.</p>
<p>This model typically explains &gt;75% of the survey costs <span class="citation">(Burrough, Beckett, and Jarvis <a href="#ref-Burrough1971">1971</a>)</span>. Further more, for the given target scale, <em>standard soil survey costs</em> can be commonly expressed as:</p>
<span class="math display">\[\begin{equation}
\theta  = \frac{{\rm X}}{A} \qquad [{\rm USD} \; {\rm km}^{-2} ]
(\#eq:mappingcosts)
\end{equation}\]</span>
<p>where <span class="math inline">\({\rm X}\)</span> is the total costs of a survey, <span class="math inline">\(A\)</span> is the size of area in km-square. So for example, according to <span class="citation">Legros (<a href="#ref-Legros2006SP">2006</a>, 75)</span>, to map 1 hectare of soil at 1:200,000 scale (at the beginning of the 21st century), one needs at least 0.48 Euros (i.e. 48 EUR to map a square-km); to map soil at 1:20 would cost about 25 EUR per ha. These are the all-inclusive costs that include salaries and time in the office needed for the work of synthesis and editing.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_scale_costs_ratio.png" alt="Some basic concepts of soil survey: (a) relationship between cartographic scale and pixel size (Hengl, 2006), (b) soil survey costs and scale relationship based on the empirical data of (Legros, 2006)." width="100%" />
<p class="caption">
(#fig:scale-costs-ratio)Some basic concepts of soil survey: (a) relationship between cartographic scale and pixel size (Hengl, 2006), (b) soil survey costs and scale relationship based on the empirical data of (Legros, 2006).
</p>
</div>
<p>Estimated standard soil survey costs per area differ from country to country. The USDA estimates that the total costs of soil mapping at their most detailed scale (1:20) are about 1.50 USD per acre i.e. about 4 USD per ha <span class="citation">(Durana <a href="#ref-eltit2008">2008</a><a href="#ref-eltit2008">a</a>)</span>; in Canada, typical costs of producing soil maps at 1:20 are in the range 3–10 CAD per ha <span class="citation">(MacMillan et al. <a href="#ref-MacMillan2010DSM">2010</a>)</span>; in the Netherlands 3.4 EUR per ha <span class="citation">(Kempen <a href="#ref-Kempen2011PhDthesis">2011</a>, 149–54)</span>; in New Zealand 4 USD per ha <span class="citation">(Carrick, Vesely, and Hewitt <a href="#ref-Carrick2010WCSS">2010</a>)</span>. Based on these national-level numbers, <span class="citation">Hengl, Nikolić, and MacMillan (<a href="#ref-Hengl2013JAG">2013</a>)</span> undertook to produce a global estimate of soil survey costs. So for example, to map 1 hectare of land at 1:20 scale, one would need (at least) 5 USD, and to map soil at 1:200,000 scale globally would cost about 8 USD per square-kilometer using conventional soil mapping methods.</p>
<p>A scale of 1:200,000 corresponds approximately to a ground resolution of 100 m (Fig. @ref(fig:scale-costs-ratio)). If we would like to open a call to map the world’s soils (assuming that total land area to map is about 104 millions of square-km) using contemporary methods at 100 m resolution, and if we would consider 8 USD per square-kilometer as a reasonable cost, then the total costs for mapping the total productive soil areas of the world would be about 872 million USD. Of course, many countries in the world have already been mapped at a scale of 1:200,000 or finer, so this number could be reduced by at least 30%, but even then we would still need a considerable budget. This is just to illustrate that soil mapping can cost an order of magnitude more than, for example, land cover mapping.</p>
<p>Producing soil information costs money, but it also leads to financial benefits. <span class="citation">Pimentel (<a href="#ref-Pimentel2006Springer">2006</a>)</span> for example shows that the costs of soil erosion, measured just by the cost of replacing lost water and nutrients, is on the order of 250 billion USD annually. Soil information, if used properly, can also lead to increased crop and livestock production. <span class="citation">Carrick, Vesely, and Hewitt (<a href="#ref-Carrick2010WCSS">2010</a>)</span>, for example, show that soil survey that costs (only) 3.99 USD per hectare, can lead to better management practices that help retain nitrogen in the soil at a rate of 42.49 USD per kg (17.30 USD per kg for farmers, 25.19 USD per kg for the community). This also demonstrates that soil mapping can be a profitable business.</p>
<p>The formula in Eq.@ref(eq:mappingcosts) is somewhat incomplete as it tells us only about the cost of mapping per unit area. Obviously, mapping efficiency has to be expressed within the context of the mapping objective. Hence, a more informative measure of <em>mapping efficiency</em> is <span class="citation">(Hengl, Nikolić, and MacMillan <a href="#ref-Hengl2013JAG">2013</a>)</span>:</p>
<span class="math display">\[\begin{equation}
\theta  = \frac{{\rm X}}{{A \cdot {\Sigma}_{\%}}} \qquad [{\rm USD} \; {\rm km}^{-2} \; \%^{-1} ]
(\#eq:efficiency)
\end{equation}\]</span>
<p>where <span class="math inline">\({\Sigma}_{\%}\)</span> is the amount of variation explained by the spatial prediction model (Eq.@ref(eq:normvar)). In other words, soil mapping efficiency is the total cost of explaining each percent of variation in target soil variables for a given area of interest.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_costs_RMSE_scheme.png" alt="General relationship between the sampling intensity (i.e. survey costs) and amount of variation in the target variable explained by a spatial prediction model. After Hengl et al. (2013) doi: 10.1016/j.jag.2012.02.005." width="80%" />
<p class="caption">
(#fig:costs-RMSE-scheme)General relationship between the sampling intensity (i.e. survey costs) and amount of variation in the target variable explained by a spatial prediction model. After Hengl et al. (2013) doi: 10.1016/j.jag.2012.02.005.
</p>
</div>
<p>An even more universal measure of mapping efficiency is the Information Production Efficiency (IPE) <span class="citation">(Hengl, Nikolić, and MacMillan <a href="#ref-Hengl2013JAG">2013</a>)</span>:</p>
<span class="math display">\[\begin{equation}
\Upsilon = \frac{{\rm X}}{{\rm gzip}} \qquad [{\rm EUR} \; {\rm B}^{-1}]
(\#eq:data-efficiency)
\end{equation}\]</span>
<p>where <span class="math inline">\({\rm gzip}\)</span> is the size of data (in Bytes) left after compression and after recoding the values to match the effective precision (<span class="math inline">\(\delta \approx {\rm RMSE}/2\)</span>). Information Production Efficiency is scale independent as the area is not included in the equation and hence can be used to compare the efficiency of various different soil mapping projects.</p>

<div class="rmdnote">
Soil mapping efficiency can be expressed as the cost of producing bytes of information about the target soil variables for a given area of interest. This allows for an objective comparison of prediction efficiency for different soil variables for different study areas.
</div>

</div>
<div id="summary-points-2" class="section level3">
<h3><span class="header-section-number">5.3.8</span> Summary points</h3>
<p>Soil mapping processes are increasingly being automated, which is mainly due to advances in software for statistical computing and growing processing speed and computing capacity. Fully automated geostatistical mapping, i.e. generation of spatial predictions with little to no human interaction, is today a growing field of geoinformation science <span class="citation">(Pebesma et al. <a href="#ref-Pebesma2011CompGeoSci">2011</a>; Brown <a href="#ref-Brown2014JSS">2015</a>; Hengl <a href="#ref-Hengl2014SoilGrids1km">2014</a>)</span>. Some key advantages of using automated soil mapping versus more conventional, traditional expert-based soil mapping are <span class="citation">(Heuvelink et al. <a href="#ref-heuvelink2010implications">2010</a>; Bivand, Pebesma, and Rubio <a href="#ref-Bivand2013Springer">2013</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>All rules required to produce outputs are formalized. The whole procedure is documented (the statistical model and associated computer script), enabling reproducible research.</p></li>
<li><p>Predicted surfaces can make use of various information sources and can be optimized relative to all available quantitative point and covariate data.</p></li>
<li><p>There is more flexibility in terms of the spatial extent, resolution and support of requested maps.</p></li>
<li><p>Automated mapping is more cost-effective: once the system is operational, maintenance and production of updates are an order of magnitude faster and cheaper. Consequently, prediction maps can be updated and improved at shorter and shorter time intervals.</p></li>
<li><p>Spatial prediction models typically provide quantitative measures of prediction uncertainty (for each prediction location), which are often not provided in the case of conventional soil mapping.</p></li>
</ol>
<p>A disadvantage of automated soil mapping is that many statistical and machine learning techniques are sensitive to errors and inconsistencies in input data. A few typos, misaligned spatial coordinates or misspecified models can create serious artifacts and reduce prediction accuracy, more so than with traditional methods. Also, fitting models using large and complex data sets can be time consuming and selection of the <em>‘best’</em> model is often problematic. Explicit incorporation of conceptual pedological (expert) knowledge, which can be important for prediction in new situations to address the above issues, can be challenging as well.</p>
<p>In contemporary soil mapping, traditional concepts such as soil map scale and size of delineations are becoming increasingly dated or secondary. The focus of contemporary soil mapping is on minimizing costs required to explain variation in the target variable, while support size of the output maps can be set by the user. The amount of variation explained by a given statistical model gradually increases with sampling intensity, until it reaches some physical limit and does not result in any further improvements. Short-range variability and measurement error, e.g. the portion of the variation that cannot be captured or expressed by the model, for many soil variables can be as great as 10–40% (Fig. @ref(fig:costs-RMSE-scheme)).</p>
<p>A useful thing for soil mapping teams is to compare a list of valid competing models and plot the differences for comparison studies using what we call <em>“predictograms”</em> (as illustrated in Fig. @ref(fig:cost-methods-scheme)). Such comparison studies permit us to determine the best performing, and most cost effective, pedometric method for an area of interest and a list of target variables.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_costs_RMSE_scheme-2.png" alt="An schematic example of a performance plot (‘predictogram’) for comparing spatial prediction models. For more details see: Hengl et al. (2013) doi: 10.1016/j.jag.2012.02.005." width="80%" />
<p class="caption">
(#fig:cost-methods-scheme)An schematic example of a performance plot (‘predictogram’) for comparing spatial prediction models. For more details see: Hengl et al. (2013) doi: 10.1016/j.jag.2012.02.005.
</p>
</div>
<p>In summary, gauging the success of soil mapping basically boils down to the amount of variation explained by the spatial prediction model i.e. quantity of effective bytes produced for the data users. The survey costs are mainly a function of sampling intensity i.e. field work and laboratory data analysis. As we collect more samples for an area of interest we explain more and more of the total variance, until we reach some maximum feasible <em>locked</em> variation (Fig. @ref(fig:cost-methods-scheme)). For a given total budget and a list of target variables an optimal (most efficient) prediction method can be determined by deriving the mapping efficiency described in Eq.@ref(eq:efficiency) or even better Eq.@ref(eq:data-efficiency).</p>

<div class="rmdnote">
Modern soil mapping is driven by the objective assessment of accuracy — emphasis is put on using methods and covariate layers that can produce the most accurate soil information given available resources, and much less on expert opinion or preference.
</div>

<p>By reporting on the RMSE, effective precision, information production efficiency, and by plotting the prediction variance estimated by the model, one gets a fairly good idea about the overall added information value in a given map. In other words, by assessing the accuracy of a map we can both recommend ways to improve the predictions (i.e. collect additional samples), and estimate the resources needed to reach some target accuracy. By assessing how the accuracy of various methods changes for various sampling intensities (Fig. @ref(fig:cost-methods-scheme)), we can distinguish between methods that are more suited for particular regions, data sets or sizes of area and optimum methods that outperform all alternatives.</p>
<!--chapter:end:Statistical_theory.Rmd-->
</div>
</div>
</div>
<div id="soilmapping-using-mla" class="section level1">
<h1><span class="header-section-number">6</span> Machine Learning Algorithms for soil mapping</h1>
<p><em>Edited by: T. Hengl</em></p>
<div id="spatial-prediction-of-soil-properties-and-classes-using-mlas" class="section level2">
<h2><span class="header-section-number">6.1</span> Spatial prediction of soil properties and classes using MLA’s</h2>
<p>This chapter looks at some common Machine learning algorithms (MLA’s) that are potentially of interest for soil mapping projects i.e. for generating spatial predictions. We put special focus on using tree-based algorithms such as <a href="https://en.wikipedia.org/wiki/Random_forest">random forest</a>, <a href="https://en.wikipedia.org/wiki/Gradient_boosting">gradient boosting</a> and <a href="https://cran.r-project.org/package=Cubist">Cubist</a>. For a more in-depth overview of machine learning algorithms used in statistics refer to the CRAN Task View on <a href="https://cran.r-project.org/web/views/MachineLearning.html">Machine Learning &amp; Statistical Learning</a>. Some other examples of how MLA’s can be used to fit Pedo-Transfer-Functions can be found in section @ref(mla-ptfs).</p>
<div id="loading-the-packages-and-data" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Loading the packages and data</h3>
<p>We start by loading all required packages:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(plotKML)
<span class="co">#&gt; plotKML version 0.5-8 (2017-05-12)</span>
<span class="co">#&gt; URL: http://plotkml.r-forge.r-project.org/</span>
<span class="kw">library</span>(sp)
<span class="kw">library</span>(randomForest)
<span class="co">#&gt; randomForest 4.6-14</span>
<span class="co">#&gt; Type rfNews() to see new features/changes/bug fixes.</span>
<span class="kw">library</span>(nnet)
<span class="kw">library</span>(e1071)
<span class="kw">library</span>(GSIF)
<span class="co">#&gt; GSIF version 0.5-4 (2017-04-25)</span>
<span class="co">#&gt; URL: http://gsif.r-forge.r-project.org/</span>
<span class="kw">library</span>(plyr)
<span class="kw">library</span>(raster)
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: &#39;raster&#39;</span>
<span class="co">#&gt; The following object is masked from &#39;package:e1071&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     interpolate</span>
<span class="kw">library</span>(caret)
<span class="co">#&gt; Loading required package: lattice</span>
<span class="co">#&gt; Loading required package: ggplot2</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: &#39;ggplot2&#39;</span>
<span class="co">#&gt; The following object is masked from &#39;package:randomForest&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     margin</span>
<span class="kw">library</span>(Cubist)
<span class="kw">library</span>(GSIF)
<span class="kw">library</span>(xgboost)</code></pre></div>
<p>Next, we load the (<a href="http://plotkml.r-forge.r-project.org/eberg.html">Ebergotzen</a>) data set which consists of point data collected using a soil auger and a stack of rasters containing all covariates:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(plotKML)
<span class="kw">data</span>(eberg)
<span class="kw">data</span>(eberg_grid)
<span class="kw">coordinates</span>(eberg) &lt;-<span class="st"> </span><span class="er">~</span>X<span class="op">+</span>Y
<span class="kw">proj4string</span>(eberg) &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="st">&quot;+init=epsg:31467&quot;</span>)
<span class="kw">gridded</span>(eberg_grid) &lt;-<span class="st"> </span><span class="er">~</span>x<span class="op">+</span>y
<span class="kw">proj4string</span>(eberg_grid) &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="st">&quot;+init=epsg:31467&quot;</span>)</code></pre></div>
<p>The covariates are then converted to principal components to reduce covariance and dimensionality:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">eberg_spc &lt;-<span class="st"> </span><span class="kw">spc</span>(eberg_grid, <span class="op">~</span><span class="st"> </span>PRMGEO6<span class="op">+</span>DEMSRT6<span class="op">+</span>TWISRT6<span class="op">+</span>TIRAST6)
<span class="co">#&gt; Converting PRMGEO6 to indicators...</span>
<span class="co">#&gt; Converting covariates to principal components...</span>
eberg_grid<span class="op">@</span>data &lt;-<span class="st"> </span><span class="kw">cbind</span>(eberg_grid<span class="op">@</span>data, eberg_spc<span class="op">@</span>predicted<span class="op">@</span>data)</code></pre></div>
<p>All further analysis is run using the so-called <em>regression matrix</em> (matrix produced using the overlay of points and grids), which contains values of the target variable and all covariates for all training points:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ov &lt;-<span class="st"> </span><span class="kw">over</span>(eberg, eberg_grid)
m &lt;-<span class="st"> </span><span class="kw">cbind</span>(ov, eberg<span class="op">@</span>data)
<span class="kw">dim</span>(m)
<span class="co">#&gt; [1] 3670   44</span></code></pre></div>
<p>In this case the regression matrix consists of 3670 observations and has 44 columns.</p>
</div>
<div id="spatial-prediction-of-soil-classes-using-mlas" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Spatial prediction of soil classes using MLA’s</h3>
<p>In the first example, we focus on mapping soil types using the auger point data. First, we need to filter out some classes that do not occur frequently enough to support statistical modelling. As a rule of thumb, a class to be modelled should have at least 5 observations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xg &lt;-<span class="st"> </span><span class="kw">summary</span>(m<span class="op">$</span>TAXGRSC, <span class="dt">maxsum=</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">length</span>(<span class="kw">levels</span>(m<span class="op">$</span>TAXGRSC))))
<span class="kw">str</span>(xg)
<span class="co">#&gt;  Named int [1:14] 71 790 86 1 186 1 704 215 252 487 ...</span>
<span class="co">#&gt;  - attr(*, &quot;names&quot;)= chr [1:14] &quot;Auenboden&quot; &quot;Braunerde&quot; &quot;Gley&quot; &quot;HMoor&quot; ...</span>
selg.levs &lt;-<span class="st"> </span><span class="kw">attr</span>(xg, <span class="st">&quot;names&quot;</span>)[xg <span class="op">&gt;</span><span class="st"> </span><span class="dv">5</span>]
<span class="kw">attr</span>(xg, <span class="st">&quot;names&quot;</span>)[xg <span class="op">&lt;=</span><span class="st"> </span><span class="dv">5</span>]
<span class="co">#&gt; [1] &quot;HMoor&quot; &quot;Moor&quot;</span></code></pre></div>
<p>this shows that two classes probably have too few observations and should be excluded from further modeling:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m<span class="op">$</span>soiltype &lt;-<span class="st"> </span>m<span class="op">$</span>TAXGRSC
m<span class="op">$</span>soiltype[<span class="kw">which</span>(<span class="op">!</span>m<span class="op">$</span>TAXGRSC <span class="op">%in%</span><span class="st"> </span>selg.levs)] &lt;-<span class="st"> </span><span class="ot">NA</span>
m<span class="op">$</span>soiltype &lt;-<span class="st"> </span><span class="kw">droplevels</span>(m<span class="op">$</span>soiltype)
<span class="kw">str</span>(<span class="kw">summary</span>(m<span class="op">$</span>soiltype, <span class="dt">maxsum=</span><span class="kw">length</span>(<span class="kw">levels</span>(m<span class="op">$</span>soiltype))))
<span class="co">#&gt;  Named int [1:11] 790 704 487 376 252 215 186 86 71 43 ...</span>
<span class="co">#&gt;  - attr(*, &quot;names&quot;)= chr [1:11] &quot;Braunerde&quot; &quot;Parabraunerde&quot; &quot;Pseudogley&quot; &quot;Regosol&quot; ...</span></code></pre></div>
<p>We can also remove all points that contain missing values for any combination of covariates and target variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span>m[<span class="kw">complete.cases</span>(m[,<span class="dv">1</span><span class="op">:</span>(<span class="kw">ncol</span>(eberg_grid)<span class="op">+</span><span class="dv">2</span>)]),]
m<span class="op">$</span>soiltype &lt;-<span class="st"> </span><span class="kw">as.factor</span>(m<span class="op">$</span>soiltype)
<span class="kw">summary</span>(m<span class="op">$</span>soiltype)
<span class="co">#&gt;     Auenboden     Braunerde          Gley    Kolluvisol Parabraunerde </span>
<span class="co">#&gt;            48           669            68           138           513 </span>
<span class="co">#&gt;  Pararendzina       Pelosol    Pseudogley        Ranker       Regosol </span>
<span class="co">#&gt;           176           177           411            17           313 </span>
<span class="co">#&gt;      Rendzina </span>
<span class="co">#&gt;            22</span></code></pre></div>
<p>We can now test fitting a MLA i.e. a random forest model using four covariate layers (parent material map, elevation, TWI and Aster thermal band):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## subset to speed-up:
s &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="kw">nrow</span>(m), <span class="dv">500</span>)
TAXGRSC.rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(<span class="dt">x=</span>m[<span class="op">-</span>s,<span class="kw">paste0</span>(<span class="st">&quot;PC&quot;</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)], <span class="dt">y=</span>m<span class="op">$</span>soiltype[<span class="op">-</span>s],
                           <span class="dt">xtest=</span>m[s,<span class="kw">paste0</span>(<span class="st">&quot;PC&quot;</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)], <span class="dt">ytest=</span>m<span class="op">$</span>soiltype[s])
## accuracy:
TAXGRSC.rf<span class="op">$</span>test<span class="op">$</span>confusion[,<span class="st">&quot;class.error&quot;</span>]
<span class="co">#&gt;     Auenboden     Braunerde          Gley    Kolluvisol Parabraunerde </span>
<span class="co">#&gt;         0.750         0.479         0.692         0.696         0.560 </span>
<span class="co">#&gt;  Pararendzina       Pelosol    Pseudogley        Ranker       Regosol </span>
<span class="co">#&gt;         0.600         0.717         0.678         1.000         0.643 </span>
<span class="co">#&gt;      Rendzina </span>
<span class="co">#&gt;         0.500</span></code></pre></div>
<p>Note that, by specifying <code>xtest</code> and <code>ytest</code>, we run both model fitting and cross-validation with 500 excluded points. The results show relatively high prediction error of about 60% i.e. relative classification accuracy of about 40%.</p>
<p>We can also test some other MLA’s that are suited for this data — multinom from the <a href="https://cran.r-project.org/package=nnet">nnet</a> package, and svm (Support Vector Machine) from the <a href="https://cran.r-project.org/package=e1071">e1071</a> package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TAXGRSC.rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(<span class="dt">x=</span>m[,<span class="kw">paste0</span>(<span class="st">&quot;PC&quot;</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)], <span class="dt">y=</span>m<span class="op">$</span>soiltype)
fm &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste</span>(<span class="st">&quot;soiltype~&quot;</span>, <span class="kw">paste</span>(<span class="kw">paste0</span>(<span class="st">&quot;PC&quot;</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>), <span class="dt">collapse=</span><span class="st">&quot;+&quot;</span>)))
TAXGRSC.mn &lt;-<span class="st"> </span>nnet<span class="op">::</span><span class="kw">multinom</span>(fm, m)
<span class="co">#&gt; # weights:  132 (110 variable)</span>
<span class="co">#&gt; initial  value 6119.428736 </span>
<span class="co">#&gt; iter  10 value 4161.338634</span>
<span class="co">#&gt; iter  20 value 4118.296050</span>
<span class="co">#&gt; iter  30 value 4054.454486</span>
<span class="co">#&gt; iter  40 value 4020.653949</span>
<span class="co">#&gt; iter  50 value 3995.113270</span>
<span class="co">#&gt; iter  60 value 3980.172669</span>
<span class="co">#&gt; iter  70 value 3975.188371</span>
<span class="co">#&gt; iter  80 value 3973.743572</span>
<span class="co">#&gt; iter  90 value 3973.073564</span>
<span class="co">#&gt; iter 100 value 3973.064186</span>
<span class="co">#&gt; final  value 3973.064186 </span>
<span class="co">#&gt; stopped after 100 iterations</span>
TAXGRSC.svm &lt;-<span class="st"> </span>e1071<span class="op">::</span><span class="kw">svm</span>(fm, m, <span class="dt">probability=</span><span class="ot">TRUE</span>, <span class="dt">cross=</span><span class="dv">5</span>)
TAXGRSC.svm<span class="op">$</span>tot.accuracy
<span class="co">#&gt; [1] 40.2</span></code></pre></div>
<p>This produces about the same accuracy levels as for random forest. Because all three methods produce comparable accuracy, we can also merge predictions by calculating a simple average:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">probs1 &lt;-<span class="st"> </span><span class="kw">predict</span>(TAXGRSC.mn, eberg_grid<span class="op">@</span>data, <span class="dt">type=</span><span class="st">&quot;probs&quot;</span>, <span class="dt">na.action =</span> na.pass) 
probs2 &lt;-<span class="st"> </span><span class="kw">predict</span>(TAXGRSC.rf, eberg_grid<span class="op">@</span>data, <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>, <span class="dt">na.action =</span> na.pass)
probs3 &lt;-<span class="st"> </span><span class="kw">attr</span>(<span class="kw">predict</span>(TAXGRSC.svm, eberg_grid<span class="op">@</span>data, 
                       <span class="dt">probability=</span><span class="ot">TRUE</span>, <span class="dt">na.action =</span> na.pass), <span class="st">&quot;probabilities&quot;</span>)</code></pre></div>
<p>derive average prediction:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">leg &lt;-<span class="st"> </span><span class="kw">levels</span>(m<span class="op">$</span>soiltype)
lt &lt;-<span class="st"> </span><span class="kw">list</span>(probs1[,leg], probs2[,leg], probs3[,leg])
probs &lt;-<span class="st"> </span><span class="kw">Reduce</span>(<span class="st">&quot;+&quot;</span>, lt) <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(lt)
## copy and make new raster object:
eberg_soiltype &lt;-<span class="st"> </span>eberg_grid
eberg_soiltype<span class="op">@</span>data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(probs)</code></pre></div>
<p>Check that all predictions sum up to 100%:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ch &lt;-<span class="st"> </span><span class="kw">rowSums</span>(eberg_soiltype<span class="op">@</span>data)
<span class="kw">summary</span>(ch)
<span class="co">#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span>
<span class="co">#&gt;       1       1       1       1       1       1</span></code></pre></div>
<p>To plot the result we can use the raster package (Fig. @ref(fig:plot-eberg-soiltype)):</p>
<div class="figure" style="text-align: center">
<img src="Soilmapping_using_mla_files/figure-html/plot-eberg-soiltype-1.png" alt="Predicted soil types for the Ebergotzen case study." width="100%" />
<p class="caption">
(#fig:plot-eberg-soiltype)Predicted soil types for the Ebergotzen case study.
</p>
</div>
<p>By using the produced predictions we can further derive Confusion Index (to map thematic uncertainty) and see if some classes should be aggregated. We can also generate a factor-type map by selecting the most probable class for each pixel, by using e.g.:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">eberg_soiltype<span class="op">$</span>cl &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">apply</span>(eberg_soiltype<span class="op">@</span>data,<span class="dv">1</span>,which.max)) 
<span class="kw">levels</span>(eberg_soiltype<span class="op">$</span>cl) =<span class="st"> </span><span class="kw">attr</span>(probs, <span class="st">&quot;dimnames&quot;</span>)[[<span class="dv">2</span>]][<span class="kw">as.integer</span>(<span class="kw">levels</span>(eberg_soiltype<span class="op">$</span>cl))]
<span class="kw">summary</span>(eberg_soiltype<span class="op">$</span>cl)
<span class="co">#&gt;     Auenboden     Braunerde          Gley    Kolluvisol Parabraunerde </span>
<span class="co">#&gt;            36          2314           150            66          2234 </span>
<span class="co">#&gt;  Pararendzina       Pelosol    Pseudogley       Regosol      Rendzina </span>
<span class="co">#&gt;           789           437          1297           321          2356</span></code></pre></div>
</div>
<div id="modelling-numeric-soil-properties-using-h2o" class="section level3">
<h3><span class="header-section-number">6.1.3</span> Modelling numeric soil properties using h2o</h3>
<p>Random forest is suited for both classification and regression problems (it is one of the most popular MLA’s for soil mapping). Consequently, we can use it also for modelling numeric soil properties i.e. to fit models and generate predictions. However, because the randomForest package in R is not suited for large data sets, we can also use some parallelized version of random forest (or more scalable) i.e. the one implemented in the <a href="http://www.h2o.ai/">h2o package</a> <span class="citation">(Richter et al. <a href="#ref-richter2015multi">2015</a>)</span>. h2o is a Java-based implementation, therefore installing the package requires Java libraries (size of package is about 80MB so it might take some to download and install) and all computing is in principle run outside of R i.e. within the JVM (Java Virtual Machine).</p>
<p>In the following example we look at mapping sand content for the upper horizons. To initiate h2o we run:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(h2o)
localH2O =<span class="st"> </span><span class="kw">h2o.init</span>(<span class="dt">startH2O=</span><span class="ot">TRUE</span>)
<span class="co">#&gt; </span>
<span class="co">#&gt; H2O is not running yet, starting it now...</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Note:  In case of errors look at the following log files:</span>
<span class="co">#&gt;     /tmp/RtmpD75UuS/h2o_jn_started_from_r.out</span>
<span class="co">#&gt;     /tmp/RtmpD75UuS/h2o_jn_started_from_r.err</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Starting H2O JVM and connecting: . Connection successful!</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; R is connected to the H2O cluster: </span>
<span class="co">#&gt;     H2O cluster uptime:         2 seconds 884 milliseconds </span>
<span class="co">#&gt;     H2O cluster timezone:       Europe/Warsaw </span>
<span class="co">#&gt;     H2O data parsing timezone:  UTC </span>
<span class="co">#&gt;     H2O cluster version:        3.20.0.8 </span>
<span class="co">#&gt;     H2O cluster version age:    2 months and 26 days  </span>
<span class="co">#&gt;     H2O cluster name:           H2O_started_from_R_jn_qts628 </span>
<span class="co">#&gt;     H2O cluster total nodes:    1 </span>
<span class="co">#&gt;     H2O cluster total memory:   4.32 GB </span>
<span class="co">#&gt;     H2O cluster total cores:    4 </span>
<span class="co">#&gt;     H2O cluster allowed cores:  4 </span>
<span class="co">#&gt;     H2O cluster healthy:        TRUE </span>
<span class="co">#&gt;     H2O Connection ip:          localhost </span>
<span class="co">#&gt;     H2O Connection port:        54321 </span>
<span class="co">#&gt;     H2O Connection proxy:       NA </span>
<span class="co">#&gt;     H2O Internal Security:      FALSE </span>
<span class="co">#&gt;     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 </span>
<span class="co">#&gt;     R Version:                  R version 3.5.1 (2018-07-02)</span></code></pre></div>
<p>This shows that multiple cores will be used for computing (to control the number of cores you can use the <code>nthreads</code> argument). Next, we need to prepare the regression matrix and prediction locations using the <code>as.h2o</code> function so that they are visible to h2o:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">eberg.hex &lt;-<span class="st"> </span><span class="kw">as.h2o</span>(m, <span class="dt">destination_frame =</span> <span class="st">&quot;eberg.hex&quot;</span>)
eberg.grid &lt;-<span class="st"> </span><span class="kw">as.h2o</span>(eberg_grid<span class="op">@</span>data, <span class="dt">destination_frame =</span> <span class="st">&quot;eberg.grid&quot;</span>)</code></pre></div>
<p>We can now fit a random forest model by using all the computing power available to us:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">RF.m &lt;-<span class="st"> </span><span class="kw">h2o.randomForest</span>(<span class="dt">y =</span> <span class="kw">which</span>(<span class="kw">names</span>(m)<span class="op">==</span><span class="st">&quot;SNDMHT_A&quot;</span>), 
                        <span class="dt">x =</span> <span class="kw">which</span>(<span class="kw">names</span>(m) <span class="op">%in%</span><span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;PC&quot;</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)), 
                        <span class="dt">training_frame =</span> eberg.hex, <span class="dt">ntree =</span> <span class="dv">50</span>)
RF.m
<span class="co">#&gt; Model Details:</span>
<span class="co">#&gt; ==============</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; H2ORegressionModel: drf</span>
<span class="co">#&gt; Model ID:  DRF_model_R_1545142934283_1 </span>
<span class="co">#&gt; Model Summary: </span>
<span class="co">#&gt;   number_of_trees number_of_internal_trees model_size_in_bytes min_depth</span>
<span class="co">#&gt; 1              50                       50              643941        20</span>
<span class="co">#&gt;   max_depth mean_depth min_leaves max_leaves mean_leaves</span>
<span class="co">#&gt; 1        20   20.00000        951       1066  1021.38000</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; H2ORegressionMetrics: drf</span>
<span class="co">#&gt; ** Reported on training data. **</span>
<span class="co">#&gt; ** Metrics reported on Out-Of-Bag training samples **</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; MSE:  221</span>
<span class="co">#&gt; RMSE:  14.9</span>
<span class="co">#&gt; MAE:  10.1</span>
<span class="co">#&gt; RMSLE:  0.431</span>
<span class="co">#&gt; Mean Residual Deviance :  221</span></code></pre></div>
<p>This shows that the model fitting R-square is about 50%. This is also indicated by the predicted vs observed plot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(scales)
<span class="kw">library</span>(lattice)
SDN.pred &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">h2o.predict</span>(RF.m, eberg.hex, <span class="dt">na.action=</span>na.pass))<span class="op">$</span>predict
plt1 &lt;-<span class="st"> </span><span class="kw">xyplot</span>(m<span class="op">$</span>SNDMHT_A <span class="op">~</span><span class="st"> </span>SDN.pred, <span class="dt">asp=</span><span class="dv">1</span>, 
               <span class="dt">par.settings=</span><span class="kw">list</span>(
                 <span class="dt">plot.symbol =</span> <span class="kw">list</span>(<span class="dt">col=</span><span class="kw">alpha</span>(<span class="st">&quot;black&quot;</span>, <span class="fl">0.6</span>), 
                 <span class="dt">fill=</span><span class="kw">alpha</span>(<span class="st">&quot;red&quot;</span>, <span class="fl">0.6</span>), <span class="dt">pch=</span><span class="dv">21</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)),
                 <span class="dt">ylab=</span><span class="st">&quot;measured&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;predicted (machine learning)&quot;</span>)
plt1</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Soilmapping_using_mla_files/figure-html/obs-pred-snd-1.png" alt="Measured vs predicted SAND content based on the Random Forest model." width="100%" />
<p class="caption">
(#fig:obs-pred-snd)Measured vs predicted SAND content based on the Random Forest model.
</p>
</div>
<p>To produce a map based on these predictions we use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">eberg_grid<span class="op">$</span>RFx &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">h2o.predict</span>(RF.m, eberg.grid, <span class="dt">na.action=</span>na.pass))<span class="op">$</span>predict</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Soilmapping_using_mla_files/figure-html/map-snd-1.png" alt="Predicted sand content based on random forest." width="100%" />
<p class="caption">
(#fig:map-snd)Predicted sand content based on random forest.
</p>
</div>
<p>h2o has another MLA of interest for soil mapping called <em>deep learning</em> (a feed-forward multilayer artificial neural network). Fitting the model is equivalent to using random forest:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">DL.m &lt;-<span class="st"> </span><span class="kw">h2o.deeplearning</span>(<span class="dt">y =</span> <span class="kw">which</span>(<span class="kw">names</span>(m)<span class="op">==</span><span class="st">&quot;SNDMHT_A&quot;</span>), 
                         <span class="dt">x =</span> <span class="kw">which</span>(<span class="kw">names</span>(m) <span class="op">%in%</span><span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;PC&quot;</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)), 
                         <span class="dt">training_frame =</span> eberg.hex)
DL.m
<span class="co">#&gt; Model Details:</span>
<span class="co">#&gt; ==============</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; H2ORegressionModel: deeplearning</span>
<span class="co">#&gt; Model ID:  DeepLearning_model_R_1545142934283_2 </span>
<span class="co">#&gt; Status of Neuron Layers: predicting SNDMHT_A, regression, gaussian distribution, Quadratic loss, 42,601 weights/biases, 508.3 KB, 25,520 training samples, mini-batch size 1</span>
<span class="co">#&gt;   layer units      type dropout       l1       l2 mean_rate rate_rms</span>
<span class="co">#&gt; 1     1    10     Input  0.00 %       NA       NA        NA       NA</span>
<span class="co">#&gt; 2     2   200 Rectifier  0.00 % 0.000000 0.000000  0.013969 0.008483</span>
<span class="co">#&gt; 3     3   200 Rectifier  0.00 % 0.000000 0.000000  0.150391 0.175266</span>
<span class="co">#&gt; 4     4     1    Linear      NA 0.000000 0.000000  0.001334 0.000842</span>
<span class="co">#&gt;   momentum mean_weight weight_rms mean_bias bias_rms</span>
<span class="co">#&gt; 1       NA          NA         NA        NA       NA</span>
<span class="co">#&gt; 2 0.000000    0.004037   0.107953  0.358530 0.054244</span>
<span class="co">#&gt; 3 0.000000   -0.018108   0.070959  0.954204 0.018889</span>
<span class="co">#&gt; 4 0.000000    0.002435   0.047210  0.102178 0.000000</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; H2ORegressionMetrics: deeplearning</span>
<span class="co">#&gt; ** Reported on training data. **</span>
<span class="co">#&gt; ** Metrics reported on full training frame **</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; MSE:  237</span>
<span class="co">#&gt; RMSE:  15.4</span>
<span class="co">#&gt; MAE:  11.2</span>
<span class="co">#&gt; RMSLE:  0.439</span>
<span class="co">#&gt; Mean Residual Deviance :  237</span></code></pre></div>
<p>Which delivers performance comparable to the random forest model. The output prediction map does show somewhat different patterns than the random forest predictions (compare Fig. @ref(fig:map-snd) and Fig. @ref(fig:map-snd-dl)).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## predictions:
eberg_grid<span class="op">$</span>DLx &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">h2o.predict</span>(DL.m, eberg.grid, <span class="dt">na.action=</span>na.pass))<span class="op">$</span>predict</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Soilmapping_using_mla_files/figure-html/map-snd-dl-1.png" alt="Predicted SAND content based on deep learning." width="100%" />
<p class="caption">
(#fig:map-snd-dl)Predicted SAND content based on deep learning.
</p>
</div>
<p>Which of the two methods should we use? Since they both have comparable performance, the most logical option is to generate ensemble (merged) predictions i.e. to produce a map that shows patterns averaged between the two methods (note: many sophisticated MLA such as random forest, neural nets, SVM and similar will often produce comparable results i.e. they are often equally applicable and there is no clear <em>winner</em>). We can use weighted average i.e. R-square as a simple approach to produce merged predictions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rf.R2 &lt;-<span class="st"> </span>RF.m<span class="op">@</span>model<span class="op">$</span>training_metrics<span class="op">@</span>metrics<span class="op">$</span>r2
dl.R2 &lt;-<span class="st"> </span>DL.m<span class="op">@</span>model<span class="op">$</span>training_metrics<span class="op">@</span>metrics<span class="op">$</span>r2
eberg_grid<span class="op">$</span>SNDMHT_A &lt;-<span class="st"> </span><span class="kw">rowSums</span>(<span class="kw">cbind</span>(eberg_grid<span class="op">$</span>RFx<span class="op">*</span>rf.R2, 
                         eberg_grid<span class="op">$</span>DLx<span class="op">*</span>dl.R2), <span class="dt">na.rm=</span><span class="ot">TRUE</span>)<span class="op">/</span>(rf.R2<span class="op">+</span>dl.R2)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Soilmapping_using_mla_files/figure-html/map-snd-ensemble-1.png" alt="Predicted SAND content based on ensemble predictions." width="100%" />
<p class="caption">
(#fig:map-snd-ensemble)Predicted SAND content based on ensemble predictions.
</p>
</div>
<p>Indeed, the output map now shows patterns of both methods and is more likely slightly more accurate than any of the individual MLA’s <span class="citation">(Sollich and Krogh <a href="#ref-krogh1996learning">1996</a>)</span>.</p>
</div>
<div id="prediction-3D" class="section level3">
<h3><span class="header-section-number">6.1.4</span> Spatial prediction of 3D (numeric) variables</h3>
<p>In the final exercise, we look at another two ML-based packages that are also of interest for soil mapping projects — cubist <span class="citation">(Kuhn et al. <a href="#ref-kuhn2012cubist">2012</a>; Kuhn and Johnson <a href="#ref-kuhn2013applied">2013</a>)</span> and xgboost <span class="citation">(Chen and Guestrin <a href="#ref-2016arXiv160302754C">2016</a>)</span>. The object is now to fit models and predict continuous soil properties in 3D. To fine-tune some of the models we will also use the <a href="http://topepo.github.io/caret/">caret</a> package, which is highly recommended for optimizing model fitting and cross-validation. Read more about how to derive soil organic carbon stock using 3D soil mapping in section @ref(ocs-3d-approach).</p>
<p>We now look at another soil mapping data set from Australia called <a href="http://gsif.r-forge.r-project.org/edgeroi.html">“Edgeroi”</a>, which is described in detail in <span class="citation">Malone et al. (<a href="#ref-Malone2009Geoderma">2009</a>)</span>. We can load the profile data and covariates by using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(edgeroi)
edgeroi.sp &lt;-<span class="st"> </span>edgeroi<span class="op">$</span>sites
<span class="kw">coordinates</span>(edgeroi.sp) &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>LONGDA94 <span class="op">+</span><span class="st"> </span>LATGDA94
<span class="kw">proj4string</span>(edgeroi.sp) &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="st">&quot;+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs&quot;</span>)
edgeroi.sp &lt;-<span class="st"> </span><span class="kw">spTransform</span>(edgeroi.sp, <span class="kw">CRS</span>(<span class="st">&quot;+init=epsg:28355&quot;</span>))
<span class="kw">load</span>(<span class="st">&quot;extdata/edgeroi.grids.rda&quot;</span>)
<span class="kw">gridded</span>(edgeroi.grids) &lt;-<span class="st"> </span><span class="er">~</span>x<span class="op">+</span>y
<span class="kw">proj4string</span>(edgeroi.grids) &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="st">&quot;+init=epsg:28355&quot;</span>)</code></pre></div>
<p>Here we are interested in modelling soil organic carbon content in g/kg for different depths. We again start by producing the regression matrix:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ov2 &lt;-<span class="st"> </span><span class="kw">over</span>(edgeroi.sp, edgeroi.grids)
ov2<span class="op">$</span>SOURCEID &lt;-<span class="st"> </span>edgeroi.sp<span class="op">$</span>SOURCEID
<span class="kw">str</span>(ov2)
<span class="co">#&gt; &#39;data.frame&#39;:    359 obs. of  7 variables:</span>
<span class="co">#&gt;  $ DEMSRT5 : num  208 199 203 202 195 201 198 210 190 195 ...</span>
<span class="co">#&gt;  $ TWISRT5 : num  19.8 19.9 19.7 19.3 19.3 19.7 19.5 19.6 19.6 19.2 ...</span>
<span class="co">#&gt;  $ PMTGEO5 : Factor w/ 7 levels &quot;Qd&quot;,&quot;Qrs&quot;,&quot;Qrt/Jp&quot;,..: 2 2 2 2 2 2 2 2 2 2 ...</span>
<span class="co">#&gt;  $ EV1MOD5 : num  -0.08 2.41 2.62 -0.39 -0.78 -0.75 1.14 5.16 -0.48 -0.84 ...</span>
<span class="co">#&gt;  $ EV2MOD5 : num  -2.47 -2.84 -2.43 5.2 1.27 -4.96 1.62 1.33 -2.66 1.01 ...</span>
<span class="co">#&gt;  $ EV3MOD5 : num  -1.59 -0.31 1.43 1.96 -0.44 2.47 -5.74 -6.78 2.29 -1.59 ...</span>
<span class="co">#&gt;  $ SOURCEID: Factor w/ 359 levels &quot;199_CAN_CP111_1&quot;,..: 1 2 3 4 5 6 7 8 9 10 ...</span></code></pre></div>
<p>Because we will run 3D modelling, we also need to add depth of horizons. We use a small function to assign depth values as the center depth of each horizon (as shown in figure below). Because we know where the horizons start and stop, we can copy the values of target variables two times so that the model knows at which depth values of properties change.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Convert soil horizon data to x,y,d regression matrix for 3D modeling:
hor2xyd &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">U=</span><span class="st">&quot;UHDICM&quot;</span>, <span class="dt">L=</span><span class="st">&quot;LHDICM&quot;</span>, <span class="dt">treshold.T=</span><span class="dv">15</span>){
  x<span class="op">$</span>DEPTH &lt;-<span class="st"> </span>x[,U] <span class="op">+</span><span class="st"> </span>(x[,L] <span class="op">-</span><span class="st"> </span>x[,U])<span class="op">/</span><span class="dv">2</span>
  x<span class="op">$</span>THICK &lt;-<span class="st"> </span>x[,L] <span class="op">-</span><span class="st"> </span>x[,U]
  sel &lt;-<span class="st"> </span>x<span class="op">$</span>THICK <span class="op">&lt;</span><span class="st"> </span>treshold.T
  ## begin and end of the horizon:
  x1 &lt;-<span class="st"> </span>x[<span class="op">!</span>sel,]; x1<span class="op">$</span>DEPTH =<span class="st"> </span>x1[,L]
  x2 &lt;-<span class="st"> </span>x[<span class="op">!</span>sel,]; x2<span class="op">$</span>DEPTH =<span class="st"> </span>x1[,U]
  y &lt;-<span class="st"> </span><span class="kw">do.call</span>(rbind, <span class="kw">list</span>(x, x1, x2))
  <span class="kw">return</span>(y)
}</code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/horizon_depths_for_3d_modeling_scheme.png" alt="Training points assigned to a soil profile with 3 horizons. Using the function from above, we assign a total of 7 training points i.e. about 2 times more training points than there are horizons." width="75%" />
<p class="caption">
(#fig:hor-3d-scheme)Training points assigned to a soil profile with 3 horizons. Using the function from above, we assign a total of 7 training points i.e. about 2 times more training points than there are horizons.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">h2 &lt;-<span class="st"> </span><span class="kw">hor2xyd</span>(edgeroi<span class="op">$</span>horizons)
## regression matrix:
m2 &lt;-<span class="st"> </span>plyr<span class="op">::</span><span class="kw">join_all</span>(<span class="dt">dfs =</span> <span class="kw">list</span>(edgeroi<span class="op">$</span>sites, h2, ov2))
<span class="co">#&gt; Joining by: SOURCEID</span>
<span class="co">#&gt; Joining by: SOURCEID</span>
## spatial prediction model:
formulaStringP2 &lt;-<span class="st"> </span>ORCDRC <span class="op">~</span><span class="st"> </span>DEMSRT5<span class="op">+</span>TWISRT5<span class="op">+</span>PMTGEO5<span class="op">+</span>
<span class="st">                            </span>EV1MOD5<span class="op">+</span>EV2MOD5<span class="op">+</span>EV3MOD5<span class="op">+</span>DEPTH
mP2 &lt;-<span class="st"> </span>m2[<span class="kw">complete.cases</span>(m2[,<span class="kw">all.vars</span>(formulaStringP2)]),]</code></pre></div>
<p>Note that <code>DEPTH</code> is used as a covariate, which makes this model 3D as one can predict anywhere in 3D space. To improve random forest modelling, we use the caret package that tries to identify also the optimal <code>mtry</code> parameter i.e. based on the cross-validation performance:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;repeatedcv&quot;</span>, <span class="dt">number=</span><span class="dv">5</span>, <span class="dt">repeats=</span><span class="dv">1</span>)
sel &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="kw">nrow</span>(mP2), <span class="dv">500</span>)
tr.ORCDRC.rf &lt;-<span class="st"> </span><span class="kw">train</span>(formulaStringP2, <span class="dt">data=</span>mP2[sel,], 
                      <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>, <span class="dt">trControl =</span> ctrl, <span class="dt">tuneLength =</span> <span class="dv">3</span>)
tr.ORCDRC.rf
<span class="co">#&gt; Random Forest </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; 500 samples</span>
<span class="co">#&gt;   7 predictor</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; No pre-processing</span>
<span class="co">#&gt; Resampling: Cross-Validated (5 fold, repeated 1 times) </span>
<span class="co">#&gt; Summary of sample sizes: 401, 399, 399, 400, 401 </span>
<span class="co">#&gt; Resampling results across tuning parameters:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;   mtry  RMSE  Rsquared  MAE </span>
<span class="co">#&gt;    2    4.18  0.490     2.78</span>
<span class="co">#&gt;    7    3.87  0.519     2.40</span>
<span class="co">#&gt;   12    3.95  0.510     2.44</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; RMSE was used to select the optimal model using the smallest value.</span>
<span class="co">#&gt; The final value used for the model was mtry = 7.</span></code></pre></div>
<p>In this case, <code>mtry = 12</code> seems to achieve the best performance. Note that we sub-set the initial matrix to speed up fine-tuning of the parameters (otherwise the computing time could easily become too great). Next, we can fit the final model by using all data (this time we also turn cross-validation off):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ORCDRC.rf &lt;-<span class="st"> </span><span class="kw">train</span>(formulaStringP2, <span class="dt">data=</span>mP2, 
                   <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>, <span class="dt">tuneGrid=</span><span class="kw">data.frame</span>(<span class="dt">mtry=</span><span class="dv">7</span>),
                   <span class="dt">trControl=</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;none&quot;</span>))
w1 &lt;-<span class="st"> </span><span class="dv">100</span><span class="op">*</span><span class="kw">max</span>(tr.ORCDRC.rf<span class="op">$</span>results<span class="op">$</span>Rsquared)</code></pre></div>
<p>The variable importance plot indicates that DEPTH is by far the most important predictor:</p>
<div class="figure" style="text-align: center">
<img src="Soilmapping_using_mla_files/figure-html/varimp-plot-edgeroi-1.png" alt="Variable importance plot for predicting soil organic carbon content (ORC) in 3D." width="70%" />
<p class="caption">
(#fig:varimp-plot-edgeroi)Variable importance plot for predicting soil organic carbon content (ORC) in 3D.
</p>
</div>
<p>We can also try fitting models using the xgboost package and the cubist packages:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tr.ORCDRC.cb &lt;-<span class="st"> </span><span class="kw">train</span>(formulaStringP2, <span class="dt">data=</span>mP2[sel,], 
                      <span class="dt">method =</span> <span class="st">&quot;cubist&quot;</span>, <span class="dt">trControl =</span> ctrl, <span class="dt">tuneLength =</span> <span class="dv">3</span>)
ORCDRC.cb &lt;-<span class="st"> </span><span class="kw">train</span>(formulaStringP2, <span class="dt">data=</span>mP2, 
                   <span class="dt">method =</span> <span class="st">&quot;cubist&quot;</span>, 
                   <span class="dt">tuneGrid=</span><span class="kw">data.frame</span>(<span class="dt">committees =</span> <span class="dv">1</span>, <span class="dt">neighbors =</span> <span class="dv">0</span>),
                   <span class="dt">trControl=</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;none&quot;</span>))
w2 &lt;-<span class="st"> </span><span class="dv">100</span><span class="op">*</span><span class="kw">max</span>(tr.ORCDRC.cb<span class="op">$</span>results<span class="op">$</span>Rsquared)
## &quot;XGBoost&quot; package:
ORCDRC.gb &lt;-<span class="st"> </span><span class="kw">train</span>(formulaStringP2, <span class="dt">data=</span>mP2, <span class="dt">method =</span> <span class="st">&quot;xgbTree&quot;</span>, <span class="dt">trControl=</span>ctrl)
w3 &lt;-<span class="st"> </span><span class="dv">100</span><span class="op">*</span><span class="kw">max</span>(ORCDRC.gb<span class="op">$</span>results<span class="op">$</span>Rsquared)
<span class="kw">c</span>(w1, w2, w3)
<span class="co">#&gt; [1] 51.9 54.5 67.3</span></code></pre></div>
<p>At the end of the statistical modelling process, we can merge the predictions by using the CV R-square estimates:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">edgeroi.grids<span class="op">$</span>DEPTH =<span class="st"> </span><span class="fl">2.5</span>
edgeroi.grids<span class="op">$</span>Random_forest &lt;-<span class="st"> </span><span class="kw">predict</span>(ORCDRC.rf, edgeroi.grids<span class="op">@</span>data, <span class="dt">na.action =</span> na.pass) 
edgeroi.grids<span class="op">$</span>Cubist &lt;-<span class="st"> </span><span class="kw">predict</span>(ORCDRC.cb, edgeroi.grids<span class="op">@</span>data, <span class="dt">na.action =</span> na.pass)
edgeroi.grids<span class="op">$</span>XGBoost &lt;-<span class="st"> </span><span class="kw">predict</span>(ORCDRC.gb, edgeroi.grids<span class="op">@</span>data, <span class="dt">na.action =</span> na.pass)
edgeroi.grids<span class="op">$</span>ORCDRC_5cm &lt;-<span class="st"> </span>(edgeroi.grids<span class="op">$</span>Random_forest<span class="op">*</span>w1 <span class="op">+</span><span class="st"> </span>
<span class="st">                               </span>edgeroi.grids<span class="op">$</span>Cubist<span class="op">*</span>w2 <span class="op">+</span><span class="st"> </span>
<span class="st">                               </span>edgeroi.grids<span class="op">$</span>XGBoost<span class="op">*</span>w3)<span class="op">/</span>(w1<span class="op">+</span>w2<span class="op">+</span>w3)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Soilmapping_using_mla_files/figure-html/maps-soc-edgeroi-1.png" alt="Comparison of three MLA's and final ensemble prediction (ORCDRC 5cm) of soil organic carbon content for 2.5 cm depth." width="100%" />
<p class="caption">
(#fig:maps-soc-edgeroi)Comparison of three MLA’s and final ensemble prediction (ORCDRC 5cm) of soil organic carbon content for 2.5 cm depth.
</p>
</div>
<p>The final plot shows that xgboost possibly over-predicts and that cubist possibly under-predicts values of <code>ORCDRC</code>, while random forest is somewhere in-between the two. Again, merged predictions are probably the safest option considering that all three MLA’s have similar measures of performance.</p>
<p>We can quickly test the overall performance using a script on github prepared for testing performance of merged predictions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">source_https &lt;-<span class="st"> </span><span class="cf">function</span>(url, ...) {
  <span class="kw">require</span>(RCurl)
  <span class="cf">if</span>(<span class="op">!</span><span class="kw">file.exists</span>(<span class="kw">paste0</span>(<span class="st">&quot;R/&quot;</span>, <span class="kw">basename</span>(url)))){
    <span class="kw">cat</span>(<span class="kw">getURL</span>(url, <span class="dt">followlocation =</span> <span class="ot">TRUE</span>,
               <span class="dt">cainfo =</span> <span class="kw">system.file</span>(<span class="st">&quot;CurlSSL&quot;</span>, <span class="st">&quot;cacert.pem&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;RCurl&quot;</span>)), 
        <span class="dt">file =</span> <span class="kw">paste0</span>(<span class="st">&quot;R/&quot;</span>, <span class="kw">basename</span>(url)))
  }
  <span class="kw">source</span>(<span class="kw">paste0</span>(<span class="st">&quot;R/&quot;</span>, <span class="kw">basename</span>(url)))
}
wdir =<span class="st"> &quot;https://raw.githubusercontent.com/ISRICWorldSoil/SoilGrids250m/&quot;</span>
<span class="kw">source_https</span>(<span class="kw">paste0</span>(wdir, <span class="st">&quot;master/grids/cv/cv_functions.R&quot;</span>))
<span class="co">#&gt; Loading required package: RCurl</span>
<span class="co">#&gt; Loading required package: bitops</span></code></pre></div>
<p>We can hence run 5-fold cross validation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mP2<span class="op">$</span>SOURCEID =<span class="st"> </span><span class="kw">paste</span>(mP2<span class="op">$</span>SOURCEID)
test.ORC &lt;-<span class="st"> </span><span class="kw">cv_numeric</span>(formulaStringP2, <span class="dt">rmatrix=</span>mP2, 
                       <span class="dt">nfold=</span><span class="dv">5</span>, <span class="dt">idcol=</span><span class="st">&quot;SOURCEID&quot;</span>, <span class="dt">Log=</span><span class="ot">TRUE</span>)
<span class="co">#&gt; Running 5-fold cross validation with model re-fitting method ranger ...</span>
<span class="co">#&gt; Subsetting observations by unique location</span>
<span class="co">#&gt; Loading required package: snowfall</span>
<span class="co">#&gt; Loading required package: snow</span>
<span class="co">#&gt; Warning in searchCommandline(parallel, cpus = cpus, type = type,</span>
<span class="co">#&gt; socketHosts = socketHosts, : Unknown option on commandline: --file</span>
<span class="co">#&gt; R Version:  R version 3.5.1 (2018-07-02)</span>
<span class="co">#&gt; snowfall 1.84-6.1 initialized (using snow 0.4-3): parallel execution on 4 CPUs.</span>
<span class="co">#&gt; Library plyr loaded.</span>
<span class="co">#&gt; Library plyr loaded in cluster.</span>
<span class="co">#&gt; Library ranger loaded.</span>
<span class="co">#&gt; Library ranger loaded in cluster.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: &#39;ranger&#39;</span>
<span class="co">#&gt; The following object is masked from &#39;package:randomForest&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     importance</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Stopping cluster</span>
<span class="kw">str</span>(test.ORC)
<span class="co">#&gt; List of 2</span>
<span class="co">#&gt;  $ CV_residuals:&#39;data.frame&#39;:    4972 obs. of  4 variables:</span>
<span class="co">#&gt;   ..$ Observed : num [1:4972] 6.5 5.1 4.9 3.3 2.2 ...</span>
<span class="co">#&gt;   ..$ Predicted: num [1:4972] 11.66 7.07 6.38 4.87 2.96 ...</span>
<span class="co">#&gt;   ..$ SOURCEID : chr [1:4972] &quot;399_EDGEROI_ed005_1&quot; &quot;399_EDGEROI_ed005_1&quot; &quot;399_EDGEROI_ed005_1&quot; &quot;399_EDGEROI_ed005_1&quot; ...</span>
<span class="co">#&gt;   ..$ fold     : int [1:4972] 1 1 1 1 1 1 1 1 1 1 ...</span>
<span class="co">#&gt;  $ Summary     :&#39;data.frame&#39;:    1 obs. of  6 variables:</span>
<span class="co">#&gt;   ..$ ME          : num -0.0784</span>
<span class="co">#&gt;   ..$ MAE         : num 2.12</span>
<span class="co">#&gt;   ..$ RMSE        : num 3.64</span>
<span class="co">#&gt;   ..$ R.squared   : num 0.566</span>
<span class="co">#&gt;   ..$ logRMSE     : num 0.483</span>
<span class="co">#&gt;   ..$ logR.squared: num 0.649</span></code></pre></div>
<p>Which shows that the R-squared based on cross-validation is about 65% i.e. the average error of predicting soil organic carbon content using ensemble method is about <span class="math inline">\(\pm 4\)</span> g/kg. The final observed-vs-predict plot shows that the model is unbiased and that the predictions generally match cross-validation points:</p>
<div class="figure" style="text-align: center">
<img src="Soilmapping_using_mla_files/figure-html/plot-measured-predicted-1.png" alt="Predicted vs observed plot for soil organic carbon ML-based model (Edgeroi data set)." width="100%" />
<p class="caption">
(#fig:plot-measured-predicted)Predicted vs observed plot for soil organic carbon ML-based model (Edgeroi data set).
</p>
</div>
</div>
<div id="ensemble-predictions-using-h2oensemble" class="section level3">
<h3><span class="header-section-number">6.1.5</span> Ensemble predictions using h2oEnsemble</h3>
<p>Ensemble models often outperform single models. There is certainly opportunity for increasing mapping accuracy by combining the power of 3–4 MLA’s. The h2o environment for ML offers automation of ensemble model fitting and predictions <span class="citation">(LeDell <a href="#ref-ledell2015scalable">2015</a>)</span>.</p>
<pre><code>#&gt; h2oEnsemble R package for H2O-3
#&gt; Version: 0.2.1
#&gt; Package created on 2017-08-02
#&gt;  Connection successful!
#&gt; 
#&gt; R is connected to the H2O cluster: 
#&gt;     H2O cluster uptime:         1 minutes 48 seconds 
#&gt;     H2O cluster timezone:       Europe/Warsaw 
#&gt;     H2O data parsing timezone:  UTC 
#&gt;     H2O cluster version:        3.20.0.8 
#&gt;     H2O cluster version age:    2 months and 26 days  
#&gt;     H2O cluster name:           H2O_started_from_R_jn_qts628 
#&gt;     H2O cluster total nodes:    1 
#&gt;     H2O cluster total memory:   4.31 GB 
#&gt;     H2O cluster total cores:    4 
#&gt;     H2O cluster allowed cores:  4 
#&gt;     H2O cluster healthy:        TRUE 
#&gt;     H2O Connection ip:          localhost 
#&gt;     H2O Connection port:        54321 
#&gt;     H2O Connection proxy:       NA 
#&gt;     H2O Internal Security:      FALSE 
#&gt;     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
#&gt;     R Version:                  R version 3.5.1 (2018-07-02)</code></pre>
<p>we first specify all learners (MLA methods) of interest:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">k.f =<span class="st"> </span>dismo<span class="op">::</span><span class="kw">kfold</span>(mP2, <span class="dt">k=</span><span class="dv">4</span>)
<span class="kw">summary</span>(<span class="kw">as.factor</span>(k.f))
<span class="co">#&gt;    1    2    3    4 </span>
<span class="co">#&gt; 1243 1243 1243 1243</span>
## split data into training and validation:
edgeroi_v.hex =<span class="st"> </span><span class="kw">as.h2o</span>(mP2[k.f<span class="op">==</span><span class="dv">1</span>,], <span class="dt">destination_frame =</span> <span class="st">&quot;eberg_v.hex&quot;</span>)
edgeroi_t.hex =<span class="st"> </span><span class="kw">as.h2o</span>(mP2[<span class="op">!</span>k.f<span class="op">==</span><span class="dv">1</span>,], <span class="dt">destination_frame =</span> <span class="st">&quot;eberg_t.hex&quot;</span>)
learner &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;h2o.randomForest.wrapper&quot;</span>, <span class="st">&quot;h2o.gbm.wrapper&quot;</span>)
fit &lt;-<span class="st"> </span><span class="kw">h2o.ensemble</span>(<span class="dt">x =</span> <span class="kw">which</span>(<span class="kw">names</span>(m2) <span class="op">%in%</span><span class="st"> </span><span class="kw">all.vars</span>(formulaStringP2)[<span class="op">-</span><span class="dv">1</span>]), 
                    <span class="dt">y =</span> <span class="kw">which</span>(<span class="kw">names</span>(m2)<span class="op">==</span><span class="st">&quot;ORCDRC&quot;</span>), 
                    <span class="dt">training_frame =</span> edgeroi_t.hex, <span class="dt">learner =</span> learner, 
                    <span class="dt">cvControl =</span> <span class="kw">list</span>(<span class="dt">V =</span> <span class="dv">5</span>))
<span class="co">#&gt; [1] &quot;Cross-validating and training base learner 1: h2o.randomForest.wrapper&quot;</span>
<span class="co">#&gt; Warning in h2o.randomForest(x = x, y = y, training_frame =</span>
<span class="co">#&gt; training_frame, : Argument offset_column is deprecated and has no use for</span>
<span class="co">#&gt; Random Forest.</span>
<span class="co">#&gt; [1] &quot;Cross-validating and training base learner 2: h2o.gbm.wrapper&quot;</span>
<span class="co">#&gt; [1] &quot;Metalearning&quot;</span>
perf &lt;-<span class="st"> </span><span class="kw">h2o.ensemble_performance</span>(fit, <span class="dt">newdata =</span> edgeroi_v.hex)
<span class="co">#&gt; Warning in doTryCatch(return(expr), name, parentenv, handler): Test/</span>
<span class="co">#&gt; Validation dataset is missing column &#39;fold_id&#39;: substituting in a column of</span>
<span class="co">#&gt; 0.0</span>
<span class="co">#&gt; Warning in doTryCatch(return(expr), name, parentenv, handler): Test/</span>
<span class="co">#&gt; Validation dataset is missing column &#39;fold_id&#39;: substituting in a column of</span>
<span class="co">#&gt; 0.0</span>
perf
<span class="co">#&gt; </span>
<span class="co">#&gt; Base learner performance, sorted by specified metric:</span>
<span class="co">#&gt;                    learner  MSE</span>
<span class="co">#&gt; 2          h2o.gbm.wrapper 12.7</span>
<span class="co">#&gt; 1 h2o.randomForest.wrapper 12.3</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; H2O Ensemble Performance on &lt;newdata&gt;:</span>
<span class="co">#&gt; ----------------</span>
<span class="co">#&gt; Family: gaussian</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Ensemble performance (MSE): 11.6494142561051</span></code></pre></div>
<p>which shows that, in this specific case, the ensemble model is only slightly better than a single model. Note that we would need to repeat testing the ensemble modeling several times until we can be certain any actual actual gain in accuracy.</p>
<p>We can also test ensemble predictions using the cookfarm data set <span class="citation">(Gasch et al. <a href="#ref-Gasch2015SPASTA">2015</a>)</span>. This data set consists of 183 profiles, each consisting of multiple soil horizons (1050 in total). To create a regression matrix we use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(cookfarm)
cookfarm.hor &lt;-<span class="st"> </span>cookfarm<span class="op">$</span>profiles
<span class="kw">str</span>(cookfarm.hor)
<span class="co">#&gt; &#39;data.frame&#39;:    1050 obs. of  9 variables:</span>
<span class="co">#&gt;  $ SOURCEID: Factor w/ 369 levels &quot;CAF001&quot;,&quot;CAF002&quot;,..: 3 3 3 3 3 5 5 5 5 5 ...</span>
<span class="co">#&gt;  $ Easting : num  493383 493383 493383 493383 493383 ...</span>
<span class="co">#&gt;  $ Northing: num  5180586 5180586 5180586 5180586 5180586 ...</span>
<span class="co">#&gt;  $ TAXSUSDA: Factor w/ 6 levels &quot;Caldwell&quot;,&quot;Latah&quot;,..: 3 3 3 3 3 4 4 4 4 4 ...</span>
<span class="co">#&gt;  $ HZDUSD  : Factor w/ 67 levels &quot;2R&quot;,&quot;A&quot;,&quot;A1&quot;,..: 12 2 7 35 36 12 2 16 43 44 ...</span>
<span class="co">#&gt;  $ UHDICM  : num  0 21 39 65 98 0 17 42 66 97 ...</span>
<span class="co">#&gt;  $ LHDICM  : num  21 39 65 98 153 17 42 66 97 153 ...</span>
<span class="co">#&gt;  $ BLD     : num  1.46 1.37 1.52 1.72 1.72 1.56 1.33 1.36 1.37 1.48 ...</span>
<span class="co">#&gt;  $ PHIHOX  : num  4.69 5.9 6.25 6.54 6.75 4.12 5.73 6.26 6.59 6.85 ...</span>
cookfarm.hor<span class="op">$</span>depth &lt;-<span class="st"> </span>cookfarm.hor<span class="op">$</span>UHDICM <span class="op">+</span>
<span class="st">  </span>(cookfarm.hor<span class="op">$</span>LHDICM <span class="op">-</span><span class="st"> </span>cookfarm.hor<span class="op">$</span>UHDICM)<span class="op">/</span><span class="dv">2</span>
sel.id &lt;-<span class="st"> </span><span class="op">!</span><span class="kw">duplicated</span>(cookfarm.hor<span class="op">$</span>SOURCEID)
cookfarm.xy &lt;-<span class="st"> </span>cookfarm.hor[sel.id,<span class="kw">c</span>(<span class="st">&quot;SOURCEID&quot;</span>,<span class="st">&quot;Easting&quot;</span>,<span class="st">&quot;Northing&quot;</span>)]
<span class="kw">str</span>(cookfarm.xy)
<span class="co">#&gt; &#39;data.frame&#39;:    183 obs. of  3 variables:</span>
<span class="co">#&gt;  $ SOURCEID: Factor w/ 369 levels &quot;CAF001&quot;,&quot;CAF002&quot;,..: 3 5 7 9 11 13 15 17 19 21 ...</span>
<span class="co">#&gt;  $ Easting : num  493383 493447 493511 493575 493638 ...</span>
<span class="co">#&gt;  $ Northing: num  5180586 5180572 5180568 5180573 5180571 ...</span>
<span class="kw">coordinates</span>(cookfarm.xy) &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>Easting <span class="op">+</span><span class="st"> </span>Northing
grid10m &lt;-<span class="st"> </span>cookfarm<span class="op">$</span>grids
<span class="kw">coordinates</span>(grid10m) &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>y
<span class="kw">gridded</span>(grid10m) =<span class="st"> </span><span class="ot">TRUE</span>
ov.cf &lt;-<span class="st"> </span><span class="kw">over</span>(cookfarm.xy, grid10m)
rm.cookfarm &lt;-<span class="st"> </span>plyr<span class="op">::</span><span class="kw">join</span>(cookfarm.hor, <span class="kw">cbind</span>(cookfarm.xy<span class="op">@</span>data, ov.cf))
<span class="co">#&gt; Joining by: SOURCEID</span></code></pre></div>
<p>Here, we are interested in predicting soil pH in 3D, hence we will use a model of form:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fm.PHI &lt;-<span class="st"> </span>PHIHOX<span class="op">~</span>DEM<span class="op">+</span>TWI<span class="op">+</span>NDRE.M<span class="op">+</span>Cook_fall_ECa<span class="op">+</span>Cook_spr_ECa<span class="op">+</span>depth
rc &lt;-<span class="st"> </span><span class="kw">complete.cases</span>(rm.cookfarm[,<span class="kw">all.vars</span>(fm.PHI)])
mP3 &lt;-<span class="st"> </span>rm.cookfarm[rc,<span class="kw">all.vars</span>(fm.PHI)]
<span class="kw">str</span>(mP3)
<span class="co">#&gt; &#39;data.frame&#39;:    997 obs. of  7 variables:</span>
<span class="co">#&gt;  $ PHIHOX       : num  4.69 5.9 6.25 6.54 6.75 4.12 5.73 6.26 6.59 6.85 ...</span>
<span class="co">#&gt;  $ DEM          : num  788 788 788 788 788 ...</span>
<span class="co">#&gt;  $ TWI          : num  4.3 4.3 4.3 4.3 4.3 ...</span>
<span class="co">#&gt;  $ NDRE.M       : num  -0.0512 -0.0512 -0.0512 -0.0512 -0.0512 ...</span>
<span class="co">#&gt;  $ Cook_fall_ECa: num  7.7 7.7 7.7 7.7 7.7 ...</span>
<span class="co">#&gt;  $ Cook_spr_ECa : num  33 33 33 33 33 ...</span>
<span class="co">#&gt;  $ depth        : num  10.5 30 52 81.5 125.5 ...</span></code></pre></div>
<p>We can again test fitting an ensemble model using two MLA’s:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">k.f3 &lt;-<span class="st"> </span>dismo<span class="op">::</span><span class="kw">kfold</span>(mP3, <span class="dt">k=</span><span class="dv">4</span>)
## split data into training and validation:
cookfarm_v.hex &lt;-<span class="st"> </span><span class="kw">as.h2o</span>(mP3[k.f3<span class="op">==</span><span class="dv">1</span>,], <span class="dt">destination_frame =</span> <span class="st">&quot;cookfarm_v.hex&quot;</span>)
cookfarm_t.hex &lt;-<span class="st"> </span><span class="kw">as.h2o</span>(mP3[<span class="op">!</span>k.f3<span class="op">==</span><span class="dv">1</span>,], <span class="dt">destination_frame =</span> <span class="st">&quot;cookfarm_t.hex&quot;</span>)
learner3 =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;h2o.glm.wrapper&quot;</span>, <span class="st">&quot;h2o.randomForest.wrapper&quot;</span>,
            <span class="st">&quot;h2o.gbm.wrapper&quot;</span>, <span class="st">&quot;h2o.deeplearning.wrapper&quot;</span>)
fit3 &lt;-<span class="st"> </span><span class="kw">h2o.ensemble</span>(<span class="dt">x =</span> <span class="kw">which</span>(<span class="kw">names</span>(mP3) <span class="op">%in%</span><span class="st"> </span><span class="kw">all.vars</span>(fm.PHI)[<span class="op">-</span><span class="dv">1</span>]), 
                    <span class="dt">y =</span> <span class="kw">which</span>(<span class="kw">names</span>(mP3)<span class="op">==</span><span class="st">&quot;PHIHOX&quot;</span>), 
                    <span class="dt">training_frame =</span> cookfarm_t.hex, <span class="dt">learner =</span> learner3, 
                    <span class="dt">cvControl =</span> <span class="kw">list</span>(<span class="dt">V =</span> <span class="dv">5</span>))
<span class="co">#&gt; [1] &quot;Cross-validating and training base learner 1: h2o.glm.wrapper&quot;</span>
<span class="co">#&gt; [1] &quot;Cross-validating and training base learner 2: h2o.randomForest.wrapper&quot;</span>
<span class="co">#&gt; Warning in h2o.randomForest(x = x, y = y, training_frame =</span>
<span class="co">#&gt; training_frame, : Argument offset_column is deprecated and has no use for</span>
<span class="co">#&gt; Random Forest.</span>
<span class="co">#&gt; [1] &quot;Cross-validating and training base learner 3: h2o.gbm.wrapper&quot;</span>
<span class="co">#&gt; [1] &quot;Cross-validating and training base learner 4: h2o.deeplearning.wrapper&quot;</span>
<span class="co">#&gt; [1] &quot;Metalearning&quot;</span>
perf3 &lt;-<span class="st"> </span><span class="kw">h2o.ensemble_performance</span>(fit3, <span class="dt">newdata =</span> cookfarm_v.hex)
<span class="co">#&gt; Warning in doTryCatch(return(expr), name, parentenv, handler): Test/</span>
<span class="co">#&gt; Validation dataset is missing column &#39;fold_id&#39;: substituting in a column of</span>
<span class="co">#&gt; 0.0</span>
<span class="co">#&gt; Warning in doTryCatch(return(expr), name, parentenv, handler): Test/</span>
<span class="co">#&gt; Validation dataset is missing column &#39;fold_id&#39;: substituting in a column of</span>
<span class="co">#&gt; 0.0</span>

<span class="co">#&gt; Warning in doTryCatch(return(expr), name, parentenv, handler): Test/</span>
<span class="co">#&gt; Validation dataset is missing column &#39;fold_id&#39;: substituting in a column of</span>
<span class="co">#&gt; 0.0</span>

<span class="co">#&gt; Warning in doTryCatch(return(expr), name, parentenv, handler): Test/</span>
<span class="co">#&gt; Validation dataset is missing column &#39;fold_id&#39;: substituting in a column of</span>
<span class="co">#&gt; 0.0</span>
perf3
<span class="co">#&gt; </span>
<span class="co">#&gt; Base learner performance, sorted by specified metric:</span>
<span class="co">#&gt;                    learner    MSE</span>
<span class="co">#&gt; 1          h2o.glm.wrapper 0.2827</span>
<span class="co">#&gt; 4 h2o.deeplearning.wrapper 0.1553</span>
<span class="co">#&gt; 3          h2o.gbm.wrapper 0.0971</span>
<span class="co">#&gt; 2 h2o.randomForest.wrapper 0.0770</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; H2O Ensemble Performance on &lt;newdata&gt;:</span>
<span class="co">#&gt; ----------------</span>
<span class="co">#&gt; Family: gaussian</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Ensemble performance (MSE): 0.0756454451033218</span></code></pre></div>
<p>In this case Ensemble performance (MSE) seems to be <em>as bad</em> as the single best spatial predictor (random forest in this case). This illustrates that ensemble predictions are sometimes not beneficial.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">h2o.shutdown</span>()
<span class="co">#&gt; Are you sure you want to shutdown the H2O instance running at http://localhost:54321/ (Y/N)?</span>
<span class="co">#&gt; [1] TRUE</span></code></pre></div>
</div>
<div id="ensemble-predictions-using-superlearner-package" class="section level3">
<h3><span class="header-section-number">6.1.6</span> Ensemble predictions using SuperLearner package</h3>
<p>Another interesting package to generate ensemble predictions of soil properties and classes is the SuperLearner package <span class="citation">(Polley and Van Der Laan <a href="#ref-polley2010super">2010</a>)</span>. This package has many more options than <code>h2o.ensemble</code> considering the number of methods available for consideration:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(SuperLearner)
<span class="co">#&gt; Loading required package: nnls</span>
<span class="co">#&gt; Super Learner</span>
<span class="co">#&gt; Version: 2.0-24</span>
<span class="co">#&gt; Package created on 2018-08-10</span>
<span class="co"># List available models:</span>
<span class="kw">listWrappers</span>()
<span class="co">#&gt; All prediction algorithm wrappers in SuperLearner:</span>
<span class="co">#&gt;  [1] &quot;SL.bartMachine&quot;      &quot;SL.bayesglm&quot;         &quot;SL.biglasso&quot;        </span>
<span class="co">#&gt;  [4] &quot;SL.caret&quot;            &quot;SL.caret.rpart&quot;      &quot;SL.cforest&quot;         </span>
<span class="co">#&gt;  [7] &quot;SL.dbarts&quot;           &quot;SL.earth&quot;            &quot;SL.extraTrees&quot;      </span>
<span class="co">#&gt; [10] &quot;SL.gam&quot;              &quot;SL.gbm&quot;              &quot;SL.glm&quot;             </span>
<span class="co">#&gt; [13] &quot;SL.glm.interaction&quot;  &quot;SL.glmnet&quot;           &quot;SL.ipredbagg&quot;       </span>
<span class="co">#&gt; [16] &quot;SL.kernelKnn&quot;        &quot;SL.knn&quot;              &quot;SL.ksvm&quot;            </span>
<span class="co">#&gt; [19] &quot;SL.lda&quot;              &quot;SL.leekasso&quot;         &quot;SL.lm&quot;              </span>
<span class="co">#&gt; [22] &quot;SL.loess&quot;            &quot;SL.logreg&quot;           &quot;SL.mean&quot;            </span>
<span class="co">#&gt; [25] &quot;SL.nnet&quot;             &quot;SL.nnls&quot;             &quot;SL.polymars&quot;        </span>
<span class="co">#&gt; [28] &quot;SL.qda&quot;              &quot;SL.randomForest&quot;     &quot;SL.ranger&quot;          </span>
<span class="co">#&gt; [31] &quot;SL.ridge&quot;            &quot;SL.rpart&quot;            &quot;SL.rpartPrune&quot;      </span>
<span class="co">#&gt; [34] &quot;SL.speedglm&quot;         &quot;SL.speedlm&quot;          &quot;SL.step&quot;            </span>
<span class="co">#&gt; [37] &quot;SL.step.forward&quot;     &quot;SL.step.interaction&quot; &quot;SL.stepAIC&quot;         </span>
<span class="co">#&gt; [40] &quot;SL.svm&quot;              &quot;SL.template&quot;         &quot;SL.xgboost&quot;</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; All screening algorithm wrappers in SuperLearner:</span>
<span class="co">#&gt; [1] &quot;All&quot;</span>
<span class="co">#&gt; [1] &quot;screen.corP&quot;           &quot;screen.corRank&quot;        &quot;screen.glmnet&quot;        </span>
<span class="co">#&gt; [4] &quot;screen.randomForest&quot;   &quot;screen.SIS&quot;            &quot;screen.template&quot;      </span>
<span class="co">#&gt; [7] &quot;screen.ttest&quot;          &quot;write.screen.template&quot;</span></code></pre></div>
<p>where <code>SL.</code> refers to an imported method from a package e.g. <code>&quot;SL.ranger&quot;</code> is the SuperLearner method from the package ranger.</p>
<p>A useful functionality of the SuperLearner package is that it displays how model average weights are estimated and which methods can safely be excluded from predictions. When using SuperLearner, however, it is highly recommended to use the parallelized / multicore version, otherwise the computing time might be quite extensive. For example, to prepare ensemble predictions using the five standard prediction techniques used in this tutorial we would run:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## detach snowfall package otherwise possible conflicts
<span class="co">#detach(&quot;package:snowfall&quot;, unload=TRUE)</span>
<span class="kw">library</span>(parallel)
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: &#39;parallel&#39;</span>
<span class="co">#&gt; The following objects are masked from &#39;package:snow&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,</span>
<span class="co">#&gt;     clusterExport, clusterMap, clusterSplit, makeCluster,</span>
<span class="co">#&gt;     parApply, parCapply, parLapply, parRapply, parSapply,</span>
<span class="co">#&gt;     splitIndices, stopCluster</span>
sl.l =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;SL.mean&quot;</span>, <span class="st">&quot;SL.xgboost&quot;</span>, <span class="st">&quot;SL.ksvm&quot;</span>, <span class="st">&quot;SL.glmnet&quot;</span>, <span class="st">&quot;SL.ranger&quot;</span>)
cl &lt;-<span class="st"> </span>parallel<span class="op">::</span><span class="kw">makeCluster</span>(<span class="kw">detectCores</span>())
x &lt;-<span class="st"> </span>parallel<span class="op">::</span><span class="kw">clusterEvalQ</span>(cl, <span class="kw">library</span>(SuperLearner))
sl &lt;-<span class="st"> </span><span class="kw">snowSuperLearner</span>(<span class="dt">Y =</span> mP3<span class="op">$</span>PHIHOX, 
                       <span class="dt">X =</span> mP3[,<span class="kw">all.vars</span>(fm.PHI)[<span class="op">-</span><span class="dv">1</span>]],
                       <span class="dt">cluster =</span> cl, 
                       <span class="dt">SL.library =</span> sl.l)
<span class="co">#&gt; Loading required package: glmnet</span>
<span class="co">#&gt; Loading required package: Matrix</span>
<span class="co">#&gt; Loading required package: foreach</span>
<span class="co">#&gt; Loaded glmnet 2.0-16</span>
sl
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:  </span>
<span class="co">#&gt; snowSuperLearner(cluster = cl, Y = mP3$PHIHOX, X = mP3[, all.vars(fm.PHI)[-1]],  </span>
<span class="co">#&gt;     SL.library = sl.l) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt;                  Risk  Coef</span>
<span class="co">#&gt; SL.mean_All    0.7540 0.000</span>
<span class="co">#&gt; SL.xgboost_All 0.0598 0.819</span>
<span class="co">#&gt; SL.ksvm_All    0.1286 0.017</span>
<span class="co">#&gt; SL.glmnet_All  0.3073 0.000</span>
<span class="co">#&gt; SL.ranger_All  0.0852 0.164</span></code></pre></div>
<p>This shows that <code>SL.xgboost_All</code> outperforms the competition by a large margin. Since this is a relatively small data set, RMSE produced by <code>SL.xgboost_All</code> is probably unrealistically small. If we only use the top three models (XGboost, ranger and ksvm) in comparison we get:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sl.l2 =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;SL.xgboost&quot;</span>, <span class="st">&quot;SL.ranger&quot;</span>, <span class="st">&quot;SL.ksvm&quot;</span>)
sl2 &lt;-<span class="st"> </span><span class="kw">snowSuperLearner</span>(<span class="dt">Y =</span> mP3<span class="op">$</span>PHIHOX, 
                       <span class="dt">X =</span> mP3[,<span class="kw">all.vars</span>(fm.PHI)[<span class="op">-</span><span class="dv">1</span>]],
                       <span class="dt">cluster =</span> cl, 
                       <span class="dt">SL.library =</span> sl.l2)
sl2
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:  </span>
<span class="co">#&gt; snowSuperLearner(cluster = cl, Y = mP3$PHIHOX, X = mP3[, all.vars(fm.PHI)[-1]],  </span>
<span class="co">#&gt;     SL.library = sl.l2) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt;                  Risk  Coef</span>
<span class="co">#&gt; SL.xgboost_All 0.0603 0.813</span>
<span class="co">#&gt; SL.ranger_All  0.0833 0.187</span>
<span class="co">#&gt; SL.ksvm_All    0.1305 0.000</span></code></pre></div>
<p>again <code>SL.xgboost</code> dominates the ensemble model, which is most likely unrealistic because most of the training data is spatially clustered and hence XGboost is probably over-fitting. To estimate actual accuracy of predicting soil pH using these two techniques we can run cross-validation where entire profiles are taken out of the training dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(rm.cookfarm<span class="op">$</span>SOURCEID)
<span class="co">#&gt;  Factor w/ 369 levels &quot;CAF001&quot;,&quot;CAF002&quot;,..: 3 3 3 3 3 5 5 5 5 5 ...</span>
cv_sl &lt;-<span class="st"> </span><span class="kw">CV.SuperLearner</span>(<span class="dt">Y =</span> mP3<span class="op">$</span>PHIHOX, 
                       <span class="dt">X =</span> mP3[,<span class="kw">all.vars</span>(fm.PHI)[<span class="op">-</span><span class="dv">1</span>]],
                       <span class="dt">parallel =</span> cl, 
                       <span class="dt">SL.library =</span> sl.l2, 
                       <span class="dt">V=</span><span class="dv">5</span>, <span class="dt">id=</span>rm.cookfarm<span class="op">$</span>SOURCEID[rc], 
                       <span class="dt">verbose=</span><span class="ot">TRUE</span>)
<span class="kw">summary</span>(cv_sl)
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:  </span>
<span class="co">#&gt; CV.SuperLearner(Y = mP3$PHIHOX, X = mP3[, all.vars(fm.PHI)[-1]], V = 5,  </span>
<span class="co">#&gt;     SL.library = sl.l2, id = rm.cookfarm$SOURCEID[rc], verbose = TRUE,  </span>
<span class="co">#&gt;     parallel = cl) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Risk is based on: Mean Squared Error</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; All risk estimates are based on V =  5 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt;       Algorithm  Ave    se   Min  Max</span>
<span class="co">#&gt;   Super Learner 0.16 0.014 0.095 0.26</span>
<span class="co">#&gt;     Discrete SL 0.17 0.015 0.116 0.25</span>
<span class="co">#&gt;  SL.xgboost_All 0.19 0.016 0.135 0.27</span>
<span class="co">#&gt;   SL.ranger_All 0.16 0.015 0.106 0.25</span>
<span class="co">#&gt;     SL.ksvm_All 0.18 0.015 0.109 0.30</span></code></pre></div>
<p>where <code>V=5</code> specifies number of folds, and <code>id=rm.cookfarm$SOURCEID</code> forces that entire profiles are removed from training and cross-validation. This gives a more realistic RMSE of about ±0.35. Note that this time <code>SL.xgboost_All</code> is even somewhat worse than the random forest model, and the ensemble model (<code>Super Learner</code>) is slightly better than each individual model. This matches our previous results with <code>h20.ensemble</code>.</p>
<p>To produce predictions of soil pH at 10 cm depth we can finally use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sl2 &lt;-<span class="st"> </span><span class="kw">snowSuperLearner</span>(<span class="dt">Y =</span> mP3<span class="op">$</span>PHIHOX, 
                       <span class="dt">X =</span> mP3[,<span class="kw">all.vars</span>(fm.PHI)[<span class="op">-</span><span class="dv">1</span>]],
                       <span class="dt">cluster =</span> cl, 
                       <span class="dt">SL.library =</span> sl.l2,
                       <span class="dt">id=</span>rm.cookfarm<span class="op">$</span>SOURCEID[rc],
                       <span class="dt">cvControl=</span><span class="kw">list</span>(<span class="dt">V=</span><span class="dv">5</span>))
sl2
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:  </span>
<span class="co">#&gt; snowSuperLearner(cluster = cl, Y = mP3$PHIHOX, X = mP3[, all.vars(fm.PHI)[-1]],  </span>
<span class="co">#&gt;     SL.library = sl.l2, id = rm.cookfarm$SOURCEID[rc], cvControl = list(V = 5)) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt;                 Risk  Coef</span>
<span class="co">#&gt; SL.xgboost_All 0.215 0.000</span>
<span class="co">#&gt; SL.ranger_All  0.166 0.461</span>
<span class="co">#&gt; SL.ksvm_All    0.163 0.539</span>
new.data &lt;-<span class="st"> </span>grid10m<span class="op">@</span>data
pred.PHI &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="ot">NULL</span>)
depths =<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">30</span>,<span class="dv">50</span>,<span class="dv">70</span>,<span class="dv">90</span>)
<span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(depths)){
  new.data<span class="op">$</span>depth =<span class="st"> </span>depths[j]
  pred.PHI[[j]] &lt;-<span class="st"> </span><span class="kw">predict</span>(sl2, new.data[,sl2<span class="op">$</span>varNames])
}
<span class="co">#&gt; Loading required package: kernlab</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: &#39;kernlab&#39;</span>
<span class="co">#&gt; The following object is masked from &#39;package:scales&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     alpha</span>
<span class="co">#&gt; The following object is masked from &#39;package:ggplot2&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     alpha</span>
<span class="co">#&gt; The following objects are masked from &#39;package:raster&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     buffer, rotated</span>
<span class="kw">str</span>(pred.PHI[[<span class="dv">1</span>]])
<span class="co">#&gt; List of 2</span>
<span class="co">#&gt;  $ pred           : num [1:3865, 1] 4.64 4.7 4.86 4.83 4.76 ...</span>
<span class="co">#&gt;  $ library.predict: num [1:3865, 1:3] 4.15 4.11 4.45 4.75 4.78 ...</span>
<span class="co">#&gt;   ..- attr(*, &quot;dimnames&quot;)=List of 2</span>
<span class="co">#&gt;   .. ..$ : NULL</span>
<span class="co">#&gt;   .. ..$ : chr [1:3] &quot;SL.xgboost_All&quot; &quot;SL.ranger_All&quot; &quot;SL.ksvm_All&quot;</span></code></pre></div>
<p>this yields two outputs:</p>
<ul>
<li>ensemble prediction in the <code>pred</code> matrix,</li>
<li>list of individual predictions in the <code>library.predict</code> matrix,</li>
</ul>
<p>To visualize the predictions (at six depths) we can run:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(depths)){
  grid10m<span class="op">@</span>data[,<span class="kw">paste0</span>(<span class="st">&quot;PHI.&quot;</span>, depths[j],<span class="st">&quot;cm&quot;</span>)] &lt;-<span class="st"> </span>pred.PHI[[j]]<span class="op">$</span>pred[,<span class="dv">1</span>]
}
<span class="kw">spplot</span>(grid10m, <span class="kw">paste0</span>(<span class="st">&quot;PHI.&quot;</span>, depths,<span class="st">&quot;cm&quot;</span>), 
       <span class="dt">col.regions=</span>R_pal[[<span class="st">&quot;pH_pal&quot;</span>]], <span class="dt">as.table=</span><span class="ot">TRUE</span>)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Soilmapping_using_mla_files/figure-html/ph-cookfarm-1.png" alt="Predicted soil pH using 3D ensemble model." width="100%" />
<p class="caption">
(#fig:ph-cookfarm)Predicted soil pH using 3D ensemble model.
</p>
</div>
<p>The second prediction matrix can be used to determine <em>model uncertainty</em>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(matrixStats)
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: &#39;matrixStats&#39;</span>
<span class="co">#&gt; The following object is masked from &#39;package:plyr&#39;:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     count</span>
grid10m<span class="op">$</span>PHI<span class="fl">.10</span>cm.sd &lt;-<span class="st"> </span><span class="kw">rowSds</span>(pred.PHI[[<span class="dv">1</span>]]<span class="op">$</span>library.predict, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)
pts =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;sp.points&quot;</span>, cookfarm.xy, <span class="dt">pch=</span><span class="st">&quot;+&quot;</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">cex=</span><span class="fl">1.4</span>)
<span class="kw">spplot</span>(grid10m, <span class="st">&quot;PHI.10cm.sd&quot;</span>, <span class="dt">sp.layout =</span> <span class="kw">list</span>(pts), <span class="dt">col.regions=</span><span class="kw">rev</span>(<span class="kw">bpy.colors</span>()))</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Soilmapping_using_mla_files/figure-html/ph-cookfarm-var-1.png" alt="Example of variance of prediction models for soil pH." width="100%" />
<p class="caption">
(#fig:ph-cookfarm-var)Example of variance of prediction models for soil pH.
</p>
</div>
<p>which highlights the especially problematic areas, in this case most likely correlated with extrapolation in feature space. Before we stop computing, we need to close the cluster session by using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stopCluster</span>(cl)</code></pre></div>
</div>
</div>
<div id="a-generic-framework-for-spatial-prediction-using-random-forest" class="section level2">
<h2><span class="header-section-number">6.2</span> A generic framework for spatial prediction using Random Forest</h2>
<p>We have seen, in the above examples, that MLA’s can be used efficiently to map soil properties and classes. Most currently used MLA’s, however, ignore the spatial locations of the observations and hence overlook any spatial autocorrelation in the data not accounted for by the covariates. Spatial auto-correlation, especially if it remains visible in the cross-validation residuals, indicates that the predictions are perhaps biased, and this is sub-optimal. To account for this, <span class="citation">T. Hengl, Nussbaum, et al. (<a href="#ref-Hengl2018RFsp">2018</a>)</span> describe a framework for using Random Forest (as implemented in the ranger package) in combination with geographical distances to sampling locations (which provide measures of relative spatial location) to fit models and predict values (RFsp).</p>
<div id="general-principle-of-rfsp" class="section level3">
<h3><span class="header-section-number">6.2.1</span> General principle of RFsp</h3>
<p>RF is, in essence, a non-spatial approach to spatial prediction, as the sampling locations and general sampling pattern are both ignored during the estimation of MLA model parameters. This can potentially lead to sub-optimal predictions and possibly systematic over- or under-prediction, especially where the spatial autocorrelation in the target variable is high and where point patterns show clear sampling bias. To overcome this problem <span class="citation">T. Hengl, Nussbaum, et al. (<a href="#ref-Hengl2018RFsp">2018</a>)</span> propose the following generic <em>“RFsp”</em> system:</p>
<span class="math display">\[\begin{equation}
Y({{\bf s}}) = f \left( {{\bf X}_G}, {{\bf X}_R}, {{\bf X}_P} \right)
(\#eq:rf-BUGP)
\end{equation}\]</span>
<p>where <span class="math inline">\({{\bf X}_G}\)</span> are covariates accounting for geographical proximity and spatial relations between observations (to mimic spatial correlation used in kriging):</p>
<span class="math display">\[\begin{equation}
{{\bf X}_G} = \left( d_{p1}, d_{p2}, \ldots , d_{pN} \right)
\end{equation}\]</span>
<p>where <span class="math inline">\(d_{pi}\)</span> is the buffer distance (or any other complex proximity upslope/downslope distance, as explained in the next section) to the observed location <span class="math inline">\(pi\)</span> from <span class="math inline">\({\bf s}\)</span> and <span class="math inline">\(N\)</span> is the total number of training points. <span class="math inline">\({{\bf X}_R}\)</span> are surface reflectance covariates, i.e. usually spectral bands of remote sensing images, and <span class="math inline">\({{\bf X}_P}\)</span> are process-based covariates. For example, the Landsat infrared band is a surface reflectance covariate, while the topographic wetness index and soil weathering index are process-based covariates. Geographic covariates are often smooth and reflect geometric composition of points, reflectance-based covariates can exhibit a significant amount of noise and usually provide information only about the surface of objects. Process-based covariates require specialized knowledge and rethinking of how to best represent processes. Assuming that the RFsp is fitted only using the <span class="math inline">\({\bf {X}_G}\)</span>, the predictions would resemble ordinary kriging (OK). If All covariates are used Eq. @ref(eq:rf-BUGP), RFsp would resemble regression-kriging (RK).</p>
</div>
<div id="geographical-covariates" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Geographical covariates</h3>
<p>One of the key principles of geography is that <em>“everything is related to everything else, but near things are more related than distant things”</em> <span class="citation">(Miller <a href="#ref-miller2004tobler">2004</a>)</span>. This principle forms the basis of geostatistics, which converts this rule into a mathematical model, i.e., through spatial autocorrelation functions or variograms. The key to making RF applicable to spatial statistics problems, therefore, lies also in preparing geographical (spatial) measures of proximity and connectivity between observations, so that spatial autocorrelation is accounted for. There are multiple options for variables that quantify proximity and geographical connection (Fig. @ref(fig:distances-examples)):</p>
<ol style="list-style-type: decimal">
<li><p>Geographical coordinates <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span>, i.e., easting and northing.</p></li>
<li><p>Euclidean distances to reference points in the study area. For example, distance to the center and edges of the study area, etc.</p></li>
<li><p>Euclidean distances to sampling locations, i.e., distances from observation locations. Here one buffer distance map can be generated per observation point or group of points. These are also the same distance measures as used in geostatistics.</p></li>
<li><p>Downslope distances, i.e., distances within a watershed: for each sampling point one can derive upslope/downslope distances to the ridges and hydrological network and/or downslope or upslope areas <span class="citation">(Gruber and Peckham <a href="#ref-GRUBER2009171">2009</a>)</span>. This requires, in addition to using a Digital Elevation Model, implementing a hydrological analysis of the terrain.</p></li>
<li><p>Resistance distances or weighted buffer distances, i.e., distances of the cumulative effort derived using terrain ruggedness and/or natural obstacles.</p></li>
</ol>
<p>The package (<strong><em>WHICH PACKAGE?</em></strong>), for example, provides a framework to derive complex distances based on terrain complexity <span class="citation">(van Etten <a href="#ref-vanEtten2017r">2017</a>)</span>. Here additional inputs required to compute complex distances are the Digital Elevation Model (DEM) and DEM-derivatives, such as slope (Fig. @ref(fig:distances-examples)b). SAGA GIS <span class="citation">(Conrad et al. <a href="#ref-gmd-8-1991-2015">2015</a>)</span> offers a wide variety of DEM derivatives that can be derived per location of interest.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_distances_examples.png" alt="Examples of distance maps to some location in space (yellow dot) based on different derivation algorithms: (a) simple Euclidean distances, (b) complex speed-based distances based on the package and Digital Elevation Model (DEM), and (c) upslope area derived based on the DEM in SAGA GIS. Image source: Hengl et al. (2018) doi: 10.7717/peerj.5518." width="100%" />
<p class="caption">
(#fig:distances-examples)Examples of distance maps to some location in space (yellow dot) based on different derivation algorithms: (a) simple Euclidean distances, (b) complex speed-based distances based on the package and Digital Elevation Model (DEM), and (c) upslope area derived based on the DEM in SAGA GIS. Image source: Hengl et al. (2018) doi: 10.7717/peerj.5518.
</p>
</div>
<p>Here, we only illustrate predictive performance using Euclidean buffer distances (to all sampling points), but the code could be adopted to include other families of geographical covariates (as shown in Fig. @ref(fig:distances-examples)). Note also that RF tolerates a high number of covariates and multicolinearity <span class="citation">(Biau and Scornet <a href="#ref-Biau2016">2016</a>)</span>, hence multiple types of geographical covariates (Euclidean buffer distances, upslope and downslope areas) could be used at the same time.</p>
</div>
<div id="spatial-prediction-2d-continuous-variable-using-rfsp" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Spatial prediction 2D continuous variable using RFsp</h3>
<p>To run these examples, it is recommended to install <a href="https://github.com/imbs-hl/ranger">ranger</a> <span class="citation">(Wright and Ziegler <a href="#ref-wright2017ranger">2017</a>)</span> directly from github:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">if</span>(<span class="op">!</span><span class="kw">require</span>(ranger)){ devtools<span class="op">::</span><span class="kw">install_github</span>(<span class="st">&quot;imbs-hl/ranger&quot;</span>) }</code></pre></div>
<p>Quantile regression random forest and derivation of standard errors using Jackknifing is available from ranger version &gt;0.9.4. Other packages that we use here include:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(GSIF)
<span class="kw">library</span>(rgdal)
<span class="co">#&gt; rgdal: version: 1.3-6, (SVN revision 773)</span>
<span class="co">#&gt;  Geospatial Data Abstraction Library extensions to R successfully loaded</span>
<span class="co">#&gt;  Loaded GDAL runtime: GDAL 2.3.2, released 2018/09/21</span>
<span class="co">#&gt;  Path to GDAL shared files: /usr/share/gdal</span>
<span class="co">#&gt;  GDAL binary built with GEOS: TRUE </span>
<span class="co">#&gt;  Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493]</span>
<span class="co">#&gt;  Path to PROJ.4 shared files: (autodetected)</span>
<span class="co">#&gt;  Linking to sp version: 1.3-1</span>
<span class="kw">library</span>(raster)
<span class="kw">library</span>(geoR)
<span class="co">#&gt; --------------------------------------------------------------</span>
<span class="co">#&gt;  Analysis of Geostatistical Data</span>
<span class="co">#&gt;  For an Introduction to geoR go to http://www.leg.ufpr.br/geoR</span>
<span class="co">#&gt;  geoR version 1.7-5.2.1 (built on 2016-05-02) is now loaded</span>
<span class="co">#&gt; --------------------------------------------------------------</span>
<span class="kw">library</span>(ranger)</code></pre></div>
<pre><code>#&gt; 
#&gt; Attaching package: &#39;gridExtra&#39;
#&gt; The following object is masked from &#39;package:randomForest&#39;:
#&gt; 
#&gt;     combine</code></pre>
<p>If no other information is available, we can use buffer distances to all points as covariates to predict values of some continuous or categorical variable in the RFsp framework. These can be derived with the help of the <a href="https://cran.r-project.org/package=raster">raster</a> package <span class="citation">(Hijmans and van Etten <a href="#ref-raster">2017</a>)</span>. Consider for example the meuse data set from the <a href="https://github.com/edzer/gstat">gstat</a> package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">demo</span>(meuse, <span class="dt">echo=</span><span class="ot">FALSE</span>)</code></pre></div>
<p>We can derive buffer distance by using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">grid.dist0 &lt;-<span class="st"> </span>GSIF<span class="op">::</span><span class="kw">buffer.dist</span>(meuse[<span class="st">&quot;zinc&quot;</span>], meuse.grid[<span class="dv">1</span>], <span class="kw">as.factor</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(meuse)))</code></pre></div>
<p>which requires a few seconds, as it generates 155 individual gridded maps. The value of the target variable <code>zinc</code> can be now modeled as a function of these computed buffer distances:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dn0 &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="kw">names</span>(grid.dist0), <span class="dt">collapse=</span><span class="st">&quot;+&quot;</span>)
fm0 &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste</span>(<span class="st">&quot;zinc ~ &quot;</span>, dn0))
fm0
<span class="co">#&gt; zinc ~ layer.1 + layer.2 + layer.3 + layer.4 + layer.5 + layer.6 + </span>
<span class="co">#&gt;     layer.7 + layer.8 + layer.9 + layer.10 + layer.11 + layer.12 + </span>
<span class="co">#&gt;     layer.13 + layer.14 + layer.15 + layer.16 + layer.17 + layer.18 + </span>
<span class="co">#&gt;     layer.19 + layer.20 + layer.21 + layer.22 + layer.23 + layer.24 + </span>
<span class="co">#&gt;     layer.25 + layer.26 + layer.27 + layer.28 + layer.29 + layer.30 + </span>
<span class="co">#&gt;     layer.31 + layer.32 + layer.33 + layer.34 + layer.35 + layer.36 + </span>
<span class="co">#&gt;     layer.37 + layer.38 + layer.39 + layer.40 + layer.41 + layer.42 + </span>
<span class="co">#&gt;     layer.43 + layer.44 + layer.45 + layer.46 + layer.47 + layer.48 + </span>
<span class="co">#&gt;     layer.49 + layer.50 + layer.51 + layer.52 + layer.53 + layer.54 + </span>
<span class="co">#&gt;     layer.55 + layer.56 + layer.57 + layer.58 + layer.59 + layer.60 + </span>
<span class="co">#&gt;     layer.61 + layer.62 + layer.63 + layer.64 + layer.65 + layer.66 + </span>
<span class="co">#&gt;     layer.67 + layer.68 + layer.69 + layer.70 + layer.71 + layer.72 + </span>
<span class="co">#&gt;     layer.73 + layer.74 + layer.75 + layer.76 + layer.77 + layer.78 + </span>
<span class="co">#&gt;     layer.79 + layer.80 + layer.81 + layer.82 + layer.83 + layer.84 + </span>
<span class="co">#&gt;     layer.85 + layer.86 + layer.87 + layer.88 + layer.89 + layer.90 + </span>
<span class="co">#&gt;     layer.91 + layer.92 + layer.93 + layer.94 + layer.95 + layer.96 + </span>
<span class="co">#&gt;     layer.97 + layer.98 + layer.99 + layer.100 + layer.101 + </span>
<span class="co">#&gt;     layer.102 + layer.103 + layer.104 + layer.105 + layer.106 + </span>
<span class="co">#&gt;     layer.107 + layer.108 + layer.109 + layer.110 + layer.111 + </span>
<span class="co">#&gt;     layer.112 + layer.113 + layer.114 + layer.115 + layer.116 + </span>
<span class="co">#&gt;     layer.117 + layer.118 + layer.119 + layer.120 + layer.121 + </span>
<span class="co">#&gt;     layer.122 + layer.123 + layer.124 + layer.125 + layer.126 + </span>
<span class="co">#&gt;     layer.127 + layer.128 + layer.129 + layer.130 + layer.131 + </span>
<span class="co">#&gt;     layer.132 + layer.133 + layer.134 + layer.135 + layer.136 + </span>
<span class="co">#&gt;     layer.137 + layer.138 + layer.139 + layer.140 + layer.141 + </span>
<span class="co">#&gt;     layer.142 + layer.143 + layer.144 + layer.145 + layer.146 + </span>
<span class="co">#&gt;     layer.147 + layer.148 + layer.149 + layer.150 + layer.151 + </span>
<span class="co">#&gt;     layer.152 + layer.153 + layer.154 + layer.155</span></code></pre></div>
<p>Subsequent analysis is similar to any regression analysis using the <a href="https://github.com/imbs-hl/ranger">ranger package</a>. First we overlay points and grids to create a regression matrix:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ov.zinc &lt;-<span class="st"> </span><span class="kw">over</span>(meuse[<span class="st">&quot;zinc&quot;</span>], grid.dist0)
rm.zinc &lt;-<span class="st"> </span><span class="kw">cbind</span>(meuse<span class="op">@</span>data[<span class="st">&quot;zinc&quot;</span>], ov.zinc)</code></pre></div>
<p>to estimate also the prediction error variance i.e. prediction intervals we set <code>quantreg=TRUE</code> which initiates the Quantile Regression RF approach <span class="citation">(Meinshausen <a href="#ref-meinshausen2006quantile">2006</a>)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.zinc &lt;-<span class="st"> </span><span class="kw">ranger</span>(fm0, rm.zinc, <span class="dt">quantreg=</span><span class="ot">TRUE</span>, <span class="dt">num.trees=</span><span class="dv">150</span>, <span class="dt">seed=</span><span class="dv">1</span>)
m.zinc
<span class="co">#&gt; Ranger result</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt;  ranger(fm0, rm.zinc, quantreg = TRUE, num.trees = 150, seed = 1) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Type:                             Regression </span>
<span class="co">#&gt; Number of trees:                  150 </span>
<span class="co">#&gt; Sample size:                      155 </span>
<span class="co">#&gt; Number of independent variables:  155 </span>
<span class="co">#&gt; Mtry:                             12 </span>
<span class="co">#&gt; Target node size:                 5 </span>
<span class="co">#&gt; Variable importance mode:         none </span>
<span class="co">#&gt; Splitrule:                        variance </span>
<span class="co">#&gt; OOB prediction error (MSE):       67501 </span>
<span class="co">#&gt; R squared (OOB):                  0.499</span></code></pre></div>
<p>This shows that, using only buffer distance explains almost 50% of the variation in the target variable. To generate predictions for the <code>zinc</code> variable and using the RFsp model, we use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">q &lt;-<span class="st"> </span><span class="kw">c</span>((<span class="dv">1</span><span class="fl">-.682</span>)<span class="op">/</span><span class="dv">2</span>, <span class="fl">0.5</span>, <span class="dv">1</span><span class="op">-</span>(<span class="dv">1</span><span class="fl">-.682</span>)<span class="op">/</span><span class="dv">2</span>)
zinc.rfd &lt;-<span class="st"> </span><span class="kw">predict</span>(m.zinc, grid.dist0<span class="op">@</span>data, 
                    <span class="dt">type=</span><span class="st">&quot;quantiles&quot;</span>, <span class="dt">quantiles=</span>q)<span class="op">$</span>predictions
<span class="kw">str</span>(zinc.rfd)
<span class="co">#&gt;  num [1:3103, 1:3] 257 257 257 257 257 ...</span>
<span class="co">#&gt;  - attr(*, &quot;dimnames&quot;)=List of 2</span>
<span class="co">#&gt;   ..$ : NULL</span>
<span class="co">#&gt;   ..$ : chr [1:3] &quot;quantile= 0.159&quot; &quot;quantile= 0.5&quot; &quot;quantile= 0.841&quot;</span></code></pre></div>
<p>this will estimate 67% probability lower and upper limits and median value. Note that “median” can often be different from the “mean”, so, if you prefer to derive mean, then the <code>quantreg=FALSE</code> needs to be used as the Quantile Regression Forests approach can only derive median.</p>
<p>To be able to plot or export the predicted values as maps, we add them to the spatial pixels object:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">meuse.grid<span class="op">$</span>zinc_rfd =<span class="st"> </span>zinc.rfd[,<span class="dv">2</span>]
meuse.grid<span class="op">$</span>zinc_rfd_range =<span class="st"> </span>(zinc.rfd[,<span class="dv">3</span>]<span class="op">-</span>zinc.rfd[,<span class="dv">1</span>])<span class="op">/</span><span class="dv">2</span></code></pre></div>
<p>We can compare the RFsp approach with the model-based geostatistics approach (see e.g. <a href="http://leg.ufpr.br/geoR/geoRdoc/geoRintro.html">geoR package</a>), where we first decide about the transformation, then fit the variogram of the target variable <span class="citation">(Diggle and Ribeiro Jr <a href="#ref-Diggle2007Springer">2007</a>; Brown <a href="#ref-Brown2014JSS">2015</a>)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">zinc.geo &lt;-<span class="st"> </span><span class="kw">as.geodata</span>(meuse[<span class="st">&quot;zinc&quot;</span>])
ini.v &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">var</span>(<span class="kw">log1p</span>(zinc.geo<span class="op">$</span>data)),<span class="dv">500</span>)
zinc.vgm &lt;-<span class="st"> </span><span class="kw">likfit</span>(zinc.geo, <span class="dt">lambda=</span><span class="dv">0</span>, <span class="dt">ini=</span>ini.v, <span class="dt">cov.model=</span><span class="st">&quot;exponential&quot;</span>)
<span class="co">#&gt; kappa not used for the exponential correlation function</span>
<span class="co">#&gt; ---------------------------------------------------------------</span>
<span class="co">#&gt; likfit: likelihood maximisation using the function optim.</span>
<span class="co">#&gt; likfit: Use control() to pass additional</span>
<span class="co">#&gt;          arguments for the maximisation function.</span>
<span class="co">#&gt;         For further details see documentation for optim.</span>
<span class="co">#&gt; likfit: It is highly advisable to run this function several</span>
<span class="co">#&gt;         times with different initial values for the parameters.</span>
<span class="co">#&gt; likfit: </span><span class="al">WARNING</span><span class="co">: This step can be time demanding!</span>
<span class="co">#&gt; ---------------------------------------------------------------</span>
<span class="co">#&gt; likfit: end of numerical maximisation.</span>
zinc.vgm
<span class="co">#&gt; likfit: estimated model parameters:</span>
<span class="co">#&gt;       beta      tausq    sigmasq        phi </span>
<span class="co">#&gt; &quot;  6.1553&quot; &quot;  0.0164&quot; &quot;  0.5928&quot; &quot;500.0001&quot; </span>
<span class="co">#&gt; Practical Range with cor=0.05 for asymptotic range: 1498</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; likfit: maximised log-likelihood = -1014</span></code></pre></div>
<p>where <code>likfit</code> function fits a log-likelihood based variogram. Note that here we need to manually specify log-transformation via the <code>lambda</code> parameter. To generate predictions and kriging variance using geoR we run:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">locs &lt;-<span class="st"> </span>meuse.grid<span class="op">@</span>coords
zinc.ok &lt;-<span class="st"> </span><span class="kw">krige.conv</span>(zinc.geo, <span class="dt">locations=</span>locs, <span class="dt">krige=</span><span class="kw">krige.control</span>(<span class="dt">obj.model=</span>zinc.vgm))
<span class="co">#&gt; krige.conv: model with constant mean</span>
<span class="co">#&gt; krige.conv: performing the Box-Cox data transformation</span>
<span class="co">#&gt; krige.conv: back-transforming the predicted mean and variance</span>
<span class="co">#&gt; krige.conv: Kriging performed using global neighbourhood</span>
meuse.grid<span class="op">$</span>zinc_ok &lt;-<span class="st"> </span>zinc.ok<span class="op">$</span>predict
meuse.grid<span class="op">$</span>zinc_ok_range &lt;-<span class="st"> </span><span class="kw">sqrt</span>(zinc.ok<span class="op">$</span>krige.var)</code></pre></div>
<p>in this case geoR automatically back-transforms values to the original scale, which is a recommended feature. Comparison of predictions and prediction error maps produced using geoR (ordinary kriging) and RFsp (with buffer distances and by just using coordinates) is given in Fig. @ref(fig:comparison-OK-RF-zinc-meuse).</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_comparison_OK_RF_zinc_meuse.png" alt="Comparison of predictions based on ordinary kriging as implemented in the geoR package (left) and random forest (right) for Zinc concentrations, Meuse data set: (first row) predicted concentrations in log-scale and (second row) standard deviation of the prediction errors for OK and RF methods. Image source: Hengl et al. (2018) doi: 10.7717/peerj.5518." width="100%" />
<p class="caption">
(#fig:comparison-OK-RF-zinc-meuse)Comparison of predictions based on ordinary kriging as implemented in the geoR package (left) and random forest (right) for Zinc concentrations, Meuse data set: (first row) predicted concentrations in log-scale and (second row) standard deviation of the prediction errors for OK and RF methods. Image source: Hengl et al. (2018) doi: 10.7717/peerj.5518.
</p>
</div>
<p>From the plot above, it can be concluded that RFsp yields very similar results to those produced using ordinary kriging via geoR. There are differences between geoR and RFsp, however. These are:</p>
<ul>
<li>RF requires no transformation i.e. works equally well with skewed and normally distributed variables; in general RF, requires fewer statistical assumptions than model-based geostatistics,</li>
<li>RF prediction error variance on average shows somewhat stronger contrast than OK variance map i.e. it emphasizes isolated, less probable, local points much more than geoR,</li>
<li>RFsp is significantly more computationally demanding as distances need to be derived from each sampling point to all new prediction locations,</li>
<li>geoR uses global model parameters and, as such, prediction patterns are also relatively uniform, RFsp on the other hand (being tree-based) will produce patterns that match the data as much as possible.</li>
</ul>
</div>
<div id="spatial-prediction-2d-variable-with-covariates-using-rfsp" class="section level3">
<h3><span class="header-section-number">6.2.4</span> Spatial prediction 2D variable with covariates using RFsp</h3>
<p>Next, we can also consider adding additional covariates that describe soil forming processes or characteristics of the land to the list of buffer distances. For example, we can add covariates for surface water occurrence <span class="citation">(Pekel et al. <a href="#ref-pekel2016high">2016</a>)</span> and elevation (<a href="http://ahn.nl">AHN</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f1 =<span class="st"> &quot;extdata/Meuse_GlobalSurfaceWater_occurrence.tif&quot;</span>
f2 =<span class="st"> &quot;extdata/ahn.asc&quot;</span>
meuse.grid<span class="op">$</span>SW_occurrence &lt;-<span class="st"> </span><span class="kw">readGDAL</span>(f1)<span class="op">$</span>band1[meuse.grid<span class="op">@</span>grid.index]
<span class="co">#&gt; extdata/Meuse_GlobalSurfaceWater_occurrence.tif has GDAL driver GTiff </span>
<span class="co">#&gt; and has 104 rows and 78 columns</span>
meuse.grid<span class="op">$</span>AHN =<span class="st"> </span><span class="kw">readGDAL</span>(f2)<span class="op">$</span>band1[meuse.grid<span class="op">@</span>grid.index]
<span class="co">#&gt; extdata/ahn.asc has GDAL driver AAIGrid </span>
<span class="co">#&gt; and has 104 rows and 78 columns</span></code></pre></div>
<p>to convert all covariates to numeric values and fill in all missing pixels we use Principal Component transformation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">grids.spc =<span class="st"> </span>GSIF<span class="op">::</span><span class="kw">spc</span>(meuse.grid, <span class="kw">as.formula</span>(<span class="st">&quot;~ SW_occurrence + AHN + ffreq + dist&quot;</span>))
<span class="co">#&gt; Converting ffreq to indicators...</span>
<span class="co">#&gt; Converting covariates to principal components...</span></code></pre></div>
<p>so that we can fit a ranger model using both geographical covariates (buffer distances) and environmental covariates imported previously:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nms &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="kw">names</span>(grids.spc<span class="op">@</span>predicted), <span class="dt">collapse =</span> <span class="st">&quot;+&quot;</span>)
fm1 &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste</span>(<span class="st">&quot;zinc ~ &quot;</span>, dn0, <span class="st">&quot; + &quot;</span>, nms))
fm1
<span class="co">#&gt; zinc ~ layer.1 + layer.2 + layer.3 + layer.4 + layer.5 + layer.6 + </span>
<span class="co">#&gt;     layer.7 + layer.8 + layer.9 + layer.10 + layer.11 + layer.12 + </span>
<span class="co">#&gt;     layer.13 + layer.14 + layer.15 + layer.16 + layer.17 + layer.18 + </span>
<span class="co">#&gt;     layer.19 + layer.20 + layer.21 + layer.22 + layer.23 + layer.24 + </span>
<span class="co">#&gt;     layer.25 + layer.26 + layer.27 + layer.28 + layer.29 + layer.30 + </span>
<span class="co">#&gt;     layer.31 + layer.32 + layer.33 + layer.34 + layer.35 + layer.36 + </span>
<span class="co">#&gt;     layer.37 + layer.38 + layer.39 + layer.40 + layer.41 + layer.42 + </span>
<span class="co">#&gt;     layer.43 + layer.44 + layer.45 + layer.46 + layer.47 + layer.48 + </span>
<span class="co">#&gt;     layer.49 + layer.50 + layer.51 + layer.52 + layer.53 + layer.54 + </span>
<span class="co">#&gt;     layer.55 + layer.56 + layer.57 + layer.58 + layer.59 + layer.60 + </span>
<span class="co">#&gt;     layer.61 + layer.62 + layer.63 + layer.64 + layer.65 + layer.66 + </span>
<span class="co">#&gt;     layer.67 + layer.68 + layer.69 + layer.70 + layer.71 + layer.72 + </span>
<span class="co">#&gt;     layer.73 + layer.74 + layer.75 + layer.76 + layer.77 + layer.78 + </span>
<span class="co">#&gt;     layer.79 + layer.80 + layer.81 + layer.82 + layer.83 + layer.84 + </span>
<span class="co">#&gt;     layer.85 + layer.86 + layer.87 + layer.88 + layer.89 + layer.90 + </span>
<span class="co">#&gt;     layer.91 + layer.92 + layer.93 + layer.94 + layer.95 + layer.96 + </span>
<span class="co">#&gt;     layer.97 + layer.98 + layer.99 + layer.100 + layer.101 + </span>
<span class="co">#&gt;     layer.102 + layer.103 + layer.104 + layer.105 + layer.106 + </span>
<span class="co">#&gt;     layer.107 + layer.108 + layer.109 + layer.110 + layer.111 + </span>
<span class="co">#&gt;     layer.112 + layer.113 + layer.114 + layer.115 + layer.116 + </span>
<span class="co">#&gt;     layer.117 + layer.118 + layer.119 + layer.120 + layer.121 + </span>
<span class="co">#&gt;     layer.122 + layer.123 + layer.124 + layer.125 + layer.126 + </span>
<span class="co">#&gt;     layer.127 + layer.128 + layer.129 + layer.130 + layer.131 + </span>
<span class="co">#&gt;     layer.132 + layer.133 + layer.134 + layer.135 + layer.136 + </span>
<span class="co">#&gt;     layer.137 + layer.138 + layer.139 + layer.140 + layer.141 + </span>
<span class="co">#&gt;     layer.142 + layer.143 + layer.144 + layer.145 + layer.146 + </span>
<span class="co">#&gt;     layer.147 + layer.148 + layer.149 + layer.150 + layer.151 + </span>
<span class="co">#&gt;     layer.152 + layer.153 + layer.154 + layer.155 + PC1 + PC2 + </span>
<span class="co">#&gt;     PC3 + PC4 + PC5 + PC6</span>
ov.zinc1 &lt;-<span class="st"> </span><span class="kw">over</span>(meuse[<span class="st">&quot;zinc&quot;</span>], grids.spc<span class="op">@</span>predicted)
rm.zinc1 &lt;-<span class="st"> </span><span class="kw">do.call</span>(cbind, <span class="kw">list</span>(meuse<span class="op">@</span>data[<span class="st">&quot;zinc&quot;</span>], ov.zinc, ov.zinc1))</code></pre></div>
<p>this finally gives:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1.zinc &lt;-<span class="st"> </span><span class="kw">ranger</span>(fm1, rm.zinc1, <span class="dt">importance=</span><span class="st">&quot;impurity&quot;</span>, 
                  <span class="dt">quantreg=</span><span class="ot">TRUE</span>, <span class="dt">num.trees=</span><span class="dv">150</span>, <span class="dt">seed=</span><span class="dv">1</span>)
m1.zinc
<span class="co">#&gt; Ranger result</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt;  ranger(fm1, rm.zinc1, importance = &quot;impurity&quot;, quantreg = TRUE,      num.trees = 150, seed = 1) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Type:                             Regression </span>
<span class="co">#&gt; Number of trees:                  150 </span>
<span class="co">#&gt; Sample size:                      155 </span>
<span class="co">#&gt; Number of independent variables:  161 </span>
<span class="co">#&gt; Mtry:                             12 </span>
<span class="co">#&gt; Target node size:                 5 </span>
<span class="co">#&gt; Variable importance mode:         impurity </span>
<span class="co">#&gt; Splitrule:                        variance </span>
<span class="co">#&gt; OOB prediction error (MSE):       53437 </span>
<span class="co">#&gt; R squared (OOB):                  0.603</span></code></pre></div>
<p>which demonstrates that there is a slight improvement relative to using only buffer distances as covariates. We can further evaluate this model to see which specific points and covariates are most important for spatial predictions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xl &lt;-<span class="st"> </span><span class="kw">as.list</span>(ranger<span class="op">::</span><span class="kw">importance</span>(m1.zinc))
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="dt">oma=</span><span class="kw">c</span>(<span class="fl">0.7</span>,<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">4</span>,<span class="fl">3.5</span>,<span class="dv">1</span>,<span class="dv">0</span>))
<span class="kw">plot</span>(vv &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">data.frame</span>(xl[<span class="kw">order</span>(<span class="kw">unlist</span>(xl), <span class="dt">decreasing=</span><span class="ot">TRUE</span>)[<span class="dv">10</span><span class="op">:</span><span class="dv">1</span>]])), <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, 
     <span class="dt">type =</span> <span class="st">&quot;n&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Variable Importance (Node Impurity)&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">lty =</span> <span class="st">&quot;dotted&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;grey60&quot;</span>)
<span class="kw">points</span>(vv, <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)
<span class="kw">axis</span>(<span class="dv">2</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">labels =</span> <span class="kw">dimnames</span>(vv)[[<span class="dv">1</span>]], <span class="dt">las =</span> <span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Soilmapping_using_mla_files/figure-html/rf-variableImportance-1.png" alt="Variable importance plot for mapping zinc content based on the Meuse data set." width="100%" />
<p class="caption">
(#fig:rf-variableImportance)Variable importance plot for mapping zinc content based on the Meuse data set.
</p>
</div>
<p>which shows, for example, that locations 54, 59 and 53 are the most influential points, and these are almost equally as important as the environmental covariates (PC2–PC4).</p>
<p>This type of modeling can be best compared to using Universal Kriging or Regression-Kriging in the geoR package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">zinc.geo<span class="op">$</span>covariate =<span class="st"> </span>ov.zinc1
sic.t =<span class="st"> </span><span class="er">~</span><span class="st"> </span>PC1 <span class="op">+</span><span class="st"> </span>PC2 <span class="op">+</span><span class="st"> </span>PC3 <span class="op">+</span><span class="st"> </span>PC4 <span class="op">+</span><span class="st"> </span>PC5
zinc1.vgm &lt;-<span class="st"> </span><span class="kw">likfit</span>(zinc.geo, <span class="dt">trend =</span> sic.t, <span class="dt">lambda=</span><span class="dv">0</span>, <span class="dt">ini=</span>ini.v, <span class="dt">cov.model=</span><span class="st">&quot;exponential&quot;</span>)
<span class="co">#&gt; kappa not used for the exponential correlation function</span>
<span class="co">#&gt; ---------------------------------------------------------------</span>
<span class="co">#&gt; likfit: likelihood maximisation using the function optim.</span>
<span class="co">#&gt; likfit: Use control() to pass additional</span>
<span class="co">#&gt;          arguments for the maximisation function.</span>
<span class="co">#&gt;         For further details see documentation for optim.</span>
<span class="co">#&gt; likfit: It is highly advisable to run this function several</span>
<span class="co">#&gt;         times with different initial values for the parameters.</span>
<span class="co">#&gt; likfit: </span><span class="al">WARNING</span><span class="co">: This step can be time demanding!</span>
<span class="co">#&gt; ---------------------------------------------------------------</span>
<span class="co">#&gt; likfit: end of numerical maximisation.</span>
zinc1.vgm
<span class="co">#&gt; likfit: estimated model parameters:</span>
<span class="co">#&gt;      beta0      beta1      beta2      beta3      beta4      beta5 </span>
<span class="co">#&gt; &quot;  5.6929&quot; &quot; -0.4351&quot; &quot;  0.0002&quot; &quot; -0.0791&quot; &quot; -0.0485&quot; &quot; -0.3725&quot; </span>
<span class="co">#&gt;      tausq    sigmasq        phi </span>
<span class="co">#&gt; &quot;  0.0566&quot; &quot;  0.1992&quot; &quot;499.9990&quot; </span>
<span class="co">#&gt; Practical Range with cor=0.05 for asymptotic range: 1498</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; likfit: maximised log-likelihood = -980</span></code></pre></div>
<p>this time geostatistical modeling produces an estimate of beta (regression coefficients) and variogram parameters (all estimated at once). Predictions using this Universal Kriging model can be generated by:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">KC =<span class="st"> </span><span class="kw">krige.control</span>(<span class="dt">trend.d =</span> sic.t, 
                   <span class="dt">trend.l =</span> <span class="op">~</span><span class="st"> </span>grids.spc<span class="op">@</span>predicted<span class="op">$</span>PC1 <span class="op">+</span><span class="st"> </span>
<span class="st">                     </span>grids.spc<span class="op">@</span>predicted<span class="op">$</span>PC2 <span class="op">+</span><span class="st"> </span>grids.spc<span class="op">@</span>predicted<span class="op">$</span>PC3 <span class="op">+</span><span class="st"> </span>
<span class="st">                     </span>grids.spc<span class="op">@</span>predicted<span class="op">$</span>PC4 <span class="op">+</span><span class="st"> </span>grids.spc<span class="op">@</span>predicted<span class="op">$</span>PC5, 
                   <span class="dt">obj.model =</span> zinc1.vgm)
zinc.uk &lt;-<span class="st"> </span><span class="kw">krige.conv</span>(zinc.geo, <span class="dt">locations=</span>locs, <span class="dt">krige=</span>KC)
<span class="co">#&gt; krige.conv: model with mean defined by covariates provided by the user</span>
<span class="co">#&gt; krige.conv: performing the Box-Cox data transformation</span>
<span class="co">#&gt; krige.conv: back-transforming the predicted mean and variance</span>
<span class="co">#&gt; krige.conv: Kriging performed using global neighbourhood</span>
meuse.grid<span class="op">$</span>zinc_UK =<span class="st"> </span>zinc.uk<span class="op">$</span>predict</code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/Fig_RF_covs_bufferdist_zinc_meuse.png" alt="Comparison of predictions (median values) produced using random forest and covariates only (left), and random forest with combined covariates and buffer distances (right)." width="100%" />
<p class="caption">
(#fig:RF-covs-bufferdist-zinc-meuse)Comparison of predictions (median values) produced using random forest and covariates only (left), and random forest with combined covariates and buffer distances (right).
</p>
</div>
<p>again, overall predictions (the spatial patterns) look fairly similar (Fig. @ref(fig:RF-covs-bufferdist-zinc-meuse)). The difference between using geoR and RFsp is that, in the case of RFsp, there are fewer choices and fewer assumptions required. Also, RFsp permits the relationship between covariates and geographical distances to be fitted all at once. This makes RFsp, in general, less cumbersome than model-based geostatistics, but then also more of a “black-box” system to a geostatistician.</p>
</div>
<div id="spatial-prediction-of-binomial-variables" class="section level3">
<h3><span class="header-section-number">6.2.5</span> Spatial prediction of binomial variables</h3>
<p>RFsp can also be used to predict (map the distribution of) binomial variables i.e. variables having only two states (TRUE or FALSE). In the model-based geostatistics equivalent methods are indicator kriging and similar. Consider for example soil type 1 from the meuse data set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">meuse<span class="op">@</span>data =<span class="st"> </span><span class="kw">cbind</span>(meuse<span class="op">@</span>data, <span class="kw">data.frame</span>(<span class="kw">model.matrix</span>(<span class="op">~</span>soil<span class="dv">-1</span>, meuse<span class="op">@</span>data)))
<span class="kw">summary</span>(<span class="kw">as.factor</span>(meuse<span class="op">$</span>soil1))
<span class="co">#&gt;  0  1 </span>
<span class="co">#&gt; 58 97</span></code></pre></div>
<p>in this case class <code>soil1</code> is the dominant soil type in the area. To produce a map of <code>soil1</code> using RFsp we have now two options:</p>
<ul>
<li><em>Option 1</em>: treat the binomial variable as numeric variable with 0 / 1 values (thus a regression problem),</li>
<li><em>Option 2</em>: treat the binomial variable as a factor variable with a single class (thus a classification problem),</li>
</ul>
<p>In the case of Option 1, we model <code>soil1</code> as:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fm.s1 &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste</span>(<span class="st">&quot;soil1 ~ &quot;</span>, <span class="kw">paste</span>(<span class="kw">names</span>(grid.dist0), <span class="dt">collapse=</span><span class="st">&quot;+&quot;</span>), 
                         <span class="st">&quot; + SW_occurrence + dist&quot;</span>))
rm.s1 &lt;-<span class="st"> </span><span class="kw">do.call</span>(cbind, <span class="kw">list</span>(meuse<span class="op">@</span>data[<span class="st">&quot;soil1&quot;</span>], 
                             <span class="kw">over</span>(meuse[<span class="st">&quot;soil1&quot;</span>], meuse.grid), 
                             <span class="kw">over</span>(meuse[<span class="st">&quot;soil1&quot;</span>], grid.dist0)))
m1.s1 &lt;-<span class="st"> </span><span class="kw">ranger</span>(fm.s1, rm.s1, <span class="dt">mtry=</span><span class="dv">22</span>, <span class="dt">num.trees=</span><span class="dv">150</span>, <span class="dt">seed=</span><span class="dv">1</span>, <span class="dt">quantreg=</span><span class="ot">TRUE</span>)
m1.s1
<span class="co">#&gt; Ranger result</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt;  ranger(fm.s1, rm.s1, mtry = 22, num.trees = 150, seed = 1, quantreg = TRUE) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Type:                             Regression </span>
<span class="co">#&gt; Number of trees:                  150 </span>
<span class="co">#&gt; Sample size:                      155 </span>
<span class="co">#&gt; Number of independent variables:  157 </span>
<span class="co">#&gt; Mtry:                             22 </span>
<span class="co">#&gt; Target node size:                 5 </span>
<span class="co">#&gt; Variable importance mode:         none </span>
<span class="co">#&gt; Splitrule:                        variance </span>
<span class="co">#&gt; OOB prediction error (MSE):       0.0579 </span>
<span class="co">#&gt; R squared (OOB):                  0.754</span></code></pre></div>
<p>which results in a model that explains about 75% of variability in the <code>soil1</code> values. We set <code>quantreg=TRUE</code> so that we can also derive lower and upper prediction intervals following the quantile regression random forest <span class="citation">(Meinshausen <a href="#ref-meinshausen2006quantile">2006</a>)</span>.</p>
<p>In the case of Option 2, we treat the binomial variable as a factor variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fm.s1c &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste</span>(<span class="st">&quot;soil1c ~ &quot;</span>, 
                           <span class="kw">paste</span>(<span class="kw">names</span>(grid.dist0), <span class="dt">collapse=</span><span class="st">&quot;+&quot;</span>), 
                           <span class="st">&quot; + SW_occurrence + dist&quot;</span>))
rm.s1<span class="op">$</span>soil1c =<span class="st"> </span><span class="kw">as.factor</span>(rm.s1<span class="op">$</span>soil1)
m2.s1 &lt;-<span class="st"> </span><span class="kw">ranger</span>(fm.s1c, rm.s1, <span class="dt">mtry=</span><span class="dv">22</span>, <span class="dt">num.trees=</span><span class="dv">150</span>, <span class="dt">seed=</span><span class="dv">1</span>, 
                <span class="dt">probability=</span><span class="ot">TRUE</span>, <span class="dt">keep.inbag=</span><span class="ot">TRUE</span>)
m2.s1
<span class="co">#&gt; Ranger result</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt;  ranger(fm.s1c, rm.s1, mtry = 22, num.trees = 150, seed = 1, probability = TRUE,      keep.inbag = TRUE) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Type:                             Probability estimation </span>
<span class="co">#&gt; Number of trees:                  150 </span>
<span class="co">#&gt; Sample size:                      155 </span>
<span class="co">#&gt; Number of independent variables:  157 </span>
<span class="co">#&gt; Mtry:                             22 </span>
<span class="co">#&gt; Target node size:                 10 </span>
<span class="co">#&gt; Variable importance mode:         none </span>
<span class="co">#&gt; Splitrule:                        gini </span>
<span class="co">#&gt; OOB prediction error (Brier s.):  0.0586</span></code></pre></div>
<p>which shows that the Out of Bag prediction error (classification error) is (only) 0.06 (in the probability scale). Note that, it is not easy to compare the results of the regression and classification OOB errors as these are conceptually different. Also note that we turn on <code>keep.inbag = TRUE</code> so that ranger can estimate the classification errors using the Jackknife-after-Bootstrap method <span class="citation">(Wager, Hastie, and Efron <a href="#ref-wager2014confidence">2014</a>)</span>. <code>quantreg=TRUE</code> obviously would not work here since it is a classification and not a regression problem.</p>
<p>To produce predictions using the two options we use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred.regr &lt;-<span class="st"> </span><span class="kw">predict</span>(m1.s1, <span class="kw">cbind</span>(meuse.grid<span class="op">@</span>data, grid.dist0<span class="op">@</span>data), <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)
pred.clas &lt;-<span class="st"> </span><span class="kw">predict</span>(m2.s1, <span class="kw">cbind</span>(meuse.grid<span class="op">@</span>data, grid.dist0<span class="op">@</span>data), <span class="dt">type=</span><span class="st">&quot;se&quot;</span>)</code></pre></div>
<p>in principle, the two options to predicting the distribution of the binomial variable are mathematically equivalent and should lead to the same predictions (also shown in the map below). In practice, there can be some small differences in numbers, due to rounding effect or random start effects.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_comparison_uncertainty_Binomial_variables_meuse.png" alt="Comparison of predictions for soil class “1” produced using (left) regression and prediction of the median value, (middle) regression and prediction of response value, and (right) classification with probabilities." width="90%" />
<p class="caption">
(#fig:comparison-uncertainty-Binomial)Comparison of predictions for soil class “1” produced using (left) regression and prediction of the median value, (middle) regression and prediction of response value, and (right) classification with probabilities.
</p>
</div>
<p>This shows that predicting binomial variables using RFsp can be implemented both as a classification and a regression problem and both are possible to implement using the ranger package and both should lead to the same results.</p>
</div>
<div id="spatial-prediction-of-soil-types" class="section level3">
<h3><span class="header-section-number">6.2.6</span> Spatial prediction of soil types</h3>
<p>Spatial prediction of a categorical variable using ranger is a form of classification problem. The target variable contains multiple states (3 in this case), but the model still follows the same formulation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fm.s =<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste</span>(<span class="st">&quot;soil ~ &quot;</span>, <span class="kw">paste</span>(<span class="kw">names</span>(grid.dist0), <span class="dt">collapse=</span><span class="st">&quot;+&quot;</span>), 
                        <span class="st">&quot; + SW_occurrence + dist&quot;</span>))
fm.s
<span class="co">#&gt; soil ~ layer.1 + layer.2 + layer.3 + layer.4 + layer.5 + layer.6 + </span>
<span class="co">#&gt;     layer.7 + layer.8 + layer.9 + layer.10 + layer.11 + layer.12 + </span>
<span class="co">#&gt;     layer.13 + layer.14 + layer.15 + layer.16 + layer.17 + layer.18 + </span>
<span class="co">#&gt;     layer.19 + layer.20 + layer.21 + layer.22 + layer.23 + layer.24 + </span>
<span class="co">#&gt;     layer.25 + layer.26 + layer.27 + layer.28 + layer.29 + layer.30 + </span>
<span class="co">#&gt;     layer.31 + layer.32 + layer.33 + layer.34 + layer.35 + layer.36 + </span>
<span class="co">#&gt;     layer.37 + layer.38 + layer.39 + layer.40 + layer.41 + layer.42 + </span>
<span class="co">#&gt;     layer.43 + layer.44 + layer.45 + layer.46 + layer.47 + layer.48 + </span>
<span class="co">#&gt;     layer.49 + layer.50 + layer.51 + layer.52 + layer.53 + layer.54 + </span>
<span class="co">#&gt;     layer.55 + layer.56 + layer.57 + layer.58 + layer.59 + layer.60 + </span>
<span class="co">#&gt;     layer.61 + layer.62 + layer.63 + layer.64 + layer.65 + layer.66 + </span>
<span class="co">#&gt;     layer.67 + layer.68 + layer.69 + layer.70 + layer.71 + layer.72 + </span>
<span class="co">#&gt;     layer.73 + layer.74 + layer.75 + layer.76 + layer.77 + layer.78 + </span>
<span class="co">#&gt;     layer.79 + layer.80 + layer.81 + layer.82 + layer.83 + layer.84 + </span>
<span class="co">#&gt;     layer.85 + layer.86 + layer.87 + layer.88 + layer.89 + layer.90 + </span>
<span class="co">#&gt;     layer.91 + layer.92 + layer.93 + layer.94 + layer.95 + layer.96 + </span>
<span class="co">#&gt;     layer.97 + layer.98 + layer.99 + layer.100 + layer.101 + </span>
<span class="co">#&gt;     layer.102 + layer.103 + layer.104 + layer.105 + layer.106 + </span>
<span class="co">#&gt;     layer.107 + layer.108 + layer.109 + layer.110 + layer.111 + </span>
<span class="co">#&gt;     layer.112 + layer.113 + layer.114 + layer.115 + layer.116 + </span>
<span class="co">#&gt;     layer.117 + layer.118 + layer.119 + layer.120 + layer.121 + </span>
<span class="co">#&gt;     layer.122 + layer.123 + layer.124 + layer.125 + layer.126 + </span>
<span class="co">#&gt;     layer.127 + layer.128 + layer.129 + layer.130 + layer.131 + </span>
<span class="co">#&gt;     layer.132 + layer.133 + layer.134 + layer.135 + layer.136 + </span>
<span class="co">#&gt;     layer.137 + layer.138 + layer.139 + layer.140 + layer.141 + </span>
<span class="co">#&gt;     layer.142 + layer.143 + layer.144 + layer.145 + layer.146 + </span>
<span class="co">#&gt;     layer.147 + layer.148 + layer.149 + layer.150 + layer.151 + </span>
<span class="co">#&gt;     layer.152 + layer.153 + layer.154 + layer.155 + SW_occurrence + </span>
<span class="co">#&gt;     dist</span></code></pre></div>
<p>to produce probability maps per soil class, we need to turn on the <code>probability=TRUE</code> option:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rm.s &lt;-<span class="st"> </span><span class="kw">do.call</span>(cbind, <span class="kw">list</span>(meuse<span class="op">@</span>data[<span class="st">&quot;soil&quot;</span>], 
                            <span class="kw">over</span>(meuse[<span class="st">&quot;soil&quot;</span>], meuse.grid), 
                            <span class="kw">over</span>(meuse[<span class="st">&quot;soil&quot;</span>], grid.dist0)))
m.s &lt;-<span class="st"> </span><span class="kw">ranger</span>(fm.s, rm.s, <span class="dt">mtry=</span><span class="dv">22</span>, <span class="dt">num.trees=</span><span class="dv">150</span>, <span class="dt">seed=</span><span class="dv">1</span>, 
              <span class="dt">probability=</span><span class="ot">TRUE</span>, <span class="dt">keep.inbag=</span><span class="ot">TRUE</span>)
m.s
<span class="co">#&gt; Ranger result</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt;  ranger(fm.s, rm.s, mtry = 22, num.trees = 150, seed = 1, probability = TRUE,      keep.inbag = TRUE) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Type:                             Probability estimation </span>
<span class="co">#&gt; Number of trees:                  150 </span>
<span class="co">#&gt; Sample size:                      155 </span>
<span class="co">#&gt; Number of independent variables:  157 </span>
<span class="co">#&gt; Mtry:                             22 </span>
<span class="co">#&gt; Target node size:                 10 </span>
<span class="co">#&gt; Variable importance mode:         none </span>
<span class="co">#&gt; Splitrule:                        gini </span>
<span class="co">#&gt; OOB prediction error (Brier s.):  0.0922</span></code></pre></div>
<p>this shows that the model is successful with an OOB prediction error of about 0.09. This number is rather abstract so we can also check the actual classification accuracy using hard classes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.s0 &lt;-<span class="st"> </span><span class="kw">ranger</span>(fm.s, rm.s, <span class="dt">mtry=</span><span class="dv">22</span>, <span class="dt">num.trees=</span><span class="dv">150</span>, <span class="dt">seed=</span><span class="dv">1</span>)
m.s0
<span class="co">#&gt; Ranger result</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt;  ranger(fm.s, rm.s, mtry = 22, num.trees = 150, seed = 1) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Type:                             Classification </span>
<span class="co">#&gt; Number of trees:                  150 </span>
<span class="co">#&gt; Sample size:                      155 </span>
<span class="co">#&gt; Number of independent variables:  157 </span>
<span class="co">#&gt; Mtry:                             22 </span>
<span class="co">#&gt; Target node size:                 1 </span>
<span class="co">#&gt; Variable importance mode:         none </span>
<span class="co">#&gt; Splitrule:                        gini </span>
<span class="co">#&gt; OOB prediction error:             10.32 %</span></code></pre></div>
<p>which shows that the classification or mapping accuracy for hard classes is about 90%. We can produce predictions of probabilities per class by running:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred.soil_rfc =<span class="st"> </span><span class="kw">predict</span>(m.s, <span class="kw">cbind</span>(meuse.grid<span class="op">@</span>data, grid.dist0<span class="op">@</span>data), <span class="dt">type=</span><span class="st">&quot;se&quot;</span>)
pred.grids =<span class="st"> </span>meuse.grid[<span class="st">&quot;soil&quot;</span>]
pred.grids<span class="op">@</span>data =<span class="st"> </span><span class="kw">do.call</span>(cbind, <span class="kw">list</span>(pred.grids<span class="op">@</span>data, 
                                      <span class="kw">data.frame</span>(pred.soil_rfc<span class="op">$</span>predictions),
                                      <span class="kw">data.frame</span>(pred.soil_rfc<span class="op">$</span>se)))
<span class="kw">names</span>(pred.grids) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;soil&quot;</span>, <span class="kw">paste0</span>(<span class="st">&quot;pred_soil&quot;</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>), <span class="kw">paste0</span>(<span class="st">&quot;se_soil&quot;</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>))
<span class="kw">str</span>(pred.grids<span class="op">@</span>data)
<span class="co">#&gt; &#39;data.frame&#39;:    3103 obs. of  7 variables:</span>
<span class="co">#&gt;  $ soil      : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 1 1 1 1 1 1 1 1 1 ...</span>
<span class="co">#&gt;  $ pred_soil1: num  0.716 0.713 0.713 0.693 0.713 ...</span>
<span class="co">#&gt;  $ pred_soil2: num  0.246 0.256 0.256 0.27 0.256 ...</span>
<span class="co">#&gt;  $ pred_soil3: num  0.0374 0.0307 0.0307 0.0374 0.0307 ...</span>
<span class="co">#&gt;  $ se_soil1  : num  0.1798 0.1684 0.1684 0.0903 0.1684 ...</span>
<span class="co">#&gt;  $ se_soil2  : num  0.1446 0.0808 0.0808 0.0796 0.0808 ...</span>
<span class="co">#&gt;  $ se_soil3  : num  0.0414 0.0413 0.0413 0.0414 0.0413 ...</span></code></pre></div>
<p>where <code>pred_soil1</code> is the probability of occurrence of class 1 and <code>se_soil1</code> is the standard error of prediction for the <code>pred_soil1</code> based on the Jackknife-after-Bootstrap method <span class="citation">(Wager, Hastie, and Efron <a href="#ref-wager2014confidence">2014</a>)</span>. The first column in <code>pred.grids</code> contains the existing map of <code>soil</code> with hard classes only.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_comparison_uncertainty_Factor_variables_meuse.png" alt="Predictions of soil types for the meuse data set based on the RFsp: (above) probability for three soil classes, and (below) derived standard errors per class." width="90%" />
<p class="caption">
(#fig:comparison-uncertainty-Factor)Predictions of soil types for the meuse data set based on the RFsp: (above) probability for three soil classes, and (below) derived standard errors per class.
</p>
</div>
<p>Spatial prediction of binomial and factor-type variables is straight forward with ranger / RFsp: buffer distance and spatial-autocorrelation can be incorporated simultaneously as opposed to geostatistical packages, where link functions and/or indicator kriging would need to be used, and which require that variograms are fitted per class.</p>
</div>
</div>
<div id="summary-points-3" class="section level2">
<h2><span class="header-section-number">6.3</span> Summary points</h2>
<p>In summary, MLA’s are fairly attractive for soil mapping and soil modelling problems in general, as they often perform better than standard linear models (as previously recognized by <span class="citation">Moran and Bui (<a href="#ref-moran2002spatial">2002</a>)</span> and <span class="citation">Henderson et al. (<a href="#ref-Henderson2004Geoderma">2004</a>)</span>) Some recent comparisons of MLA’s performance for operational soil mapping can be found in <span class="citation">Nussbaum et al. (<a href="#ref-nussbaum2018evaluation">2018</a>)</span>). MLA’s often perform better than linear techniques for soil mapping; possibly for the following three reasons:</p>
<ol style="list-style-type: decimal">
<li>Non-linear relationships between soil forming factors and soil properties can be more efficiently modeled using MLA’s,</li>
<li>Tree-based MLA’s (random forest, gradient boosting, cubist) are suitable for representing <em>local</em> soil-landscape relationships, nested within larger areas, which is often important for achieving accuracy of spatial prediction models,</li>
<li>In the case of MLA, statistical properties such as multicolinearity and non-Gaussian distribution are dealt with inside the models, which simplifies statistical modeling steps,</li>
</ol>
<p>On the other hand, MLA’s can be computationally very intensive and consequently require careful planning, especially as the number of points goes beyond a few thousand and the number of covariates beyond a dozen. Note also that some MLA’s, such as for example Support Vector Machines (<code>svm</code>), are computationally very intensive and are probably not well suited for large data sets.</p>
<p>Within PSM, there is increasing interest in doing ensemble predictions, model averages or model stacks. Stacking models can improve upon individual best techniques, achieving improvements of up to 30%, with the additional demands consistinf of only higher computation loads <span class="citation">(Michailidis <a href="#ref-michailidis2017investigating">2017</a>)</span>. In the example above, the extensive computational load from derivation of models and product predictions already achieved improved accuracies, making increasing computing loads further a matter of diminishing returns. Some interesting Machine Learning Algorithms for soil mapping based on regression include: Random Forest <span class="citation">(Biau and Scornet <a href="#ref-Biau2016">2016</a>)</span>, Gradient Boosting Machine (GBM) <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-hastie2009elements">2009</a>)</span>, Cubist <span class="citation">(Kuhn et al. <a href="#ref-kuhn2014cubist">2014</a>)</span>, Generalized Boosted Regression Models <span class="citation">(Ridgeway <a href="#ref-ridgeway2010gbm">2018</a>)</span>, Support Vector Machines <span class="citation">(Chang and Lin <a href="#ref-chang2011libsvm">2011</a>)</span>, and the Extreme Gradient Boosting approach available via the xgboost package <span class="citation">(Chen and Guestrin <a href="#ref-2016arXiv160302754C">2016</a>)</span>. None of these techniques is universally recognized as the best spatial predictor for all soil variables. Instead, we recommend comparing MLA’s using robust cross-validation methods as explained above. Also combining MLA’s into ensemble predictions might not be beneficial in all situations. Less is better sometimes.</p>
<p>The RFsp method seems to be suitable for generating spatial and spatiotemporal predictions. Computing time, however, can be demanding and working with data sets with &gt;1000 point locations (hence 1000+ buffer distance maps) is probably not yet feasible or recommended. Also cross-validation of accuracy of predictions produced using RFsp needs to be implemented using leave-location-out CV to account for spatial autocorrelation in data. The key to the success of the RFsp framework might be the training data quality — especially quality of spatial sampling (to minimize extrapolation problems and any type of bias in data), and quality of model validation (to ensure that accuracy is not effected by over-fitting). For all other details about RFsp refer to <span class="citation">T. Hengl, Nussbaum, et al. (<a href="#ref-Hengl2018RFsp">2018</a>)</span>.</p>
<!--chapter:end:Soilmapping_using_mla.Rmd-->
</div>
</div>
<div id="SOC-chapter" class="section level1">
<h1><span class="header-section-number">7</span> Spatial prediction and assessment of Soil Organic Carbon</h1>
<p><em>Edited by: Hengl T. &amp; Sanderman J.</em></p>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">7.1</span> Introduction</h2>
<p>This chapter was prepared as supplementary material for the <span class="citation">Sanderman, Hengl, and Fiske (<a href="#ref-sanderman2018soil">2018</a>)</span> article. It explains how to map Soil Organic Carbon Stocks (OCS) using soil samples (point data). It also demonstrates derivation of values at both the site level (per profile) and by using raster calculus (per pixel). We then illustrate how to estimate total OCS for any area of interest (which can be a field plot, farm and/or administrative region). For an introduction to soil mapping using Machine Learning Algorithms refer to section @ref(soilmapping-using-mla). To access ISRIC’s global compilation of soil profiles please refer to: <a href="http://www.isric.org/explore/wosis" class="uri">http://www.isric.org/explore/wosis</a>.</p>
</div>
<div id="measurement-and-derivation-of-soil-organic-carbon" class="section level2">
<h2><span class="header-section-number">7.2</span> Measurement and derivation of soil organic carbon</h2>
<p>Carbon below ground can be organic or non-organic/ mineral (usually carbonates and bicarbonates) i.e. CaCO<span class="math inline">\(_3\)</span> in the rocks. Organic carbon stocks below ground (0–2 m) in terrestrial ecosystems consist of two major components:</p>
<ol style="list-style-type: decimal">
<li>Living organism biomass i.e. mainly:
<ul>
<li>Plant roots,</li>
<li>Microbial biomass <span class="citation">(Xu, Thornton, and Post <a href="#ref-xu2013global">2013</a>)</span>,</li>
</ul></li>
<li>Plant and animal residues at various stages of decomposition (organic matter).</li>
</ol>
<p><span class="citation">Xu, Thornton, and Post (<a href="#ref-xu2013global">2013</a>)</span> estimated that total global microbial biomass is about 17 Pg C, which is only about 2% of the total for organic matter. Therefore, the amounts of C in microbial biomass can reasonably be considered negligible, in comparison to the total C stock. However, if one includes all living organisms, and especially tree roots, then the portion of total C found in living organisms could be more significant, especially in areas under dense forests.</p>
<p>Soil Organic Carbon Stock (<strong>OCS</strong>) is the mass of soil organic carbon per standard area and for a specific depth interval, usually expressed in kg/m<span class="math inline">\(^2\)</span> or t/ha. It can be derived using (laboratory and/or field) measurement of soil organic carbon content (ORC; expressed in % or g/kg of &lt;2mm mineral earth), then taking into account bulk density (BLD), thickness of the soil layer, and volume percentage of coarse fragments (CRF) <span class="citation">(Nelson and Sommers <a href="#ref-Nelson1982">1982</a>; Poeplau, Vos, and Axel <a href="#ref-poeplau2017soil">2017</a>)</span>:</p>
<span class="math display">\[\begin{equation}
{\rm OCS} [{\rm kg/m^2}] = {\rm ORC} [\%] / 100 \cdot {\rm BLD} [{\rm kg/m^3}] \cdot (1- {\rm CRF} [\%]/100) \cdot {\rm HOT} [m]
(\#eq:ocs)
\end{equation}\]</span>
<p>Note that if one has soil organic carbon content measured in g/kg then one should divide by 1000 instead of 100. A correction for gravel content is necessary because only material less than 2 mm is analyzed for ORC concentration. Ignoring the gravel content can result in an overestimation of the organic carbon stock. Note also that OCS always refers to a specific depth interval or horizon thickness (HOT), e.g.:</p>
<ul>
<li>kg/m<span class="math inline">\(^2\)</span> for depth 0–30 cm <span class="citation">(Berhongaray and Alvarez <a href="#ref-berhongaray2013ipcc">2013</a>)</span>,</li>
</ul>
<p>Values of OCS in kg/m<span class="math inline">\(^2\)</span> can also be expressed in tons/ha units, in which case a simple conversion formula can be applied:</p>
<span class="math display">\[\begin{equation}
1 \cdot {\rm kg/m^2} = 10 \cdot {\rm tons/ha}
(\#eq:kgm2)
\end{equation}\]</span>
<p>Total OCS for an area of interest can be derived by multiplying OCS by total area e.g.:</p>
<span class="math display">\[\begin{equation}
120 {\rm tons/ha} \cdot 1 {\rm km^2} = 120 \cdot 100 = 12,000 {\rm tons}
(\#eq:tonsha)
\end{equation}\]</span>
<p>Another way to express soil organic carbon is through <strong>soil organic carbon density</strong> (<strong>OCD</strong> in kg/m<span class="math inline">\(^3\)</span>), which is in fact equivalent to OCS divided by the horizon thickness:</p>
<span class="math display">\[\begin{equation}
{\rm OCD} [{\rm kg/m^3}] = {\rm ORC} [\%]/100 \cdot {\rm BLD} [{\rm kg/m^3}] \cdot (1- {\rm CRF} [\%]/100) = {\rm OCS} / {\rm HOT}
(\#eq:ocd)
\end{equation}\]</span>
<p>While OCS is a summary measure of SOC, always associated with a specific depth interval, OCD is a relative measure of soil organic carbon distribution and can be associated with any support size i.e. to an arbitrary depth. In principle, OCD (kg/m<span class="math inline">\(^3\)</span>) is strongly correlated with ORC (g/kg) as indicated in the figure below. However, depending on soil mineralogy and coarse fragment content, OCD can be lower or higher than what the smoothed line indicates (notice the range of values around the smoothed line is relatively wide). It is important to understand, however, that, as long as ORC, BLD and CRF are known, one can convert the values from ORC to OCD and OCS and <em>vice versa</em>, without loosing any information about the soil organic carbon stock.</p>
<div class="figure" style="text-align: center">
<img src="figures/rplot_soilcarbon_density_vs_orc.png" alt="Correlation between soil organic carbon density and soil organic carbon content (displayed on a log-scale) created using a global compilations of soil profile data (WoSIS). Values 1, 2, 3, 4, 5 and 6 in the plot (log scale) correspond to values 2, 6, 19, 54, 147 and 402. Note that for ORC &gt;12%, the OCD line flattens, which means that, organic carbon density practically stops to increase with the increase of ORC content." width="55%" />
<p class="caption">
(#fig:carbon-density-plot)Correlation between soil organic carbon density and soil organic carbon content (displayed on a log-scale) created using a global compilations of soil profile data (WoSIS). Values 1, 2, 3, 4, 5 and 6 in the plot (log scale) correspond to values 2, 6, 19, 54, 147 and 402. Note that for ORC &gt;12%, the OCD line flattens, which means that, organic carbon density practically stops to increase with the increase of ORC content.
</p>
</div>
<p>In summary, there are four main variables used to represent soil organic carbon:</p>
<ol style="list-style-type: decimal">
<li><strong>Soil Organic Carbon fraction or content</strong> (ORC) in g/kg (permille) or dg/kg (percent),</li>
<li><strong>Soil Organic Carbon Density</strong> (OCD) in kg/m<span class="math inline">\(^3\)</span>,</li>
<li><strong>Soil Organic Carbon Stock</strong> (OCS) in kg/m<span class="math inline">\(^2\)</span> or in tons/ha and for the given soil depth interval,</li>
<li><strong>Total Soil Organic Carbon Stock</strong> (TOCS) in million tonnes or Pg i.e. OCS multiplied by surface area,</li>
</ol>
<p>Global estimates of the total soil organic carbon stock are highly variable <span class="citation">(Scharlemann et al. <a href="#ref-Scharlemann2014CM">2014</a>)</span>. Current estimates of the present total soil organic carbon stock range between 800–2100 Pg C (for 0–100 cm), with a median estimate of about 1500 Pg C (for 0–100 cm). This means that the average OCS for the 0–100 cm depth interval for the global land mask (148,940,000 km<span class="math inline">\(^2\)</span>) is about 11 kg/m<span class="math inline">\(^2\)</span> or 110 tons/ha, and that average soil organic carbon density (OCD) is about 11 kg/m<span class="math inline">\(^3\)</span> (compare to the standard bulk density of fine earth of 1250 kg/m<span class="math inline">\(^3\)</span>); standard OCS for 0–30 cm depth interval is 7 kg/m<span class="math inline">\(^2\)</span> i.e. the average OCD is about 13 kg/m<span class="math inline">\(^3\)</span>.</p>
<div class="rmdnote">
<p>
The average Organic Carbon Stock for the 0–100 cm depth interval for the land mask (148,940,000 km<span class="math inline"><span class="math inline">\(^2\)</span></span>) is about 11 kg/m<span class="math inline"><span class="math inline">\(^2\)</span></span> or 110 tons/ha. The average soil Organic Carbon Density (OCD) is about 11 kg/m<span class="math inline"><span class="math inline">\(^3\)</span></span> (compared to the standard bulk density of fine earth of 1250 kg/m<span class="math inline"><span class="math inline">\(^3\)</span></span>). Standard Organic Carbon Stock for 0–30 cm depth interval is 7 kg/m<span class="math inline"><span class="math inline">\(^2\)</span></span> i.e. the average OCD is about 13 kg/m<span class="math inline"><span class="math inline">\(^3\)</span></span>.
</p>
</div>
<p>The distribution of soil organic carbon in the world is, however, highly patchy with large areas with OCS <span class="math inline">\(\ll 100\)</span> tons/ha, and then some <em>pockets</em> of accumulated organic material i.e. organic soil types (histosols) with OCS up to 850tons/ha (for 0–30 cm depth interval). The world’s soil organic matter accumulation areas are usually found in the following biomes / land cover classes: wetlands and peatlands, mangroves, tundras and taigas.</p>
<p>Land use and agriculture, in particular, have led to dramatic decreases in soil carbon stocks in last 200+ years (agricultural and industrial revolutions). <span class="citation">Lal (<a href="#ref-Lal2004Science">2004</a>)</span> estimated that approximately 54 Pg C have been added to the atmosphere due to agricultural activities with another 26 Pg C being lost from soils due to erosion. <span class="citation">Wei et al. (<a href="#ref-wei2014global">2014</a>)</span> estimated that, on average, conversion from forests to various agricultural land results in a 30–50% decrease in SOCS. Modelling and monitoring of soil organic carbon dynamics is therefore of increasing importance (see e.g. FAO report <a href="http://www.fao.org/documents/card/en/c/25eaf720-94e4-4f53-8f50-cdfc2487e1f8/">“Unlocking the Potential of Soil Organic Carbon”</a>).</p>
</div>
<div id="derivation-of-ocs-and-ocd-using-soil-profile-data" class="section level2">
<h2><span class="header-section-number">7.3</span> Derivation of OCS and OCD using soil profile data</h2>
<p>As mentioned previously, OCS stock is most commonly derived from measurements of the organic carbon (ORC) content, soil bulk density (BLD) and the volume fraction of gravel (CRF). These are usually sampled either per soil layers or soil <strong>horizons</strong> (a sequence of horizons makes a soil profile), which can refer to variable soil depth intervals i.e. are non-standard. That means that, before one can determine OCS for standard fixed depth intervals (e.g. 0–30 cm or 0–100 cm), values of ORC, BLD and CRF need to be standardized so they refer to common depth intervals.</p>
<p>Consider, for example, the following two real life examples of soil profile data for a standard agricultural soil and an organic soil. In the first example, <a href="http://www.asris.csiro.au/mapping/hyperdocs/NatSoil/399%5EEDGEROI%5Eed079.pdf">profile from Australia</a>, the soil profile data shows <span class="citation">(Karssies <a href="#ref-Karssies2011CSIRO">2011</a>)</span>:</p>
<table>
<caption>(#tab:profile-edgeroi)Laboratory data for a profile <em>399 EDGEROI ed079</em> from Australia (Karssies 2011).</caption>
<thead>
<tr class="header">
<th align="right">upper_limit</th>
<th align="right">lower_limit</th>
<th align="right">carbon_content</th>
<th align="right">bulk_density</th>
<th align="right">CF</th>
<th align="right">SOCS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">10</td>
<td align="right">8.2</td>
<td align="right">1340</td>
<td align="right">6</td>
<td align="right">1.1</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">20</td>
<td align="right">7.5</td>
<td align="right">1367</td>
<td align="right">6</td>
<td align="right">1.0</td>
</tr>
<tr class="odd">
<td align="right">20</td>
<td align="right">55</td>
<td align="right">6.1</td>
<td align="right">1382</td>
<td align="right">7</td>
<td align="right">3.0</td>
</tr>
<tr class="even">
<td align="right">55</td>
<td align="right">90</td>
<td align="right">3.3</td>
<td align="right">1433</td>
<td align="right">8</td>
<td align="right">1.7</td>
</tr>
<tr class="odd">
<td align="right">90</td>
<td align="right">116</td>
<td align="right">1.6</td>
<td align="right">1465</td>
<td align="right">8</td>
<td align="right">0.6</td>
</tr>
</tbody>
</table>
<p>Note that BLD data were not given for the described horizons (the original soil profile description / laboratory data indicates that no BLD were recorded for this profile). In the absence of measured field BLD we can substitute BLD estimated using LandGIS data. It (unfortunately) commonly happens that soil profile observations lack BLD measurements, and consequently BLD needs to be generated using a Pedo-Transfer function or estimated from soil maps.</p>
<p>To determine OCS for standard depth intervals 0–30, 0–100 and 0–200 cm, we first fit a mass-preserving spline <span class="citation">(Malone et al. <a href="#ref-Malone2009Geoderma">2009</a>)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(GSIF)
<span class="co">#&gt; GSIF version 0.5-4 (2017-04-25)</span>
<span class="co">#&gt; URL: http://gsif.r-forge.r-project.org/</span>
<span class="kw">library</span>(aqp)
<span class="co">#&gt; This is aqp 1.16-3</span>
<span class="kw">library</span>(sp)
<span class="kw">library</span>(plyr)
lon =<span class="st"> </span><span class="fl">149.73</span>; lat =<span class="st"> </span><span class="fl">-30.09</span>; 
id =<span class="st"> &quot;399_EDGEROI_ed079&quot;</span>; TIMESTRR =<span class="st"> &quot;1987-01-05&quot;</span>
top =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">55</span>, <span class="dv">90</span>) 
bottom =<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">55</span>, <span class="dv">90</span>, <span class="dv">116</span>)
ORC =<span class="st"> </span><span class="kw">c</span>(<span class="fl">8.2</span>, <span class="fl">7.5</span>, <span class="fl">6.1</span>, <span class="fl">3.3</span>, <span class="fl">1.6</span>)
BLD =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1340</span>, <span class="dv">1367</span>, <span class="dv">1382</span>, <span class="dv">1433</span>, <span class="dv">1465</span>)
CRF =<span class="st"> </span><span class="kw">c</span>(<span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">8</span>)
<span class="co">#OCS = OCSKGM(ORC, BLD, CRF, HSIZE=bottom-top)</span>
prof1 &lt;-<span class="st"> </span><span class="kw">join</span>(<span class="kw">data.frame</span>(id, top, bottom, ORC, BLD, CRF), 
               <span class="kw">data.frame</span>(id, lon, lat, TIMESTRR), <span class="dt">type=</span><span class="st">&#39;inner&#39;</span>)
<span class="co">#&gt; Joining by: id</span>
<span class="kw">depths</span>(prof1) &lt;-<span class="st"> </span>id <span class="op">~</span><span class="st"> </span>top <span class="op">+</span><span class="st"> </span>bottom
<span class="co">#&gt; Warning: converting IDs from factor to character</span>
<span class="kw">site</span>(prof1) &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>lon <span class="op">+</span><span class="st"> </span>lat <span class="op">+</span><span class="st"> </span>TIMESTRR
<span class="kw">coordinates</span>(prof1) &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>lon <span class="op">+</span><span class="st"> </span>lat
<span class="kw">proj4string</span>(prof1) &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="st">&quot;+proj=longlat +datum=WGS84&quot;</span>)
ORC.s &lt;-<span class="st"> </span><span class="kw">mpspline</span>(prof1, <span class="dt">var.name=</span><span class="st">&quot;ORC&quot;</span>, <span class="dt">d=</span><span class="kw">t</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">30</span>,<span class="dv">100</span>,<span class="dv">200</span>)), <span class="dt">vhigh =</span> <span class="dv">2200</span>)
<span class="co">#&gt; Fitting mass preserving splines per profile...</span>
<span class="co">#&gt; </span>
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|</span><span class="st">                                                                 </span><span class="er">|</span><span class="st">   </span><span class="dv">0</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|=================================================================|</span><span class="st"> </span><span class="dv">100</span>%
BLD.s &lt;-<span class="st"> </span><span class="kw">mpspline</span>(prof1, <span class="dt">var.name=</span><span class="st">&quot;BLD&quot;</span>, <span class="dt">d=</span><span class="kw">t</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">30</span>,<span class="dv">100</span>,<span class="dv">200</span>)), <span class="dt">vhigh =</span> <span class="dv">2200</span>)
<span class="co">#&gt; Fitting mass preserving splines per profile...</span>
<span class="co">#&gt; </span>
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|</span><span class="st">                                                                 </span><span class="er">|</span><span class="st">   </span><span class="dv">0</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|=================================================================|</span><span class="st"> </span><span class="dv">100</span>%
CRF.s &lt;-<span class="st"> </span><span class="kw">mpspline</span>(prof1, <span class="dt">var.name=</span><span class="st">&quot;CRF&quot;</span>, <span class="dt">d=</span><span class="kw">t</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">30</span>,<span class="dv">100</span>,<span class="dv">200</span>)), <span class="dt">vhigh =</span> <span class="dv">2200</span>)
<span class="co">#&gt; Fitting mass preserving splines per profile...</span>
<span class="co">#&gt; </span>
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|</span><span class="st">                                                                 </span><span class="er">|</span><span class="st">   </span><span class="dv">0</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|=================================================================|</span><span class="st"> </span><span class="dv">100</span>%</code></pre></div>
<p>now we can derive OCS for top-soil by using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">OCSKGM</span>(ORC.s<span class="op">$</span>var.std<span class="op">$</span><span class="st">`</span><span class="dt">0-30 cm</span><span class="st">`</span>, 
       BLD.s<span class="op">$</span>var.std<span class="op">$</span><span class="st">`</span><span class="dt">0-30 cm</span><span class="st">`</span>, 
       CRF.s<span class="op">$</span>var.std<span class="op">$</span><span class="st">`</span><span class="dt">0-30 cm</span><span class="st">`</span>, <span class="dt">HSIZE=</span><span class="dv">30</span>)
<span class="co">#&gt; [1] 2.88</span>
<span class="co">#&gt; attr(,&quot;measurementError&quot;)</span>
<span class="co">#&gt; [1] 3.84</span>
<span class="co">#&gt; attr(,&quot;units&quot;)</span>
<span class="co">#&gt; [1] &quot;kilograms per square-meter&quot;</span></code></pre></div>
<p>and for sub-soil using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">OCSKGM</span>(ORC.s<span class="op">$</span>var.std<span class="op">$</span><span class="st">`</span><span class="dt">30-100 cm</span><span class="st">`</span>, 
       BLD.s<span class="op">$</span>var.std<span class="op">$</span><span class="st">`</span><span class="dt">30-100 cm</span><span class="st">`</span>, 
       CRF.s<span class="op">$</span>var.std<span class="op">$</span><span class="st">`</span><span class="dt">30-100 cm</span><span class="st">`</span>, <span class="dt">HSIZE=</span><span class="dv">70</span>)
<span class="co">#&gt; [1] 3.62</span>
<span class="co">#&gt; attr(,&quot;measurementError&quot;)</span>
<span class="co">#&gt; [1] 9.18</span>
<span class="co">#&gt; attr(,&quot;units&quot;)</span>
<span class="co">#&gt; [1] &quot;kilograms per square-meter&quot;</span></code></pre></div>
<p>Note that the OCSKGM function requires soil organic carbon content in g/kg. If one has contents measured in % then first multiply the values by 10. Bulk density data should be provided in kg/m3, gravel content in %, and layer depth in cm. Running the OCSKGM function for the Edgeroi profile gives the following estimates of OCS for standard depth intervals (Fig. @ref(fig:scheme-soc-prof1)):</p>
<ul>
<li><p>0–30 cm: 2.9 kg / m-square</p></li>
<li><p>0–100 cm: 6.5 kg / m-square</p></li>
<li><p>0–200 cm: 8.5 kg / m-square (85 tonnes / ha)</p></li>
</ul>
<p>Values of OCS between 5–35 kg/m<span class="math inline">\(^2\)</span> for 0–100 cm are commonly reported for a variety of mineral soils with e.g. 1–3% of soil organic carbon.</p>
<div class="rmdnote">
<p>
Organic Carbon Stock for standard depths can be determined from legacy soil profile data either by fitting a spline function to organic carbon, bulk density values, or by aggregating data using simple conversion formulas. A standard mineral soil with 1–3% soil organic carbon for the 0–100 cm depth interval should have about 5–35 kg/m<span class="math inline"><span class="math inline">\(^2\)</span></span> or 50–350 tonnes/ha. An organic soil with &gt;30% soil organic carbon may have as much as 60–90 kg/m<span class="math inline"><span class="math inline">\(^2\)</span></span> for the 0–100 cm depth interval.
</p>
</div>
<p>Note that the measurement error is computed from default uncertainty values (expressed in standard deviations) for organic carbon (10 g/kg), bulk density (100 kg/m3) and coarse fraction content (5%). When these are not provided by the user, the outcome should be interpreted with care.</p>
<div class="figure" style="text-align: center">
<img src="figures/fig_2_profiles_ocs_edgeroi.png" alt="Determination of soil organic carbon density and stock for standard depth intervals: example of a mineral soil profile from Australia." width="90%" />
<p class="caption">
(#fig:scheme-soc-prof1)Determination of soil organic carbon density and stock for standard depth intervals: example of a mineral soil profile from Australia.
</p>
</div>
<p>In the second example we look at a profile from Canada (a histosol with &gt;40% of organic carbon; <span class="citation">Shaw, Bhatti, and Sabourin (<a href="#ref-shaw2005ecosystem">2005</a>)</span>):</p>
<table>
<caption>(#tab:profile-can)Laboratory data for an organic soil profile from Canada (Shaw, Bhatti, and Sabourin 2005).</caption>
<thead>
<tr class="header">
<th align="right">upper_limit</th>
<th align="right">lower_limit</th>
<th align="right">carbon_content</th>
<th align="right">bulk_density</th>
<th align="right">CF</th>
<th align="right">SOCS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">31</td>
<td align="right">472</td>
<td align="right">185</td>
<td align="right">5</td>
<td align="right">25.7</td>
</tr>
<tr class="even">
<td align="right">31</td>
<td align="right">61</td>
<td align="right">492</td>
<td align="right">172</td>
<td align="right">6</td>
<td align="right">23.9</td>
</tr>
<tr class="odd">
<td align="right">61</td>
<td align="right">91</td>
<td align="right">487</td>
<td align="right">175</td>
<td align="right">6</td>
<td align="right">24.1</td>
</tr>
<tr class="even">
<td align="right">91</td>
<td align="right">122</td>
<td align="right">502</td>
<td align="right">166</td>
<td align="right">6</td>
<td align="right">24.3</td>
</tr>
<tr class="odd">
<td align="right">122</td>
<td align="right">130</td>
<td align="right">59</td>
<td align="right">830</td>
<td align="right">6</td>
<td align="right">3.7</td>
</tr>
</tbody>
</table>
<p>Here also BLD values were not provided and so had to be estimated. To estimate BLD, we use a simple Pedo-Transfer rule e.g. from <span class="citation">Köchy, Hiederer, and Freibauer (<a href="#ref-kochy2015global">2015</a>)</span>:</p>
<span class="math display">\[\begin{equation}
BLD.f = (-0.31 \cdot log(ORC/10) + 1.38) \cdot 1000
(\#eq:kochy)
\end{equation}\]</span>
<p>We divide the organic carbon content here by 10 to convert the organic carbon content from g/kg to % as required by the PTF. Note that one might want to use different PTFs for different soil layers. For mineral soils the bulk density of subsoil layers is often somewhat higher than for topsoil layers. For organic soils this typically is the inverse. For instance, <span class="citation">Köchy, Hiederer, and Freibauer (<a href="#ref-kochy2015global">2015</a>)</span> propose the following PTF for the subsoil (for layers with SOC &gt; 3%):</p>
<span class="math display">\[\begin{equation}
BLD = -0.32 * log({\rm ORC}[\%]) + 1.38
(\#eq:kochy)
\end{equation}\]</span>
<p>which gives slightly lower bulk density values. Another useful source for PTFs for organic soils is work by <span class="citation">Hossain, Chen, and Zhang (<a href="#ref-hossain2015bulk">2015</a>)</span>. For illustrative purposes, we have here used only one PTF for all soil layers.</p>
<p>We can again fit mass-preserving splines and determine OCS for standard depth intervals by using the functions applied to profile 1. This ultimately gives the following estimates (Fig. @ref(fig:scheme-soc-prof2)):</p>
<ul>
<li><p>0–30 cm: 24.8 kg / m-square</p></li>
<li><p>0–100 cm: 75.3 kg / m-square</p></li>
<li><p>0–200 cm: 114.5 kg / m-square (1145 tonnes / ha)</p></li>
</ul>
<div class="figure" style="text-align: center">
<img src="figures/fig_2_profiles_ocs_organic.png" alt="Determination of soil organic carbon density and stock for standard depth intervals: example of an organic soil profile from Canada." width="90%" />
<p class="caption">
(#fig:scheme-soc-prof2)Determination of soil organic carbon density and stock for standard depth intervals: example of an organic soil profile from Canada.
</p>
</div>
<p>Note that only 3–4% of the total soil profiles in the world have organic carbon content above 8% (soils with ORC &gt;12% are classified as organic soils or histosols in USDA and/or WRB classifications and are even less frequent), hence soil-depth functions of organic carbon content and derivation of OCS for organic soils are specific for patches of organic soils. On the other hand, organic soils carry much more total OCS. Precise processing and mapping of organic soils is, therefore, often crucial for accurate estimation of total OCS for large areas. Therefore, it is fairly important to use a good PTF to fill in missing values for BLD for organic soils. As a rule of thumb, organic soil will rarely exhibit a density greater than some minimum value e.g. 120 kg/m<span class="math inline">\(^3\)</span> because even though SOC content can be &gt;50%, bulk density of such soil gets proportionally lower and bulk density is physically bound with how material is organized in soil (unless soils is artificially compacted). Also, using correct estimates for coarse fragments is important as otherwise, if CRF is ignored, total stock will be over-estimated <span class="citation">(Poeplau, Vos, and Axel <a href="#ref-poeplau2017soil">2017</a>)</span>.</p>
<p>A somewhat more straightforward way to estimate OCS for a list of soil profiles vs spline fitting is:</p>
<ol style="list-style-type: decimal">
<li>Fill in bulk densities using some PTF or global data,</li>
<li>Use information about the depth to bedrock to correct for shallow soils,</li>
<li>Use information on CRF to correct stocks for stony / skeletoidal component,</li>
<li>Aggregate non-standard horizon depth values using some simple rules (Fig. @ref(fig:scheme-profiles-ocs)).</li>
</ol>
<div class="figure" style="text-align: center">
<img src="figures/Fig_standard_soil_profiles_SOC_calc.png" alt="Estimation of OCS values 0–30 cm using some typical soil profile data without fitting splines." width="95%" />
<p class="caption">
(#fig:scheme-profiles-ocs)Estimation of OCS values 0–30 cm using some typical soil profile data without fitting splines.
</p>
</div>
</div>
<div id="estimation-of-bulk-density-using-a-globally-calibrated-ptf" class="section level2">
<h2><span class="header-section-number">7.4</span> Estimation of Bulk Density using a globally-calibrated PTF</h2>
<p>Where values for bulk density are missing, and no local PTF exists, WoSIS points (global compilation of soil profiles) can be used to fit a PTF that can fill-in gaps in bulk density measurements globally. A regression matrix extracted on 15th of May 2017 (and which contains harmonized values for BD, organic carbon content, pH, sand and clay content, depth of horizon and USDA soil type at some 20,000 soil profiles world-wide), can be fitted using a random forest model (see also <span class="citation">Ramcharan et al. (<a href="#ref-ramcharan2017soil">2018</a><a href="#ref-ramcharan2017soil">b</a>)</span>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dfs_tbl =<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;extdata/wosis_tbl.rds&quot;</span>)
ind.tax =<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;extdata/ov_taxousda.rds&quot;</span>)
<span class="kw">library</span>(ranger)
fm.BLD =<span class="st"> </span><span class="kw">as.formula</span>(
  <span class="kw">paste</span>(<span class="st">&quot;BLD ~ ORCDRC + CLYPPT + SNDPPT + PHIHOX + DEPTH.f +&quot;</span>, 
        <span class="kw">paste</span>(<span class="kw">names</span>(ind.tax), <span class="dt">collapse=</span><span class="st">&quot;+&quot;</span>)))
m.BLD_PTF &lt;-<span class="st"> </span><span class="kw">ranger</span>(fm.BLD, dfs_tbl, <span class="dt">num.trees =</span> <span class="dv">85</span>, <span class="dt">importance=</span><span class="st">&#39;impurity&#39;</span>)
m.BLD_PTF
<span class="co">#&gt; Ranger result</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt;  ranger(fm.BLD, dfs_tbl, num.trees = 85, importance = &quot;impurity&quot;) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Type:                             Regression </span>
<span class="co">#&gt; Number of trees:                  85 </span>
<span class="co">#&gt; Sample size:                      98650 </span>
<span class="co">#&gt; Number of independent variables:  70 </span>
<span class="co">#&gt; Mtry:                             8 </span>
<span class="co">#&gt; Target node size:                 5 </span>
<span class="co">#&gt; Variable importance mode:         impurity </span>
<span class="co">#&gt; Splitrule:                        variance </span>
<span class="co">#&gt; OOB prediction error (MSE):       32379 </span>
<span class="co">#&gt; R squared (OOB):                  0.549</span></code></pre></div>
<p>This shows somewhat lower accuracy i.e. an RMSE of ±180 kg/m<span class="math inline">\(^3\)</span> (R squared (OOB) = 0.54), but is still probably preferable to completely excluding all observations without bulk density from SOC assessment. A disadvantage of this model is that, in order to predict BD for new locations, we need to also have measurements of texture fractions, pH and organic carbon of course. For example, an Udalf with 1.1% organic carbon, 22% clay, pH of 6.5, sand content of 35% and at depth of 5 cm would result in an estimate for bulk density of:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ind.tax.new =<span class="st"> </span>ind.tax[<span class="kw">which</span>(ind.tax<span class="op">$</span>TAXOUSDA84<span class="op">==</span><span class="dv">1</span>)[<span class="dv">1</span>],]
<span class="kw">predict</span>(m.BLD_PTF, <span class="kw">cbind</span>(<span class="kw">data.frame</span>(<span class="dt">ORCDRC=</span><span class="dv">11</span>, 
                                    <span class="dt">CLYPPT=</span><span class="dv">22</span>, 
                                    <span class="dt">PHIHOX=</span><span class="fl">6.5</span>, 
                                    <span class="dt">SNDPPT=</span><span class="dv">35</span>, 
                                    <span class="dt">DEPTH.f=</span><span class="dv">5</span>), ind.tax.new))<span class="op">$</span>predictions
<span class="co">#&gt; [1] 1526</span></code></pre></div>
<p>Note also that the PTF above needs USDA suborder values per point location following the LandGIS legend for USDA suborders, and formatted as in the <code>ind.tax</code> object. Unfortunately, the model above probably over-estimates bulk density for organic soils as these are usually under-represented i.e. often not available (consider for example a Saprist with 32% organic carbon):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ind.tax.new =<span class="st"> </span>ind.tax[<span class="kw">which</span>(ind.tax<span class="op">$</span>TAXOUSDA13<span class="op">==</span><span class="dv">1</span>)[<span class="dv">1</span>],]
<span class="kw">predict</span>(m.BLD_PTF, 
        <span class="kw">cbind</span>(<span class="kw">data.frame</span>(<span class="dt">ORCDRC=</span><span class="dv">320</span>, <span class="dt">CLYPPT=</span><span class="dv">8</span>, <span class="dt">PHIHOX=</span><span class="fl">5.5</span>, 
                         <span class="dt">SNDPPT=</span><span class="dv">45</span>, <span class="dt">DEPTH.f=</span><span class="dv">10</span>), ind.tax.new))<span class="op">$</span>predictions
<span class="co">#&gt; [1] 773</span></code></pre></div>
<p>An alternative to estimating BLD is to just use ORC values, e.g. (see plot below):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.BLD_ls =<span class="st"> </span><span class="kw">loess</span>(BLD <span class="op">~</span><span class="st"> </span>ORCDRC, dfs_tbl, <span class="dt">span=</span><span class="dv">1</span><span class="op">/</span><span class="dv">18</span>)
<span class="kw">predict</span>(m.BLD_ls, <span class="kw">data.frame</span>(<span class="dt">ORCDRC=</span><span class="dv">320</span>))
<span class="co">#&gt;   1 </span>
<span class="co">#&gt; 664</span></code></pre></div>
<p>This gives about 30% lower value than the random forest-based PTF from above. Over-estimating BLD will always result in higher OCS, hence clearly accurate information on BLD can be crucial for any OCS monitoring project. This means that the PTF fitted using random forest above is likely over-estimating BLD values for organic soils, mainly because there are not enough training points in organic soils that have measurements data for all of ORC, BLD, soil pH and texture fractions (if ANY of the calibration measurements are missing, the whole horizons are taken out of calibration and hence different ranges of BLD could be completely misrepresented).</p>
<div class="rmdnote">
<p>
Soil Bulk density (BLD) is an important soil property that is required to estimate stocks of nutrients especially soil organic carbon. Measurements of BLD are often not available and need to be estimated using some PTF or similar. Most PTF’s for BLD are based on correlating BLD with soil organic carbon, clay and sand content, pH, soil type and climate zone.
</p>
</div>
<div class="figure" style="text-align: center">
<img src="figures/rplot_bulk_dens_function_of_soc.png" alt="Correlation plot between soil organic carbon density and bulk density (fine earth), created using the global compilations of soil profile data (http://www.isric.org/content/wosis-data-sets). Black line indicates fitted loess polynomial surface (stats::loess). There is still quite some scatter around the fitted line: many combinations of BLD and ORC, that do not fall close to the correlation line, can still be observed." width="60%" />
<p class="caption">
(#fig:plot-bld-soc)Correlation plot between soil organic carbon density and bulk density (fine earth), created using the global compilations of soil profile data (<a href="http://www.isric.org/content/wosis-data-sets" class="uri">http://www.isric.org/content/wosis-data-sets</a>). Black line indicates fitted loess polynomial surface (stats::loess). There is still quite some scatter around the fitted line: many combinations of BLD and ORC, that do not fall close to the correlation line, can still be observed.
</p>
</div>
<p>To fill-in missing values for BLD, a combination of two global Pedo-Transfer functions can be used for example: (1) PTF fitted using random forest model that locally predicts BLD as a function of organic carbon content, clay and sand content, pH and coarse fragments, and (2) a simpler model that predicts BLD based on only ORC. The average RMSE of these PTFs for BLD is about ±150 kg/m<span class="math inline">\(^3\)</span>.</p>
<p>For mineral soils the relationship between soil organic carbon and soil depth follows a log-log relationship which can be also approximated with the following (global) model (R-square: 0.36; see Fig. @ref(fig:soc-depth-plot)):</p>
<span class="math display">\[\begin{equation}
ORC (depth) = exp[ 4.1517 −0.60934 \cdot log(depth) ]
(\#eq:orc-depth)
\end{equation}\]</span>
<p>This also illustrates that any organic carbon spatial prediction model can significantly profit from including depth in the statistical modelling.</p>
<div class="figure" style="text-align: center">
<img src="figures/journal_pone_0105992_g005.png" alt=" Globally fitted regression model for predicting soil organic carbon using depth only (log-log regression) and (a) individual soil profile from the ISRIC soil monolith collection. Image source: Hengl et al. (2014) doi: 10.1371/journal.pone.0105992." width="90%" />
<p class="caption">
(#fig:soc-depth-plot) Globally fitted regression model for predicting soil organic carbon using depth only (log-log regression) and (a) individual soil profile from the ISRIC soil monolith collection. Image source: Hengl et al. (2014) doi: 10.1371/journal.pone.0105992.
</p>
</div>
<p>In summary, PTFs can be used to efficiently estimate missing BLD values (BLD is usually highly correlated with organic carbon content and depth, texture fractions, soil classification and soil pH can also help improve accuracy of the PTFs). However, for organic soils there is, in general, less calibration data and therefore errors are potentially higher. Mistakes in estimating BLD can result in systematic and significant over/under-estimations of the actual carbon stock. On the other hand, removing all soil horizons from OCS assessment that lack BLD measurements leads also to reduced accuracy as fewer points are then available for training of the spatial prediction models. Especially for organic soils (&gt;12% organic carbon), there is no easy solution for estimating missing values for BLD. Collecting additional (local) calibration points might become unavoidable. <span class="citation">Lobsey and Viscarra Rossel (<a href="#ref-lobsey2016sensing">2016</a>)</span> recently proposed a method that combines gamma-ray attenuation and visible–near infrared (vis–NIR) spectroscopy to measure bulk density <em>ex situ</em> using samples that are freshly collected under wet field conditions. Hopefully BLD measurements (or their complete absence) will become less of a problem in the future.</p>
</div>
<div id="generating-maps-of-ocs" class="section level2">
<h2><span class="header-section-number">7.5</span> Generating maps of OCS</h2>
<p>Most projects focused on monitoring OCS require an estimate of OCS to be provided for the entire area of interest, so that users can visually explore spatial patterns of OCS. In this tutorial we demonstrate how to generate maps of OCS using point samples and RS based covariates. The output of this process is usually a gridded map (<code>SpatialPixelsDataFrame</code>) covering the entire area of interest (plot, farm, administrative unit or similar). Once OCS is mapped, we can multiply OCS densities by the area of each pixel and summing up all numbers we can compute the total OCS in total tonnes using the formula above. Predicted OCS values can also be aggregated per land cover class or similar. If a series of OCS maps is produced for the same area of interest (time-series of OCS), these can then be used to derive OCS change through time per pixel.</p>
<p>In principle, there are three main approaches to estimating total OCS for an area of interest (Fig. @ref(fig:ocs-three-approaches)):</p>
<ul>
<li><p>By directly predicting OCS, here called the <strong>the 2D approach to OCS mapping</strong> (this often requires vertical aggregation / modeling of soil variable depth curves as indicated above),</p></li>
<li><p>By predicting ORC, BLD and CRF, and then deriving OCS per layer, here called <strong>the 3D approach to OCS mapping with ORC, BLD and CRF mapped separately</strong>,</p></li>
<li><p>By deriving OCD (organic carbon density) and then directly predicting OCD and converting it to OCS, here called <strong>the 3D approach to OCS mapping via direct modeling of OCD</strong>,</p></li>
</ul>
<div class="rmdnote">
<p>
Soil Organic Carbon stock can be mapped by using at least differing approaches: (1) the 2D approach where estimation of OCS is done at the site level, (2) the 3D approach where soil organic carbon content, bulk density and coarse fragments are mapped separately, then used to derive OCS for standard depths at each grid cell, and (3) the 3D approach based on mapping Organic Carbon Density, then converting to stocks.
</p>
</div>
<div class="figure" style="text-align: center">
<img src="figures/fig_derivation_socs_scheme.png" alt="Three main computational paths (2D and 3D) to producing maps of organic carbon stock." width="100%" />
<p class="caption">
(#fig:ocs-three-approaches)Three main computational paths (2D and 3D) to producing maps of organic carbon stock.
</p>
</div>
<p>Although 2D prediction of OCS from point data seems to be more straightforward, many soil profiles contain measurements at non-standard depth intervals (varying support sizes also) and therefore 2D modeling of OCS can often be a cumbersome. In most situations where legacy soil profile data are used, 3D modeling of OCD is probably the most elegant solution to mapping OCS because:</p>
<ul>
<li><p>No vertical aggregation of values via spline fitting or similar is needed to standardize values per standard depths,</p></li>
<li><p>No additional uncertainty is introduced (in the case of the 2D approach splines likely introduce some extra uncertainty in the model),</p></li>
<li><p>Predictions of OCD/OCS can be generated for any depth interval using the same model (i.e. predictions are based on a single 3D model),</p></li>
</ul>
<p>A disadvantage of doing 3D modeling of OCD is, however, that correlation with covariate layers could be less clear than if separate models are built for ORC, BLD and CRF. Because OCD is a composite variable, it can often be difficult to distinguish whether the values are lower or higher due to differences in ORC, BLD or CRF. We leave it to users to compare various approaches to OCS mapping and then select the method that achieves best accuracy and/or is most suitable for use for their applications.</p>
</div>
<div id="predicting-ocs-from-point-data-the-2d-approach" class="section level2">
<h2><span class="header-section-number">7.6</span> Predicting OCS from point data (the 2D approach)</h2>
<p>The <a href="https://cran.r-project.org/package=geospt">geospt package</a> contains 125 samples of OCS from Colombia already at standard depth intervals, hence this data set is suitable and ready for 2D mapping of OCS. The data set consists of tabular values for points and a raster map containing the borders of the study area:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;extdata/COSha10.rda&quot;</span>)
<span class="kw">load</span>(<span class="st">&quot;extdata/COSha30.rda&quot;</span>)
<span class="kw">str</span>(COSha30)
<span class="co">#&gt; &#39;data.frame&#39;:    118 obs. of  10 variables:</span>
<span class="co">#&gt;  $ ID        : Factor w/ 118 levels &quot;S1&quot;,&quot;S10&quot;,&quot;S100&quot;,..: 1 44 61 89 100 110 2 9 15 21 ...</span>
<span class="co">#&gt;  $ x         : int  669030 669330 670292 669709 671321 670881 670548 671340 671082 670862 ...</span>
<span class="co">#&gt;  $ y         : int  448722 448734 448697 448952 448700 448699 448700 448969 448966 448968 ...</span>
<span class="co">#&gt;  $ DA30      : num  1.65 1.6 1.5 1.32 1.41 1.39 1.51 1.39 1.55 1.63 ...</span>
<span class="co">#&gt;  $ CO30      : num  0.99 1.33 1.33 1.09 1.04 1.19 1.21 1.36 1.09 1.19 ...</span>
<span class="co">#&gt;  $ COB1r     : Factor w/ 6 levels &quot;Az&quot;,&quot;Ci&quot;,&quot;Cpf&quot;,..: 5 5 2 5 2 5 2 2 2 5 ...</span>
<span class="co">#&gt;  $ S_UDS     : Factor w/ 19 levels &quot;BJa1&quot;,&quot;BQa1&quot;,..: 12 5 12 5 11 12 12 12 12 12 ...</span>
<span class="co">#&gt;  $ COSha30   : num  49.2 64 59.8 43.1 44.2 ...</span>
<span class="co">#&gt;  $ Cor4DAidep: num  43.3 56.3 54 37.9 39.9 ...</span>
<span class="co">#&gt;  $ CorT      : num  1.37 1.39 1.38 1.36 1.36 ...</span></code></pre></div>
<p>where <code>COSha10</code> = 0–10 cm, <code>COSha30</code> = 0–30 cm in tons / ha are values for OCS aggregated to standard soil depth intervals, so there is no need to do any spline fitting and/or vertical aggregation. We can also load the raster map for the area by using (Fig. @ref(fig:libertad-soc)):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;extdata/COSha30map.rda&quot;</span>)
<span class="kw">proj4string</span>(COSha30map) =<span class="st"> &quot;+proj=utm +zone=18 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;</span>
<span class="kw">str</span>(COSha30map<span class="op">@</span>data)
<span class="co">#&gt; &#39;data.frame&#39;:    10000 obs. of  2 variables:</span>
<span class="co">#&gt;  $ var1.pred: num  39.9 39.8 39.9 40.3 40.7 ...</span>
<span class="co">#&gt;  $ var1.var : num  1.91e-05 6.39e-05 1.05e-04 1.39e-04 1.66e-04 ...</span></code></pre></div>
<p>which shows predictions and kriging variances for <code>COSha30</code>.</p>
<div class="figure" style="text-align: center">
<img src="figures/fig_la_libertad_research_center_socs.jpg" alt="Example of a data set with OCS samples (for 2D prediction). Case study in Colombia available via the geospt package (https://cran.r-project.org/package=geospt)." width="75%" />
<p class="caption">
(#fig:libertad-soc)Example of a data set with OCS samples (for 2D prediction). Case study in Colombia available via the geospt package (<a href="https://cran.r-project.org/package=geospt" class="uri">https://cran.r-project.org/package=geospt</a>).
</p>
</div>
<p>We can import a number of RS-based covariates into R by (these were derived from the global 30 m layers listed previously):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">covs30m =<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;extdata/covs30m.rds&quot;</span>)
<span class="kw">proj4string</span>(covs30m) =<span class="st"> </span><span class="kw">proj4string</span>(COSha30map)
<span class="kw">names</span>(covs30m)
<span class="co">#&gt;  [1] &quot;SRTMGL1_SRTMGL1.2_cprof&quot;                  </span>
<span class="co">#&gt;  [2] &quot;SRTMGL1_SRTMGL1.2_devmean&quot;                </span>
<span class="co">#&gt;  [3] &quot;SRTMGL1_SRTMGL1.2_openn&quot;                  </span>
<span class="co">#&gt;  [4] &quot;SRTMGL1_SRTMGL1.2_openp&quot;                  </span>
<span class="co">#&gt;  [5] &quot;SRTMGL1_SRTMGL1.2_slope&quot;                  </span>
<span class="co">#&gt;  [6] &quot;SRTMGL1_SRTMGL1.2_twi&quot;                    </span>
<span class="co">#&gt;  [7] &quot;SRTMGL1_SRTMGL1.2_vbf&quot;                    </span>
<span class="co">#&gt;  [8] &quot;SRTMGL1_SRTMGL1.2_vdepth&quot;                 </span>
<span class="co">#&gt;  [9] &quot;SRTMGL1_SRTMGL1.2&quot;                        </span>
<span class="co">#&gt; [10] &quot;COSha30map_var1pred_&quot;                     </span>
<span class="co">#&gt; [11] &quot;GlobalForestChange2000.2014_first_NIRL00&quot; </span>
<span class="co">#&gt; [12] &quot;GlobalForestChange2000.2014_first_REDL00&quot; </span>
<span class="co">#&gt; [13] &quot;GlobalForestChange2000.2014_first_SW1L00&quot; </span>
<span class="co">#&gt; [14] &quot;GlobalForestChange2000.2014_first_SW2L00&quot; </span>
<span class="co">#&gt; [15] &quot;GlobalForestChange2000.2014_treecover2000&quot;</span>
<span class="co">#&gt; [16] &quot;GlobalSurfaceWater_extent&quot;                </span>
<span class="co">#&gt; [17] &quot;GlobalSurfaceWater_occurrence&quot;            </span>
<span class="co">#&gt; [18] &quot;Landsat_bare2010&quot;</span></code></pre></div>
<p>This contains a number of covariates from SRTM DEM derivatives, to Global Surface Water occurrence values and similar (see section @ref(soil-covs-30m) for more details). All these could potentially prove useful for mapping OCS. We can also derive buffer distances from observations points and use these (as measures of spatial context) to improve predictions <span class="citation">(T. Hengl, Nussbaum, et al. <a href="#ref-Hengl2018RFsp">2018</a>)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">proj4string</span>(COSha30map) =<span class="st"> &quot;+proj=utm +zone=18 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;</span>
<span class="kw">coordinates</span>(COSha30) =<span class="st"> </span><span class="er">~</span><span class="st"> </span>x<span class="op">+</span>y
<span class="kw">proj4string</span>(COSha30) =<span class="st"> </span><span class="kw">proj4string</span>(COSha30map)
covs30mdist =<span class="st"> </span>GSIF<span class="op">::</span><span class="kw">buffer.dist</span>(COSha30[<span class="st">&quot;COSha30&quot;</span>], covs30m[<span class="dv">1</span>], <span class="kw">as.factor</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(COSha30)))</code></pre></div>
<p>We can convert the original covariates to Principal Components, also to fill in all missing pixels:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">covs30m<span class="op">@</span>data =<span class="st"> </span><span class="kw">cbind</span>(covs30m<span class="op">@</span>data, covs30mdist<span class="op">@</span>data)
sel.rm =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;GlobalSurfaceWater_occurrence&quot;</span>, <span class="st">&quot;GlobalSurfaceWater_extent&quot;</span>,
           <span class="st">&quot;Landsat_bare2010&quot;</span>, <span class="st">&quot;COSha30map_var1pred_&quot;</span>)
rr =<span class="st"> </span><span class="kw">which</span>(<span class="kw">names</span>(covs30m<span class="op">@</span>data) <span class="op">%in%</span><span class="st"> </span>sel.rm)
fm.spc =<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste</span>(<span class="st">&quot; ~ &quot;</span>, <span class="kw">paste</span>(<span class="kw">names</span>(covs30m)[<span class="op">-</span>rr], <span class="dt">collapse =</span> <span class="st">&quot;+&quot;</span>)))
<span class="kw">proj4string</span>(covs30m) =<span class="st"> </span><span class="kw">proj4string</span>(COSha30)
covs30m.spc =<span class="st"> </span>GSIF<span class="op">::</span><span class="kw">spc</span>(covs30m, fm.spc)
<span class="co">#&gt; Converting covariates to principal components...</span>
ov.COSha30 =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">as.data.frame</span>(COSha30), <span class="kw">over</span>(COSha30, covs30m.spc<span class="op">@</span>predicted))</code></pre></div>
<p>By using the above listed covariates, we can fit a spatial prediction 2D model using an available model, such as <a href="https://cran.r-project.org/package=ranger">ranger</a> <span class="citation">(Wright and Ziegler <a href="#ref-wright2017ranger">2017</a>)</span>, <a href="https://cran.r-project.org/package=xgboost">xgboost</a> and/or <a href="https://cran.r-project.org/package=GAMBoost">gamboost</a>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
<span class="co">#&gt; Loading required package: lattice</span>
<span class="co">#&gt; Loading required package: ggplot2</span>
<span class="kw">library</span>(ranger)
fm.COSha30 =<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste</span>(<span class="st">&quot;COSha30 ~ &quot;</span>, <span class="kw">paste</span>(<span class="kw">names</span>(covs30m.spc<span class="op">@</span>predicted), <span class="dt">collapse =</span> <span class="st">&quot;+&quot;</span>)))
fm.COSha30
<span class="co">#&gt; COSha30 ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9 + </span>
<span class="co">#&gt;     PC10 + PC11 + PC12 + PC13 + PC14 + PC15 + PC16 + PC17 + PC18 + </span>
<span class="co">#&gt;     PC19 + PC20 + PC21 + PC22 + PC23 + PC24 + PC25 + PC26 + PC27 + </span>
<span class="co">#&gt;     PC28 + PC29 + PC30 + PC31 + PC32 + PC33 + PC34 + PC35 + PC36 + </span>
<span class="co">#&gt;     PC37 + PC38 + PC39 + PC40 + PC41 + PC42 + PC43 + PC44 + PC45 + </span>
<span class="co">#&gt;     PC46 + PC47 + PC48 + PC49 + PC50 + PC51 + PC52 + PC53 + PC54 + </span>
<span class="co">#&gt;     PC55 + PC56 + PC57 + PC58 + PC59 + PC60 + PC61 + PC62 + PC63 + </span>
<span class="co">#&gt;     PC64 + PC65 + PC66 + PC67 + PC68 + PC69 + PC70 + PC71 + PC72 + </span>
<span class="co">#&gt;     PC73 + PC74 + PC75 + PC76 + PC77 + PC78 + PC79 + PC80 + PC81 + </span>
<span class="co">#&gt;     PC82 + PC83 + PC84 + PC85 + PC86 + PC87 + PC88 + PC89 + PC90 + </span>
<span class="co">#&gt;     PC91 + PC92 + PC93 + PC94 + PC95 + PC96 + PC97 + PC98 + PC99 + </span>
<span class="co">#&gt;     PC100 + PC101 + PC102 + PC103 + PC104 + PC105 + PC106 + PC107 + </span>
<span class="co">#&gt;     PC108 + PC109 + PC110 + PC111 + PC112 + PC113 + PC114 + PC115 + </span>
<span class="co">#&gt;     PC116 + PC117 + PC118 + PC119 + PC120 + PC121 + PC122 + PC123 + </span>
<span class="co">#&gt;     PC124 + PC125 + PC126 + PC127 + PC128 + PC129 + PC130 + PC131 + </span>
<span class="co">#&gt;     PC132</span>
rf.tuneGrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">.mtry =</span> <span class="kw">seq</span>(<span class="dv">2</span>, <span class="dv">60</span>, <span class="dt">by=</span><span class="dv">5</span>),
                           <span class="dt">.splitrule =</span> <span class="st">&quot;maxstat&quot;</span>,
                           <span class="dt">.min.node.size =</span> <span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">20</span>))
gb.tuneGrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">eta =</span> <span class="kw">c</span>(<span class="fl">0.3</span>,<span class="fl">0.4</span>), 
                           <span class="dt">nrounds =</span> <span class="kw">c</span>(<span class="dv">50</span>,<span class="dv">100</span>), 
                           <span class="dt">max_depth =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">gamma =</span> <span class="dv">0</span>, 
                           <span class="dt">colsample_bytree =</span> <span class="fl">0.8</span>, 
                           <span class="dt">min_child_weight =</span> <span class="dv">1</span>, <span class="dt">subsample=</span><span class="dv">1</span>)
fitControl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;repeatedcv&quot;</span>, <span class="dt">number=</span><span class="dv">4</span>, <span class="dt">repeats=</span><span class="dv">1</span>)
mFit1 &lt;-<span class="st"> </span><span class="kw">train</span>(fm.COSha30, <span class="dt">data=</span>ov.COSha30, <span class="dt">method=</span><span class="st">&quot;ranger&quot;</span>, 
               <span class="dt">trControl=</span>fitControl, <span class="dt">importance=</span><span class="st">&#39;impurity&#39;</span>, 
               <span class="dt">tuneGrid=</span>rf.tuneGrid)
mFit1
<span class="co">#&gt; Random Forest </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; 118 samples</span>
<span class="co">#&gt; 132 predictors</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; No pre-processing</span>
<span class="co">#&gt; Resampling: Cross-Validated (4 fold, repeated 1 times) </span>
<span class="co">#&gt; Summary of sample sizes: 87, 88, 89, 90 </span>
<span class="co">#&gt; Resampling results across tuning parameters:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;   mtry  min.node.size  RMSE  Rsquared  MAE </span>
<span class="co">#&gt;    2    10             11.0  0.0225    8.75</span>
<span class="co">#&gt;    2    20             11.0  0.0450    8.69</span>
<span class="co">#&gt;    7    10             11.0  0.0442    8.66</span>
<span class="co">#&gt;    7    20             11.0  0.0442    8.68</span>
<span class="co">#&gt;   12    10             11.0  0.0608    8.61</span>
<span class="co">#&gt;   12    20             10.9  0.0588    8.62</span>
<span class="co">#&gt;   17    10             11.0  0.0519    8.65</span>
<span class="co">#&gt;   17    20             11.0  0.0583    8.60</span>
<span class="co">#&gt;   22    10             10.9  0.0684    8.59</span>
<span class="co">#&gt;   22    20             10.9  0.0699    8.59</span>
<span class="co">#&gt;   27    10             10.9  0.0710    8.57</span>
<span class="co">#&gt;   27    20             10.9  0.0737    8.57</span>
<span class="co">#&gt;   32    10             10.9  0.0836    8.55</span>
<span class="co">#&gt;   32    20             10.9  0.0836    8.56</span>
<span class="co">#&gt;   37    10             10.9  0.0887    8.63</span>
<span class="co">#&gt;   37    20             10.9  0.0739    8.57</span>
<span class="co">#&gt;   42    10             10.9  0.0606    8.64</span>
<span class="co">#&gt;   42    20             10.8  0.0930    8.50</span>
<span class="co">#&gt;   47    10             10.9  0.0731    8.58</span>
<span class="co">#&gt;   47    20             10.9  0.0753    8.60</span>
<span class="co">#&gt;   52    10             10.9  0.0848    8.55</span>
<span class="co">#&gt;   52    20             10.9  0.0849    8.53</span>
<span class="co">#&gt;   57    10             10.9  0.0748    8.61</span>
<span class="co">#&gt;   57    20             10.9  0.0772    8.61</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Tuning parameter &#39;splitrule&#39; was held constant at a value of maxstat</span>
<span class="co">#&gt; RMSE was used to select the optimal model using the smallest value.</span>
<span class="co">#&gt; The final values used for the model were mtry = 42, splitrule =</span>
<span class="co">#&gt;  maxstat and min.node.size = 20.</span>
mFit2 &lt;-<span class="st"> </span><span class="kw">train</span>(fm.COSha30, <span class="dt">data=</span>ov.COSha30, <span class="dt">method=</span><span class="st">&quot;xgbTree&quot;</span>, 
               <span class="dt">trControl=</span>fitControl, <span class="dt">tuneGrid=</span>gb.tuneGrid)
mFit2
<span class="co">#&gt; eXtreme Gradient Boosting </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; 118 samples</span>
<span class="co">#&gt; 132 predictors</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; No pre-processing</span>
<span class="co">#&gt; Resampling: Cross-Validated (4 fold, repeated 1 times) </span>
<span class="co">#&gt; Summary of sample sizes: 87, 89, 89, 89 </span>
<span class="co">#&gt; Resampling results across tuning parameters:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;   eta  max_depth  nrounds  RMSE  Rsquared  MAE </span>
<span class="co">#&gt;   0.3  2           50      11.8  0.0361    9.33</span>
<span class="co">#&gt;   0.3  2          100      11.8  0.0367    9.33</span>
<span class="co">#&gt;   0.3  3           50      11.7  0.0366    9.48</span>
<span class="co">#&gt;   0.3  3          100      11.7  0.0367    9.48</span>
<span class="co">#&gt;   0.4  2           50      11.9  0.0480    9.53</span>
<span class="co">#&gt;   0.4  2          100      11.9  0.0478    9.54</span>
<span class="co">#&gt;   0.4  3           50      12.1  0.0429    9.56</span>
<span class="co">#&gt;   0.4  3          100      12.1  0.0429    9.56</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Tuning parameter &#39;gamma&#39; was held constant at a value of 0</span>
<span class="co">#&gt;  0.8</span>
<span class="co">#&gt; Tuning parameter &#39;min_child_weight&#39; was held constant at a value of</span>
<span class="co">#&gt;  1</span>
<span class="co">#&gt; Tuning parameter &#39;subsample&#39; was held constant at a value of 1</span>
<span class="co">#&gt; RMSE was used to select the optimal model using the smallest value.</span>
<span class="co">#&gt; The final values used for the model were nrounds = 100, max_depth = 3,</span>
<span class="co">#&gt;  eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1</span>
<span class="co">#&gt;  and subsample = 1.</span></code></pre></div>
<p>This example illustrates that no significant spatial prediction models (with an R-square exceeding 10%) can be fitted using these data. It is very common in soil mapping projects for models to explain only low amounts of the total variation, resulting in large average errors of prediction and/or wide prediction intervals. This can occur because of high measurement errors, and/or because there are missing covariates, but it could also happen because the natural complexity of soils in the area is simply high.</p>
<p>Note that the absolute values of our predictions of OCS are somewhat different than those produced by the <a href="https://cran.r-project.org/package=geospt">geospt package</a> authors, although the main patterns are comparable.</p>
<div class="figure" style="text-align: center">
<img src="Soil_organic_carbon_files/figure-html/plot-cosha30map-rf-1.png" alt="Comparison of predictions generated using ordinary kriging (left) and machine learning with the help of 30 m resolution covariates and buffer distances (right)." width="100%" />
<p class="caption">
(#fig:plot-cosha30map-rf)Comparison of predictions generated using ordinary kriging (left) and machine learning with the help of 30 m resolution covariates and buffer distances (right).
</p>
</div>
<p>We can compare the difference between mean predicted OCS and measured OCS:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(COSha30.pr<span class="op">$</span>COSha30map_RF, <span class="dt">na.rm=</span><span class="ot">TRUE</span>); <span class="kw">mean</span>(COSha30<span class="op">$</span>COSha30, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)
<span class="co">#&gt; [1] 48.6</span>
<span class="co">#&gt; [1] 50.6</span>
## 48 tonnes/ha vs 51 tonnes / ha</code></pre></div>
<p>and derive the total SOC in tonnes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(COSha30.pr<span class="op">$</span>COSha30map_RF<span class="op">*</span><span class="dv">30</span><span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="fl">1e4</span>, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)
<span class="co">#&gt; [1] 102089</span></code></pre></div>
</div>
<div id="ocs-3d-approach" class="section level2">
<h2><span class="header-section-number">7.7</span> Deriving OCS from soil profile data (the 3D approach)</h2>
<p>In the following example, we will demonstrate, using a known data set, (<a href="http://gsif.r-forge.r-project.org/edgeroi.html">Edgeroi</a>, from Australia) which has been well documented in the literature <span class="citation">(Malone et al. <a href="#ref-Malone2009Geoderma">2009</a>)</span>, how to derive OCS in t/ha using soil profile data and a 3D approach to spatial prediction based on mapping the Organic Carbon Density (OCD) in kg/m-cubic. The Edgeroi data set is a typical example of a soil profile data set that is relatively comprehensive, but still missing BLD measurements.</p>
<p>The Edgeroi data set can be loaded from the GSIF package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(GSIF)
<span class="kw">data</span>(edgeroi)
edgeroi.sp =<span class="st"> </span>edgeroi<span class="op">$</span>sites
<span class="kw">coordinates</span>(edgeroi.sp) &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>LONGDA94 <span class="op">+</span><span class="st"> </span>LATGDA94
<span class="kw">proj4string</span>(edgeroi.sp) &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="st">&quot;+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs&quot;</span>)
edgeroi.sp &lt;-<span class="st"> </span><span class="kw">spTransform</span>(edgeroi.sp, <span class="kw">CRS</span>(<span class="st">&quot;+init=epsg:28355&quot;</span>))</code></pre></div>
<p>This data set comes with a list of covariate layers which can be used to model the distribution of soil organic carbon:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;extdata/edgeroi.grids.rda&quot;</span>)
<span class="kw">gridded</span>(edgeroi.grids) &lt;-<span class="st"> </span><span class="er">~</span>x<span class="op">+</span>y
<span class="kw">proj4string</span>(edgeroi.grids) &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="st">&quot;+init=epsg:28355&quot;</span>)
<span class="kw">names</span>(edgeroi.grids)
<span class="co">#&gt; [1] &quot;DEMSRT5&quot; &quot;TWISRT5&quot; &quot;PMTGEO5&quot; &quot;EV1MOD5&quot; &quot;EV2MOD5&quot; &quot;EV3MOD5&quot;</span></code></pre></div>
<p>Because some of the covariate layers are factors e.g. <code>PMTGEO5</code> (parent material map) and because random forest requires numeric covariates, we can convert factors to numeric PCs by using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">edgeroi.spc =<span class="st"> </span><span class="kw">spc</span>(edgeroi.grids, <span class="op">~</span>DEMSRT5<span class="op">+</span>TWISRT5<span class="op">+</span>PMTGEO5<span class="op">+</span>EV1MOD5<span class="op">+</span>EV2MOD5<span class="op">+</span>EV3MOD5)
<span class="co">#&gt; Converting PMTGEO5 to indicators...</span>
<span class="co">#&gt; Converting covariates to principal components...</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/edgeroi_overview.jpeg" alt="Edgeroi data set: locations of soil profiles and Australian soil classification codes. For more details see Malone et al. (2009)." width="100%" />
<p class="caption">
(#fig:edgeroi-overview)Edgeroi data set: locations of soil profiles and Australian soil classification codes. For more details see Malone et al. (2009).
</p>
</div>
<p>Note that Edgeroi completely lacks any BLD values, therefore before we can compute OCD values, we need to estimate BLD values for each corresponding horizon. Here the easiest option is probably to use BLD values sourced from LandGIS predictions (and which you can download from <a href="https://landgis.opengeohub.org" class="uri">https://landgis.opengeohub.org</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">landgis.bld =<span class="st"> </span><span class="kw">list.files</span>(<span class="st">&quot;/mnt/DATA/LandGIS/predicted250m&quot;</span>, 
                     <span class="dt">pattern=</span><span class="kw">glob2rx</span>(<span class="st">&quot;sol_bulkdens.fineearth_usda.4a1h_m_*.tif$&quot;</span>), <span class="dt">full.names=</span><span class="ot">TRUE</span>)
<span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(landgis.bld)){
  <span class="kw">system</span>(<span class="kw">paste0</span>(<span class="st">&#39;gdalwarp &#39;</span>, landgis.bld[j], <span class="st">&#39; extdata/edgeroi_&#39;</span>, <span class="kw">basename</span>(landgis.bld[j]), 
         <span class="st">&#39; -t_srs </span><span class="ch">\&quot;</span><span class="st">&#39;</span>, <span class="kw">proj4string</span>(edgeroi.grids), <span class="st">&#39;</span><span class="ch">\&quot;</span><span class="st"> -tr 250 250 -co </span><span class="ch">\&quot;</span><span class="st">COMPRESS=DEFLATE</span><span class="ch">\&quot;</span><span class="st">&#39;</span>, 
         <span class="st">&#39; -te &#39;</span>, <span class="kw">paste</span>(<span class="kw">as.vector</span>(edgeroi.grids<span class="op">@</span>bbox), <span class="dt">collapse =</span> <span class="st">&quot; &quot;</span>)))
}</code></pre></div>
<p>Matching between the irregularly distributed soil horizons and LandGIS bulk density at standard depths can be implemented in three steps. First, we overlay the points and LandGIS GeoTIFFs to get the BLD values in kg / cubic-m at standard depths:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sg &lt;-<span class="st"> </span><span class="kw">list.files</span>(<span class="st">&quot;extdata&quot;</span>, <span class="st">&quot;edgeroi_sol_bulkdens.fineearth&quot;</span>, <span class="dt">full.names =</span> <span class="ot">TRUE</span>)
ov &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(raster<span class="op">::</span><span class="kw">extract</span>(<span class="kw">stack</span>(sg), edgeroi.sp)<span class="op">*</span><span class="dv">10</span>)
ov.edgeroi.BLD =<span class="st"> </span>ov[,<span class="kw">c</span>(<span class="kw">grep</span>(<span class="st">&quot;b0..&quot;</span>, <span class="kw">names</span>(ov), <span class="dt">fixed =</span> <span class="ot">TRUE</span>), <span class="kw">grep</span>(<span class="st">&quot;b10..&quot;</span>, <span class="kw">names</span>(ov), <span class="dt">fixed =</span> <span class="ot">TRUE</span>), 
                       <span class="kw">grep</span>(<span class="st">&quot;b30..&quot;</span>, <span class="kw">names</span>(ov), <span class="dt">fixed =</span> <span class="ot">TRUE</span>), <span class="kw">grep</span>(<span class="st">&quot;b60..&quot;</span>, <span class="kw">names</span>(ov), <span class="dt">fixed =</span> <span class="ot">TRUE</span>), 
                       <span class="kw">grep</span>(<span class="st">&quot;b100..&quot;</span>, <span class="kw">names</span>(ov), <span class="dt">fixed =</span> <span class="ot">TRUE</span>), <span class="kw">grep</span>(<span class="st">&quot;b200..&quot;</span>, <span class="kw">names</span>(ov), <span class="dt">fixed =</span> <span class="ot">TRUE</span>))]</code></pre></div>
<p>Second, we derive averaged estimates of BLD for standard depth intervals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ov.edgeroi.BLDm =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">BLD.f=</span><span class="kw">as.vector</span>(<span class="kw">sapply</span>(<span class="dv">2</span><span class="op">:</span><span class="kw">ncol</span>(ov.edgeroi.BLD),                                                <span class="cf">function</span>(i){<span class="kw">rowMeans</span>(ov.edgeroi.BLD[,<span class="kw">c</span>(i<span class="dv">-1</span>,i)])})),
                    <span class="dt">DEPTH.c=</span><span class="kw">as.vector</span>(<span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="cf">function</span>(i){<span class="kw">rep</span>(<span class="kw">paste0</span>(<span class="st">&quot;sd&quot;</span>,i),
                        <span class="kw">nrow</span>(edgeroi<span class="op">$</span>sites))})), <span class="dt">SOURCEID=</span><span class="kw">rep</span>(edgeroi<span class="op">$</span>sites<span class="op">$</span>SOURCEID, <span class="dv">5</span>))
<span class="kw">str</span>(ov.edgeroi.BLDm)
<span class="co">#&gt; &#39;data.frame&#39;:    1795 obs. of  3 variables:</span>
<span class="co">#&gt;  $ BLD.f   : num  1370 1335 1310 1500 1490 ...</span>
<span class="co">#&gt;  $ DEPTH.c : Factor w/ 5 levels &quot;sd1&quot;,&quot;sd2&quot;,&quot;sd3&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</span>
<span class="co">#&gt;  $ SOURCEID: Factor w/ 359 levels &quot;199_CAN_CP111_1&quot;,..: 1 2 3 4 5 6 7 8 9 10 ...</span></code></pre></div>
<p>Third, we match BLD values by matching horizon depths (center of horizon) with the standard depth intervals <code>sd1</code> to <code>sd5</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">edgeroi<span class="op">$</span>horizons<span class="op">$</span>DEPTH =<span class="st"> </span>edgeroi<span class="op">$</span>horizons<span class="op">$</span>UHDICM <span class="op">+</span><span class="st"> </span>(edgeroi<span class="op">$</span>horizons<span class="op">$</span>LHDICM <span class="op">-</span><span class="st"> </span>edgeroi<span class="op">$</span>horizons<span class="op">$</span>UHDICM)<span class="op">/</span><span class="dv">2</span>
edgeroi<span class="op">$</span>horizons<span class="op">$</span>DEPTH.c =<span class="st"> </span><span class="kw">cut</span>(edgeroi<span class="op">$</span>horizons<span class="op">$</span>DEPTH, <span class="dt">include.lowest=</span><span class="ot">TRUE</span>,
                               <span class="dt">breaks=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">30</span>,<span class="dv">60</span>,<span class="dv">100</span>,<span class="dv">1000</span>), <span class="dt">labels=</span><span class="kw">paste0</span>(<span class="st">&quot;sd&quot;</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>))
<span class="kw">summary</span>(edgeroi<span class="op">$</span>horizons<span class="op">$</span>DEPTH.c)
<span class="co">#&gt; sd1 sd2 sd3 sd4 sd5 </span>
<span class="co">#&gt; 391 379 408 391 769</span>
edgeroi<span class="op">$</span>horizons<span class="op">$</span>BLD.f =<span class="st"> </span>plyr<span class="op">::</span><span class="kw">join</span>(edgeroi<span class="op">$</span>horizons[,<span class="kw">c</span>(<span class="st">&quot;SOURCEID&quot;</span>,<span class="st">&quot;DEPTH.c&quot;</span>)], ov.edgeroi.BLDm)<span class="op">$</span>BLD.f
<span class="co">#&gt; Joining by: SOURCEID, DEPTH.c</span></code></pre></div>
<p>which shows relatively equal distribution of soil horizons for within the standard depths. Now that we have a rough estimate of the bulk density for all horizons, we can derive OCD in kg/m-cubic by using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">edgeroi<span class="op">$</span>horizons<span class="op">$</span>OCD =<span class="st"> </span>edgeroi<span class="op">$</span>horizons<span class="op">$</span>ORCDRC<span class="op">/</span><span class="dv">1000</span> <span class="op">*</span><span class="st"> </span>edgeroi<span class="op">$</span>horizons<span class="op">$</span>BLD.f
<span class="kw">summary</span>(edgeroi<span class="op">$</span>horizons<span class="op">$</span>OCD)
<span class="co">#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s </span>
<span class="co">#&gt;     0.1     2.4     7.2     9.5    13.2   124.9     262</span></code></pre></div>
<p>This shows that OCD values range from 0–110 kg/m-cubic, with an average of 9.5 kg/m-cubic (this corresponds to an average organic carbon content of about 0.8%).</p>
<p>For further 3D spatial prediction of OCD we use the ranger package, which fits a random forest model to this 3D data. We start by overlaying points and rasters so that we can create a regression matrix:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ov2 &lt;-<span class="st"> </span><span class="kw">over</span>(edgeroi.sp, edgeroi.spc<span class="op">@</span>predicted)
ov2<span class="op">$</span>SOURCEID =<span class="st"> </span>edgeroi.sp<span class="op">$</span>SOURCEID
h2 =<span class="st"> </span><span class="kw">hor2xyd</span>(edgeroi<span class="op">$</span>horizons)
m2 &lt;-<span class="st"> </span>plyr<span class="op">::</span><span class="kw">join_all</span>(<span class="dt">dfs =</span> <span class="kw">list</span>(edgeroi<span class="op">$</span>sites, h2, ov2))
<span class="co">#&gt; Joining by: SOURCEID</span>
<span class="co">#&gt; Joining by: SOURCEID</span></code></pre></div>
<p>The spatial prediction model can be fitted using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fm.OCD =<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste0</span>(<span class="st">&quot;OCD ~ DEPTH + &quot;</span>, <span class="kw">paste</span>(<span class="kw">names</span>(edgeroi.spc<span class="op">@</span>predicted), <span class="dt">collapse =</span> <span class="st">&quot;+&quot;</span>)))
fm.OCD
<span class="co">#&gt; OCD ~ DEPTH + PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + </span>
<span class="co">#&gt;     PC9 + PC10 + PC11 + PC12</span>
m.OCD &lt;-<span class="st"> </span><span class="kw">ranger</span>(fm.OCD, m2[<span class="kw">complete.cases</span>(m2[,<span class="kw">all.vars</span>(fm.OCD)]),], 
                <span class="dt">quantreg =</span> <span class="ot">TRUE</span>, <span class="dt">importance =</span> <span class="st">&quot;impurity&quot;</span>)
m.OCD
<span class="co">#&gt; Ranger result</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt;  ranger(fm.OCD, m2[complete.cases(m2[, all.vars(fm.OCD)]), ],      quantreg = TRUE, importance = &quot;impurity&quot;) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Type:                             Regression </span>
<span class="co">#&gt; Number of trees:                  500 </span>
<span class="co">#&gt; Sample size:                      4972 </span>
<span class="co">#&gt; Number of independent variables:  13 </span>
<span class="co">#&gt; Mtry:                             3 </span>
<span class="co">#&gt; Target node size:                 5 </span>
<span class="co">#&gt; Variable importance mode:         impurity </span>
<span class="co">#&gt; Splitrule:                        variance </span>
<span class="co">#&gt; OOB prediction error (MSE):       18.6 </span>
<span class="co">#&gt; R squared (OOB):                  0.694</span></code></pre></div>
<p>Which shows that the average error with Out-of-bag training points is ±4.2 kg/m-cubic. Note that setting <code>quantreg = TRUE</code> allows us to derive also a map of the prediction errors (Fig. @ref(fig:plot-edgeroi-ocd)), following the method of <span class="citation">Meinshausen (<a href="#ref-meinshausen2006quantile">2006</a>)</span>.</p>
<p>To derive OCS in tons/ha we can compute OCD at two depths (0 and 30 cm) and then take the mean value to produce a more representative value:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">30</span>)){
   edgeroi.spc<span class="op">@</span>predicted<span class="op">$</span>DEPTH =<span class="st"> </span>i
   OCD.rf &lt;-<span class="st"> </span><span class="kw">predict</span>(m.OCD, edgeroi.spc<span class="op">@</span>predicted<span class="op">@</span>data)
   nm1 =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;OCD.&quot;</span>, i, <span class="st">&quot;cm&quot;</span>)
   edgeroi.grids<span class="op">@</span>data[,nm1] =<span class="st"> </span>OCD.rf<span class="op">$</span>predictions
   OCD.qrf &lt;-<span class="st"> </span><span class="kw">predict</span>(m.OCD, edgeroi.spc<span class="op">@</span>predicted<span class="op">@</span>data, 
                      <span class="dt">type=</span><span class="st">&quot;quantiles&quot;</span>, <span class="dt">quantiles=</span><span class="kw">c</span>((<span class="dv">1</span><span class="fl">-.682</span>)<span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="op">-</span>(<span class="dv">1</span><span class="fl">-.682</span>)<span class="op">/</span><span class="dv">2</span>))
   nm2 =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;OCD.&quot;</span>, i, <span class="st">&quot;cm_se&quot;</span>)
   edgeroi.grids<span class="op">@</span>data[,nm2] =<span class="st"> </span>(OCD.qrf<span class="op">$</span>predictions[,<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>OCD.qrf<span class="op">$</span>predictions[,<span class="dv">1</span>])<span class="op">/</span><span class="dv">2</span>
}</code></pre></div>
<p>so that the final Organic carbon stocks in t/ha is:</p>
<pre><code>#&gt; 
#&gt; Attaching package: &#39;raster&#39;
#&gt; The following objects are masked from &#39;package:aqp&#39;:
#&gt; 
#&gt;     metadata, metadata&lt;-
#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#&gt;    20.7    39.5    47.6    48.5    56.4   105.7</code></pre>
<div class="figure" style="text-align: center">
<img src="Soil_organic_carbon_files/figure-html/plot-edgeroi-ocd-1.png" alt="Predicted organic carbon stock for 0–30 cm depth and error map for the Edgeroi data set. All values expressed in tons/ha." width="100%" />
<p class="caption">
(#fig:plot-edgeroi-ocd)Predicted organic carbon stock for 0–30 cm depth and error map for the Edgeroi data set. All values expressed in tons/ha.
</p>
</div>
<p>Note that deriving the error map in the ranger package can be computationally intensive, especially if the number of covariates is high. It is therefore not yet recommended for large rasters.</p>
<p>Next, we can derive the total soil organic carbon stock per <a href="http://data.environment.nsw.gov.au/dataset/nsw-landuseac11c">land use class</a> (2007). For this we can use the aggregation function from the plyr package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rgdal)
<span class="co">#&gt; rgdal: version: 1.3-6, (SVN revision 773)</span>
<span class="co">#&gt;  Geospatial Data Abstraction Library extensions to R successfully loaded</span>
<span class="co">#&gt;  Loaded GDAL runtime: GDAL 2.3.2, released 2018/09/21</span>
<span class="co">#&gt;  Path to GDAL shared files: /usr/share/gdal</span>
<span class="co">#&gt;  GDAL binary built with GEOS: TRUE </span>
<span class="co">#&gt;  Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493]</span>
<span class="co">#&gt;  Path to PROJ.4 shared files: (autodetected)</span>
<span class="co">#&gt;  Linking to sp version: 1.3-1</span>
edgeroi.grids<span class="op">$</span>LandUse =<span class="st"> </span><span class="kw">readGDAL</span>(<span class="st">&quot;extdata/edgeroi_LandUse.sdat&quot;</span>)<span class="op">$</span>band1
<span class="co">#&gt; extdata/edgeroi_LandUse.sdat has GDAL driver SAGA </span>
<span class="co">#&gt; and has 128 rows and 190 columns</span>
lu.leg =<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;extdata/LandUse.csv&quot;</span>)
edgeroi.grids<span class="op">$</span>LandUseClass =<span class="st"> </span><span class="kw">paste</span>(<span class="kw">join</span>(<span class="kw">data.frame</span>(<span class="dt">LandUse=</span>edgeroi.grids<span class="op">$</span>LandUse), 
                                        lu.leg, <span class="dt">match=</span><span class="st">&quot;first&quot;</span>)<span class="op">$</span>LU_NSWDeta)
<span class="co">#&gt; Joining by: LandUse</span>
OCS_agg.lu &lt;-<span class="st"> </span>plyr<span class="op">::</span><span class="kw">ddply</span>(edgeroi.grids<span class="op">@</span>data, .(LandUseClass), summarize,
                          <span class="dt">Total_OCS_kt=</span><span class="kw">round</span>(<span class="kw">sum</span>(OCS<span class="fl">.30</span>cm<span class="op">*</span><span class="dv">250</span><span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="fl">1e4</span>, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)<span class="op">/</span><span class="fl">1e3</span>),
                          <span class="dt">Area_km2=</span><span class="kw">round</span>(<span class="kw">sum</span>(<span class="op">!</span><span class="kw">is.na</span>(OCS<span class="fl">.30</span>cm))<span class="op">*</span><span class="dv">250</span><span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="fl">1e6</span>))
OCS_agg.lu<span class="op">$</span>LandUseClass.f =<span class="st"> </span><span class="kw">strtrim</span>(OCS_agg.lu<span class="op">$</span>LandUseClass, <span class="dv">34</span>)
OCS_agg.lu<span class="op">$</span>OCH_t_ha_M =<span class="st"> </span><span class="kw">round</span>(OCS_agg.lu<span class="op">$</span>Total_OCS_kt<span class="op">*</span><span class="dv">1000</span><span class="op">/</span>(OCS_agg.lu<span class="op">$</span>Area_km2<span class="op">*</span><span class="dv">100</span>))
OCS_agg.lu[OCS_agg.lu<span class="op">$</span>Area_km2<span class="op">&gt;</span><span class="dv">5</span>,<span class="kw">c</span>(<span class="st">&quot;LandUseClass.f&quot;</span>,<span class="st">&quot;Total_OCS_kt&quot;</span>,<span class="st">&quot;Area_km2&quot;</span>,<span class="st">&quot;OCH_t_ha_M&quot;</span>)]
<span class="co">#&gt;                        LandUseClass.f Total_OCS_kt Area_km2 OCH_t_ha_M</span>
<span class="co">#&gt; 2  Constructed grass waterway for wat           55       11         50</span>
<span class="co">#&gt; 3                              Cotton           42        8         52</span>
<span class="co">#&gt; 4                  Cotton - irrigated          828      203         41</span>
<span class="co">#&gt; 5   Cropping - continuous or rotation         1788      402         44</span>
<span class="co">#&gt; 6  Cropping - continuous or rotation           235       59         40</span>
<span class="co">#&gt; 10                           Farm dam           54       10         54</span>
<span class="co">#&gt; 11 Farm Infrastructure - house, machi           90       18         50</span>
<span class="co">#&gt; 12 Grazing - Residual strips (block o           49       10         49</span>
<span class="co">#&gt; 13 Grazing of native vegetation. Graz          685      129         53</span>
<span class="co">#&gt; 14 Grazing of native vegetation. Graz           68       13         52</span>
<span class="co">#&gt; 16                     Irrigation dam           66       16         41</span>
<span class="co">#&gt; 21                      Native forest          226       37         61</span>
<span class="co">#&gt; 26                  Research facility           40        9         44</span>
<span class="co">#&gt; 27 River, creek or other incised drai           69       11         63</span>
<span class="co">#&gt; 28               Road or road reserve          117       23         51</span>
<span class="co">#&gt; 29                       State forest          407       83         49</span>
<span class="co">#&gt; 32 Volunteer, naturalised, native or          1378      238         58</span>
<span class="co">#&gt; 33 Volunteer, naturalised, native or            64       16         40</span>
<span class="co">#&gt; 34 Volunteer, naturalised, native or            75       14         54</span>
<span class="co">#&gt; 35 Volunteer, naturalised, native or           471       99         48</span>
<span class="co">#&gt; 37 Wide road reserve or TSR, with som          468       90         52</span></code></pre></div>
<p>Which shows that, for the <code>Cropping - continuous or rotation</code>, which is the dominant land use class in the area, the average OCS is 43 tons/ha for the 0–30 cm depth. In this case, the total soil organic carbon stock for the whole area (for all land use classes) is ca 7154 thousand tons of C. There do not appear to be large differences in OCS between the natural vegetation and croplands.</p>
</div>
<div id="deriving-ocs-using-spatiotemporal-models" class="section level2">
<h2><span class="header-section-number">7.8</span> Deriving OCS using spatiotemporal models</h2>
<p>Assuming that measurements of ORC have also been referenced temporally (at least to the year of sampling), point data can be used to build spatiotemporal models of soil organic carbon. Consider, for example, the soil profile data available for the conterminous USA:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">OCD_stN &lt;-<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;extdata/usa48.OCD_spacetime_matrix.rds&quot;</span>)
<span class="kw">dim</span>(OCD_stN)
<span class="co">#&gt; [1] 250428    134</span></code></pre></div>
<p>This data shows that there are actually sufficient observations spread through time (last 60+ years) to fit a spatiotemporal model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(OCD_stN<span class="op">$</span>YEAR, <span class="dt">xlab=</span><span class="st">&quot;Year&quot;</span>, <span class="dt">main=</span><span class="st">&quot;&quot;</span>, <span class="dt">col=</span><span class="st">&quot;darkgrey&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="Soil_organic_carbon_files/figure-html/thist-usa48-1.png" alt="Distribution of soil observations based on sampling year." width="100%" />
<p class="caption">
(#fig:thist-usa48)Distribution of soil observations based on sampling year.
</p>
</div>
<p>In fact, because the data set above represents values of OCD at variable depths, we can use this data to fit a full 3D+T spatiotemporal model in the form:</p>
<span class="math display">\[\begin{equation}
{\rm OCD}(xydt) = d + X_1 (xy) + \ldots + X_k (xy) + \ldots + X_p (xyt)
(\#eq:stmodel)
\end{equation}\]</span>
<p>where <span class="math inline">\(d\)</span> is the depth, <span class="math inline">\(X_k (xy)\)</span> are static covariates i.e. the covariates that do not change in time, and <span class="math inline">\(X_p (xyt)\)</span> are spatiotemporal covariates i.e. covariates that change with time. Here we can assume that the static covariates are mainly landform and lithology: these have probably not changed significantly in the last 100 years. Land cover, land use and climate, on the other hand, have probably changed drastically in the last 100 years and have to be represented with a time-series of images. There are, indeed, several time-series data sets now available that can be used to represent land cover dynamics:</p>
<ul>
<li><p><a href="ftp://ftp.pbl.nl/hyde/hyde3.2/">HYDE 3.2 Historic land use data set</a> <span class="citation">(Klein Goldewijk et al. <a href="#ref-klein2011hyde">2011</a>)</span>: portrays the distribution of major agricultural systems from 10,000 BC (pre-historic no land-use condition) to the present time. 10 categories of land use have been represented: total cropping, total grazing, pasture (improved grazing-land), rangeland (unimproved grazing-land), total rainfed cropping, total irrigated cropping, with further subdivisions for rice and non-rice cropping systems for both rainfed and irrigated cropping.</p></li>
<li><p><a href="http://www.ipcc-data.org/observ/clim/">CRU TS2.1 climatic surfaces</a> for period 1960–1990 <span class="citation">(Harris et al. <a href="#ref-harris2014updated">2014</a>)</span>.</p></li>
<li><p><a href="http://www.unep-wcmc.org/resources-and-data/generalised-original-and-current-forest">UNEP-WCMC Generalized Original and Current Forest cover map</a> showing global dynamics of forest cover.</p></li>
</ul>
<p>All these layers are available only at a relatively coarse resolution of 10 km, but then cover longer time spans. Note also that, since these are time-series images, spatiotemporal overlay can take time as spatial overlay must be repeated for each time period. The spatiotemporal matrix file already contains the results of overlay, so that we can focus directly on building spatiotemporal models of OCD e.g.:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pr.lst &lt;-<span class="st"> </span><span class="kw">names</span>(OCD_stN)[<span class="op">-</span><span class="kw">which</span>(<span class="kw">names</span>(OCD_stN) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;SOURCEID&quot;</span>, <span class="st">&quot;DEPTH.f&quot;</span>, <span class="st">&quot;OCDENS&quot;</span>, 
                                                      <span class="st">&quot;YEAR&quot;</span>, <span class="st">&quot;YEAR_c&quot;</span>, <span class="st">&quot;LONWGS84&quot;</span>, <span class="st">&quot;LATWGS84&quot;</span>))]
fm0.st &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste</span>(<span class="st">&#39;OCDENS ~ DEPTH.f + &#39;</span>, <span class="kw">paste</span>(pr.lst, <span class="dt">collapse=</span><span class="st">&quot;+&quot;</span>)))
sel0.m =<span class="st"> </span><span class="kw">complete.cases</span>(OCD_stN[,<span class="kw">all.vars</span>(fm0.st)])
## takes &gt;2 mins
rf0.OCD_st &lt;-<span class="st"> </span><span class="kw">ranger</span>(fm0.st, <span class="dt">data=</span>OCD_stN[sel0.m,<span class="kw">all.vars</span>(fm0.st)], 
                     <span class="dt">importance=</span><span class="st">&quot;impurity&quot;</span>, <span class="dt">write.forest=</span><span class="ot">TRUE</span>, <span class="dt">num.trees=</span><span class="dv">120</span>)</code></pre></div>
<p>the most important covariates being:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xl &lt;-<span class="st"> </span><span class="kw">as.list</span>(ranger<span class="op">::</span><span class="kw">importance</span>(rf0.OCD_st))
<span class="kw">print</span>(<span class="kw">t</span>(<span class="kw">data.frame</span>(xl[<span class="kw">order</span>(<span class="kw">unlist</span>(xl), <span class="dt">decreasing=</span><span class="ot">TRUE</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]])))</code></pre></div>
<p>which shows that the most important soil covariate by far is soil depth, followed by elevation, grazing, MODIS cloud fraction images, cropland and similar. For a full description of codes please refer to <span class="citation">Sanderman, Hengl, and Fiske (<a href="#ref-sanderman2018soil">2018</a>)</span> (also available in <a href="https://github.com/whrc/Soil-Carbon-Debt/blob/master/SOCS/WHRC_soilcarbon_list_of_covariates.csv">this table</a>).</p>
<p>Finally, based on this model, we can generate predictions for 3–4 specific time periods and for some arbitrary depth e.g. 10 cm. The maps below clearly show that ca 8% of soil organic carbon has been lost in the last 90 years, most likely due to increases in grazing and croplands. The maps also show, however, that some areas in the northern latitudes are experiencing an increase in SOC, possibly due to higher rainfall i.e. based on the CRU data set.</p>
<div class="figure" style="text-align: center">
<img src="figures/usa48_ocd_10cm_year2014.png" alt="Predicted OCD (in kg/cubic-m) at 10 cm depth for the year 2014. Blue colors indicate low values, red high values." width="100%" />
<p class="caption">
(#fig:usa48-ocd-2014)Predicted OCD (in kg/cubic-m) at 10 cm depth for the year 2014. Blue colors indicate low values, red high values.
</p>
</div>
<div class="figure" style="text-align: center">
<img src="figures/usa48_ocd_10cm_year1925.png" alt="Predicted OCD (in kg/cubic-m) at 10 cm depth for the year 1925." width="100%" />
<p class="caption">
(#fig:usa48-ocd-1925)Predicted OCD (in kg/cubic-m) at 10 cm depth for the year 1925.
</p>
</div>
<p>This demonstrates that, as long as there is enough training data spread through time, and as long as covariates are available for the corresponding time ranges, machine learning can also be used to fit full 3D+T spatiotemporal prediction models <span class="citation">(Gasch et al. <a href="#ref-Gasch2015SPASTA">2015</a>)</span>. Once we produce a time-series of images for some target soil variable of interest, the next step would be to implement time-series analysis methods to e.g. detect temporal trends and areas of highest apparent soil degradation. An R package that is fairly useful for such analysis is the <a href="http://greenbrown.r-forge.r-project.org/">greenbrown</a> package, primarily used to map and quantify degradation of land cover <span class="citation">(Forkel et al. <a href="#ref-forkel2015codominant">2015</a>)</span>.</p>
<p>We can focus on the time-series of predicted organic carbon density for USA48:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(greenbrown)
<span class="kw">library</span>(raster)
<span class="kw">setwd</span>()
tif.lst &lt;-<span class="st"> </span><span class="kw">list.files</span>(<span class="st">&quot;extdata/USA48&quot;</span>, <span class="dt">pattern=</span><span class="st">&quot;_10km.tif&quot;</span>, <span class="dt">full.names =</span> <span class="ot">TRUE</span>)
g10km &lt;-<span class="st"> </span><span class="kw">as</span>(<span class="kw">readGDAL</span>(tif.lst[<span class="dv">1</span>]), <span class="st">&quot;SpatialPixelsDataFrame&quot;</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="kw">length</span>(tif.lst)){ g10km<span class="op">@</span>data[,i] =<span class="st"> </span><span class="kw">readGDAL</span>(tif.lst[i], <span class="dt">silent=</span><span class="ot">TRUE</span>)<span class="op">$</span>band1[g10km<span class="op">@</span>grid.index] }
<span class="kw">names</span>(g10km) =<span class="st"> </span><span class="kw">basename</span>(tif.lst)
g10km =<span class="st"> </span><span class="kw">as.data.frame</span>(g10km)
<span class="kw">gridded</span>(g10km) =<span class="st"> </span><span class="er">~</span>x<span class="op">+</span>y
<span class="kw">proj4string</span>(g10km) =<span class="st"> &quot;+proj=longlat +datum=WGS84&quot;</span></code></pre></div>
<p>to speed up processing we can subset grids and focus on the State of Texas:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(maps)
<span class="kw">library</span>(maptools)
states &lt;-<span class="st"> </span><span class="kw">map</span>(<span class="st">&#39;state&#39;</span>, <span class="dt">plot=</span><span class="ot">FALSE</span>, <span class="dt">fill=</span><span class="ot">TRUE</span>)
states =<span class="st"> </span><span class="kw">SpatialPolygonsDataFrame</span>(<span class="kw">map2SpatialPolygons</span>(states, <span class="dt">IDs=</span><span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(states<span class="op">$</span>names)),
                                  <span class="kw">data.frame</span>(<span class="dt">names=</span>states<span class="op">$</span>names))
<span class="kw">proj4string</span>(states) =<span class="st"> &quot;+proj=longlat +datum=WGS84&quot;</span>
ov.g10km =<span class="st"> </span><span class="kw">over</span>(<span class="dt">y=</span>states, <span class="dt">x=</span>g10km)
txg10km =<span class="st"> </span>g10km[<span class="kw">which</span>(ov.g10km<span class="op">$</span>names<span class="op">==</span><span class="st">&quot;texas&quot;</span>),]
txg10km =<span class="st"> </span><span class="kw">as.data.frame</span>(txg10km)
<span class="kw">gridded</span>(txg10km) =<span class="st"> </span><span class="er">~</span>x<span class="op">+</span>y
<span class="kw">proj4string</span>(txg10km) =<span class="st"> &quot;+proj=longlat +datum=WGS84&quot;</span>
<span class="kw">spplot</span>(<span class="kw">log1p</span>(<span class="kw">stack</span>(txg10km)), <span class="dt">col.regions=</span>SAGA_pal[[<span class="dv">1</span>]])
g10km.b =<span class="st"> </span>raster<span class="op">::</span><span class="kw">brick</span>(txg10km)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/rplot_timeseries_ocd_maps_texas.png" alt="Time-series of predictions of organic carbon density for Texas." width="100%" />
<p class="caption">
(#fig:time-series-texas)Time-series of predictions of organic carbon density for Texas.
</p>
</div>
<p>We can analyze this time-series data to see where the decrease in organic carbon is most significant, for example the slope of the change:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trendmap &lt;-<span class="st"> </span><span class="kw">TrendRaster</span>(g10km.b, <span class="dt">start=</span><span class="kw">c</span>(<span class="dv">1935</span>, <span class="dv">1</span>), <span class="dt">freq=</span><span class="dv">1</span>, <span class="dt">breaks=</span><span class="dv">1</span>) 
## can be computationally intensive
<span class="kw">plot</span>(trendmap[[<span class="st">&quot;SlopeSEG1&quot;</span>]], 
     <span class="dt">col=</span><span class="kw">rev</span>(SAGA_pal[[<span class="st">&quot;SG_COLORS_GREEN_GREY_RED&quot;</span>]]), 
     <span class="dt">zlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">1.5</span>,<span class="fl">1.5</span>), <span class="dt">main=</span><span class="st">&quot;Slope SEG1&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/rplot_splope_ocd_change.png" alt="Predicted slope of change of soil organic carbon density for Texas for the period 1935–2014. Negative values indicate loss of soil organic carbon." width="80%" />
<p class="caption">
(#fig:ocd-slope-texas)Predicted slope of change of soil organic carbon density for Texas for the period 1935–2014. Negative values indicate loss of soil organic carbon.
</p>
</div>
<p>which shows that loss of soil organic carbon is especially distinct in the southern part of Texas. The slope coefficient map is, on average, negative, which indicates that most of the state has lost organic carbon for the period of interest. Note that running such time-series analysis is not trivial as a sufficient number of observations in time (if possible: repetitions) is needed to be able to extract significant patterns. Also <code>TrendRaster</code> function can be quite computationally intensive, hence some careful planning of the processing steps / processing infrastructure is usually recommended.</p>
</div>
<div id="summary-points-4" class="section level2">
<h2><span class="header-section-number">7.9</span> Summary points</h2>
<p>Based on all the examples and discussion above, the following key points can be emphasized:</p>
<ol style="list-style-type: decimal">
<li>OCS for an area of interest can be derived either using 2D or 3D approaches. 3D approaches typically include modeling ORC, BLD and CRF separately (and then deriving OCS per pixel), or modeling OCD for standard depths and then converting to OCS.</li>
<li>Publicly available RS-based covariates (SRTM / ALOS DEM, Landsat, Sentinel satellites) are available for improving the mapping accuracy of OCS. Improving the accuracy of OCS maps is becoming less expensive, given the increasing availability of RS data.</li>
<li>PT (Pedo-Transfer) rules can be used to fill in (estimate) missing BLD values and to estimate ORC for deeper soil depths. Also global maps with predictions of BLD and CRF can be used to supply missing values, if there are no other alternatives.</li>
<li>Machine learning techniques such as Random Forest, neural nets, gradient boosting and similar, can be used to predict soil organic carbon in 2D, 3D and in spatiotemporal modeling frameworks. The accuracy of these predictions is improved relative to linear statistical models, especially where the relationship between soil organic carbon distribution and climatic, land cover, hydrological, relief and similar covariates is complex (i.e. non-linear).</li>
<li>Global estimates of ORC, BLD and CRF can be used as covariates so that consistent predictions can be produced (as explained in <span class="citation">Ramcharan et al. (<a href="#ref-ramcharan2018soil">2018</a><a href="#ref-ramcharan2018soil">a</a>)</span>).</li>
<li>By producing spatial predictions of OCS for specific time periods, one can derive estimates of OCS change (loss or gain).</li>
<li>Most of the statistical / analytical tools required for running spatial analysis, time series analysis, export and visualization of soil carbon data are available in R, especially thanks to the contributed packages: aqp, caret, ranger, xgboost, GSIF, greenbrown and similar.</li>
</ol>
<!--chapter:end:Soil_organic_carbon.Rmd-->
</div>
</div>
<div id="practical-tips" class="section level1">
<h1><span class="header-section-number">8</span> Practical tips for organizing Predictive Soil Mapping</h1>
<p><em>Edited by: T. Hengl, R. A. MacMillan and I. Wheeler</em></p>
<div id="critical-aspects-of-predictive-soil-mapping" class="section level2">
<h2><span class="header-section-number">8.1</span> Critical aspects of Predictive Soil Mapping</h2>
<p>The previous chapters have reviewed many of the technical aspects of PSM. To a statistical, following the right procedure and applying the right statistical framework will be the key element of success of a PSM project. In practice, it is really the combination of all elements and aspects that determines a success of PSM project. In this chapter we provide some practical tips on how to organize work and what to be especially carefull about. We refer to these as the <em>critical aspects of PSM projects</em>.</p>
<p>At the end of the chapter we also try to give a more practical tips in sense of check-lists and simple economic calculus, to try to prevent from making unrealistic plans or producing maps that nobody will eventually use.</p>
<div id="psm-main-steps" class="section level3">
<h3><span class="header-section-number">8.1.1</span> PSM main steps</h3>
<p>Based on all theory explained previously, we can say that PSM is a six-step process:</p>
<ol style="list-style-type: decimal">
<li><p>Preparation of point data (training data).</p></li>
<li><p>Preparation of covariate data (the explanatory variables).</p></li>
<li><p>Model fitting and validation (building rules by overlay, model fitting and cross-validation).</p></li>
<li><p>Predictions and generation of (currently best-possible) final maps (applying the rules).</p></li>
<li><p>Archiving and distribution of maps (usually via soil geographical databases and/or web services).</p></li>
<li><p>Updates and improvements (support).</p></li>
</ol>
<p>In principle, there are three main types of PSM projects:</p>
<ol style="list-style-type: upper-alpha">
<li><p>PSM projects in new areas — no point observations or samples currently exist.</p></li>
<li><p>PSM projects using legacy points — sufficient point data to support PSM exist and are available, but no previous PSM modelling has been implemented in this area.</p></li>
<li><p>PSM projects aiming at optimizing predictions and usability — Previous PSM models have already been completed but previous results can still be improved / optimized.</p></li>
</ol>
<p>If point data are not available, then collecting new point data, via field work and laboratory analysis will usually consume a majority of any PSM project budget (PSM projects type A). Otherwise, if point data are already available (and only need to be imported and harmonized), the most time consuming part of PSM will likely be preparation of covariate data layers (PSM projects type B). Predictions can also take a long time and computing costs per update can be significant (see further sections). Personnel costs can be more significant than server costs as programming can demand weeks of staff time. However, if programming is done at a high level (e.g. through generic functions and objects), subsequent updates should require less personnel time as predictions can be increasingly automated.</p>
<p>Another aspect of PSM is the time dimension i.e. will maps be continuously updated or do they need to produced only once and then remain relevant and useful for years (often up to decades), so that PSM projects can also be classified into:</p>
<ol style="list-style-type: upper-roman">
<li><p>PSM projects for the purpose of mapping static (stable) spatial patterns only.</p></li>
<li><p>PSM projects for the purpose of one-time change detection (e.g. two time intervals).</p></li>
<li><p>PSM projects for the purpose of monitoring soil conditions / status (continuous updates at regular intervals).</p></li>
</ol>
<p>To date, almost all conventional soil mapping ignores time and change and instead tries to pretend that soil properties are static and persist through time virtually unaltered. Increasingly, however, new generation PSM projects aim to monitor changes in soil resources, with special focus given to changes in soil organic carbon, soil nutrients and similar (Fig. @ref(fig:psm-types-monitoring)). For PSM project type III spatio-temporal prediction models can be used (as in meteorology for example), but then this requires that sufficient training data are available in the time domain e.g. at least five measurement intervals / repetitions.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_types_of_PSM_projects.png" alt="Types of PSM projects depending on whether maps are generated for single usage, or for detecting change or soil monitoring." width="75%" />
<p class="caption">
(#fig:psm-types-monitoring)Types of PSM projects depending on whether maps are generated for single usage, or for detecting change or soil monitoring.
</p>
</div>
</div>
<div id="psm-input-and-output-spatial-data-layers" class="section level3">
<h3><span class="header-section-number">8.1.2</span> PSM input and output spatial data layers</h3>
<p>In PSM, there are, in principle, three (3) main types of spatial input layers <span class="citation">(Hengl, Mendes de Jesus, et al. <a href="#ref-Hengl2017SoilGrids250m">2017</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Soil samples (usually points or transects) are spatially incomplete. They are used as evidence in generating spatial predictions. In vertical and horizontal dimensions, soil points might refer to volumes i.e. have a block support. Often only the horizontal (2D) support is mentioned, and the 3D support has to be inferred from the description of the depth slice(s) sampled.</p></li>
<li><p>Soil mask i.e.a raster map delineating the spatial domain of interest for PSM. Commonly derived from a land cover map with water bodies, permanent ice and similar removed from predictions.</p></li>
<li><p>Covariates i.e. grid maps that depict environmental conditions. Ideally all covariates are “stacked” to exactly match the same grid and all missing values and inconsistencies are resolved prior to PSM.</p></li>
</ol>
<p>And three (3) main types of spatial output layers:</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Spatial predictions of (primary) soil variables that are spatially complete i.e. are produced and available for the entire extent of the soil mask.</p></li>
<li><p>Maps of (secondary) soil variables which are derived using calculations applied to combinations of the primary predicted soil variables. These are usually less expensive to produce, by an order of magnitude, than spatial predictions of primary soil variables.</p></li>
<li><p>Maps quantifying uncertainty in terms of prediction error, prediction interval, confusion index or similar metrics. These may be derived at the same time as predictions are made or can be made completely independently of predictions.</p></li>
</ol>
<p>Each element of the map types listed above needs to have a consistent spatio-temporal reference, which typically includes:</p>
<ul>
<li><p>Geographic location in local or geographic coordinates (for global modelling we usually prefer initial georeferencing that uses longitude and latitude in the WGS84 coordinate system);</p></li>
<li><p>Depth interval expressed in cm from the land surface (upper and lower depth) for layers and point depth for point predictions;</p></li>
<li><p>Support size or referent soil volume (or voxel) i.e. the horizontal sampling area multiplied by the thickness of the sampling block e.g. 30 ✕ 30 ✕ 0.3 m.</p></li>
<li><p>Temporal reference i.e. a begin and an end date/time of the period of measurements/estimations. Specifying exact spatial and temporal references in the metadata can is vital for optimal production and use of maps.</p></li>
</ul>
<p>Spatial predictions of primary soil properties can be used to:</p>
<ul>
<li><p>Derive spatial aggregates (upscaling to coarser resolution).</p></li>
<li><p>Derive vertical aggregates e.g. mean pH in 0–100 cm of soil (for this we usually recommend using the trapezoidal rule as explained in <span class="citation">Hengl, Mendes de Jesus, et al. (<a href="#ref-Hengl2017SoilGrids250m">2017</a>)</span>).</p></li>
<li><p>Derive secondary soil properties e.g. available water capacity, organic carbon stock etc.</p></li>
</ul>
<p>Spatial predictions of primary soil variables and derived soil variables are used for decision making and further modeling i.e. they are used to construct a <strong>Soil Information System</strong> once all values of all variables are known for all pixels within the soil mask. A SIS should ideally provide information that can directly support input to modeling, planning and decision-making.</p>
</div>
</div>
<div id="technical-specifications-affecting-the-majority-of-production-costs" class="section level2">
<h2><span class="header-section-number">8.2</span> Technical specifications affecting the majority of production costs</h2>
<p>The majority of the costs of a PSM project are controlled by the following:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Spatial resolution</strong> (commonly 30 m, 100 m or 250 m): Spatial resolution is crucial in determining the total costs of PSM, especially in terms of computing, storage, network traffic and hardware requirements. Changing the spatial resolution from 100 to 30 m means that about 10 times more pixels will need to be produced, stored and shared via the network. This does not always imply that the costs of PSM will also be 10 times greater than for a 100 m resolution project, but the increase in costs is often going to follow a quadratic function. Also note that, for even finer resolutions e.g. 5 m, very limited free public covariate data are available and additional purchases of commercial RS products will typically be required. For example the latest 12 m resolution WorldDEM (<a href="https://worlddem-database.terrasar.com/" class="uri">https://worlddem-database.terrasar.com/</a>) can cost up to 10 USD per square km, and which can increase PSM costs significantly.</p></li>
<li><p><strong>List of target variables and their complexity</strong>: Some PSM projects focus on mapping 1–2 soil variables only, and as such can be rather straightforward to implement. Any PSM project that requires creation of a complete Soil Information System (tens of quantitative soil variables and soil types), will definitely demand more effort and hence potentially significantly increase costs. Typically, evaluation and quality control of maps in a SIS requires an analyst to open and visually compare patterns from different maps and to make use of considerable empirical knowledge of soils. Costs of production can also be significantly increased depending on whether lower and upper prediction intervals are required. As with increasing spatial resolution, requesting lower and upper prediction intervals means that two times more pixels will need to be produced.</p></li>
<li><p><strong>Targeted accuracy/quality levels</strong>: Often the agencies that order spatial soil information expect that predictions will achieve some desired accuracy targets. Accuracy of predictions can, indeed, often be improved (but only up to a certain level), by simply improving the modelling framework (PSM projects type C). In practice, if a contractor requires significant improvements in accuracy, then this often means that both additional point records and improved covariate data (for example at finer spatial resolution) will need to be collected and/or purchased. This can often mean that the original budget will have to be increased until the required accuracy level can be reached.</p></li>
<li><p><strong>List of targeted services / user domain</strong>: Is the goal of the PSM project to produce data only, or to serve this data for a number of applications (use-cases)? How broad is the user domain? Is the data being made for a few targeted clients or for the widest possible user base? Is high traffic expected and, if so, how will the costs of hosting and serving the data and processes be met? Producing a robust, scalable web-system that can serve thousands of users at the same time requires additional investments in programming and maintenance.</p></li>
<li><p><strong>Commercialization options</strong>: Commercialization of data and services can also significantly increase costs, since the development team needs to prepare also workflows where invoices and bills are generated on demand, or where efficient support and security are now critically important. Even though many companies exist that offer outsourcing of this functionality, many organizations and companies prefer to have full control of the commercialization steps, hence such functionality needs to be then developed internally within the project or organization.</p></li>
</ol>
<div id="field-observations-and-measurements" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Field observations and measurements</h3>
<p><strong>Observations and measurements</strong> (O&amp;M) are at the heart of all advances in scientific endeavour. One cannot describe, or attempt to understand, what one cannot see, or measure. Great leaps in scientific understanding have always followed from major improvements in the ability to see, and measure, phenomenon or objects. Think of the telescope and astronomy, the microscope and microbiology, the X-ray and atomic structure or crystallography and so on.</p>
<p>In the area of resource inventories, observations and measurements carried out in the field (field data) provide the evidence critical to developing the understanding of spatial patterns and spatial processes that underpins all models that predict the spatial distribution of properties or classes. This applies equally to subjective, empirical mental, or conceptual, models and objective, quantitative statistical models. The more and better the observations and measurements we obtain, the better will be our ability to understand and predict spatial patterns of soils and other natural phenomena. Consider here some general observations on how to maximize efficiency of O&amp;M:</p>
<ul>
<li><p>For maximum utility, field data should be objective and reproducible.</p></li>
<li><p>They should be collected using some kind of unbiased sampling design that supports reproducibility and return sampling.</p></li>
<li><p>They should be located as accurately as possible in both space (geolocation) and time (temporal location).</p></li>
<li><p>They should describe and measure actual conditions in their present state (and current land use) and not in some assumed natural, climax or equilibrium condition.</p></li>
<li><p>They should capture and permit description of spatial and temporal variation across multiple spatial scales and time frames.</p></li>
</ul>
<p>It is widely assumed that collecting new field data to produce new and improved inventory products is prohibitively expensive and will never be possible or affordable in the foreseeable future. Consequently, most current projects or programs that aim to produce new maps of soils or other terrestrial entities have explicitly embraced the assumption that the only feasible way to produce new soil maps is to locate, and make use of, existing legacy data consisting of previously reported field observations or existing laboratory analysed field samples. However, recent activities in Africa (www.Africasoils.net), for example, have demonstrated conclusively that it is feasible, affordable and beneficial to collect new field observations and samples and to analyse new soil samples affordably and to a high standard <span class="citation">(Shepherd and Walsh <a href="#ref-ShepherdWalsh2007JNIS">2007</a>)</span>.</p>
</div>
<div id="preparation-of-point-data" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Preparation of point data</h3>
<p>Import of basic O&amp;M field data (e.g. soil point data) can be time consuming and require intensive checking and harmonization. Communicating with the original data producers is highly recommended to reduce errors during import. Getting original data producers involved can be best achieved by inviting them to become full participants (e.g. join in joint publications) or by at least providing adequate and visible acknowledgement (e.g. listing names and affiliations in metadata or on project websites).</p>
<p>Documenting all import, filtering and translation steps applied to source data is highly recommended, as these steps can then be communicated to the original field data producers to help filter out further bugs. We typically generate a single tabular object with the following properties as our final output of point data preparation :</p>
<ul>
<li><p>Consistent column names are used; metadata explaining column names is provided,</p></li>
<li><p>All columns contain standardized data (same variable type, same measurement units) with harmonized values (no significant bias in values from sub-methods),</p></li>
<li><p>All artifacts, outliers and typos have been identified and corrected to the best of our ability,</p></li>
<li><p>Missing values have been imputed (replaced with estimated values) as much as possible,</p></li>
<li><p>Spatial coordinates, including depths, (x,y,z) are available for all rows (point locations).</p></li>
</ul>
</div>
<div id="preparation-of-covariates" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Preparation of covariates</h3>
<p>As mentioned previously, preparation of covariate layers can require significant effort, even if RS data is publicly available and well documented. For example, MODIS land products are among the most used RS layers for global to regional PSM. Using raw reflectance data, such as the mid-infrared MODIS bands from a single day can, however, be of limited use for soil mapping in areas with dynamic vegetation, i.e. with strong seasonal changes in vegetation cover. To account for seasonal fluctuation and for inter-annual variations in surface reflectance, we instead advise using long-term temporal signatures of the soil surface derived as monthly averages from long-term MODIS imagery (18+ years of data). We assume here that, for each location in the world, long-term average seasonal signatures of surface reflectance or vegetation index provide a better indication of site environmental characteristics than just a single day snapshot of surface reflectance. Computing temporal signatures of the land surface requires a large investment of time (comparable to the generation of climatic images vs temporary weather maps), but it is possibly the only way to represent the cumulative influence of living organisms on soil formation <span class="citation">(Hengl, Mendes de Jesus, et al. <a href="#ref-Hengl2017SoilGrids250m">2017</a>)</span>.</p>
<p>Typical operations to prepare soil covariates for PSM thus include:</p>
<ul>
<li><p>Downloading the original source RS data,</p></li>
<li><p>Filtering missing pixels using neighbourhood filters and/or simple rules,</p></li>
<li><p>Running aggregation functions (usually via some tiling system),</p></li>
<li><p>Running hydrological and morphological analysis on source DEM data</p></li>
<li><p>Calculation of a Gaussian pyramid, for some relevant covariates, at multiple coarser resolutions, in order to capture multi-scale variation at appropriate (longer range) process scales.</p></li>
<li><p>Preparing final mosaics to be used for PSM (e.g. convert to GeoTIFFS and compress using internal compression <code>&quot;COMPRESS=DEFLATE&quot;</code> or similar),</p></li>
</ul>
<p>For processing the covariates we currently use a combination of Open Source GIS software, primarily SAGA GIS, GRASS GIS, Whitebox tools, R packages raster, sp, GSIF and GDAL for reprojecting, mosaicking and merging tiles. SAGA GIS and GDAL were found to be highly suitable for processing massive data sets, as parallelization of computing was relatively easy to implement.</p>
<p>Preparation of covariate layers is completed once:</p>
<ul>
<li><p>all layers have been resampled to exactly the same grid resolution and spatial reference frame (downscaling or aggregation applied where necessary),</p></li>
<li><p>all layers are complete (present for &gt;95% of the soil mask at least; the remaining 5% of missing pixels can usually be filled-in using some algorithm),</p></li>
<li><p>there are no visibly obvious artifacts or blunders in the input covariate layers,</p></li>
</ul>
</div>
<div id="soil-mask-and-the-grid-system" class="section level3">
<h3><span class="header-section-number">8.2.4</span> Soil mask and the grid system</h3>
<p>We recommend using a raster mask file to define the spatial domain of interest (i.e. total number of pixels to be mapped), and the spatial reference framework for PSM. The mask file defines the maximum extent, or bounds, of the area for which predictions will be made. It also identifies any grid cells, within the maximum bounds, which are to be excluded from prediction for various reasons (e.g. water, ice or human disturbance). Finally, the mask file establishes the resolution (pixel size) and spatial coordinate system that all other layers included in the analysis must conform to, to ensure consistent overlay of all grids. In most of our PSM projects we typically restrict ourselves to making predictions only for pixels that exhibit some evidence of having vegetative cover at some point in time. We tend to exclude from prediction any grid cells that have no evidence of vegetative cover at any time, such as permanent bodies of water or ice, bare rock and man made features such as roads, bridges and buildings. A generic definition of a soil mask can differ somewhat from the one we use, but this has been our practice.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_soil_mask_scheme.png" alt="Example of a soil mask scheme." width="100%" />
<p class="caption">
(#fig:soil-mask-scheme)Example of a soil mask scheme.
</p>
</div>
<p>From the perspective of global soil mapping, any place on terrestrial Earth can be considered to belong to one and only one of six categories (excluding oceans):</p>
<p>A. Fresh water (lakes, rivers)</p>
<p>B. Permanent ice or snow</p>
<p>C. Bare rocks</p>
<p>D. Urban areas</p>
<p>E. Bare soil areas</p>
<p>F. Soils with vegetation cover</p>
<p>This gives the following formulas:</p>
<p>F = Land mask - ( A + B + C + D + E )</p>
<p>Soil mask = D + E + F</p>
<p>Hence the values in the soil mask can be typically coded as:</p>
<ul>
<li><p>0 = NA or non-soil</p></li>
<li><p>1 = soils with vegetation cover</p></li>
<li><p>2 = urban areas</p></li>
<li><p>3 = bare soil areas</p></li>
</ul>
<p>If no other layers are available, global maps of land cover can be used to produce a soil mask file (following the simple formula from above). Some known global land cover layers are:</p>
<ul>
<li><p>300 m resolution: ESA CCI Land cover — 300 m annual global land cover time series from 1992 to 2015 (<a href="https://www.esa-landcover-cci.org/"><em>https://www.esa-landcover-cci.org/</em></a>),</p></li>
<li><p>100 m resolution: ESA PROBA-V 100 m land cover map (<a href="http://land.copernicus.eu/global"><em>http://land.copernicus.eu/global</em></a>),</p></li>
<li><p>30 m resolution: Chinese GLC data product (GlobeLand30) with 10 classes for the years 2000 and 2010 (<a href="http://www.globallandcover.com"><em>http://www.globallandcover.com</em></a>),</p></li>
</ul>
<p>Using widely accepted, published, global land cover maps to define a soil mask is highly recommended. This allows users to validate the maps and also ensures future consistency in case there is a need in the future to merge multiple maps covering larger areas.</p>
<p>Another important technical consideration for a PSM project is the <strong>grid system</strong>. The grid system is defined by the bounding box, pixel size and number of rows and columns:</p>
<ul>
<li>Xmin, Xmax, Ymin, Ymax,</li>
<li>Spatial resolution in m (projected),</li>
<li>Spatial resolution in DD,</li>
<li>Number of rows (X) and columns (Y),</li>
</ul>
<p>Maps referenced by geographical coordinates (<a href="http://spatialreference.org/ref/epsg/wgs-84/"><em>EPSG:4326</em></a>; used by the GPS satellite navigation system and for NATO military geodetic surveying) have spatial resolution given in abstract decimal degrees (which do not relate 1:1 with metric resolution). Some standard spatial resolutions (in decimal degrees) can be derived using the following simple rules of thumb (d.d. = decimal degrees):</p>
<ul>
<li><p>30 m ≈ 1/4000 d.d. = 0.00025</p></li>
<li><p>100 m ≈ 1/1200 d.d. = 0.0008333333</p></li>
<li><p>250 m ≈ 1/480 d.d. = 0.002083333</p></li>
<li><p>500 m ≈ 1/240 d.d. = 0.004166667</p></li>
<li><p>1 km ≈ 1/120 d.d. = 0.008333333</p></li>
</ul>
<p>Again, these are only approximate conversions. Differences in resolution in x/y coordinates in projected 2D space and geographical coordinates can be large, especially around poles and near the equator.</p>
<p>Another highly recommended convention is to use some widely accepted Equal area projection system for all output maps. This ensures the best possible precision in determining area measures, which is often important e.g. for derivation of total stocks, volumes of soil and soil components and similar. Every country tends to use a specific equal area projection system for it’s mapping, which is usually available from the National mapping agency. For continental scale maps we recommend using e.g. the <a href="https://github.com/TUW-GEO/Equi7Grid"><em>Equi7 grid system</em></a>. Some recognized advantages of the Equi7 Grid system are:</p>
<ul>
<li><p>The projections of the Equi7 Grid are equidistant and hence suitable for various geographic analyses, especially for derivation of buffer distances and for hydrological DEM modeling, i.e. to derive all DEM-based soil covariates,</p></li>
<li><p>Areal and shape distortions stemming from the Equi7 Grid projection are relatively small, yielding a small grid oversampling factor,</p></li>
<li><p>The Equi7 Grid system ensures an efficient raster data storage while suppressing inaccuracies during spatial transformation.</p></li>
</ul>
</div>
<div id="uncertainty-of-psm-maps" class="section level3">
<h3><span class="header-section-number">8.2.5</span> Uncertainty of PSM maps</h3>
<p>For soil maps to be considered trustworthy and used appropriately, producers are often required to report mapping accuracy (usually per soil variable) and identify limitations of the produced maps. There are many measures of mapping accuracy, but usually these can be grouped around the following two approaches:</p>
<ol style="list-style-type: decimal">
<li><p>Prediction intervals at each prediction point, i.e. lower and upper limits for 90% probability range.</p></li>
<li><p>Global (whole-map) measures of the mapping accuracy (RMSE, ME, CCC, z-scores, variogram of CV residuals).</p></li>
</ol>
<p>The mean width of prediction intervals and global measures of mapping accuracy should, in principle, match, although it is possible that the mean width of prediction intervals can often be somewhat wider (a consequence of extrapolation). In some cases measures of uncertainty can be over-optimistic or biased (which will eventually be exposed by new observations), which can decrease confidence in the product, hence providing realistic estimates of uncertainty of uncertainty is often equally as important as optimising predictions.</p>
<p>Common approaches to improving the accuracy of predicted maps i.e. narrowing down the prediction intervals are to (a) collect new additional data at point locations where models perform the poorest (e.g. exhibit the widest prediction intervals), and (b) invest in preparing more meaningful covariates, especially finer resolution covariates. Technical specifications, however, influence the production costs and have to be considered carefully as production costs can significantly increase with e.g. finer pixel size. Aiming at 30% lower RMSE might seem trivial but the costs of such improvement could exceed the original budget by several times <span class="citation">(Hengl, Nikolić, and MacMillan <a href="#ref-Hengl2013JAG">2013</a>)</span>.</p>
</div>
<div id="computing-costs" class="section level3">
<h3><span class="header-section-number">8.2.6</span> Computing costs</h3>
<p>To achieve efficient computing, experienced data scientists understand the importance of utilizing the full capacity of the available hardware to its maximum potential (100%). This usually implies:</p>
<ul>
<li><p>that the most up-to-date software is used for all computing tasks,</p></li>
<li><p>that the software is installed in such a way that it can achieve maximum computing capacity,</p></li>
<li><p>that any function, or process, that can be parallelized in theory is also parallelized in practice,</p></li>
<li><p>that running functions on the system will not result in system shutdowns, failures or artifacts,</p></li>
</ul>
<p>As mentioned previously, applying PSM for large areas at finer resolutions (billions of pixels) benefits from use of a high performance computing (HPC) server to run overlay, model fitting and predictions and to then generate mosaics. The current code presented in this PSM with R book is more or less 90% optimized so that running of the most important functions can be easily scaled up. The total time required to run one update on a single dedicated HPC server (e.g. via Amazon AWS) for a soil mask that contains &gt;100 million pixels can require weeks of computing time. Copying and uploading files can also be a lengthy process.</p>
<p>A configuration we adopt, and recommend, for processing large stacks of grids with a large number of evidence points is e.g. the <a href="https://www.ovh.nl/dedicated_servers/HG/">OVH server</a>:</p>
<ul>
<li><a href="https://www.ovh.nl/dedicated_servers/infrastructure/1801eg08.xml"><em>EG-512-H</em></a> (512GB RAM takes 3 weeks of computing; costs ca € 950,00 per month)</li>
</ul>
<p>An alternative to using OVH is the Amazon AWS (Fig. @ref(fig:aws-htop-server)). Amazon AWS server, with a similar configuration, might appear to cost much more than an OVH server (especially if used continuously over a month period), but Amazon offers computing costs to be paid by the hour, which provides more flexibility for less intensive users. As a rule of thumb, a dedicated server at Amazon AWS, if used continuously 100% for the whole month, could cost up to 2.5 times more than an OVH server.</p>
<p>The recommended server for running PSM on Amazon AWS to produce predictions for billions of pixels is:</p>
<ul>
<li><a href="https://aws.amazon.com/ec2/pricing/on-demand/"><em>AWS m4.16xlarge</em></a> ($3.84 per Hour);</li>
</ul>
<div class="figure" style="text-align: center">
<img src="figures/Fig_htop_96treads.png" alt="Example of an AWS dedicated server running spatial predictions on 96 threads and using almost 500GB or RAM. Renting out this server can costs up to 8 USD per hour." width="100%" />
<p class="caption">
(#fig:aws-htop-server)Example of an AWS dedicated server running spatial predictions on 96 threads and using almost 500GB or RAM. Renting out this server can costs up to 8 USD per hour.
</p>
</div>
<p>A HPC server should also have at least 2–3TB of hard disk space to host all input and output data. In addition to computing costs, one also needs to carefully consider web hosting and web traffic costs. For large datasets these can almost equal actual computing production costs.</p>
</div>
</div>
<div id="final-delivery-of-maps" class="section level2">
<h2><span class="header-section-number">8.3</span> Final delivery of maps</h2>
<div id="delivery-data-formats" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Delivery data formats</h3>
<p>A highly suitable and flexible data format for delivering raster images of soil variables is GeoTIFF. We prefer using this format for sharing raster data for the following reasons <span class="citation">(Mitchell and GDAL Developers <a href="#ref-mitchell2014geospatial">2014</a>)</span>:</p>
<ul>
<li><p>It is GDAL’s default data format and much functionality for subsetting, reprojecting, reading and writing GeoTIFFs already exists (see <a href="http://www.gdal.org/gdal_utilities.html"><em>GDAL utils</em></a>).</p></li>
<li><p>It supports internal compression via creation options (e.g. <code>COMPRESS=DEFLATE</code>).</p></li>
<li><p>Extensive overlay, subset, index, translate functionality is available via GDAL and other open source software. Basically, the GeoTIFF format functions as a raster DB.</p></li>
</ul>
<p>By exporting spatial data to GeoTiffs, one can create a soil spatial DB or a soil information system. This does not necessarily mean that its targeted users will be able to find all information without problems and/or questions. The usability and popularity of a data set reflect many considerations in addition to data quality.</p>
<p>Another useful aspect of final delivery of maps is compression of the GeoTIFFs. To avoid large file sizes, we recommend always using integers inside GeoTIFF formats because floating point formats can result in increases in file sizes of up four times (with no increase in accuracy). This might require multiplication of original values of the soil property of interest by 10 or 100, in order to maintain precision and accuracy (e.g. multiply pH values by 10 before exporting your raster into integer GeoTIFF format).</p>
</div>
<div id="general-recommendations" class="section level3">
<h3><span class="header-section-number">8.3.2</span> General recommendations</h3>
<p>Even maps of perfect quality might still not attract users, if they are not properly designed. Some things to consider to increase both use and usability of map data are:</p>
<ol style="list-style-type: decimal">
<li><p>Make a landing page for your map data that includes: (a) simple access/download instructions, (b) screenshots of your data in action (people prefer visual explanations with examples), (c) links to key documents explaining how the data were produced, and (d) workflows explaining how to request support (who to contact and how).</p></li>
<li><p>Make data accessible from multiple independent systems e.g. via WCS, FTP and through a mirror site (in case one of the access sites goes offline). This might be inefficient considering there will be multiple copies of the same data, but it can quadruple data usage.</p></li>
<li><p>Explain the data formats used to share data, and provide tutorials, for both beginners and advanced users, that instruct how to access and use the data.</p></li>
<li><p>Consider installing and using a version control system (or simply use github or a similar repository) so that the users can track earlier versions of map data.</p></li>
<li><p>Consider closely following principles of <a href="https://ropensci.org/blog/2014/06/09/reproducibility/">reproducible research</a> (all processing steps, inputs and outputs are accessible). For example, making the R code available via github so that anyone is theoretically able to reproduce all examples shown in the text. Transparency increases trust.</p></li>
</ol>
</div>
<div id="technical-specifications-psm-project" class="section level3">
<h3><span class="header-section-number">8.3.3</span> Technical specifications PSM project</h3>
<p>A way to increase planning of a PSM project is to spend more time on preparing the technical aspects of data production. This includes listing the general specifications of the study area, listing target variables and their collection methods, listing covariate layers of interest to be used to improve mapping accuracy and listing targeted spatial prediction algorithms to be compared.</p>
<p>General specifications of the study area includes:</p>
<ul>
<li>G.1 Project title:</li>
<li>G.2 PSM project type:
<ul>
<li>PSM project in a new area</li>
<li>PSM project using legacy points</li>
<li>PSM project aiming at optimizing predictions and usability</li>
</ul></li>
<li>G.3 Target spatial resolution:
<ul>
<li>10 m</li>
<li>30 m</li>
<li>100 m</li>
<li>250 m</li>
<li>1000 m</li>
</ul></li>
<li>G.4 Target temporal span:
<ul>
<li>Begin date,</li>
<li>End date,</li>
</ul></li>
<li>G.5 Soil mask:
<ul>
<li><em>raster image or land cover classes in the referent land cover map covering the study area</em></li>
</ul></li>
<li>G.6 Grid definition:
<ul>
<li>Xmin,</li>
<li>Ymin,</li>
<li>Xmax,</li>
<li>Ymax,</li>
</ul></li>
<li>G.7 Target projection system:
<ul>
<li><a href="https://proj4.org/"><em>proj4</em></a> code,</li>
</ul></li>
<li>G.8 Total area:
<ul>
<li>in square-km,</li>
</ul></li>
<li>G.9 Inspection density (observations per square-km):
<ul>
<li>Detailed soil profiles,</li>
<li>Soil semi-profiles,</li>
<li>Top-soil / sub-soil samples (with laboratory analysis),</li>
<li>Quick observations (no lab data),</li>
</ul></li>
<li>G.10 Total budget (planned):</li>
<li>G.11 Total pixels in millions:
<ul>
<li>amount of pixels for all predictions</li>
</ul></li>
<li>G.12 Total planned production costs per 1M pixels (divide G.10 by G.11):</li>
<li>G.13 Target data license:</li>
<li>G.14 Target user groups and workflows (targeted services):</li>
<li>G.15 Further updates of maps:
<ul>
<li>Continuous updates in regular intervals,</li>
<li>Two prediction time intervals (start, end period),</li>
<li>No updates required except fixes and corrections,</li>
</ul></li>
<li>G.16 Commercialization of the PSM outputs:
<ul>
<li>No commercial data nor services,</li>
<li>Commercial data products,</li>
<li>Commercial services,</li>
</ul></li>
<li>G.17 Support options:
<ul>
<li>Dedicated staff / live contact,</li>
<li>Mailing list,</li>
<li>Github / code repository issues,</li>
</ul></li>
</ul>
</div>
<div id="standard-soil-data-production-costs" class="section level3">
<h3><span class="header-section-number">8.3.4</span> Standard soil data production costs</h3>
<p>Standard production costs can roughly be split into three main categories:</p>
<ul>
<li><p>Fixed costs (e.g. project initiation, equipment, materials, workshops etc),</p></li>
<li><p>Data production costs expressed per M (million) of pixels of data produced,</p></li>
<li><p>Data maintenance and web-serving costs, usually expressed as monthly/yearly costs,</p></li>
</ul>
<p>Although in the introduction chapter we have mentioned that the production costs are mainly function of resolution i.e. cartographic scale, in practice several other factors determine the total costs. Standard soil data production costs (approximate estimates) per soil data quality category (see below) is connected to the quality level of the output maps. Consider that there are four main quality levels:</p>
<ul>
<li><p>L0 = initial product with only few soil properties, no quality/accuracy requirements,</p></li>
<li><p>L1 = final complete product with no quality/accuracy requirements,</p></li>
<li><p>L2 = final complete product matching standard accuracy requirements,</p></li>
<li><p>L3 = final complete certified product according to the ISO or similar standards.</p></li>
</ul>
<table>
<caption>(#tab:psm-table-costs)Example of a costs table for various quality levels PSM products. Prices expressed per M (million) of pixels produced.</caption>
<thead>
<tr class="header">
<th align="left">X.Type.of.PSM.project</th>
<th align="left">L0</th>
<th align="left">L1</th>
<th align="left">L2</th>
<th align="left">L3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">PSM projects in new area (single state)</td>
<td align="left">500-1000 US$ / Mpix</td>
<td align="left">1,000-5,000 US$ / Mpix</td>
<td align="left">5,000-50,000 US$ / Mpix</td>
<td align="left">&gt;50,000 US$ / Mpix</td>
</tr>
<tr class="even">
<td align="left">PSM projects using legacy points (single state)</td>
<td align="left">0.8 US$ / Mpix</td>
<td align="left">2 US$ / Mpix</td>
<td align="left">2–50 US$ / Mpix</td>
<td align="left">&gt;50 US$ / Mpix</td>
</tr>
<tr class="odd">
<td align="left">PSM projects aiming at optimizing predictions and usability (single state)</td>
<td align="left">0.5 US$ / Mpix</td>
<td align="left">0.8 US$ / Mpix</td>
<td align="left">NA</td>
<td align="left">NA</td>
</tr>
<tr class="even">
<td align="left">PSM change detection project (two states)</td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="left">NA</td>
</tr>
<tr class="odd">
<td align="left">PSM monitoring projects (multiple states)</td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="left">NA</td>
</tr>
</tbody>
</table>
<p>To convert average costs / M pixels to total costs we run the following calculus:</p>
<ul>
<li><p>Pixel resolution = 100 m</p></li>
<li><p>USA48 area = 8,080,464.3 square-km</p></li>
<li><p>Total pixels 6 depths 3 soil properties = 14,544 Mpix</p></li>
<li><p>Average production costs (L1) = 0.8 US$ / Mpix</p></li>
<li><p>Total production costs PSM projects using legacy points (single state, L1) = 11,635 US$</p></li>
<li><p>Average production costs (L2) = 2 US$ / Mpix</p></li>
<li><p>Total production costs PSM projects using legacy points (single state, L2) = 29,088 US$</p></li>
</ul>
<p>Note: this is a very generic estimate of the production costs and actual numbers might be significantly different. Additional fixed costs + monthly/yearly costs need to be added to these numbers to account also for the web hosting, support and costs of updates.</p>
<p>Compare this costs with the following standard estimated costs to deliver completed soil survey products:</p>
<ul>
<li><p>USDA estimate of the total soil survey costs: 4 US$ per ha for 1:20,000 scale maps <span class="citation">(Durana <a href="#ref-Durana2008">2008</a><a href="#ref-Durana2008">b</a>)</span> to map USA48 area = 3.2 billion US$,</p></li>
<li><p>New Zealand: 4 US$ per ha for 1:20,000 scale maps <span class="citation">(Carrick, Vesely, and Hewitt <a href="#ref-Carrick2010WCSS">2010</a>)</span>,</p></li>
<li><p>Canada: 3–10 CA$ per ha for 1:20,000 scale maps <span class="citation">(MacMillan et al. <a href="#ref-MacMillan2010DSM">2010</a>)</span>,</p></li>
</ul>
</div>
</div>
<div id="summary-notes" class="section level2">
<h2><span class="header-section-number">8.4</span> Summary notes</h2>
<p>Predictive soil mapping applies statistical and/or machine learning techniques to fit models for the purpose of producing spatial and/or spatiotemporal predictions of soil variables i.e. to produce maps of soil properties or soil classes at various resolutions. This chapter identifies and discusses the key technical specifications users need to consider to prepare for data production and to obtain realistic estimates of workloads and production costs.</p>
<p>The key technical specifications of a PSM project are considered to consist of defining the following: a soil mask, a spatial resolution, a list of target variables and standard depth intervals (for 3D soil variables), prediction intervals (if required), any secondary soil variables (and how they will be derived) and required accuracy levels. Technical specifications determine the production costs and need to be considered carefully as production costs are sensitive to specifications, (e.g. 3 times finer pixel size can increase production costs up to 10 times, or setting targets such as 30% lower RMSE can increase costs as either more points or more covariates, or both, need to be included. General forms at the end of the chapter provide a detailed list of target soil variables and covariate layers typically used in PSM projects to date.</p>
<!--chapter:end:PSM_practical_tips.Rmd-->
</div>
</div>
<div id="the-future-of-predictive-soil-mapping" class="section level1">
<h1><span class="header-section-number">9</span> The future of predictive soil mapping</h1>
<p><em>Edited by: R. A. MacMillan and T. Hengl</em></p>
<div id="introduction-2" class="section level2">
<h2><span class="header-section-number">9.1</span> Introduction</h2>
<p>This chapter presents some opinions and speculation about how predictive soil mapping (PSM) may evolve, develop and improve in the near term future. These thoughts were originally prepared for a discussion document about whether national to provincial scale soil inventory programs in Canada could, or indeed should, be reinvented and reinvigorated and, if so, how this reinvention might be best designed and accomplished.</p>
<p>The solutions proposed for reinvigorating presently moribund soil inventory programs in Canada were largely based on adopting new methods and ideas associated with PSM within a new collaborative, collective and open operational framework. These earlier thoughts were considered to be relevant to the more general topic of the future of predictive soil mapping (PSM). As such, the original discussion document was slightly modified, extended and included as a chapter in this book.</p>
<p>This chapter addresses the following two main issues:</p>
<ul>
<li><p>What went wrong with with past national to state level conventional soil, and other terrestrial resource, inventory programs and can they now be renewed and resurrected?</p></li>
<li><p>How can the methods and ideas behind PSM be adopted and applied to accomplish the goal of renewing and reviving conventional soil and terrestrial resource inventory programs?</p></li>
</ul>
</div>
<div id="past-conventional-terrestrial-resource-inventories" class="section level2">
<h2><span class="header-section-number">9.2</span> Past conventional terrestrial resource inventories</h2>
<div id="why-have-most-national-resource-inventories-been-discontinued" class="section level3">
<h3><span class="header-section-number">9.2.1</span> Why have most national resource inventories been discontinued?</h3>
<p>Historically, almost all past terrestrial resource inventory agencies were slow, expensive to maintain and failed to produce complete, consistent, current and correct map coverage (4Cs) for entire areas of political jurisdiction or interest. Most national agencies were unable to completely map an entire administrative area affordably at any useful scale using a single consistent methodology applied over a relatively short time span to produce a single wall to wall map product. Instead almost all past inventory efforts have been piecemeal and incomplete.</p>
<p>This resulted in what we consider to be <em>“the embarrassment of the index map”</em>. Virtually every jurisdiction produced an index map to illustrate which parts of the jurisdiction had been mapped at all, which different mapping methods were used to map different parts, which eras or years each bit of mapping represented and what scale of mapping had been carried out in any part. This index map was often proudly displayed and circulated to illustrate how much progress had been made towards mapping an entire jurisdiction of interest. In actual fact, the index map represented a powerful demonstration of all that was wrong with mapping and mapping progress in that jurisdiction.</p>
<p>The first thing that index maps clearly demonstrated was that there was no complete map coverage for the area at any scale. The second thing highlighted was that there was no consistency in scale, methods or legend across even areas that had been mapped. Different areas had been mapped at different scales, over different times, using different concepts and legends and no effort had been expended to achieve consistency across the entire area. The third thing that would also become immediately obvious was that, at current rates, complete mapping of any jurisdiction would never be achieved in anyone’s lifetime. Not particularly encouraging information to impart. And yet, every agency maintained an index map and loved to share it.</p>
<p>Another significant historical misjudgement was the failure to make the information and services provided by terrestrial inventory service agencies critically important and absolutely necessary to at least one essential decision making process, preferably a legally mandated one. Where inventory agencies still survive, they have linked their products and services intimately to one or more clearly defined, and legally mandated, decision making processes that involve the expenditure of considerable sums of (usually public) money. Soil survey survives in the USA (at least for now) largely because county scale soil maps are a critical requirement for calculating eligibility for financial support payments for many agricultural subsidy and support payment programs. You cannot apply for, or obtain, a subsidy payment unless you have a soil survey map to justify your eligibility for the payment and to document where and how the required supported activity will be implemented.</p>
<p>It can be argued that many terrestrial resource inventory programs failed (and disappeared) because they viewed themselves as their own primary customer and designed and delivered products and services meant to satisfy their own desires and expectations and not those of real, downstream users. They became convinced of the rightness, value and importance of their maps and databases the way they wanted to make them and did not effectively listen to, or respond to, criticism of these products. Users would criticise conventional soil polygon maps and reports filled with complicated jargon and impenetrable legends and be dismissed as simply not being able to understand a soil map and to appreciate how complicated and complex it was to portray the spatial variation in soils in a simple way. Rather than trying to design and make simpler representations of more easily understood spatial patterns, terrestrial inventory agencies would suggest that an expert in making the maps was required to assist users in interpretation and use of any map.</p>
</div>
<div id="is-there-a-future-for-conventional-terrestrial-inventory-programs" class="section level3">
<h3><span class="header-section-number">9.2.2</span> Is there a future for conventional terrestrial inventory programs ?</h3>
<p>We have asked ourselves, <em>“can conventional comprehensive soil and similar terrestrial inventory programs be saved or renewed?”</em> The short answer is probably no, at least not in their present format. Conventional resource inventory programs have become too expensive, too slow to deliver needed outputs and too slow to change to produce innovative and needed products. There is now probably insufficient will, money, demand or benefit to support continuation, or re-establishment, of conventional, government-funded, comprehensive inventory programs as we have known them in the past. However, that does not mean that all needs and benefits previously provided by comprehensive inventory programs are being met now or that they do not need to be met. There are a large number of benefits associated with the existence of comprehensive inventories and we ask if these may not be important to continue to service and if they might still be provided under some newly redesigned framework.</p>
</div>
<div id="can-terrestrial-inventory-programs-be-renewed-and-revived" class="section level3">
<h3><span class="header-section-number">9.2.3</span> Can terrestrial inventory programs be renewed and revived?</h3>
<p>One of our key hopes (which we especially promote through the OpenGeoHub Foundation), is to contribute to a discussion of how comprehensive terrestrial resource inventory programs (or equivalent frameworks) might be re-imagined, re-designed, re-invented, re-implemented and renewed at regional to national to global scales, for the next generation, and by whom.</p>
<p>We consider here that we are now at a nexus where it has become possible to address and redress many of the past inconsistencies and oversights in terrestrial resource mapping. It is now completely feasible to aspire to affordably and expeditiously produce new predictive maps that achieve the 4 Cs and are:</p>
<ul>
<li><p><strong>Complete</strong> (e.g. cover entire areas of interest),</p></li>
<li><p><strong>Consistent</strong> (e.g. are made using a single methodology, applied at a single scale and over a single short period of time)</p></li>
<li><p><strong>Current</strong> (e.g. represent conditions as they are today, at a specific moment in time)</p></li>
<li><p><strong>Correct</strong> (e.g. are as accurate as is possible to achieve given available data and methods)</p></li>
</ul>
<p>We consider that it is also now possible to redesign any new output maps so that they are capable of acting directly as inputs to well established processes and programs for planning and decision making at national to regional to operational scales. And we consider that we have a unique opportunity to work collaboratively with numerous actual and potential users of spatial inventory data to ensure that new output products directly meet their spatial data needs.</p>
</div>
<div id="how-can-terrestrial-inventory-programs-be-renewed-and-revived-and-by-whom" class="section level3">
<h3><span class="header-section-number">9.2.4</span> How can terrestrial inventory programs be renewed and revived and by whom?</h3>
<p>In light of developments in science, technology, methods of societal interaction and new models of funding and cooperative action, we suggest that looking back at how things were done in the past no longer provides the most appropriate model for how inventory activities ought to be designed and conducted in the future. We argue that it is preferable to re-imagine an entirely new framework for cooperation, which takes advantage of new scientific and organizational advances and within which many of the acknowledged benefits of previous, government-funded, programs can be delivered within a new model of cooperative, collective action and sharing.</p>
<p>In this age of Facebook and Twitter and Wikipedia and Google Earth, it is no longer the purview, or responsibility, of any single, government funded, agency to collect, produce, maintain and distribute comprehensive information about the spatial distribution of soils, eco-systems, terrain units, wetlands or any other terrestrial attributes. We believe that it should instead become a collective responsibility, for a large variety of overlapping groups and institutions, to create, maintain and share spatial information of common interest. It is incumbent on these diverse interest groups to identify mechanisms by which willing collaborators can join together to produce, maintain and distribute basic levels of spatially distributed land resource information jointly and collectively.</p>
</div>
</div>
<div id="the-future-of-psm-embracing-scientific-and-technical-advances" class="section level2">
<h2><span class="header-section-number">9.3</span> The future of PSM: Embracing scientific and technical advances</h2>
<div id="overview" class="section level3">
<h3><span class="header-section-number">9.3.1</span> Overview</h3>
<p>We consider that any new, future collaborative PSM activity should take advantage of recent scientific and technical advances in the following areas:</p>
<ul>
<li><p>Collection of field observations and samples</p>
<ul>
<li><p>Collating and harmonizing existing legacy soils data</p></li>
<li><p>New field sampling designs and programs and new data collection strategies</p></li>
</ul></li>
<li><p>Characterization of soils in the field and in the laboratory</p>
<ul>
<li><p>New field sensors for characterizing soils in situ</p></li>
<li><p>New faster, cheaper and more accurate methods of laboratory analysis</p></li>
</ul></li>
<li><p>Creation, collation and distribution of comprehensive sets of environmental covariates</p>
<ul>
<li><p>Introduce new covariate data sets based on new remote, air and space sensors</p></li>
<li><p>Include new varieties and resolutions of DEM and other environmental covariate data</p></li>
<li><p>Maximize use and relevance of existing data sets of environmental covariates</p></li>
</ul></li>
<li><p>Automated spatial prediction models.</p>
<ul>
<li><p>Replace previous qualitative and subjective mental models with new quantitative and objective statistical models</p></li>
<li><p>Adopt new methods of automated space-time modelling and prediction</p></li>
</ul></li>
<li><p>New options for hosting, publishing, sharing and using spatial data via cloud services</p>
<ul>
<li><p>Develop new platforms for collaborative data sharing and geo-publishing</p></li>
<li><p>Develop open services to deliver on-demand, real time online mapping</p></li>
</ul></li>
</ul>
</div>
<div id="collection-of-field-observations-and-samples" class="section level3">
<h3><span class="header-section-number">9.3.2</span> Collection of field observations and samples</h3>
<p>We can improve how we locate and obtain data on field observations and measurements. These O&amp;M field data provide the evidence that is essential for developing all spatial prediction models and outputs. First consider the challenges and opportunities associated with identifying, obtaining and using existing, or legacy, field observations and measurements.</p>
<p>Legacy field data refers to any field observations or measurements that were collected in the past and that remain discoverable and available for present use. Typically, these legacy field data consist of either field observations and classifications made at point locations to support the development of conventional, manually prepared maps or of laboratory analysed samples, collected to support characterization of point locations considered to be typical or representative of a particular soil class or individual. Legacy field data may already be in digital format and stored in digital databases. More often, legacy data are found in paper reports, manuals, scientific publications and other hard copy formats that require the data to first be transformed into digital format and then harmonized into a standardized format before they can be used effectively.</p>
<p>Legacy field data typically possess several characteristics which can make their use for producing new inventory map products problematic. Some common limitations of legacy field data are:</p>
<ul>
<li><p>They are rarely collected using any kind of rigorous, statistically valid, sampling design</p></li>
<li><p>Their locations in space (geolocations) are often not measured or reported accurately</p></li>
<li><p>Their locations in time (sampling dates) are often unknown or are spread over decades</p></li>
<li><p>The methods used in description or analysis can vary greatly by source, location or time</p></li>
<li><p>They can be difficult and costly to find, to obtain, to digitize and to harmonize</p></li>
</ul>
<p>Despite these limitations, legacy field data have other attributes that make them useful and worth assembling, collating, harmonizing and using. The advantages associated with using legacy field data can be summarized as follows:</p>
<ul>
<li><p>Legacy point data provide the only source of baseline information about past time periods</p>
<ul>
<li><p>We can’t go back in time to collect new samples or make new observations applicable to past time periods</p></li>
<li><p>They establish prior probabilities which are essentially starting points that describe what we know now before we start making new predictions and new maps using new data</p></li>
</ul></li>
<li><p>Legacy point data are all we have initially to work with until new field data can be obtained</p>
<ul>
<li><p>Use of legacy field data can help us to learn and to improve methods and approaches.</p></li>
<li><p>Working through the full cycle required to produce predictive maps lets us learn a lot about how to do it and, more importantly, how we might do it better the next time around.</p></li>
<li><p>They give us something to work with to provide real-world, worked examples, for ourselves and for potential users, of the kinds of maps and other products that can now be produced using modern automated prediction methods</p></li>
</ul></li>
<li><p>Legacy point data help us to illustrate problems, weaknesses and opportunities for improvement</p>
<ul>
<li><p>Gaps in existing legacy data (missing data in space and time) help to illustrate the need to have samples that comprehensively cover all areas of space and time of interest.</p></li>
<li><p>Errors and uncertainties in initial predictive maps based on legacy field data provide a clear illustration of the need for more and better field data to improve future mapping</p></li>
<li><p>The spatial distribution of uncertainties computed for initial maps created using legacy data can identify locations where new observations and samples are most needed and will contribute most to improving subsequent predictions.</p></li>
</ul></li>
</ul>
<p>Legacy point data can be surprisingly difficult and costly to find, obtain, harmonize and digitize. One can only imagine how many hundreds of thousands, even millions, of site observations may have been made by field personnel undertaking many different types of inventories for many different agencies over the years. Similarly, laboratories have definitely analyzed millions of soil samples over the years for samples collected by government agencies, private sector consulting companies, NGOs, advocacy groups, farmers or landowners. Unfortunately, very few of these observations or samples have survived to enter the public domain where they can now be easily located and obtained.</p>
<p>In an ideal world, it would be possible to identify and obtain hundreds of thousands to perhaps even millions of laboratory analysis results for point locations globally. These samples surely were taken and analysed but they no longer remain accessible. Instead, best efforts to date have resulted in rescuing some 100,000 to 200,000 records globally for which soil analytical data exist for geolocated point locations. What has happened to all of the thousands to millions of other analysed samples that were undeniably collected and analysed? Essentially they may be considered to be lost in the mists of time, victims of lack of will and lack of resources to support maintaining a viable archive of observation and sample results over the years. Unfortunately, no entity or agency had the mandate to maintain such a global archive and no one had the vision or resources to take on such a challenge.</p>
<p>The world can do a much better job of locating, harmonizing, archiving and sharing global legacy field and laboratory data than it has done to date <span class="citation">(Arrouays et al. <a href="#ref-arrouays2017soil">2017</a>)</span>. It is incumbent on agencies, companies, organizations and individuals that hold, or are aware of, collections of legacy field data to step forward to offer to contribute such data to a comprehensive and open repository of field observations and laboratory measurements. We would hope that the evidence of beneficial use of legacy point data by OpenGeoHub to produce concrete examples of needed and useful spatial outputs would encourage entities that hold field O&amp;M data that are not currently publically available to contribute them for future use by a community of global mappers. Techniques developed by OpenGeoHub to collate and harmonize legacy point data could be applied to any new, previously overlooked, datasets contributed, in the future, by interested parties.</p>
</div>
<div id="collecting-new-field-om-data" class="section level3">
<h3><span class="header-section-number">9.3.3</span> Collecting New Field O&amp;M Data</h3>
<p>Next, consider the challenges and opportunities associated with selecting, collecting, recording and using new field observations and laboratory measurements.</p>
<p>The Africa Soil Information Service (AfSIS) project (<a href="http://www.africasoils.net" class="uri">http://www.africasoils.net</a>) provides a powerful example of how new field observations and laboratory analysed field data can be collected in a manner that is reliable, feasible and affordable. AfSIS is one of the very few global examples of an entity that has not accepted that collection of new field data is too difficult and too expensive to contemplate. Instead, AfSIS asked the question <em>“how can we make it feasible and affordable to collect new, high quality, field data?”</em> And then AfSIS (and several partner countries) went ahead and collected new field data using modern, robust and affordable methods of field sampling and laboratory analysis.</p>
<p>Following the example of AfSIS, we can identify the following major considerations for how the collection of new field O&amp;M data can be made both more affordable and more effective.</p>
<ul>
<li><p>Select locations for field sampling using a formal, rigorous sampling design</p>
<ul>
<li><p>Design based sampling schemes</p>
<ul>
<li><p>Random sampling</p></li>
<li><p>Stratified random sampling</p></li>
<li><p>Systematic sampling (confluence point or grid sampling)</p></li>
<li><p>Nested, multi-scale hierarchical sampling</p></li>
</ul></li>
<li><p>Model based sampling schemes</p>
<ul>
<li><p>Conditioned Latin Hypercube (cLHC) sampling</p></li>
<li><p>Multi-stage sampling at locations of maximum uncertainty</p></li>
</ul></li>
</ul></li>
<li><p>Systematize and automate all field sampling and recording procedures as much as possible</p>
<ul>
<li><p>Create custom tools and apps to support</p>
<ul>
<li><p>Locating sample sites and recording observations</p></li>
<li><p>Assigning unique identifier sample numbers to all locations and samples</p></li>
<li><p>Tracking progress of samples from the field through the lab to the database</p></li>
</ul></li>
</ul></li>
</ul>
<p>Adopting formal sampling designs to identify where to best collect new field O&amp;M samples offers several significant advantages.</p>
<p>Firstly, statistically valid sampling schemes ensure that the fewest number of samples are required to achieve the most correct and representative values to characterize any area of interest. This minimizes field data collection costs while maximizing usefulness of the samples. Secondly, there is rapidly growing interest in, and need for, measuring and monitoring of changes in environmental conditions through time (e.g. carbon sequestration or losses, fertility changes). Quantitative statements can only be made about the accuracy of changes in values for any given area if there is an ability to replicate those values with a subsequent comparable sampling effort. The ability to return to any given area at some later time to collect a second set of statistically representative field samples is essential to any effort to quantify and monitor changes through time. Only statistically based sampling frameworks support repeat sampling.</p>
<p>Design based sampling schemes generally require little to no advance knowledge about the patterns of spatial variation within an area to be sampled. They are best used for situations where there is little existing knowledge about spatial variation and where there is a need to collect a representative sample with the fewest possible sample points.</p>
<p>Of the design based options available we suggest using a nested, multiscale sampling design based on a stratified random sample framework. In nested sampling, explicit attention is given to ensuring that multiple samples are collected at a succession of point locations with increasingly large interpoint separation distances (e.g. 1 m, 10 m, 100 m, 1 km). These multiple points support construction of semi-variograms that quantify the amounts of variation in any attribute that occur across different distances. Knowing how much of the total observed variation occurs across different distances can be very helpful for identifying and selecting the most appropriate grid resolution(s) to use for predictive mapping. If 100% of the observed variation occurs over distances shorter than the minimum feasible grid resolution, then there is really no point in trying to map the attribute spatially at that resolution. Similarly, if most of the observed variation occurs across longer distances, there is really little point in using a very fine resolution grid for prediction. Most past purposive sampling undertaken for conventional inventories was not particularly well suited to supporting geostatistics and the production of semi-variograms.</p>
<p>Model based sampling frameworks are recommended for situations where there is some existing (<em>a-priori</em>) knowledge about the spatial pattern of distribution of properties or classes of interest. Conditioned Latin Hypercube (cLHC) sampling is based on first identifying all significant combinations of environmental conditions that occur in an area based on overlay and intersection of grid maps that depict the spatial distribution of environmental covariates. Potential point sample locations are then identified and selected in such a way that they represent all significant combinations of environmental conditions in an area. Point samples are typically selected so that the numbers of samples taken are more or less proportional to the frequency of occurrence of each significant combination of environmental covariates. This ensures that samples cover the full range of combinations of environmental conditions (e.g. the covariate space) in an area and sample numbers are proportional to the relative extent of each major combination of environmental conditions in an area.</p>
<p>Field sampling programs can also be designed to collect new point samples at locations of maximum uncertainty or error in a current set of spatial predictions. The spatially located measures of uncertainty computed as one output of a prediction model can be used to provide an indication of the locations where it may be most beneficial to collect new samples to reduce uncertainty to the maximum extent possible. This type of sampling approach can proceed sequentially, with predictions updated for both estimated values and computed uncertainty at all locations after any new point sample data have been included in a new model run. It is often not efficient to collect just one new point sample prior to rerunning a model and updating all predictions of values and uncertainties. So, it is often recommended to collect a series of new point observations at a number of locations that exhibit the largest estimates of uncertainty and then update all predictions based on this series of new field point data. Collecting a series of new multistage samples can be repeated as many times as is deemed necessary to achieve some specified maximum acceptable level of uncertainty everywhere.</p>
<p>Field sampling can also be made more efficient, and less expensive, by creating and adopting more systematic and automated procedures to support field description and sampling. Custom apps can be developed to help to choose, and then locate, sampling points in the field rapidly and accurately. These field apps can be extended to automate and systematize most aspects of making and recording observations in the field, thereby increasing speed and accuracy and reducing costs. Unique sample numbers can be generated to automatically assign unique and persistent identifiers to every site and to every soil sample collected in the field. This can reduce costs and errors associated with assigning different sample IDs at different stages in a sampling campaign (e.g. field, lab, data entry). Persistent and unique identifiers can help to support continuous, real-time tracking of the progress of field descriptions and soil samples from initial collection in the field through laboratory analysis to final collation in a soil information system. This consistency and reliability of tracking can also improve efficiency, decrease errors and reduce costs for field description and laboratory analysis. Taken all together, improvements that automate and systematize field descriptions and field sampling can make it much more affordable and feasible to collect new field data through new field sampling programs.</p>
</div>
<div id="characterization-of-soils-in-the-field-and-the-laboratory" class="section level3">
<h3><span class="header-section-number">9.3.4</span> Characterization of soils in the field and the laboratory</h3>
<p>Characterization of field profiles and samples can be made more affordable and feasible again by making maximum use of new technologies that enable field descriptions and laboratory analyses to be completed more rapidly, more affordably and more accurately.</p>
<p>Field characterizations can be improved by making use of a number of new technologies. Simply taking geotagged digital photos of soil profiles and sample sites can provide effective information that is located with accuracy in both space and time. New sensors based on handheld spectrophotometers are just beginning to become available. These may soon support fast, efficient and accurate characterization of many soil physical and chemical attributes directly in the field. Other field instruments such as ground penetrating radar, electrical conductivity and gamma ray spectroscopy are also becoming increasingly available and useful. Field sensors for monitoring soil moisture and soil temperature in real time and transmitting these data to a central location are also becoming increasingly common and affordable to deploy. Simple field description protocols based on using mobile phones to crowdsource a set of basic observations and measurements could enable massive public participation in collecting new field data.</p>
<p>Recent developments in the use of new, rapid and accurate pharmaceutical grade analytical devices have reduced the costs of typical laboratory analyses dramatically, while, at the same time, significantly improving on reproducibility and accuracy. A modern soil laboratory now entails making use of mid and near infrared spectrophotometers, X-ray diffraction and X-Ray diffusion and laser based particle size analysis. Using these new instruments, it has been demonstrated that total costs for running a complete set of common soil analyses on a full soil profile can be reduced from a current cost of US$ 2,000 to as little as US$ 2-10 per profile (Need to cite something from Keith Shepherd here). This reduction in cost, along with the associated increase in reproducibility is a game changer. It makes it, once again, feasible and affordable to consider taking new field soil samples and analysing them in the laboratory.</p>
</div>
<div id="creation-collation-and-distribution-of-effective-environmental-covariates" class="section level3">
<h3><span class="header-section-number">9.3.5</span> Creation, collation and distribution of effective environmental covariates,</h3>
<p>Any future soil inventory activities will inevitably be largely based on development and application of automated predictive soil mapping (PSM) methods. These methods are themselves based on developing statistical relationships between environmental conditions that have been mapped extensively, over an entire area of interest (e.g. environmental covariates), and geolocated point observations that provide objective evidence about the properties or classes of soils (or any other environmental attribute of interest) at specific sampled locations.</p>
<p>The quality of outputs generated by predictive mapping models is therefore highly dependent on the quality of the point evidence and also the environmental covariates available for use in any model. For environmental covariates to be considered effective and useful, they must capture and describe spatial variation in the most influential environmental conditions accurately and at the appropriate level of spatial resolution (detail) and spatial abstraction (generalization). They must also describe those specific environmental conditions that exhibit the most direct influence on the development and distribution of soils or soil properties (or of whatever else one wishes to predict). The degree to which available environmental covariates can act as reliable and accurate proxies for the main (scorpan) soil forming factors has a profound influence on the success of PSM methods. If available covariates describe the environment comprehensively, accurately and correctly, it is likely that predictive models will also achieve high levels of prediction accuracy and effectiveness, if provided with sufficient suitable point training data.</p>
<p>Fortunately, advances in remote sensing and mapping continue to provide us with more and better information on the global spatial distribution of many key (scorpan) environmental conditions. Climate data (c) is becoming increasingly detailed, accurate and available. Similarly, many currently available kinds of remotely sensed imagery provide increasingly useful proxies for describing spatial patterns of vegetation (o) and land use. Topography, or relief (r), is being described with increasing detail, precision and accuracy by ever finer resolution global digital elevation models (DEMs).</p>
<p>Unfortunately, several key environmental conditions are still not as well represented, by currently available environmental covariates, as one would wish. Improvements need to be made in acquiring global covariates that describe parent material (p), age (a) and spatial context or spatial position (n) better than they currently are. In addition, the scorpan model recognizes that available information about some aspect of the soil (s) can itself be used as a covariate in predicting some other (related) aspect of the soil. Only recently have we begun to see complete and consistent global maps of soil classes and soil properties emerge that can be used as covariates to represent the soil (s) factor in prediction models based on the scorpan concept.</p>
<p>Advances are being made in developing new covariates that provide improved proxies for describing parent material (p). Perhaps the best known of these, and the most directly relevant, is airborne gamma ray spectroscopy. This sensor can provide very direct and interpretable information from which inferences can be made about both the mineralogy and the texture of the top few centimeters of the land surface. A number of countries (e.g. Australia, Uganda, Ireland) already possess complete, country-wide coverage of gamma ray spectroscopy surveys. More are likely to follow. Similarly, advances are being made in interpreting satellite based measurements of spatio-temporal variations in ground surface temperature and near surface soil moisture to infer properties of the parent material such as texture, and to a lesser extent, mineralogy. These act as very indirect proxies but they do help to distinguish warmer and more rapidly drying sands, for example, from colder and slower drying wet clays. Identifying and acquiring more detailed and more accurate covariates from which parent material type and texture can be inferred is a major ongoing challenge for which progress has been slow.</p>
<p>Only recently have a number of investigators begun to suggest a variety of covariates that can be calculated and used as proxies to describe spatial context or spatial position (n) in the scorpan model. These measures of spatial context or position help to account for the effects of spatial autocorrelation in prediction models for many soil properties and attributes. They also help to coax out effects related to spatial context and spatial scale. The old adage that “what you see depends upon how closely you look” certainly applies to predictive soil mapping. If one only looks at the finest detail, one overlooks the broader context and broader patterns. Similarly, if one only looks at broad patterns (coarser resolutions) one can easily miss seeing, and predicting, important shorter range variation. Soils are known to form in response to a number of different soil forming processes and these processes are themselves known to operate over quite different ranges of process scales (or distances). So, if one looks only at the most detailed scales (e.g. finest spatial resolution) one can easily fail to observe, describe and account for important influences that operate across longer distances and larger scales. Increasingly, it is becoming evident that prediction models generate more accurate results when they incorporate consideration of a hierarchical pyramid of environmental covariates computed across a wide range of resolutions to represent a wide range of process scales and formative influences.</p>
<p>A final, and very significant, consideration, for environmental covariates is one of degree of availability and ease of use. For covariates to be effective, they must be relatively easy to identify, locate and use. Many existing spatial data sets need some form of preprocessing or transformation in order to become useful inputs as environmental covariates in predictive mapping. Difficulties and costs involved in locating, downloading and transforming these source data sets can severely restrict their effective use. Equally, many of these same covariates are often located, downloaded and processed multiple times by multiple entities for use in a single project and then archived (or disposed of) and not made easily available for subsequent use and reuse. A mentality of “protecting my data” leads to limitations on sharing and reuse of spatial data with large resulting costs from redoing the same work over and over for each new project. Significant efficiencies could be realized if spatial datasets, once assembled, corrected and preprocessed, could be widely shared and widely used.</p>
<p>In many PSM projects, as much as 80% of the time and effort expended can go into preparing and collating the environmental covariates used in the modelling process. If modellers could work collectively and collaboratively to share entire collections of relevant covariates at global to regional to national scales, considerable efficiencies could be realized. Time and effort now spent in assembling covariates could instead be devoted to locating and assembling more and better point O&amp;M data and on discovering and applying improved models. So, one key way in which future inventory activities could be made much more efficient and cost-effective would be to develop mechanisms and platforms whereby comprehensive stacks of environmental covariates, covering entire regions of interest, could be jointly collated and freely shared. OpenGeoHub aims to provide a fully worked example of such a platform for sharing geodata.</p>
</div>
<div id="automated-spatial-prediction-models-psm" class="section level3">
<h3><span class="header-section-number">9.3.6</span> Automated spatial prediction models (PSM)</h3>
<p>Rapid adoption of new, automated, spatial prediction methods is the most fundamental change envisaged as being central to all efforts to redesign land resource inventories such that they can, once again, become affordable and feasible to conduct. These models are quantitative, objective, repeatable and updateable. They capture and codify understanding of how soils are arranged spatially in the landscape, and why, in ways that are systematic, rigorous and verifiable. Results of models can be updated regularly and easily, as new O&amp;M point data, new covariates, or even new modelling algorithms become available. The time and costs associated with constructing prediction models is minimal in comparison with traditional manual mapping methods. Even more dramatically, once constructed, models can be rerun, against improved data, to update predictions regularly or to track changes in conditions through time.</p>
<p>Prediction models have changed, and improved, quite substantially, over the last few years. Most initial PSM models were linear (simple) and universal (applied equally to entire areas). Newer PSM models are increasingly non-linear and hierarchical with different mathematical combinations of predictors operating in different ways under different regional combinations of environmental conditions. More powerful methods involving Deep Learning and Artificial Intelligence have recently demonstrated improved prediction accuracies, compared to earlier, more simple, linear regression or tree models.</p>
<p>Automated prediction models have several other clear advantages over conventional manual mapping methods. Consider again, the previously discussed manual approaches of top-down versus bottom up mapping. Up until now, almost all previous manual (or indeed automated) mapping programs have been bottom up approaches applicable to bounded areas of some defined and limited extent such as individual farm fields, map sheets, counties, provinces, states or, at a maximum, entire countries. Any project that applies only to a bounded area of limited extent will, as a consequence, only collect, analyse and use observations and data that exist within the boundaries of the defined map extent.</p>
<p>Automated mapping methods have the advantage that they can be truly global. That is, they can use, and consider, all available point data, everywhere in the world, as evidence when constructing prediction rules. This means that all possible point data get used and no data go to waste. Global models, that use all available global point data are, in fact, an elegant and simple way of implementing the concept of Homosoil that has been advanced by <span class="citation">Mallavan, Minasny, and McBratney (<a href="#ref-Mallavan2010PSS">2010</a>)</span>. The Homosol concept suggests that, if O&amp;M data are not available for any particular point of interest in the world, then one should search to identify and locate a point somewhere else in the world that has the most similar possible combination of environmental conditions as the current unsampled point but that has also been sampled. Data for this sampled site are then used to characterize the current unsampled site. Global models simply reverse this search process by 180 degrees while at the same time making it much more efficient and simpler to implement. Global models take all available point data and then identify all other locations anywhere in the world that possess similar combinations of environmental conditions. All these similar locations are then assigned, via application of the prediction model, values for soil properties or soil classes that are similar to those observed at the sampled reference location, or multiple similar locations.</p>
<p>Global models not only make use of all available point data to develop rules, they also capture and quantify variation in soil classes and soil properties that operates over longer distances (10s to 100s of km) and coarser scales. This longer range variation is usually related to soil forming processes that themselves operate over longer distances, such as gradual, long distance variation in climate, vegetation or even topography (at the level of regional physiography). Long range variation may require consideration of patterns that express themselves over very large distances that may exist partially, or entirely, outside the boundaries of some current bounded area of interest. Local, bounded studies can easily fail to observe and quantify this long range variation.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_LandGIS_workflow_soil.png" alt="General workflow of the spatial prediction system used to produce soil property and class maps via the LandGIS." width="85%" />
<p class="caption">
(#fig:landgis-soil)General workflow of the spatial prediction system used to produce soil property and class maps via the LandGIS.
</p>
</div>
<p>We can consider global models as providing a kind of elegant implementation of top down mapping (Fig. @ref(fig:landgis-soil)). Global models capture, describe and quantify that portion of the total spatial variation in soil properties and soil classes that occurs over longer distances in response to longer range soil forming processes. This longer range variation may only constitute some rather small percentage of the total range in spatial variation in some property (typically some 10-30% of total variation). But it does represent a component of the total variation that would likely be missed, and not properly observed or accounted for, by local, bounded, models that do not consider patterns of spatial variation that extend outside their maximum boundaries or that occur entirely outside the boundaries of a contained study area.</p>
<p>In a top down mapping approach based on automated mapping, predictions made globally, using all globally available point data, can be used to account for longer range patterns of variation and can provide initial, <em>a priori,</em> estimates of the most likely values for soil properties or soil classes at a point. These initial, <em>a priori,</em> estimates can subsequently be updated and improved upon by more detailed local studies that have access to much larger volumes of local O&amp;M point data. The values computed for soil properties by global models can be merged with values estimated by local models to create some form of merged weighted average. Alternately, the global estimates of soil property values can be used to represent soil type covariates (s) in a scorpan prediction model. Here, globally estimated property values are used as s-type covariates in predicting equivalent soil property values at local scales using local models.</p>
<p>Automated spatial prediction models also permit us to recognize that otherwise similar soils develop and express different properties under different types of human management. They don’t just permit this recognition, they force us to recognize differences in soils that arise from differences in land use. This is because automated prediction models are driven by the data that are fed to them and field O&amp;M data collected from managed landscapes will invariably report different values for key soil properties than would be reported for otherwise similar soils under natural or unmanaged conditions. Thus, for automated predictive models to actually work, they have to observe and then predict differences in soils and soil properties between managed and natural landscapes. This was never something that was considered feasible to do with manual soil mapping. Consequently managed soils were usually named and mapped as if they were identical to their natural (unmanaged) equivalents. Differences might be described in reports or tables of laboratory analyses, but the two variations of the same soil (managed and natural) were rarely, if ever, mapped as separately described entities.</p>
<p>In a similar way, automated prediction methods force us to recognize and account for temporal variations that arise from changes in soil conditions or soil attributes at the same locations over time. The models will predict values similar to those provided to them as input from field observations and measurements. If we have point O&amp;M data for the same point location that is separated in time and that reflects changes in soil property values through time, we need to be able to recognize this and adapt to it. We need to recognize that all predictions apply to a specific time period and that different predictions (maps) need to be produced for different time periods, if the available point O&amp;M data reference widely different time periods.</p>
<p>In the context of automated mapping and High Performance Computing, opportunities for producing high quality soil maps using Open Source software are becoming more and more attractive. However, not all Open Source Machine Learning packages are equally applicable for processing large national or international data sets at resolutions of 250 m or better. LandGIS predictions are, for example, possible only thanks to the following packages that can be fully parallelized and are ready for upscaling predictions (all three have been written in C++ in fact):</p>
<ul>
<li><p>ranger (<a href="https://github.com/imbs-hl/ranger" class="uri">https://github.com/imbs-hl/ranger</a>),</p></li>
<li><p>xgboost (<a href="https://xgboost.readthedocs.io/en/latest/" class="uri">https://xgboost.readthedocs.io/en/latest/</a>),</p></li>
<li><p>liquidSVM (<a href="https://github.com/liquidSVM/liquidSVM" class="uri">https://github.com/liquidSVM/liquidSVM</a>),</p></li>
</ul>
<p>these can be further efficiently combined with accuracy assessment and fine-tuning packages (also ready for parallelization):</p>
<ul>
<li><p>SuperLearner (<a href="https://cran.r-project.org/web/packages/SuperLearner/" class="uri">https://cran.r-project.org/web/packages/SuperLearner/</a>),</p></li>
<li><p>caret (<a href="https://topepo.github.io/caret/" class="uri">https://topepo.github.io/caret/</a>),</p></li>
<li><p>mlr (<a href="https://mlr.mlr-org.com/" class="uri">https://mlr.mlr-org.com/</a>),</p></li>
</ul>
<p>Beyond that it is not trivial to use R for production of large rasters where millions of points with hundreds of covariates are used for model building.</p>
</div>
<div id="hosting-publishing-sharing-and-using-spatial-data" class="section level3">
<h3><span class="header-section-number">9.3.7</span> Hosting, publishing, sharing and using spatial data</h3>
<p>Finally, we need to consider how future inventory activities can benefit from improved approaches for hosting, publishing, sharing and using spatial data, with special attention paid to predictions of soil properties or soil classes.</p>
<p>The value of data is in its use. Thus, we only get full value for our data if we can maximize its distribution and use. Developments in mechanisms and communities for sharing digital data online provide promise of greatly improved access to, and use of, new digital data sets, including predictive soil maps. Major developments in hosting and delivering spatial data online include new and increased interest in, and adherence to, principles of FAIR Data, FAST Data and, most importantly, OPEN Data.</p>
<p>FAIR Data principles aim to make data findable, accessible, interoperable and reusable <span class="citation">(Wilkinson et al. <a href="#ref-wilkinson2016fair">2016</a>)</span>. The easier data are to locate and access, the greater the use is likely to be. Similarly, data that are interoperable are easier to ingest into end user applications, and so, will receive greater use. Data that are reusable also ensure maximum benefit by facilitating regular use and reuse.</p>
<p>FAST data is the application of big data analytics to smaller data sets in near-real or real-time in order to solve a problem or create business value. The goal of fast data is to quickly gather and mine structured and unstructured data so that action can be taken (<a href="https://whatis.techtarget.com/definition/fast-data" class="uri">https://whatis.techtarget.com/definition/fast-data</a>). Fast data is fundamentally different from Big Data in many ways. Big Data is most typically data at rest, hundreds of terabytes or even petabytes of it, taking up lots of space on disk drives. Fast data is data in motion (<a href="https://www.voltdb.com/why-voltdb/big-data/" class="uri">https://www.voltdb.com/why-voltdb/big-data/</a>). OpenGeoHub aims to use Big Data analytics to rapidly and affordably turn static and unstructured data into easily used, and widely used information. The objective should be to rapidly generate agile, flexible and user oriented data.</p>
<p>Future soil inventory projects based on application of predictive soil modelling will also benefit from adopting the following principles of OPEN Data based on the Sunlight Foundation’s “Ten Principles for Opening up Government Information”. (<a href="https://open.canada.ca/en/open-data-principles#toc95)\" class="uri">https://open.canada.ca/en/open-data-principles#toc95)\</a><br />
<strong>1. Completeness</strong><br />
Datasets should be as complete as possible, reflecting the entirety of what is recorded about a particular subject. All raw information from a dataset should be released to the public, unless there are Access to Information or Privacy issues. Metadata that defines and explains the raw data should be included, along with explanations for how the data was calculated.<br />
<br />
<strong>2. Primacy</strong><br />
Datasets should come from a primary source. This includes the original information collected by the original sources and available details on how the data was collected. Public dissemination will allow users to verify that information was collected properly and recorded accurately.<br />
<br />
<strong>3. Timeliness</strong><br />
Datasets released should be made available to the public in a timely fashion. Whenever feasible, information collected by original entities should be released as quickly as it is gathered and collected. Priority should be given to data whose utility is time sensitive.<br />
<br />
<strong>4. Ease of Physical and Electronic Access</strong><br />
Datasets released by their producers should be as accessible as possible, with accessibility defined as the ease with which information can be obtained. Barriers to electronic access include making data accessible only via submitted forms or systems that require browser-oriented technologies (e.g., Flash, Javascript, cookies or Java applets). By contrast, providing an interface for users to make specific calls for data through an Application Programming Interface (API) make data much more readily accessible.<br />
<br />
<strong>5. Machine readability</strong><br />
Machines can handle certain kinds of inputs much better than others. Datasets should be released in widely-used file formats that easily lend themselves to machine processing (e.g. CSV, XML). These files should be accompanied by documentation related to the format and how to use it in relation to the data.<br />
<br />
<strong>6. Non-discrimination</strong><br />
Non-discrimination refers to who can access data and how they must do so. Barriers to use of data can include registration or membership requirements. Released datasets should have as few barriers to use as possible. Non-discriminatory access to data should enable any person to access the data at any time without having to identify him/herself or provide any justification for doing so.<br />
<br />
<strong>7. Use of Commonly Owned Standards</strong><br />
Commonly owned standards refer to who owns the format in which data is stored. For example, if only one company manufactures the program that can read a file where data is stored, access to that information is dependent upon use of that company’s program. Sometimes that program is unavailable to the public at any cost, or is available, but for a fee. Removing this cost makes the data available to a wider pool of potential users. Released datasets should be in freely available file formats as often as possible.<br />
<br />
<strong>8. Licencing</strong><br />
All datasets should be released under a recognized Open Data Licence. Such licences are designed to increase openness and minimize restrictions on the use of the data.<br />
<br />
<strong>9. Permanence</strong><br />
The capability of finding information over time is referred to as permanence. For best use by the public, information made available online should remain online, with appropriate version-tracking and archiving over time.<br />
<br />
<strong>10. Usage Costs</strong><br />
All open data should be provided free of charge.</p>
<p>A preferred way of achieving FAIR, FAST and OPEN data distribution is to develop and maintain new, online platforms that support collaborative compilation, sharing and geopublishing. OpenGeoHub aims to provide a viable, worked example of how a new, open and collaborative web-based platform can deliver soil spatial information on-demand and in nearly real time.</p>
</div>
<div id="new-visualization-and-data-analysis-tools" class="section level3">
<h3><span class="header-section-number">9.3.8</span> New visualization and data analysis tools</h3>
<p>Terrestrial resource inventories, and indeed spatial inventories of almost all environmental conditions, will increasingly benefit from adopting and using new tools and platforms that enhance interactive, real time data visualization and data analysis.</p>
<p>Spatial data increasingly needs to be presented in ways that support interactive, real time visualization of 3 dimensions plus time. What is increasingly being referred to as 4D or 3D+ time. We need to help users visualize, and appreciate, how soils vary with depth as well as in horizontal space. And, also increasingly, we need to be able to help users visualize and understand how soils can vary through time. OpenGeoHub is attempting to demonstrate newly available facilities for visualizing, and interacting with, 3D and 3D+ time spatio-temporal data.</p>
<p>Every effort needs to be made to facilitate easy use of terrestrial resource inventory spatial data. This should entail releasing spatial data that has both the content and the format required for immediate ingestion into, and use in, critical end user applications. Users should be able to link their applications to data supplier platforms and simple call up needed data.</p>
</div>
</div>
<div id="the-future-of-psm-embracing-new-organizational-and-governance-models" class="section level2">
<h2><span class="header-section-number">9.4</span> The future of PSM: Embracing new organizational and governance models</h2>
<div id="overview-1" class="section level3">
<h3><span class="header-section-number">9.4.1</span> Overview</h3>
<p>In the same way that new scientific and technological advances can be embraced to improve future PSM any new, future, PSM activities should also take advantage of newer organizational models that improve how collective activities can be organized and managed collaboratively and cooperatively through innovations such as:</p>
<ul>
<li><p>Open data and platforms and procedures for acquiring and sharing data,</p></li>
<li><p>Open cloud-based processing capabilities,</p></li>
<li><p>Collaborative production of inputs and new outputs,</p></li>
<li><p>Crowdsourcing and voluntary collaboration,</p></li>
<li><p>Crowdfunding and blockchain funding systems,</p></li>
<li><p>Web-based sponsorship and revenue opportunities,</p></li>
</ul>
</div>
<div id="open-data-and-platforms-and-procedures-for-acquiring-and-sharing-it" class="section level3">
<h3><span class="header-section-number">9.4.2</span> Open data and platforms and procedures for acquiring and sharing it</h3>
<p>Open data is, of course, the key requirement for enabling maximum access to, and use of, point and covariate data required to support collaborative PSM. Firewalls, paywalls and data silos typically act to restrict access to, and use of, valuable point and covariate data, Open data can be used and reused, multiple times, often for unanticipated applications. Data need to be not only open but also easily discoverable and accessible. This is where open platforms, such as OpenGeoHub, come in. They can facilitate easy and effective access to, and use of, shared open data.</p>
</div>
<div id="open-cloud-based-processing-capabilities" class="section level3">
<h3><span class="header-section-number">9.4.3</span> Open cloud-based processing capabilities</h3>
<p>At the moment, most PSM activities take place on local computers using local processors. As PSM proceeds and increasingly deals with much larger datasets at much finer spatial resolutions, it may become less and less viable to download all data for local processing. Imagine trying to download 5 to 10 m resolution raster data for an entire continent, or even the entire world, for tens to perhaps hundreds of layers of covariate data. We may rapidly arrive at a situation where it could take months to years to simply download such large volumes of data before any analyses could take place. It such a situation, it no longer makes sense to try to download covariate datasets to work with them locally.</p>
<p>Similarly, many big data applications have now accepted that it is far more efficient and affordable to conduct their processing and analysis in the cloud using services such as Amazon cloud services, Google Earth Engine or Microsoft cloud services. It has become too costly to assemble and maintain the massive amounts of processing power, and memory, in house that are increasingly required to process massive datasets using big data analytics.</p>
<p>Modellers can easily obtain and download all available covariate data for all point locations for which they possess point observations or measurements. Typically, entire stacks of covariate data can be identified and downloaded for thousands of point locations within just a few seconds of submitting a query. This covariate data is all that is needed to create the matrices required to support development, evaluation and finalization of multiple predictive models for PSM. Once an optimum model (or models) has been developed, the model itself can be uploaded to a cloud based processing service and the model can be run against all covariate data stored in the cloud, using cloud based memory and processing capabilities. This is perhaps a preferable and more practical way to implement PSM modelling for very large datasets.</p>
</div>
<div id="collaborative-production-of-inputs-and-new-outputs" class="section level3">
<h3><span class="header-section-number">9.4.4</span> Collaborative production of inputs and new outputs</h3>
<p>It is likely that it will increasingly only be possible to produce next generation national to state level PSM inventory products through some form of collaborative effort.</p>
<p>It is very unusual for any one agency or entity to have both the mandate and the resources to assume responsibility for producing maps for entire countries or even entire provinces or states. Mapping and field data collection activities tend to be fragmented in response to various jurisdictional mandates and operational responsibilities. Agricultural departments are responsible for agricultural lands, forestry for forested areas, parks departments for public parklands and environmental departments for conservation areas. No one entity ever seems to have responsibility for mapping an entire country or state. In addition, a majority of mapping and field data collection programs are now typically undertaken by private engineering and environmental consulting companies on contract to industry or government clients. The people charged with collecting the field data seldom have any responsibility or mandate for ongoing custodianship and preservation of the collected data. The companies or government agencies that contracted to have the data collected themselves typically lack the resources, expertise or motivation to conserve and maintain the field data delivered to them, let alone to share it widely with others.</p>
<p>So, how can a situation be achieved where a large proportion of point data collected in the field, or analysed in a lab, are collated, stored and maintained for widespread distribution and use in PSM? We believe that what is required are both physical (or virtual) platforms where collaboration and sharing can be facilitated and legal and organisational protocols that encourage, and indeed require, saving, maintaining and sharing of point observation data collectively and collaboratively.</p>
<p>What is required is a change in attitude that is reflected by equivalent changes in regulations and procedures. Governments and private sector industries that require, or commission, field data collection activities need to adopt procedures and rules that require any new data to be deposited in an open repository where it can be widely and easily accessed and shared. Similarly, laboratories that undertake analysis of field collected samples need to be encouraged, or even obliged, to submit analytical results for samples from point locations to some shared and open repository. If this were to occur, then anyone interested in producing maps for any area would have access to all potentially available and useful point data to inform and drive their predictive models. We offer OpoenGeoHub as an example of a physical platform where all of point data, covariate data and output predictive maps can be widely and freely published, archived and shared.</p>
<p>The production of output maps can also be undertaken as a collective and collaborative exercise. Individuals and agencies can work together to share input data sets (point data and covariates) and models and to jointly produce maps for areas of common interest. The more people that get involved in producing new maps using common, open databases, the greater the number, variety and utility of maps we are likely to see produced.</p>
</div>
<div id="crowdsourcing-and-voluntary-collaboration" class="section level3">
<h3><span class="header-section-number">9.4.5</span> Crowdsourcing and voluntary collaboration,</h3>
<p>There is a role in PSM for crowdsourcing and voluntary contributions from citizen scientists. Sampling plans can be developed and volunteers can be encouraged to go to identified sampling locations to collect a series of easy to make observations using a provided template. One active example of this approach is the Degree Confluence Project. This project aims to have people visit each of the exact integer degree intersections of latitude and longitude on Earth, posting photographs and a narrative of each visit online (<a href="https://en.wikipedia.org/wiki/Degree_Confluence_Project" class="uri">https://en.wikipedia.org/wiki/Degree_Confluence_Project</a>). The project describes itself as “an organized sampling of the world”.</p>
<p>Monitoring programs can vary significantly, ranging from community based monitoring on a local scale, to large-scale collaborative global monitoring programs such as those focused on climate change <span class="citation">(Lovett et al. <a href="#ref-lovett2007needs">2007</a>)</span>. There is a global recognition that “environmental issues are best handled with the participation of all concerned citizens”, a principal first articulated in the United Nation’s Earth Summit Agenda 21 (UN, 1992). This principle was strengthened further in July, 2009, with the formal ratification of the Aarhus Convention which mandates participation by the public in environmental decision-making and access to justice in environmental matters (UNECE, 2008).</p>
<p>If volunteers can be advised where to go and what to observe, or collect, at sample locations following a defined format or template, much useful data can be collected for use in PSM. For example, it is relatively easy to make very useful field observations about whether a location is a wetland organic soil or an upland mineral soil. This, in itself, is very useful to know. Similarly, citizen scientists can be instructed to obtain valuable measurements such as depth of a soil to bedrock, thickness of a topsoil horizon, color of a topsoil or subsoil horizon or presence or absence of bare rock, water, stones or gullies. It is even possible to provide detailed instructions that permit volunteers to estimate soil texture by application of manual hand texture assessments. Increasingly, apps are likely to be developed for mobile phones that will support quantitative assessments of soil color or structure and, possibly very soon, spectroscopic analysis of soil properties on-site. So, future PSM activities should look for opportunities to engage citizen volunteers in collecting field observations and measurements to extend what PSM is able to accomplish now using data collected data by professionals..</p>
</div>
<div id="sponsorship-subscription-crowdfunding-and-blockchain-funding-systems" class="section level3">
<h3><span class="header-section-number">9.4.6</span> Sponsorship, subscription, crowdfunding and blockchain funding systems,</h3>
<p>Someone has to pay to finance the collection of new field and laboratory point data, the assembly, storage and distribution of databases of relevant point data or environmental covariates at relevant resolutions and the production, publication, maintenance and distribution of any models, output maps or other end products. We can imagine several possible revenue streams that could be adopted to fund a collaborative platform in a sustainable way. These include:</p>
<ul>
<li><p>Sponsorship</p></li>
<li><p>Subscriptions by participating partners</p></li>
<li><p>Crowdfunding</p></li>
<li><p>Blockchain funding</p></li>
<li><p>Advertising revenue</p></li>
</ul>
<p>Sponsors are those who provide funds willingly to support operations and activities that they consider to be beneficial, possibly to themselves but, more importantly, to wider society. Sponsors typically regard the services and products provided by the funded entity as delivering a desirable public good or public service and to therefore be worthy of their financial support. Sponsors typically do not dictate what gets done, or how, but sponsors do have some expectations that their funding will ensure that the public good activities they support will continue to be undertaken and delivered by the funding recipient in a consistent and responsible manner.</p>
<p>We can imagine that an open collaborative for natural resource inventory products might attract sponsorship from philanthropic donors who elect to fund environmental activities undertaken in support of the broader public interest. Some government agencies, or even commercial companies, might also elect to offer ongoing sponsorship funding. The main role of sponsorship funding ought to be to provide some minimum base level of income that can pay for the ongoing costs associated with maintaining basic operational capabilities. Examples of basic operational costs include ongoing charges for paying for website development and maintenance, online storage, web traffic and web based processing arising from user activity and system administration. These are basically just the costs associated with keeping the lights on and the doors open. They ensure continuity from year to year but do not usually fund major expansions or annual production activities.</p>
<p>Active contributors to, and users of, the products and services generated by a consortium or collective of partners can help to self-fund the collective’s activities by agreeing to contribute funds via some sort of continuing subscription. Partners may be able to justify paying an annual subscription to sustain the collective activity because they, themselves, obtain benefits or reduce internal expenditures they would otherwise normally pay for the same sets of services or activities internally and individually. Sharing platforms for collecting, hosting, publishing and disseminating spatial environmental data could be more cost effective than building and maintaining multiple separate platforms and functionalities individually. These reduced, or avoided, costs could justify contributing some funds to pay for, and sustain, the operations of the collective. Sustaining subscriptions are a more stable and reliable way to fund the ongoing development and maintenance of the collective’s activities and infrastructure because they can be more reliably estimated and counted on. These funds can also help pay for new work and new data.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_workflow_web_mapper.png" alt="Recommended decision tree for design of PSM. In essence, users' perspective and technical settings should be the key for most of decisions when it comes to design of PSM projects." width="95%" />
<p class="caption">
(#fig:psm-design-users)Recommended decision tree for design of PSM. In essence, users’ perspective and technical settings should be the key for most of decisions when it comes to design of PSM projects.
</p>
</div>
<p>One can imagine quite a large number of potential partners, that might be willing to agree to contributing an annual sustaining subscription. Examples of the kinds of partners that might subscribe include:</p>
<ul>
<li><p>Government agencies with a mandate or need to produce or use environmental data,</p></li>
<li><p>Industrial and commercial companies that have responsibilities for submitting environmental data,</p></li>
<li><p>Engineering and environmental consulting companies that are typically paid to collect environmental data and produce maps,</p></li>
<li><p>NGOs and public interest groups that advocate for sustainability or environmental protection</p></li>
<li><p>Universities and research institutions with interests in teaching or conducting environmental research</p></li>
<li><p>Industry associations and professional certification bodies,</p></li>
<li><p>Commercial companies that provide products or services in the areas of computing, spatial analysis and the environment</p></li>
</ul>
<p>One would expect that partners willing to contribute a sustaining subscription would have something to gain from being active participants. They might contribute actively to adding to the collections of field data or new maps produced or distributed by themselves or by the collective. They might find it convenient and effective to use the platforms and infrastructure maintained by the collective to assist them to not only produce new data but to publish and disseminate, widely and effectively, any new data or maps they might produce. Producers of new maps and data would have their capabilities and products highlighted and gain from exposure and credibility. High volume users of data would gain from savings realized by having a single, one-stop platform for finding and obtaining data they need and from the security they would feel in using credible data produced using transparent processes in full public view. Universities and research institutions would gain from having access to the latest and most complete data and methods and to new facilities and new approaches to expose their students to. And commercial companies that provide software, hardware or services to the environmental community can gain by being associated with providing support for an entity that is providing high quality information as a public good.</p>
<p>Crowdfunding is becoming an increasingly common way to secure money to conduct, and sustain, public good activities. Users who benefit from accessing the site and using free data may well be motivated to offer their support in the form of voluntary donations. Donors can be acknowledged and thanked on the site by maintaining a web page that lists voluntary contributions.</p>
<p>It is becoming increasingly common for sites that offer free data or services to sustain themselves by accepting revenue from advertisers that want to be associated with the product or services provided by the site or just want to benefit from the exposure gained from high volumes of traffic to the site.</p>
</div>
<div id="a-proposal-for-organizing-and-managing-a-new-open-collective" class="section level3">
<h3><span class="header-section-number">9.4.7</span> A proposal for organizing and managing a new open collective</h3>
<p>So, what is it that we would like to promote and implement as we proceed into the future? The basic concept is to imagine, design and build a new, collaborative and cooperative framework for collecting, analysing, maintaining, producing and distributing spatial information about attributes and classes of terrestrial landscapes. The idea may be thought of as a virtual network for creating and supporting ongoing, comprehensive terrestrial resource inventories of all kinds and their required supporting infrastructures. The concept adopts, and extends, many of the elements of the existing Global Soil Information Facilities (GSIF). (<a href="http://www.isric.org/content/soilgrids" class="uri">http://www.isric.org/content/soilgrids</a>) and of the recently launched OpenGeoHub Foundation’s LandGIS (<a href="http://landgis.opengeohub.org" class="uri">http://landgis.opengeohub.org</a>).</p>
<p>We imagine harnessing the possibilities for collective and cooperative action offered by new and emerging methods for social networking and scientific cooperation. The concept aims to promote, incorporate and make use of all relevant new scientific and technical advances in the assembly and processing of terrestrial spatial data. But the vision is not solely driven by technology push from new scientific and technical methods. Rather, it is primarily driven by recognition of the power inherent in emerging trends in crowd-sourcing and facilitated collective action.</p>
<p>Companies, such as Dupont, advertise that they are part of a global collaboratory, and recognize that their businesses, and profit, benefit greatly from sharing much of their proprietary internal research data with partners and even competitors. This recognizes the fact that many agencies and companies are in the business of collecting and analysing data to try to extract useful knowledge or information from that data. The larger and more comprehensive the pool of data, the more likely it is to yield new and valuable understandings or knowledge and then, from this, enable the creation of new and useful, or profitable, products.</p>
<p>Automatically creating maps of the spatial distribution of terrestrial spatial entities, or their attributes, represents one particular application of data mining techniques for extracting understanding and knowledge from data to produce new and useful products. We propose that by cooperating to maximize the assembly and sharing of data about terrestrial entities, using best available methods, we can expect to also maximize the knowledge, the information and the variety, quality and value of new products that can be extracted from the assembled data. Our LandGIS is now a proof of concept of an Open Data system where any group can contribute and use as a publishing platform. We anticipate that our LandGIS will be further combined with data channels produced by other groups, e.g. the landpotential.org project or similar, so that a top-down, bottom-up (predictions based on global models combined with local verification and adjustments; Fig. @ref(fig:scheme-soc-prof1)), can be generated.</p>
<div class="figure" style="text-align: center">
<img src="figures/Fig_future_mobile_phone.png" alt="Future Global Land Information System (built on the top of the LandGIS concept) and targeted functionality: users on ground send local O&amp;M through their mobile phones, which are then used to run and re-callibrate global models." width="100%" />
<p class="caption">
(#fig:mobile-phone-scheme)Future Global Land Information System (built on the top of the LandGIS concept) and targeted functionality: users on ground send local O&amp;M through their mobile phones, which are then used to run and re-callibrate global models.
</p>
</div>
<p>There are probably many viable ways in which a collective could be set up to organize and manage the various collaborative activities required to implement a new virtual terrestrial resource inventory entity. Many are likely to be attracted to the idea of setting up a semi-independent institute affiliated with a university or research institute. For example, for many years, soil survey activities in Canadian provinces, such as Alberta, were conducted officially under the auspices of Institutes of Pedology that formalized cooperation among federal and provincial government departments and university departments of soil science.. Others might be attracted to the idea of spinning off a notionally independent private sector company within which other entities could collaborate to produce or distribute their data. Examples of private companies involved in distributing spatial data include AltaLIS (<a href="https://beta.altalis.com" class="uri">https://beta.altalis.com</a>) and Valtis (<a href="http://www.valtus.com" class="uri">http://www.valtus.com</a>) which operate in Alberta.</p>
<p>We, quite obviously, favor an approach of creating a small, agile, not-for-profit foundation that can act as a core entity for a larger network of partners and collaborators. The foundation can provide essential back office types of support as well as shared infrastructure that supports and facilitates all technical facets of the collection, assembly, production, publishing, dissemination and use of spatial environmental data.</p>
<p>This is the concept behind the newly formed OpenGeoHub Foundation (<a href="https://opengeohub.org/" class="uri">https://opengeohub.org/</a>). We see OpenGeoHub as an entity that can build, operate and maintain a core set of functionalities and infrastructure required to support a wide variety of inventory type activities. This core functionality can provide back office facilities and support that can be made use of by any entity that desires to be actively involved in the collection, production or distribution of spatial environmental information. Just as many companies and agencies have increasingly begun to outsource their data storage, data processing and even key functions such as payroll and human resources so too could entities involved in the production or use or spatial data outsource many of their functions to OpenGeoHub. It is expensive and time consuming to build and maintain custom functionality in house to support the production and distribution of inventory spatial data. Why not build it well once and make this functionality available to everyone? If some desired functionality is missing, then build it inside the foundation so that all can benefit from using the new functionality. Why spend money and time building multiple versions of systems with equivalent functionality and purpose when one will do for all? Then the partner entities can concentrate on doing what their mandates instruct them to do, and not on building and maintaining spatial analysis and spatial distribution systems and infrastructures.</p>
<p>We would hope that OpenGeoHub could act as a fully functional worked example of how collaboration and collective action in the area of production and delivery of environmental spatial data could be organized and implemented efficiently and effectively. Once the concept has been demonstrated and accepted, it might well prove useful to replicate or clone the concept, including all of its functionalities, for use at national, state or regional levels. As with any other concept that works, cloning to set up franchise operations is widely accepted. It is not necessary to reinvent the wheel to set up a cloned franchise operation. Most of the design and functionality can simply be replicated for use at the local franchise level. We envisage OpenGeoHub as a test case and an incubator that, if successful, could form a template for many other successful spinoffs.</p>
<!--chapter:end:PSM_future.Rmd-->
</div>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<!--chapter:end:references.Rmd-->
<div id="refs" class="references">
<div id="ref-Adams1973JSS">
<p>Adams, W. A. 1973. “The effect of organic matter on the bulk and true density of some uncultivated podzolic soils.” <em>Journal of Soil Science</em> 24: 10–17.</p>
</div>
<div id="ref-Aitken1991AJSR">
<p>Aitken, R.L., and P.W. Moody. 1991. “Interrelations between Soil pH Measurements in Various Electrolytes and Soil Solution pH in Acidic Soils.” <em>Australian Journal of Soil Research</em> 29: 483–91.</p>
</div>
<div id="ref-Alexander1980SSSAJ">
<p>Alexander, E.B. 1980. “Bulk density of Californian soils in relation to other soil properties.” <em>Soil Science Society of America Journal</em> 44: 689–92.</p>
</div>
<div id="ref-Arrouays201493">
<p>Arrouays, Dominique, Michael G. Grundy, Alfred E. Hartemink, Jonathan W. Hempel, Gerard B.M. Heuvelink, S. Young Hong, Philippe Lagacherie, et al. 2014. “Chapter Three — GlobalSoilMap: Toward a Fine-Resolution Global Grid of Soil Properties.” In <em>Soil Carbon</em>, edited by Donald L. Sparks, 125:93–134. Advances in Agronomy. Academic Press. doi:<a href="https://doi.org/10.1016/B978-0-12-800137-0.00003-0">10.1016/B978-0-12-800137-0.00003-0</a>.</p>
</div>
<div id="ref-arrouays2017soil">
<p>Arrouays, Dominique, Johan GB Leenaars, Anne C Richer-de-Forges, Kabindra Adhikari, Cristiano Ballabio, Mogens Greve, Mike Grundy, et al. 2017. “Soil legacy data rescue via GlobalSoilMap and other international and national initiatives.” <em>GeoResJ</em> 14. Elsevier: 1–19.</p>
</div>
<div id="ref-Arya1981SSSAJ">
<p>Arya, L.M., and J.F. Paris. 1981. “A physico-empirical approach to predict the soil water moisture characteristic from particle size distribution and bulk density data.” <em>Soil Science Society of America Journal</em> 45: 1023–30.</p>
</div>
<div id="ref-Avery1987">
<p>Avery, B. 1987. <em>Soil Survey Methods: A Review</em>. Technical Monograph No. 18. Silsoe: Soil Survey &amp; Land Resource Centre.</p>
</div>
<div id="ref-Rainer2010">
<p>Baritz, Rainer, Guenther Seufert, Luca Montanarella, and Eric Van Ranst. 2010. “Carbon Concentrations and Stocks in Forest Soils of Europe.” <em>Forest Ecology and Management</em> 260 (06/2010): 262–77. doi:<a href="https://doi.org/10.1016/j.foreco.2010.03.025">10.1016/j.foreco.2010.03.025</a>.</p>
</div>
<div id="ref-Batjes2009SUM">
<p>Batjes, N. H. 2009. “Harmonized soil profile data for applications at global and continental scales: Updates to the WISE database.” <em>Soil Use and Management</em> 25 (2): 124–27. doi:<a href="https://doi.org/10.1111/j.1475-2743.2009.00202.x">10.1111/j.1475-2743.2009.00202.x</a>.</p>
</div>
<div id="ref-Beaudette2009CG">
<p>Beaudette, D., and A.T. O’Geen. 2009. “Soil-web: An online soil survey for California, Arizona, and Nevada.” <em>Computers &amp; Geosciences</em> 35: 2119–28.</p>
</div>
<div id="ref-Beaudette2013CompGeo">
<p>Beaudette, D.E., P. Roudier, and A.T. O’Geen. 2013. “Algorithms for Quantitative Pedology: A Toolkit for Soil Scientists.” <em>Computers &amp; Geosciences</em> 52 (0): 258–68. doi:<a href="https://doi.org/10.1016/j.cageo.2012.10.020">10.1016/j.cageo.2012.10.020</a>.</p>
</div>
<div id="ref-Behrens2018128">
<p>Behrens, T., K. Schmidt, R.A. MacMillan, and R.A. Viscarra Rossel. 2018. “Multiscale Contextual Spatial Modelling with the Gaussian Scale Space.” <em>Geoderma</em> 310: 128–37. doi:<a href="https://doi.org/10.1016/j.geoderma.2017.09.015">10.1016/j.geoderma.2017.09.015</a>.</p>
</div>
<div id="ref-berhongaray2013ipcc">
<p>Berhongaray, Gonzalo, and Roberto Alvarez. 2013. “The Ipcc Tool for Predicting Soil Organic Carbon Changes Evaluated for the Pampas, Argentina.” <em>Agriculture, Ecosystems &amp; Environment</em> 181. Elsevier: 241–45.</p>
</div>
<div id="ref-Bernoux1998SSSAJ">
<p>Bernoux, M., D. Arrouays, C. Cerri, B. Valkoff, and C. Jolivet. 1998. “Bulk densities of Brazilian Amazon soils related to other soil properties.” <em>Soil Science Society of America Journal</em> 62: 743–49.</p>
</div>
<div id="ref-Biau2016">
<p>Biau, Gérard, and Erwan Scornet. 2016. “A Random Forest Guided Tour.” <em>TEST</em> 25 (2). Springer: 197–227. doi:<a href="https://doi.org/10.1007/s11749-016-0481-7">10.1007/s11749-016-0481-7</a>.</p>
</div>
<div id="ref-BieUlph1972JAE">
<p>Bie, S.W., and A. Ulph. 1972. “The Economic Value of Survey Information.” <em>Journal of Agricultural Economics</em> 13: 285–97.</p>
</div>
<div id="ref-Bie1973JSS">
<p>Bie, S.W., A. Uph, and P.H. T. Beckett. 1973. “Calculating the Economic Benefits of Soil Survey.” <em>Journal of Soil Science</em> 24: 429–35.</p>
</div>
<div id="ref-Bishop1999Geoderma">
<p>Bishop, T.F.A., A. B. McBratney, and G.M. Laslett. 1999. “Modelling Soil Attribute Depth Functions with Equal-Area Quadratic Smoothing Splines.” <em>Geoderma</em> 91 (1-2): 27–45.</p>
</div>
<div id="ref-Bisutti2004TAC">
<p>Bisutti, I., I. Hilke, and M. Raessler. 2004. “Determination of total organic carbon — an overview of current methods.” <em>Trends in Analytical Chemistry</em> 23 (10-11): 716–26.</p>
</div>
<div id="ref-Bivand2008Springer">
<p>Bivand, R., E. Pebesma, and V. Rubio. 2008. <em>Applied Spatial Data Analysis with R</em>. Use R Series. Heidelberg: Springer.</p>
</div>
<div id="ref-Bivand2013Springer">
<p>———. 2013. <em>Applied Spatial Data Analysis with R</em>. 2nd ed. Use R Series. Heidelberg: Springer.</p>
</div>
<div id="ref-Boettinger2010Springer">
<p>Boettinger, J. L., D. W. Howell, A. C. Moore, A. E. Hartemink, and S. Kienast-Brown, eds. 2010. <em>Digital Soil Mapping: Bridging Research, Environmental Application, and Operation</em>. Vol. 2. Progress in Soil Science. Springer.</p>
</div>
<div id="ref-Bouma1989S">
<p>Bouma, J. 1989. “Land qualities in space and time.” In <em>Proceedings of a symposium organized by the International Society of Soil Science</em>, edited by J. Bouma and A.K. Bregt, 3–13. Wageningen: Wageningen University.</p>
</div>
<div id="ref-bouma1998exploring">
<p>Bouma, J., N. H. Batjes, and J. J. R. Groot. 1998. “Exploring Land Quality Effects on World Food Supply.” <em>Geoderma</em> 86 (1). Elsevier: 43–59.</p>
</div>
<div id="ref-breiman1993classification">
<p>Breiman, Leo. 1993. <em>Classification and Regression Trees</em>. CRC press.</p>
</div>
<div id="ref-breiman2001random">
<p>———. 2001. “Random Forests.” <em>Machine Learning</em> 45 (1). Springer: 5–32.</p>
</div>
<div id="ref-Brennan1998">
<p>Brennan, R.F., and M.D.A. Bolland. 1998. “Relationship Between Ph Measured in Water and Calcium Chloride for Soils of Southwestern Australia.” <em>Communications in Soil Science and Plant Analysis</em> 29 (17-18). Taylor &amp; Francis: 2683–9. doi:<a href="https://doi.org/10.1080/00103629809370143">10.1080/00103629809370143</a>.</p>
</div>
<div id="ref-Brenning2012">
<p>Brenning, A. 2012. “Spatial cross-validation and bootstrap for the assessment of prediction rules in remote sensing: The R package sperrorest.” In <em>2012 Ieee International Geoscience and Remote Sensing Symposium</em>, 5372–5. doi:<a href="https://doi.org/10.1109/IGARSS.2012.6352393">10.1109/IGARSS.2012.6352393</a>.</p>
</div>
<div id="ref-brodlie2012review">
<p>Brodlie, Ken, Rodolfo Allendes Osorio, and Adriano Lopes. 2012. “A Review of Uncertainty in Data Visualization.” In <em>Expanding the Frontiers of Visual Analytics and Visualization</em>, 81–109. Springer.</p>
</div>
<div id="ref-Brown2014JSS">
<p>Brown, Patrick E. 2015. “Model-Based Geostatistics the Easy Way.” <em>Journal of Statistical Software</em> 63 (12). <a href="http://www.jstatsoft.org/v63/i12" class="uri">http://www.jstatsoft.org/v63/i12</a>.</p>
</div>
<div id="ref-Brus2007Geoderma">
<p>Brus, D. J., and G. B. M. Heuvelink. 2007. “Optimization of sample patterns for universal kriging of environmental variables.” <em>Geoderma</em> 138 (1-2). Elsevier: 86–95.</p>
</div>
<div id="ref-Brus2011EJSS">
<p>Brus, D.J., B. Kempen, and G.B.M. Heuvelink. 2011. “Sampling for Validation of Digital Soil Maps.” <em>European Journal of Soil Science</em> 62 (3). Blackwell Publishing Ltd: 394–407. doi:<a href="https://doi.org/10.1111/j.1365-2389.2011.01364.x">10.1111/j.1365-2389.2011.01364.x</a>.</p>
</div>
<div id="ref-Brye2003CSSPA">
<p>Brye, K.R., and N.A. Slaton. 2003. “Carbon and Nitrogen Storage in a Typic Albaqualf as Affected by Assessment Method.” <em>Communications in Soil Science and Plant Analysis</em> 34: 1637–55.</p>
</div>
<div id="ref-Bui2004Geoderma">
<p>Bui, E. 2004. “Soil Survey as a Knowledge System.” <em>Geoderma</em> 120 (05/2004): 17–26. doi:<a href="https://doi.org/10.1016/j.geoderma.2003.07.006">10.1016/j.geoderma.2003.07.006</a>.</p>
</div>
<div id="ref-Bui2003Geoderma">
<p>Bui, E. 2003. “A Strategy to Fill Gaps in Soil Survey over Large Spatial Extents: An Example from the Murray-Darling Basin of Australia.” <em>Geoderma</em> 111 (01/2003): 21–44. doi:<a href="https://doi.org/10.1016/S0016-7061(02)00238-0">10.1016/S0016-7061(02)00238-0</a>.</p>
</div>
<div id="ref-buol2011soil">
<p>Buol, S.W., R.J. Southard, R.C. Graham, and P. A. McDaniel. 2011. <em>Soil Genesis and Classification</em>. Wiley.</p>
</div>
<div id="ref-Burrough1989JSS">
<p>Burrough, P. A. 1989. “Fuzzy Mathematical Methods for Soil Survey and Land Evaluation.” <em>Journal of Soil Science</em> 40 (3): 477–92. doi:<a href="https://doi.org/10.1111/j.1365-2389.1989.tb01290.x">10.1111/j.1365-2389.1989.tb01290.x</a>.</p>
</div>
<div id="ref-Burrough1998OUP">
<p>Burrough, P. A., and R. A. McDonnell. 1998. <em>Principles of Geographical Information Systems</em>. 2nd ed. Oxford: Oxford University Press.</p>
</div>
<div id="ref-Burrough1971">
<p>Burrough, P. A., P.H. T. Beckett, and M.G. Jarvis. 1971. “The Relation Between Cost and Utility in Soil Survey (I–Iii).” <em>Journal of Soil Science</em> 22 (3). Blackwell Publishing Ltd: 359–94. doi:<a href="https://doi.org/10.1111/j.1365-2389.1971.tb01624.x">10.1111/j.1365-2389.1971.tb01624.x</a>.</p>
</div>
<div id="ref-Campbell2008NCST">
<p>Campbell, Andrew. 2008. <em>Managing Australia’s Soils: A policy discussion paper</em>. Canberra: CSIRO.</p>
</div>
<div id="ref-Campbell1989">
<p>Campbell, G.S., and S. Shiozawa. 1989. “Prediction of hydraulic properties of soils using particle-size distribution and bulk density data.” In <em>Proc. Int. Worksh. On Indirect Methods for Estimating the Hydraulic Properties of Unsaturated Soils</em>, edited by van Genuchten, M.Th. et al., 317–28. Riverside, CA: Univ. of California, Riverside.</p>
</div>
<div id="ref-Canarache1993ST">
<p>Canarache, A. 1993. “Physical-technological soil maps — A possible product of soil survey for direct use in agriculture.” <em>Soil Technology</em> 6: 3–15.</p>
</div>
<div id="ref-Carrick2010WCSS">
<p>Carrick, Sam, Éva-Terézia Vesely, and Allan Hewitt. 2010. “Economic Value of Improved Soil Natural Capital Assessment: A Case Study on Nitrogen Leaching.” In <em>19th World Congress of Soil Science</em>, 1–6. Brisbane, Australia: IUSS.</p>
</div>
<div id="ref-carter2007soil">
<p>Carter, M.R., and E.G. Gregorich. 2007. <em>Soil Sampling and Methods of Analysis</em>. CRC PRESS.</p>
</div>
<div id="ref-CAUBET201999">
<p>Caubet, Manon, Mercedes Román Dobarco, Dominique Arrouays, Budiman Minasny, and Nicolas P.A. Saby. 2019. “Merging Country, Continental and Global Predictions of Soil Texture: Lessons from Ensemble Modelling in France.” <em>Geoderma</em> 337: 99–110. doi:<a href="https://doi.org/10.1016/j.geoderma.2018.09.007">10.1016/j.geoderma.2018.09.007</a>.</p>
</div>
<div id="ref-chang2011libsvm">
<p>Chang, Chih-Chung, and Chih-Jen Lin. 2011. “LIBSVM: a library for support vector machines.” <em>ACM Transactions on Intelligent Systems and Technology (TIST)</em> 2 (3). Acm: 27.</p>
</div>
<div id="ref-Chen2014">
<p>Chen, Jun, Jin Chen, Anping Liao, Xin Cao, Lijun Chen, Xuehong Chen, Chaoying He, et al. 2015. “Global land cover mapping at 30 m resolution: A POK-based operational approach.” <em>ISPRS Journal of Photogrammetry and Remote Sensing</em> 103: 7–27. doi:<a href="https://doi.org/10.1016/j.isprsjprs.2014.09.002">10.1016/j.isprsjprs.2014.09.002</a>.</p>
</div>
<div id="ref-2016arXiv160302754C">
<p>Chen, T., and C. Guestrin. 2016. “XGBoost: A Scalable Tree Boosting System.” <em>ArXiv E-Prints</em>, March.</p>
</div>
<div id="ref-chesworth2008encyclopedia">
<p>Chesworth, W. 2008. <em>Encyclopedia of soil science</em>. Encyclopedia of Earth Sciences Series. Springer.</p>
</div>
<div id="ref-christakos2001temporal">
<p>Christakos, G., P. Bogaert, and M.L. Serre. 2001. <em>Temporal GIS: advanced functions for field-based applications</em>. v. 1. Springer.</p>
</div>
<div id="ref-Christensen2001Springer">
<p>Christensen, R. 2001. <em>Linear Models for Multivariate, Time Series, and Spatial Data</em>. 2nd ed. New York: Springer Verlag.</p>
</div>
<div id="ref-Conant2010">
<p>Conant, Richard T, Stephen M Ogle, Eldor A Paul, and Keith Paustian. 2010. “Measuring and Monitoring Soil Organic Carbon Stocks in Agricultural Lands for Climate Mitigation.” <em>Frontiers in Ecology and the Environment</em> 9 (3). Ecological Society of America: 169–73. <a href="http://dx.doi.org/10.1890/090153" class="uri">http://dx.doi.org/10.1890/090153</a>.</p>
</div>
<div id="ref-gmd-8-1991-2015">
<p>Conrad, O., B. Bechtel, M. Bock, H. Dietrich, E. Fischer, L. Gerlitz, J. Wehberg, V. Wichmann, and J. Böhner. 2015. “System for Automated Geoscientific Analyses (Saga) V. 2.1.4.” <em>Geoscientific Model Development</em> 8 (7). Copernicus GmbH: 1991–2007. doi:<a href="https://doi.org/10.5194/gmd-8-1991-2015">10.5194/gmd-8-1991-2015</a>.</p>
</div>
<div id="ref-cooper2005national">
<p>Cooper, Miguel, Lúcia Maria Silveira Mendes, Wellinton Luiz Costa Silva, and Gerd Sparovek. 2005. “A National Soil Profile Database for Brazil Available to International Scientists.” <em>Soil Science Society of America Journal</em> 69 (3). Soil Science Society: 649–52.</p>
</div>
<div id="ref-cressie2011statistics">
<p>Cressie, N.A. C., and C.K. Wikle. 2011. <em>Statistics for Spatio-Temporal Data</em>. Wiley Series in Probability and Statistics. Wiley.</p>
</div>
<div id="ref-Cresswell2006SUM">
<p>Cresswell, H.P., Y. Coquet, A. Bruand, and N.J. McKenzie. 2006. “The transferability of Australian pedotransfer functions for predicting water retention characteristics of French soils.” <em>Soil Use and Management</em> 22: 62–70.</p>
</div>
<div id="ref-Curtis1964SSSAP">
<p>Curtis, R.O., and B.W. Post. 1964. “Estimating bulk density from organic matter content in some Vermont forest soils.” <em>Soil Science Society of America Proceedings</em> 28: 285–86.</p>
</div>
<div id="ref-DAvello1998SSH">
<p>D’Avello, T.P., and R.L. McLeese. 1998. “Why Are Those Lines Placed Where They Are? An Investigation of Soil Map Recompilation Methods.” <em>Soil Survey Horiz.</em> 39: 119–26.</p>
</div>
<div id="ref-Danielson2011GMTED">
<p>Danielson, J.J., and D.B. and Gesch. 2011. <em>Global multi-resolution terrain elevation data 2010 (GMTED2010)</em>. Open-File Report 2011-1073. U.S. Geological Survey.</p>
</div>
<div id="ref-Davis1943SS">
<p>Davis, L.E. 1943. “Measurements of pH with the Glass Electrode as Affected by Soil Moisture.” <em>Soil Science</em> 56 (6): 405–22.</p>
</div>
<div id="ref-DeGruijter1997Geoderma">
<p>de Gruijter, J. J., D. J. J. Walvoort, and P. F. M. van Gaans. 1997. “Continuous Soil Maps — a Fuzzy Set Approach to Bridge the Gap Between Aggregation Levels of Process and Distribution Models.” <em>Geoderma</em> 77 (2-4): 169–95.</p>
</div>
<div id="ref-deGruijter2006sampling">
<p>de Gruijter, J.J., D.J. Brus, M.F.P. Bierkens, and M. Knotters. 2006. <em>Sampling for Natural Resource Monitoring</em>. London: Springer.</p>
</div>
<div id="ref-DeVos2007SUM">
<p>De Vos, B., S. Lettens, B. Muys, and J.A. Deckers. 2007. “Walkley-Black analysis of forest soil organic carbon: Recovery,limitations and uncertainty.” <em>Soil Use and Management</em> 23: 221–29.</p>
</div>
<div id="ref-JPLN:JPLN200900269">
<p>Desaules, André, Stefan Ammann, and Peter Schwab. 2010. “Advances in Long-Term Soil-Pollution Monitoring of Switzerland.” <em>Journal of Plant Nutrition and Soil Science</em> 173 (4). WILEY-VCH Verlag: 525–35. doi:<a href="https://doi.org/10.1002/jpln.200900269">10.1002/jpln.200900269</a>.</p>
</div>
<div id="ref-Diggle2007Springer">
<p>Diggle, P. J., and P. J. Ribeiro Jr. 2007. <em>Model-based Geostatistics</em>. Springer Series in Statistics. Springer.</p>
</div>
<div id="ref-Dobos2006digital">
<p>Dobos, E., F. Carré, T. Hengl, H. I. Reuter, and G. Tóth. 2006. <em>Digital Soil Mapping: As a Support to Production of Functional Maps</em>. Luxemburg: Office for Official Publications of the European Communities.</p>
</div>
<div id="ref-driessen1992land">
<p>Driessen, Paul M, and Nicolaas T Konijn. 1992. <em>Land-Use Systems Analysis</em>. Wageningen Agricultural University.</p>
</div>
<div id="ref-eltit2008">
<p>Durana, Patricia J. 2008a. “Appendix A: Chronology of the U.S. Soil Survey.” In <em>Profiles in the History of the U.s. Soil Survey</em>, 315–20. Iowa State Press. doi:<a href="https://doi.org/10.1002/9780470376959.app1">10.1002/9780470376959.app1</a>.</p>
</div>
<div id="ref-Durana2008">
<p>———. 2008b. “Appendix A: Chronology of the U.S. Soil Survey.” In <em>Profiles in the History of the U.S. Soil Survey</em>, 315–20. Iowa State Press. doi:<a href="https://doi.org/10.1002/9780470376959.app1">10.1002/9780470376959.app1</a>.</p>
</div>
<div id="ref-eswaran2010soil">
<p>Eswaran, H., R. Ahrens, T.J. Rice, and B.A. Stewart. 2010. <em>Soil Classification: A Global Desk Reference</em>. Taylor &amp; Francis.</p>
</div>
<div id="ref-fan2013global">
<p>Fan, Y, H Li, and G Miguez-Macho. 2013. “Global Patterns of Groundwater Table Depth.” <em>Science</em> 339 (6122). American Association for the Advancement of Science: 940–43.</p>
</div>
<div id="ref-FAO1990">
<p>FAO. 1990. <em>Guidelines for Soil Profile Description</em>. 3rd ed. Rome: Food; Agriculture Organization of the United Nations.</p>
</div>
<div id="ref-FAO2006">
<p>———. 2006. <em>Guidelines for Soil Profile Description</em>. 4th ed. Rome: Food; Agriculture Organization of the United Nations.</p>
</div>
<div id="ref-FAO2012HWSD">
<p>FAO/IIASA/ISRIC/ISS-CAS/JRC. 2012. <em>Harmonized World Soil Database (Version 1.2)</em>. Rome: FAO.</p>
</div>
<div id="ref-Federer1993CJFR">
<p>Federer, C.A., D.E. Turcotte, and C.T. Smith. 1993. “The organic- bulk-density relationship and the expression of nutrient content in forest soils.” <em>Canadian Journal of Forest Research</em> 23: 1026–32.</p>
</div>
<div id="ref-fernandez1988color">
<p>Fernandez, R.N., D.G. Schulze, D.L. Coffin, and G.E. Van Scoyoc. 1988. “Color, Organic Matter, and Pesticide Adsorption Relationships in a Soil Landscape.” <em>Soil Science Society of America Journal</em> 52 (4). Soil Science Society of America: 1023–6.</p>
</div>
<div id="ref-Finke2006Elsevier">
<p>Finke, P. 2006. “Quality assessment of digital soil maps: producers and users perspectives.” In <em>Digital Soil Mapping: An Introductory Perspective</em>, edited by P. Lagacherie, A. B. McBratney, and M. Voltz, 523–41. Developments in Soil Science. Amsterdam: Elsevier.</p>
</div>
<div id="ref-Finke2008462">
<p>Finke, Peter A., and John L. Hutson. 2008. “Modelling Soil Genesis in Calcareous Loess.” <em>Geoderma</em> 145 (3?4): 462–79. doi:<a href="https://doi.org/10.1016/j.geoderma.2008.01.017">10.1016/j.geoderma.2008.01.017</a>.</p>
</div>
<div id="ref-Florinsky2012Dokuchaev">
<p>Florinsky, I. V. 2012. “The Dokuchaev Hypothesis as a Basis for Predictive Digital Soil Mapping (on the 125th Anniversary of Its Publication).” <em>Eurasian Soil Science</em> 45 (4): 445–51. doi:<a href="https://doi.org/10.1134/S1064229312040047">10.1134/S1064229312040047</a>.</p>
</div>
<div id="ref-food1977framework">
<p>Food, Development Agriculture Organization of the United Nations. Soil Resources, and Conservation Service. 1977. <em>A Framework for Land Evaluation</em>. Publication (International Institute for Land Reclamation and Improvement). International Institute for Land Reclamation; Improvement.</p>
</div>
<div id="ref-food2006guidelines">
<p>Food, and Agriculture Organization of the United Nations. 2006. <em>Guidelines for Soil Description</em>. Food; Agriculture Organization of the United Nations.</p>
</div>
<div id="ref-forkel2015codominant">
<p>Forkel, Matthias, Mirco Migliavacca, Kirsten Thonicke, Markus Reichstein, Sibyll Schaphoff, Ulrich Weber, and Nuno Carvalhais. 2015. “Codominant Water Control on Global Interannual Variability and Trends in Land Surface Phenology and Greenness.” <em>Global Change Biology</em> 21 (9). Wiley Online Library: 3414–35.</p>
</div>
<div id="ref-Fran01">
<p>Franklin, J. 1995. “Predictive Vegetation Mapping: Geographic Modelling of Biospatial Patterns in Relation to Environmental Gradients.” <em>Progress in Physical Geography</em> 19 (4): 474–99.</p>
</div>
<div id="ref-frossard2006function">
<p>Frossard, E., W.E.H. Blum, B.P. Warkentin, and Geological Society of London. 2006. <em>Function of soils for human societies and the environment</em>. Special Publication — Geological Society of London. Geological Society.</p>
</div>
<div id="ref-Gasch2015SPASTA">
<p>Gasch, Caley, Tomislav Hengl, Benedikt Gräler, Hanna Meyer, Troy Magney, and David Brown. 2015. “Spatio-temporal interpolation of soil water, temperature, and electrical conductivity in 3D+T: The Cook Agronomy Farm data set.” <em>Spatial Statistics</em> 14 (Part A): 70–90. doi:<a href="https://doi.org/10.1016/j.spasta.2015.04.001">10.1016/j.spasta.2015.04.001</a>.</p>
</div>
<div id="ref-GehlRice2005">
<p>Gehl, RonaldJ., and CharlesW. Rice. 2007. “Emerging Technologies for in Situ Measurement of Soil Carbon.” <em>Climatic Change</em> 80 (1-2). Kluwer Academic Publishers: 43–54. doi:<a href="https://doi.org/10.1007/s10584-006-9150-2">10.1007/s10584-006-9150-2</a>.</p>
</div>
<div id="ref-gibbons1964study">
<p>Gibbons, Frank R., Ronald Geoffrey Downes, and others. 1964. <em>A study of the land in south-western Victoria</em>. Soil Conservation Authority Melbourne.</p>
</div>
<div id="ref-Gijsman2007CEA">
<p>Gijsman, A.J., P.K. Thornton, and G. Hoogenboom. 2007. “Using the WISE database to parameterize soil inputs for crop simulation models.” <em>Computers and Electronics in Agriculture</em> 56: 85–100.</p>
</div>
<div id="ref-giri2011status">
<p>Giri, Chandra, E Ochieng, Larry L Tieszen, Z Zhu, Ashbindu Singh, T Loveland, J Masek, and Norm Duke. 2011. “Status and Distribution of Mangrove Forests of the World Using Earth Observation Satellite Data.” <em>Global Ecology and Biogeography</em> 20 (1). Wiley Online Library: 154–59.</p>
</div>
<div id="ref-Goodchild2008Accuracy">
<p>Goodchild, M.F. 2008. “Spatial Accuracy 2.0.” In <em>Proceedings of the 8th International Symposium on Spatial Accuracy Assessment in Natural Resources and Environmental Sciences</em>, edited by Wan, Y. et al., 1–7. World Academic Union (Press).</p>
</div>
<div id="ref-Goovaerts1997Oxford">
<p>Goovaerts, P. 1997. <em>Geostatistics for Natural Resources Evaluation (Applied Geostatistics)</em>. New York: Oxford University Press.</p>
</div>
<div id="ref-goovaerts1999geostatistics">
<p>Goovaerts, Pierre. 1999. “Geostatistics in Soil Science: State-of-the-Art and Perspectives.” <em>Geoderma</em> 89 (1). Elsevier: 1–45.</p>
</div>
<div id="ref-goovaerts2001geostatistical">
<p>———. 2001. “Geostatistical Modelling of Uncertainty in Soil Science.” <em>Geoderma</em> 103 (1). Elsevier: 3–26.</p>
</div>
<div id="ref-Grewal1991JSS">
<p>Grewal, K.S., G.D. Buchan, and R.R. Sherlock. 1991. “A comparison of three methods of organic carbon determination in some New Zealand soils.” <em>Journal of Soil Science</em> 42: 251–57.</p>
</div>
<div id="ref-Grinand2008Geoderma">
<p>Grinand, C., D. Arrouays, B. Laroche, and M. Martin. 2008. “Extrapolating regional soil landscapes from an existing soil map: Sampling intensity, validation procedures, and integration of spatialcontext.” <em>Geoderma</em> 143 (01/2008): 180–90. doi:<a href="https://doi.org/10.1016/j.geoderma.2007.11.004">10.1016/j.geoderma.2007.11.004</a>.</p>
</div>
<div id="ref-GRUBER2009171">
<p>Gruber, S., and S. Peckham. 2009. “Chapter 7 Land-Surface Parameters and Objects in Hydrology.” In <em>Geomorphometry</em>, edited by Tomislav Hengl and Hannes I. Reuter, 33:171–94. Developments in Soil Science. Elsevier. doi:<a href="https://doi.org/10.1016/S0166-2481(08)00007-X">10.1016/S0166-2481(08)00007-X</a>.</p>
</div>
<div id="ref-grunwald2005environmental">
<p>Grunwald, S. 2005a. <em>Environmental Soil-Landscape Modeling: Geographic Information Technologies and Pedometrics</em>. Books in Soils, Plants, and the Environment Series. Taylor &amp; Francis.</p>
</div>
<div id="ref-Grunwald2005CRCPress">
<p>———. 2005b. “What do we really know about the space-time continuum of soil-landscapes.” In <em>Environmental Soil-Landscape Modeling: Geographic Information Technologies and Pedometrics</em>, edited by S. Grunwald, 3–36. Boca Raton, Florida: CRC Press.</p>
</div>
<div id="ref-Haining2010GEAN780">
<p>Haining, Robert P., Ruth Kerry, and Margaret A. Oliver. 2010. “Geography, Spatial Data Analysis, and Geostatistics: An Overview.” <em>Geographical Analysis</em> 42 (1). Blackwell Publishing Inc: 7–31. doi:<a href="https://doi.org/10.1111/j.1538-4632.2009.00780.x">10.1111/j.1538-4632.2009.00780.x</a>.</p>
</div>
<div id="ref-Hall1977">
<p>Hall, D.G., M.J. Reeve, A.J. Thomasson, and V.F. Wright. 1977. <em>Water Retention, Porosity and Density of Field Soils</em>. Soil Survey of England and Wales. Harpenden: Transport; Road Research Laboratory.</p>
</div>
<div id="ref-hall2007accuracy">
<p>Hall, Dorothy K, and George A Riggs. 2007. “Accuracy assessment of the MODIS snow products.” <em>Hydrological Processes</em> 21 (12). Wiley Online Library: 1534–47.</p>
</div>
<div id="ref-hansen2013high">
<p>Hansen, M. C., P. V. Potapov, R. Moore, M. Hancher, S. A. Turubanova, A. Tyukavina, D. Thau, et al. 2013. “High-resolution global maps of 21st-century forest cover change.” <em>Science</em> 342 (6160). American Association for the Advancement of Science: 850–53.</p>
</div>
<div id="ref-harpstead2001soil">
<p>Harpstead, M.I., T.J. Sauer, and W.F. Bennett. 2001. <em>Soil Science Simplified</em>. Wiley.</p>
</div>
<div id="ref-harrell2001regression">
<p>Harrell, F.E. 2001. <em>Regression Modeling Strategies: With Applications to Linear Models, Logistic Regression, and Survival Analysis</em>. Graduate Texts in Mathematics. Springer.</p>
</div>
<div id="ref-harris2014updated">
<p>Harris, IPDJ, PD Jones, TJ Osborn, and DH Lister. 2014. “Updated High-Resolution Grids of Monthly Climatic Observations–the Cru Ts3. 10 Dataset.” <em>International Journal of Climatology</em> 34 (3). Wiley Online Library: 623–42.</p>
</div>
<div id="ref-Hartemink2010Springer">
<p>Hartemink, A. E., J. Hempel, P. Lagacherie, A. B. McBratney, N. McKenzie, R. A. MacMillan, B. Minasny, et al. 2010. “GlobalSoilMap.net — A New Digital Soil Map of the World.” In <em>Digital Soil Mapping: Bridging Research, Environmental Application, and Operation</em>, edited by J. L. Boettinger, D. W. Howell, A. C. Moore, A. E. Hartemink, and S. Kienast-Brown, 2:423–27. Progress in Soil Science. Springer.</p>
</div>
<div id="ref-Hartemink2008SMD">
<p>Hartemink, Alfred. 2008. “Soil Map Density and a Nation’s Wealth and Income.” In <em>Digital Soil Mapping with Limited Data</em>, edited by Alfred Hartemink, Alex McBratney, and Mariade Lourdes Mendonça-Santos, 53–66. Springer Netherlands. doi:<a href="https://doi.org/10.1007/978-1-4020-8592-5_1">10.1007/978-1-4020-8592-5_1</a>.</p>
</div>
<div id="ref-Hartemink2008Springer">
<p>Hartemink, Alfred E., Alex McBratney, and Maria de Lourdes Mendonça-Santos, eds. 2008. <em>Digital Soil Mapping with Limited Data</em>. Vol. 1. Progress in Soil Science. Springer.</p>
</div>
<div id="ref-GGGE:GGGE2352">
<p>Hartmann, Jens, and Nils Moosdorf. 2012. “The new global lithological map database GLiM: A representation of rock properties at the Earth surface.” <em>Geochemistry, Geophysics, Geosystems</em> 13 (12): n/a–n/a. doi:<a href="https://doi.org/10.1029/2012GC004370">10.1029/2012GC004370</a>.</p>
</div>
<div id="ref-hastie2009elements">
<p>Hastie, T.J., R.J. Tibshirani, and J.J.H. Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Springer Series in Statistics. Springer-Verlag New York.</p>
</div>
<div id="ref-hearst1998support">
<p>Hearst, Marti A., S.T. Dumais, E. Osman, John Platt, and Bernhard Scholkopf. 1998. “Support Vector Machines.” <em>Intelligent Systems and Their Applications, IEEE</em> 13 (4). IEEE: 18–28.</p>
</div>
<div id="ref-Henderson2004Geoderma">
<p>Henderson, B. L., E. N. Bui, C. J. Moran, and D. A. P. Simon. 2004. “Australia-wide predictions of soil properties using decision trees.” <em>Geoderma</em> 124 (3-4): 383–98.</p>
</div>
<div id="ref-Hengl2015AfSoilGrids250m">
<p>Hengl, Gerard B.M. AND Kempen, Tomislav AND Heuvelink. 2015. “Mapping Soil Properties of Africa at 250 m Resolution: Random Forests Significantly Improve Current Predictions.” <em>PLoS ONE</em> 10 (e0125814). Public Library of Science. doi:<a href="https://doi.org/10.1371/journal.pone.0125814">10.1371/journal.pone.0125814</a>.</p>
</div>
<div id="ref-Hengl2014SoilGrids1km">
<p>Hengl, Jorge AND MacMillan, Tomislav AND Mendes de Jesus. 2014. “SoilGrids1km — Global Soil Information Based on Automated Mapping.” <em>PLoS ONE</em> 9 (e105992). Public Library of Science. doi:<a href="https://doi.org/10.1371/journal.pone.0105992">10.1371/journal.pone.0105992</a>.</p>
</div>
<div id="ref-Hengl2006CG">
<p>Hengl, T. 2006. “Finding the right pixel size.” <em>Computers &amp; Geosciences</em> 32 (9): 1283–98.</p>
</div>
<div id="ref-hengl2017soil">
<p>Hengl, T., J.G.B. Leenaars, K.D. Shepherd, M.G. Walsh, G.B.M. Heuvelink, T. Mamo, H. Tilahun, E. Berkhout, M. Cooper, and E. Fegraus. 2017. “Soil nutrient maps of Sub-Saharan Africa: assessment of soil nutrient content at 250 m spatial resolution using machine learning.” <em>Nutrient Cycling in Agroecosystems</em> 109 (1). Springer: 77–102.</p>
</div>
<div id="ref-Hengl2017SoilGrids250m">
<p>Hengl, T., J. Mendes de Jesus, G. B. M. Heuvelink, M. Ruiperez Gonzalez, M. Kilibarda, A. Blagotic, W. Shangguan, et al. 2017. “SoilGrids250m: Global Gridded Soil Information Based on Machine Learning.” <em>PLoS One</em> 12 (2): e0169748.</p>
</div>
<div id="ref-Hengl2013JAG">
<p>Hengl, T., M. Nikolić, and R. A. MacMillan. 2013. “Mapping Efficiency and Information Content.” <em>International Journal of Applied Earth Observation and Geoinformation</em> 22: 127–38. doi:<a href="https://doi.org/10.1016/j.jag.2012.02.005">10.1016/j.jag.2012.02.005</a>.</p>
</div>
<div id="ref-Hengl2006SSSAJ">
<p>Hengl, Tomislav, and Stjepan Husnjak. 2006. “Evaluating Adequacy and Usability of Soil Maps in Croatia.” <em>Soil Science Society of America Journal</em> 70 (3): 920–29. doi:<a href="https://doi.org/10.2136/sssaj2004.0141">10.2136/sssaj2004.0141</a>.</p>
</div>
<div id="ref-hengl2007regression">
<p>Hengl, Tomislav, Gerard Heuvelink, and David G Rossiter. 2007. “About Regression-Kriging: From Equations to Case Studies.” <em>Computers &amp; Geosciences</em> 33 (10). Elsevier: 1301–15.</p>
</div>
<div id="ref-Hengl2018RFsp">
<p>Hengl, Tomislav, Madlene Nussbaum, Marvin N Wright, Gerard B.M. Heuvelink, and Benedikt Gräler. 2018. “Random Forest as a Generic Framework for Predictive Modeling of Spatial and Spatio-Temporal Variables.” <em>PeerJ</em> 6 (August): e5518. doi:<a href="https://doi.org/10.7717/peerj.5518">10.7717/peerj.5518</a>.</p>
</div>
<div id="ref-Hengl2007Geoderma">
<p>Hengl, Tomislav, Norair Toomanian, Hannes I. Reuter, and Mohammad J. Malakouti. 2007. “Methods to Interpolate Soil Categorical Variables from Profile Observations: Lessons from Iran.” <em>Geoderma</em> 140 (4). Elsevier: 417–27. doi:<a href="https://doi.org/10.1016/j.geoderma.2007.04.022">10.1016/j.geoderma.2007.04.022</a>.</p>
</div>
<div id="ref-Hengl2018PNV">
<p>Hengl, Tomislav, Markus G. Walsh, Jonathan Sanderman, Ichsani Wheeler, Sandy P. Harrison, and Iain C. Prentice. 2018. “Global Mapping of Potential Natural Vegetation: An Assessment of Machine Learning Algorithms for Estimating Land Potential.” <em>PeerJ</em> 6 (August): e5457. doi:<a href="https://doi.org/10.7717/peerj.5457">10.7717/peerj.5457</a>.</p>
</div>
<div id="ref-Herold2016">
<p>Herold, Martin, Linda See, Nandin-Erdene Tsendbazar, and Steffen Fritz. 2016. “Towards an Integrated Global Land Cover Monitoring and Mapping System.” <em>Remote Sensing</em> 8 (12). doi:<a href="https://doi.org/10.3390/rs8121036">10.3390/rs8121036</a>.</p>
</div>
<div id="ref-Heuscher2005SSSAJ">
<p>Heuscher, S.A., C.C. Brandt, and P.M. Jardine. 2005. “Using Soil Physical and Chemical Properties to Estimate Bulk Density.” <em>Soil Science Society of America Journal</em> 69: 1–7.</p>
</div>
<div id="ref-Heuvelink1999Geoderma">
<p>Heuvelink, G. B. M., and E. J. Pebesma. 1999. “Spatial aggregation and soil process modelling.” <em>Geoderma</em> 89 (1-2): 47–65.</p>
</div>
<div id="ref-Heuvelink1998a">
<p>Heuvelink, G.B.M. 1998. <em>Error Propagation in Environmental Modelling with Gis</em>. London, UK: Taylor &amp; Francis.</p>
</div>
<div id="ref-Heuvelink19921">
<p>Heuvelink, G.B.M., and M.F.P. Bierkens. 1992. “Combining Soil Maps with Interpolations from Point Observations to Predict Quantitative Soil Properties.” <em>Geoderma</em> 55 (1-2): 1–15. doi:<a href="https://doi.org/10.1016/0016-7061(92)90002-O">10.1016/0016-7061(92)90002-O</a>.</p>
</div>
<div id="ref-Heuvelink2006Elsevier">
<p>Heuvelink, G.B.M., and J. Brown. 2006. “Towards a Soil Information System for Uncertain Soil Data.” In <em>Digital Soil Mapping: An Introductory Perspective</em>, edited by P. Lagacherie, A. B. McBratney, and M. Voltz, 112–18. Developments in Soil Science. Amsterdam: Elsevier.</p>
</div>
<div id="ref-Heuvelink2001Geoderma">
<p>Heuvelink, G.B.M., and R. Webster. 2001. “Modelling soil variation: past, present, and future.” <em>Geoderma</em> 100 (3-4).</p>
</div>
<div id="ref-heuvelink2010implications">
<p>Heuvelink, GBM, DJ Brus, F De Vries, R Vašát, DJJ Walvoort, B Kempen, and M Knotters. 2010. “Implications of Digital Soil Mapping for Soil Information Systems.” In <em>4th Global Workshop on Digital Soil Mapping, Rome</em>, 24–26.</p>
</div>
<div id="ref-Heuvelink2014GSM">
<p>Heuvelink, Gerard BM. 2014. “Uncertainty Quantification of Globalsoilmap Products.” In <em>GlobalSoilMap: Basis of the Global Spatial Soil Information System</em>, edited by D. Arrouays, N. McKenzie, J. Hempel, A.R. de Forges, and A.B. McBratney, 327–32. Taylor &amp; Francis.</p>
</div>
<div id="ref-Hijmans2005IJC">
<p>Hijmans, R. J., S. E. Cameron, J. L. Parra, P. G. Jones, and A. Jarvis. 2005. “Very High Resolution Interpolated Climate Surfaces for Global Land Areas.” <em>International Journal of Climatology</em> 25: 1965–78.</p>
</div>
<div id="ref-raster">
<p>Hijmans, Robert J., and Jacob van Etten. 2017. <em>Raster: Geographic Data Analysis and Modeling</em>. <a href="http://CRAN.R-project.org/package=raster" class="uri">http://CRAN.R-project.org/package=raster</a>.</p>
</div>
<div id="ref-hodnett2002marked">
<p>Hodnett, M.G., and J. Tomasella. 2002. “Marked differences between van Genuchten soil water-retention parameters for temperate and tropical soils: a new water-retention pedo-transfer functions developed for tropical soils.” <em>Geoderma</em> 108 (3). Elsevier: 155–80.</p>
</div>
<div id="ref-hollis2006spade">
<p>Hollis, J. M., R. J. A. Jones, C. J. Marshall, A. Holden, J. R. Van de Veen, and L. Montanarella. 2006. <em>SPADE-2: The soil profile analytical database for Europe, version 1.0</em>. Luxembourg: Office for official publications of the European Communities.</p>
</div>
<div id="ref-hossain2015bulk">
<p>Hossain, MF, W Chen, and Yu Zhang. 2015. “Bulk Density of Mineral and Organic Soils in the Canada’s Arctic and Sub-Arctic.” <em>Information Processing in Agriculture</em> 2 (3-4). Elsevier: 183–90.</p>
</div>
<div id="ref-Hudson2000SSSAJ">
<p>Hudson, B. D. 2004. “The soil survey as a paradigm-based science.” <em>Soil Science Society of America Journal</em> 56: 836–41.</p>
</div>
<div id="ref-huffman2009gpcp">
<p>Huffman, George J, and David T Bolvin. 2009. <em>GPCP version 2 combined precipitation data set documentation</em>. Vol. 1. Laboratory for Atmospheres, NASA Goddard Space Flight Center; Science Systems; Applications.</p>
</div>
<div id="ref-essd-5-3-2013">
<p>Hugelius, G., C. Tarnocai, G. Broll, J. G. Canadell, P. Kuhry, and D. K. Swanson. 2013. “The Northern Circumpolar Soil Carbon Database: Spatially Distributed Datasets of Soil Coverage and Soil Carbon Storage in the Northern Permafrost Regions.” <em>Earth System Science Data</em> 5 (1): 3–13. doi:<a href="https://doi.org/10.5194/essd-5-3-2013">10.5194/essd-5-3-2013</a>.</p>
</div>
<div id="ref-INEGI2000">
<p>Instituto Nacional de Estadística y Geografía (INEGI). 2000. <em>Conjunto de Datos de Perfiles de Suelos, Escala 1: 250 000 Serie Ii</em>. (Continuo Nacional). Aguascalientes, Ags. México: INEGI.</p>
</div>
<div id="ref-FAO2006WRB">
<p>IUSS Working Group WRB. 2006. <em>World reference base for soil resources 2006: a framework for international classification, correlation and communication</em>. World Soil Resources Reports No. 103. Rome: Food; Agriculture Organization of the United Nations.</p>
</div>
<div id="ref-Jagtap2004TASAE">
<p>Jagtap, S.S., U. Lal, J.W. Jones, A.J. Gijsman, and J.T. Ritchie. 2004. “A dynamic nearest-neighbor method for estimating soil water parameters.” <em>Trans. ASAE</em> 47: 1437–44.</p>
</div>
<div id="ref-Jankauskas2006CSSPA">
<p>Jankauskas, B., G. Jankauskiene, A. Slepetiene, M.A. Fullen, and C.A. Booth. 2006. “International Comparison of Analytical Methods of Determining the Soil Organic Matter Content of Lithuanian Eutric Albeluvisols.” <em>Communications in Soil Science and Plant Analysis</em> 37: 707–20.</p>
</div>
<div id="ref-Jeffrey1970JE">
<p>Jeffrey, D. W. 1970. “A note on the use of ignition loss as a means for the approximate estimation of soil bulk density.” <em>Journal of Ecology</em> 58: 297–99.</p>
</div>
<div id="ref-jenny1994factors">
<p>Jenny, H. 1994. <em>Factors of Soil Formation: A System of Quantitative Pedology</em>. Dover Books on Earth Sciences. Dover Publications.</p>
</div>
<div id="ref-Jenny1968">
<p>Jenny, H., A. E. Salem, and J. R. Wallis. 1968. “Organic Matter and Soil Fertility.” In, edited by P. Salviucci, 32:5–36. Pontif. Acad. Sci. Scripta Varia. New York: North Holland Publishing Co., Amsterdam, &amp; Wiley Interscience Division, John Wiley &amp; Sons, Inc.</p>
</div>
<div id="ref-Jolivet1998CSSPA">
<p>Jolivet, C., D. Arrouays, and M. Bernoux. 1998. “Comparison between analytical methods for organic carbon and organic matter determination in sandy Spodosols of France.” <em>Communications in Soil Science and Plant Analysis</em> 29: 2227–33.</p>
</div>
<div id="ref-jost2005analysing">
<p>Jost, G., G. B. M. Heuvelink, and A. Papritz. 2005. “Analysing the Space–Time Distribution of Soil Water Storage of a Forest Ecosystem Using Spatio-Temporal Kriging.” <em>Geoderma</em> 128 (3). Elsevier: 258–73.</p>
</div>
<div id="ref-Kalembasa1973JSFA">
<p>Kalembasa, S.J., and D.S. Jenkinson. 1973. “A Comparative Study of Titrimetric and Gravimetric Methods for the Determination of Organic Carbon in Soil.” <em>Journal of the Science of Food and Agriculture</em> 24: 1085–90.</p>
</div>
<div id="ref-kanevski2009machine">
<p>Kanevski, M., V. Timonin, and A. Pozdnukhov. 2009. <em>Machine Learning for Spatial Environmental Data: Theory, Applications, and Software</em>. Environmental Sciences. EFPL Press.</p>
</div>
<div id="ref-Karssies2011CSIRO">
<p>Karssies, Linda. 2011. <em>CSIRO National Soil Archive and the National Soil Database (Natsoil)</em>. Data Collection v1. Canberra: CSIRO.</p>
</div>
<div id="ref-Kempen2011PhDthesis">
<p>Kempen, B. 2011. “Updating Soil Information with Digital Soil Mapping.” PhD thesis, Wageningen: Wageningen University. <a href="http://edepot.wur.nl/187198" class="uri">http://edepot.wur.nl/187198</a>.</p>
</div>
<div id="ref-Kempen2011Geoderma">
<p>Kempen, B., D.J. Brus, and J.J. Stoorvogel. 2011. “Three-Dimensional Mapping of Soil Organic Matter Content Using Soil Type-Specific Depth Functions.” <em>Geoderma</em> 162 (1-2): 107–23. doi:<a href="https://doi.org/10.1016/j.geoderma.2011.01.010">10.1016/j.geoderma.2011.01.010</a>.</p>
</div>
<div id="ref-Kempen2009Geoderma">
<p>Kempen, B., D.J. Brus, G.B.M. Heuvelink, and J.J. Stoorvogel. 2009. “Updating the 1:50,000 Dutch soil map using legacy soil data: A multinomial logistic regression approach.” <em>Geoderma</em> 151 (3-4): 311–26. doi:<a href="https://doi.org/10.1016/j.geoderma.2009.04.023">10.1016/j.geoderma.2009.04.023</a>.</p>
</div>
<div id="ref-klein2011hyde">
<p>Klein Goldewijk, Kees, Arthur Beusen, Gerard Van Drecht, and Martine De Vos. 2011. “The Hyde 3.1 Spatially Explicit Database of Human-Induced Global Land-Use Change over the Past 12,000 Years.” <em>Global Ecology and Biogeography</em> 20 (1). Wiley Online Library: 73–86.</p>
</div>
<div id="ref-kochy2015global">
<p>Köchy, M, R Hiederer, and A Freibauer. 2015. “Global Distribution of Soil Organic Carbon–Part 1: Masses and Frequency Distributions of Soc Stocks for the Tropics, Permafrost Regions, Wetlands, and the World.” <em>Soil</em> 1 (1). Copernicus GmbH: 351–65.</p>
</div>
<div id="ref-kondolf2003tools">
<p>Kondolf, G.M., and H. Piégay. 2003. <em>Tools in Fluvial Geomorphology</em>. Wiley.</p>
</div>
<div id="ref-Konen2002SSSAJ">
<p>Konen, M.E., P.M. Jacobs, C.L. Burras, B.J. Talaga, and J.A. Mason. 2002. “Equations for Predicting Soil Organic Carbon Using Loss-on-Ignitionfor North Central U.S. Soils.” <em>Soil Science Society of America Journal</em> 66: 1878–81.</p>
</div>
<div id="ref-Krasilnikov2009handbook">
<p>Krasilnikov, P., J.J.I. Marti, R. Arnold, and S. Shoba, eds. 2009. <em>A Handbook of Soil Terminology, Correlation and Classification</em>. Earthscan LLC.</p>
</div>
<div id="ref-kuhn2013applied">
<p>Kuhn, Max, and Kjell Johnson. 2013. <em>Applied Predictive Modeling</em>. Vol. 810. Springer.</p>
</div>
<div id="ref-kuhn2012cubist">
<p>Kuhn, Max, Steve Weston, Chris Keefer, and Nathan Coulter. 2012. <em>Cubist Models for Regression</em>. <a href="https://cran.r-project.org/package=cubist" class="uri">https://cran.r-project.org/package=cubist</a>.</p>
</div>
<div id="ref-kuhn2014cubist">
<p>Kuhn, Max, Steve Weston, Chris Keefer, Nathan Coulter, and R Quinlan. 2014. <em>Cubist: Rule-and Instance-Based Regression Modeling</em>. <a href="http://CRAN.R-project.org/package=Cubist" class="uri">http://CRAN.R-project.org/package=Cubist</a>.</p>
</div>
<div id="ref-Kutner2004McGraw">
<p>Kutner, M. H., C. J. Nachtsheim, J. Neter, and W. Li, eds. 2004. <em>Applied Linear Statistical Models</em>. 5th ed. McGraw-Hill.</p>
</div>
<div id="ref-kutner2005applied">
<p>Kutner, M.H., J. Neter, C.J. Nachtsheim, and W. Li. 2005. <em>Applied Linear Statistical Models</em>. Operations and Decision Sciences Series. McGraw-Hill Irwin.</p>
</div>
<div id="ref-Kyriakidis2004GEAN1135">
<p>Kyriakidis, Phaedon C. 2004. “A Geostatistical Framework for Area-to-Point Spatial Interpolation.” <em>Geographical Analysis</em> 36 (3). Blackwell Publishing Ltd: 259–89. doi:<a href="https://doi.org/10.1111/j.1538-4632.2004.tb01135.x">10.1111/j.1538-4632.2004.tb01135.x</a>.</p>
</div>
<div id="ref-Lagacherie1992PhD">
<p>Lagacherie, P. 1992. “Formalisation Des Lois de Distribution Des Sols Pour Automatiser La Cartographie Pédologique à Partir d’un Secteur Pris Comme Référence. Cas de La Petite Région ‘Moyenne Vallée de L’Hérault’.” PhD thesis, Montpellier: Université des Sciences et Techniques du Languedoc.</p>
</div>
<div id="ref-Lagacherie1995Geoderma">
<p>———. 1995. “A Soil Survey Procedure Using the Knowledge of Soil Pattern Established on a Previously Mapped Reference Area.” <em>Geoderma</em> 65 (03/1995): 283–301. doi:<a href="https://doi.org/10.1016/0016-7061(94)00040-H">10.1016/0016-7061(94)00040-H</a>.</p>
</div>
<div id="ref-Lagacherie2001Geoderm">
<p>———. 2001. “Mapping of Reference Area Representativity Using a Mathematical Soilscape Distance.” <em>Geoderma</em> 101: 105–18. doi:<a href="https://doi.org/10.1016/S0016-7061(00)00101-4">10.1016/S0016-7061(00)00101-4</a>.</p>
</div>
<div id="ref-Lagacherie2006Elsevier">
<p>Lagacherie, P., A. B. McBratney, and M. Voltz, eds. 2006. <em>Digital Soil Mapping: An Introductory Perspective</em>. Developments in Soil Science. Amsterdam: Elsevier.</p>
</div>
<div id="ref-Lal2004Science">
<p>Lal, R. 2004. “Soil Carbon Sequestration Impacts on Global Climate Change and Food Security.” <em>Science</em> 304 (5677): 1623–7. doi:<a href="https://doi.org/10.1126/science.1097396">10.1126/science.1097396</a>.</p>
</div>
<div id="ref-lark2003fitting">
<p>Lark, R. M., and A. Papritz. 2003. “Fitting a Linear Model of Coregionalization for Soil Properties Using Simulated Annealing.” <em>Geoderma</em> 115 (3). Elsevier: 245–60.</p>
</div>
<div id="ref-lark2006spatial">
<p>Lark, RM, BR Cullis, and SJ Welham. 2006. “On spatial prediction of soil properties in the presence of a spatial trend: the empirical best linear unbiased predictor (E-BLUP) with REML.” <em>European Journal of Soil Science</em> 57 (6). Wiley Online Library: 787–99.</p>
</div>
<div id="ref-LathropJr19951">
<p>Lathrop Jr., Richard G., John D. Aber, and John A. Bognar. 1995. “Spatial Variability of Digital Soil Maps and Its Impact on Regional Ecosystem Modeling.” <em>Ecological Modelling</em> 82 (1): 1–10. doi:<a href="https://doi.org/10.1016/0304-3800(94)00068-S">10.1016/0304-3800(94)00068-S</a>.</p>
</div>
<div id="ref-ledell2015scalable">
<p>LeDell, Erin E. 2015. <em>Scalable Ensemble Learning and Computationally Efficient Variance Estimation</em>. University of California, Berkeley.</p>
</div>
<div id="ref-Leenaars2012">
<p>Leenaars, Johan G.B. 2014. <em>Africa Soil Profiles Database, Version 1.2. a Compilation of Geo-Referenced and Standardized Legacy Soil Profile Data for Sub Saharan Africa (with Dataset)</em>. Wageningen, the Netherlands: Africa Soil Information Service (AfSIS) project; ISRIC — World Soil Information.</p>
</div>
<div id="ref-Legros2006SP">
<p>Legros, J.-P. 2006. <em>Mapping of the Soil</em>. 1st ed. Enfield, New Hampshire: Science Publishers.</p>
</div>
<div id="ref-Lei1998">
<p>Lei, Simon A. 1998. “Soil Properties of the Kelso Sand Dunes in the Mojave Desert.” <em>The Southwestern Naturalist</em> 43 (1). Southwestern Association of Naturalists: 47–52.</p>
</div>
<div id="ref-LiHeap2010EI">
<p>Li, Jin, and Andrew D. Heap. 2010. “A Review of Comparative Studies of Spatial Interpolation Methods in Environmental Sciences: Performance and Impact Factors.” <em>Ecological Informatics</em> 6 (3, 3-4). Elsevier: 228–41. doi:<a href="https://doi.org/10.1016/j.ecoinf.2010.12.003">10.1016/j.ecoinf.2010.12.003</a>.</p>
</div>
<div id="ref-lindsay2016whitebox">
<p>Lindsay, John B. 2016. “Whitebox GAT: A case study in geomorphometric analysis.” <em>Computers &amp; Geosciences</em> 95. Elsevier: 75–84.</p>
</div>
<div id="ref-Liu2009">
<p>Liu, Jian, and A-Xing Zhu. 2009. “Mapping with Words: A New Approach to Automated Digital Soil Survey.” <em>International Journal of Intelligent Systems</em> 24 (3). Wiley Subscription Services, Inc., A Wiley Company: 293–311. doi:<a href="https://doi.org/10.1002/int.20337">10.1002/int.20337</a>.</p>
</div>
<div id="ref-lobsey2016sensing">
<p>Lobsey, CR, and RA Viscarra Rossel. 2016. “Sensing of Soil Bulk Density for More Accurate Carbon Accounting.” <em>European Journal of Soil Science</em> 67 (4). Wiley Online Library: 504–13.</p>
</div>
<div id="ref-lovett2007needs">
<p>Lovett, Gary M, Douglas A Burns, Charles T Driscoll, Jennifer C Jenkins, Myron J Mitchell, Lindsey Rustad, James B Shanley, Gene E Likens, and Richard Haeuber. 2007. “Who Needs Environmental Monitoring?” <em>Frontiers in Ecology and the Environment</em> 5 (5). Wiley Online Library: 253–60.</p>
</div>
<div id="ref-macdonald1992cansis">
<p>MacDonald, K. B., and K. W. G. Valentine. 1992. <em>CanSIS/Nsdb. a General Description</em>. Ottawa: Centre for Land; Biological Resources Research, Research Branch, Agriculture Canada.</p>
</div>
<div id="ref-MacMillan2010DSM">
<p>MacMillan, R. A., D.E. Moon, R. A. Coupé, and N. Phillips. 2010. “Predictive Ecosystem Mapping (Pem) for 8.2 Million Ha of Forestland,British Columbia, Canada.” In <em>Digital Soil Mapping: Bridging Research, Environmental Application, and Operation</em>, edited by J. L. Boettinger et al., 2:335–54. Progress in Soil Science. Springer. doi:<a href="https://doi.org/10.1007/978-90-481-8863-5_27">10.1007/978-90-481-8863-5_27</a>.</p>
</div>
<div id="ref-MacMillan2005CJSS">
<p>MacMillan, R. A., W. W. Pettapiece, and J. A. Brierley. 2005. “An Expert System for Allocating Soils to Landforms Through the Application of Soil Survey Tacit Knowledge.” <em>Canadian Journal of Soil Science</em>, 7/2005, 103–12.</p>
</div>
<div id="ref-Mallavan2010PSS">
<p>Mallavan, B.P., B. Minasny, and A. B. McBratney. 2010. “Homosoil, a Methodology for Quantitative Extrapolation of Soil Information Across the Globe.” In <em>Digital Soil Mapping</em>, edited by J. L. Boettinger, 2:137–49. Progress in Soil Science. Elsevier; Elsevier. doi:<a href="https://doi.org/10.1007/978-90-481-8863-5_12">10.1007/978-90-481-8863-5_12</a>.</p>
</div>
<div id="ref-Malone2009Geoderma">
<p>Malone, B.P., A. B. McBratney, B. Minasny, and G.M. Laslett. 2009. “Mapping Continuous Depth Functions of Soil Carbon Storage and Available Water Capacity.” <em>Geoderma</em> 154 (1-2): 138–52. doi:<a href="https://doi.org/10.1016/j.geoderma.2009.10.007">10.1016/j.geoderma.2009.10.007</a>.</p>
</div>
<div id="ref-malone2016using">
<p>Malone, B.P., B. Minasny, and A.B. McBratney. 2016. <em>Using R for Digital Soil Mapping</em>. Progress in Soil Science. Springer International Publishing.</p>
</div>
<div id="ref-Manrique1991SSSAJ">
<p>Manrique, L.A., and C.A. Jones. 1991. “Bulk-density of soils in relation to soil physical and chemical properties.” <em>Soil Science Society of America Journal</em> 55: 476–81.</p>
</div>
<div id="ref-Mansuy201459">
<p>Mansuy, Nicolas, Evelyne Thiffault, David Pare, Pierre Bernier, Luc Guindon, Philippe Villemaire, Vincent Poirier, and Andre Beaudoin. 2014. “Digital Mapping of Soil Properties in Canadian Managed Forests at 250 M of Resolution Using the K-Nearest Neighbor Method.” <em>Geoderma</em> 235-236 (0): 59–73. doi:<a href="https://doi.org/10.1016/j.geoderma.2014.06.032">10.1016/j.geoderma.2014.06.032</a>.</p>
</div>
<div id="ref-Markus2001399">
<p>Markus, Julie, and Alex B. McBratney. 2001. “A Review of the Contamination of Soil with Lead: II. Spatial Distribution and Risk Assessment of Soil Lead.” <em>Environment International</em> 27 (5): 399–411. doi:<a href="https://doi.org/10.1016/S0160-4120(01)00049-6">10.1016/S0160-4120(01)00049-6</a>.</p>
</div>
<div id="ref-Marsman1986ALTERRA">
<p>Marsman, B.A., and J.J. de Gruijter. 1986. <em>Quality of Soil Maps: A Comparison of Soil Survey Methods in a Sandyarea</em>. Vol. 15. Soil Survey Papers. Wageningen: Netherlands Soil Survey Institute.</p>
</div>
<div id="ref-Matheron1969PhD">
<p>Matheron, G. 1969. <em>Le krigeage universel</em>. Vol. 1. Fontainebleau: Cahiers du Centre de Morphologie Mathématique, École des Mines de Paris.</p>
</div>
<div id="ref-McBratney2003Geoderma">
<p>McBratney, A. B., M. L. Mendoça Santos, and B. Minasny. 2003. “On digital soil mapping.” <em>Geoderma</em> 117 (1-2): 3–52.</p>
</div>
<div id="ref-McBratney2002Geoderma">
<p>McBratney, A. B., B. Minasny, S.R. Cattle, and R.W. Vervoort. 2002. “From Pedotransfer Functions to Soil Inference Systems.” <em>Geoderma</em> 109: 41–73.</p>
</div>
<div id="ref-McBratney2011HSS">
<p>McBratney, A. B., B. Minasny, R. A. MacMillan, and F. Carré. 2011. “Digital Soil Mapping.” In <em>Handbook of Soil Science</em>, edited by H. Li and M.E. Sumner, 37:1–45. CRC Press.</p>
</div>
<div id="ref-MCBRATNEY20033">
<p>McBratney, A.B, M.L Mendonça Santos, and B Minasny. 2003. “On Digital Soil Mapping.” <em>Geoderma</em> 117 (1): 3–52. doi:<a href="https://doi.org/10.1016/S0016-7061(03)00223-4">10.1016/S0016-7061(03)00223-4</a>.</p>
</div>
<div id="ref-mcbratney2018pedometrics">
<p>McBratney, A.B., B. Minasny, and U. Stockmann. 2018. <em>Pedometrics</em>. Progress in Soil Science. Springer International Publishing.</p>
</div>
<div id="ref-McBratney2006WCSS">
<p>McBratney, Alex, Nathan Odgers, and Budiman Minasny. 2006. “Random Catena Sampling for Establishing Soil-Landscape Rules forDigital Soil Mapping.” In <em>18th World Congress of Soil Science</em>, 4. Philadelphia, Pennsylvania: IUSS.</p>
</div>
<div id="ref-Meersmans2009SUM">
<p>Meersmans, J., B. Van Wesemael, and M. Van Molle. 2009. “Determining soil organic carbon for a agricultural soils: A comparison between the Walkley &amp; Black and the dry combustion methods (north Belgium).” <em>Soil Use and Management</em> 25: 346–53.</p>
</div>
<div id="ref-meinshausen2006quantile">
<p>Meinshausen, Nicolai. 2006. “Quantile Regression Forests.” <em>The Journal of Machine Learning Research</em> 7 (Jun). JMLR.org: 983–99.</p>
</div>
<div id="ref-michailidis2017investigating">
<p>Michailidis, Marios. 2017. “Investigating Machine Learning Methods in Recommender Systems.” PhD thesis, UCL (University College London).</p>
</div>
<div id="ref-Mikhailova2003CSSPA">
<p>Mikhailova, E.A., R.R.P. Noble, and C.J. Post. 2003. “Comparison of Soil Organic Carbon Recovery by Walkley-Black andDry Combustion Methods in the Russian Chernozem.” <em>Communications in Soil Science and Plant Analysis</em> 34: 1853–60.</p>
</div>
<div id="ref-Miller1979">
<p>Miller, F. P., D. E. McCormack, and J. R. Talbot. 1979. “The Mechanics of Track Support, Piles and Geotechnical Data.” In, 57–65. Symposium, Trans. Res. Record 733. Washington D.C.: Board. Natl. Acad. Sci.</p>
</div>
<div id="ref-miller2004tobler">
<p>Miller, Harvey J. 2004. “Tobler’s First Law and Spatial Analysis.” <em>Annals of the Association of American Geographers</em> 94 (2). Wiley Online Library: 284–89.</p>
</div>
<div id="ref-Miller2010SSSAJ">
<p>Miller, R.O., and D.E. Kissel. 2010. “Comparison of Soil pH Methods on Soils of North America.” <em>Soil Science Society of America Journal</em> 74: 310–16.</p>
</div>
<div id="ref-Minasny2007JITL">
<p>Minasny, B. 2007. “Predicting soil properties.” <em>Jurnal Ilmu Tanah Dan Lingkungan</em> 7 (1): 54–67.</p>
</div>
<div id="ref-Minasny2001AJSR">
<p>Minasny, B., and A. B. McBratney. 2001. “The Australian soil texture boomerang: A comparison of the Australian and USDA/FAO soil particle-size classification systems.” <em>Australian Journal of Soil Research</em> 39: 1443–51.</p>
</div>
<div id="ref-Minasny2007Geoderma">
<p>———. 2007. “Spatial prediction of soil properties using EBLUP with Matérn covariance function.” <em>Geoderma</em> 140: 324–36.</p>
</div>
<div id="ref-Minasny2008140">
<p>Minasny, Budiman, Alex B. McBratney, and S. Salvador-Blanes. 2008. “Quantitative Models for Pedogenesis — a Review.” <em>Geoderma</em> 144 (1?2): 140–57. doi:<a href="https://doi.org/10.1016/j.geoderma.2007.12.013">10.1016/j.geoderma.2007.12.013</a>.</p>
</div>
<div id="ref-mira2015modis">
<p>Mira, Maria, Marie Weiss, Frédéric Baret, Dominique Courault, Olivier Hagolle, Belen Gallego-Elvira, and Albert Olioso. 2015. “The Modis (Collection V006) Brdf/Albedo Product Mcd43d: Temporal Course Evaluated over Agricultural Landscape.” <em>Remote Sensing of Environment</em> 170. Elsevier: 216–28.</p>
</div>
<div id="ref-mitchell2014geospatial">
<p>Mitchell, T., and GDAL Developers. 2014. <em>Geospatial Power Tools: GDAL Raster &amp; Vector Commands</em>. Locate Press.</p>
</div>
<div id="ref-moran2002spatial">
<p>Moran, C.J., and E.N. Bui. 2002. “Spatial Data Mining for Enhanced Soil Map Modelling.” <em>International Journal of Geographical Information Science</em> 16 (6). Taylor &amp; Francis: 533–49.</p>
</div>
<div id="ref-Morel2005202">
<p>Morel, J. L., C. Schwartz, L. Florentin, and C. de Kimpe. 2005. “Urban Soils.” In <em>Encyclopedia of Soils in the Environment</em>, edited by Daniel Hillel, 202–8. Oxford: Elsevier. doi:<a href="https://doi.org/10.1016/B0-12-348530-4/00305-2">10.1016/B0-12-348530-4/00305-2</a>.</p>
</div>
<div id="ref-mowrer2000quantifying">
<p>Mowrer, H. T., and R.G. Congalton. 2000. <em>Quantifying spatial uncertainty in natural resources: theory and applications for GIS and remote sensing</em>. Ann Arbor Press.</p>
</div>
<div id="ref-Nachtergaele2010press">
<p>Nachtergaele, F. O., V.W.P. Van Engelen, and N. H. Batjes. 2010. “Qualitative and quantitative aspects of world and regional soildatabases and maps.” In <em>Handbook of Soil Science</em>, edited by Yuncong Li and M. Sumner, 2nd ed., in press. CRC Press, Taylor; Francis Group.</p>
</div>
<div id="ref-Burt2004SSIR">
<p>Natural Resources Conservation Service. 2004. <em>Soil Survey Laboratory Methods Manual Version 4.0.</em> Edited by Rebecca Burt. Soil Survey Investigations Report No. 42. United States Department of Agriculture.</p>
</div>
<div id="ref-Nelson1982">
<p>Nelson, D. W., and L.E. Sommers. 1982. “Total Carbon, Organic Carbon, and Organic Matter.” In <em>Methods of Soil Analysis, Part 2</em>, edited by A.L. Page, R.H. Miller, and D.R. Keeney, 2nd ed., 539–79. Agron. Monogr. 9. Madison, WI: ASA; SSSA.</p>
</div>
<div id="ref-Nemes1999">
<p>Nemes, A., M.G. Schaap, and F.J. Leij. 1999. <em>The UNSODA unsaturated soil hydraulic database Version 2.0.</em> Riverside, CA: US Salinity Laboratory.</p>
</div>
<div id="ref-Nemes2003SSSAJ">
<p>Nemes, A., M.G. Schaap, and J.H.M. Wösten. 2003. “Functional evaluation of pedotransfer functions derived from different scales of data collection.” <em>Soil Science Society of America Journal</em> 67: 1093–1102.</p>
</div>
<div id="ref-Nemes1999G">
<p>Nemes, A., J.H.M. Wösten, A. Lilly, and J.H. Oude Voshaar. 1999. “Evaluation of different procedures to interpolate particle-size distributions to achieve compatibility within soil databases.” <em>Geoderma</em> 90: 187–202.</p>
</div>
<div id="ref-Ng2018">
<p>Ng, Wartini, Budiman Minasny, Brendan Malone, and Patrick Filippi. 2018. “In Search of an Optimum Sampling Algorithm for Prediction of Soil Properties from Infrared Spectra.” <em>PeerJ</em> 6 (October): e5722. doi:<a href="https://doi.org/10.7717/peerj.5722">10.7717/peerj.5722</a>.</p>
</div>
<div id="ref-nussbaum2018evaluation">
<p>Nussbaum, Madlene, Kay Spiess, Andri Baltensweiler, Urs Grob, Armin Keller, Lucie Greiner, Michael E Schaepman, and Andreas Papritz. 2018. “Evaluation of Digital Soil Mapping Approaches with Large Sets of Environmental Covariates.” <em>Soil</em> 4 (1): 1.</p>
</div>
<div id="ref-OGeen2017soilweb">
<p>O’Geen, Anthony, Mike Walkinshaw, and Dylan Beaudette. 2017. “SoilWeb: A Multifaceted Interface to Soil Survey Information.” <em>Soil Science Society of America Journal</em> 81 (4). The Soil Science Society of America, Inc.: 853–62.</p>
</div>
<div id="ref-Odgers201130">
<p>Odgers, Nathan P., Alex B. McBratney, and Budiman Minasny. 2011. “Bottom-up Digital Soil Mapping. Ii. Soil Series Classes.” <em>Geoderma</em> 163 (1-2): 30–37. doi:<a href="https://doi.org/10.1016/j.geoderma.2011.03.013">10.1016/j.geoderma.2011.03.013</a>.</p>
</div>
<div id="ref-Omuto2012GSP">
<p>Omuto, C., F. Nachtergaele, and R. Vargas Rojas. 2012. <em>State of the Art Report on Global and Regional Soil Information: Where are we? Where to go?</em> Global Soil Partnership Technical Report. Rome: FAO.</p>
</div>
<div id="ref-Oosterveld1980CAE">
<p>Oosterveld, M., and C. Chang. 1980. “Empirical relationship between laboratory determinations of soil texture and moisture retention.” <em>Can. Agric. Eng.</em> 22: 149–51.</p>
</div>
<div id="ref-Oreskes04021994">
<p>Oreskes, Naomi, Kristin Shrader-Frechette, and Kenneth Belitz. 1994. “Verification, Validation, and Confirmation of Numerical Models in the Earth Sciences.” <em>Science</em> 263 (5147): 641–46. doi:<a href="https://doi.org/10.1126/science.263.5147.641">10.1126/science.263.5147.641</a>.</p>
</div>
<div id="ref-Panagos2013439">
<p>Panagos, Panos, Roland Hiederer, Marc Van Liedekerke, and Francesca Bampa. 2013. “Estimating soil organic carbon in Europe based on data collected through an European network.” <em>Ecological Indicators</em> 24 (0): 439–50. doi:<a href="https://doi.org/10.1016/j.ecolind.2012.07.020">10.1016/j.ecolind.2012.07.020</a>.</p>
</div>
<div id="ref-pansu2001soil">
<p>Pansu, M., J. Gautheyrou, and J.Y. Loyer. 2001. <em>Soil Analysis: Sampling, Instrumentation and Quality Control</em>. A.A. Balkema Publishers.</p>
</div>
<div id="ref-Pebesma2006TiG">
<p>Pebesma, E.J. 2006. “The Role of External Variables and GIS Databases in Geostatistical Analysis.” <em>Transactions in GIS</em> 10 (4): 615–32.</p>
</div>
<div id="ref-pebesma2005classes">
<p>Pebesma, Edzer J, and Roger S Bivand. 2005. “Classes and Methods for Spatial Data in R.” <em>R News</em> 5 (2): 9–13.</p>
</div>
<div id="ref-Pebesma2004CG">
<p>Pebesma, Edzer J. 2004. “Multivariable geostatistics in S: the gstat package.” <em>Computers &amp; Geosciences</em> 30 (7). Elsevier: 683–91.</p>
</div>
<div id="ref-Pebesma2011CompGeoSci">
<p>Pebesma, Edzer, Dan Cornford, Gregoire Dubois, Gerard B.M. Heuvelink, Dionisis Hristopulos, Jürgen Pilz, Ulrich Stöhlker, Gary Morin, and Jon O. Skøien. 2011. “INTAMAP: The design and implementation of an interoperable automated interpolation web service.” <em>Computers &amp; Geosciences</em> 37 (3). Elsevier: 343–52. doi:<a href="https://doi.org/10.1016/j.cageo.2010.03.019">10.1016/j.cageo.2010.03.019</a>.</p>
</div>
<div id="ref-pekel2016high">
<p>Pekel, Jean-François, Andrew Cottam, Noel Gorelick, and Alan S Belward. 2016. “High-Resolution Mapping of Global Surface Water and Its Long-Term Changes.” <em>Nature</em> 504. Nature Research: 418–22.</p>
</div>
<div id="ref-Pelletier2016">
<p>Pelletier, J. D., P. D. Broxton, P. Hazenberg, X. Zeng, P. A. Troch, G.-Y. Niu, Z. Williams, M. A. Brunke, and D. Gochis. 2016. “A Gridded Global Data Set of Soil, Immobile Regolith, and Sedimentary Deposit Thicknesses for Regional and Global Land Surface Modeling.” <em>Journal of Advances in Modeling Earth Systems</em> 8. doi:<a href="https://doi.org/10.1002/2015MS000526">10.1002/2015MS000526</a>.</p>
</div>
<div id="ref-Smith2010CUP">
<p>Pete Smith, Pete Falloon, and Werner L. Kutsch. 2010. “The role of soils in the Kyoto Protocol.” In <em>Soil Carbon Dynamics</em>, edited by M. Bahn, 245–56. Cambridge University Press. doi:<a href="https://doi.org/10.1017/CBO9780511711794.014">10.1017/CBO9780511711794.014</a>.</p>
</div>
<div id="ref-peverill1999soil">
<p>Peverill, K.I., L.A. Sparrow, and D.J. Reuter. 1999. <em>Soil analysis: an interpretation manual</em>. CSIRO Publishing.</p>
</div>
<div id="ref-Pimentel2006Springer">
<p>Pimentel, David. 2006. “Soil Erosion: A Food and Environmental Threat.” <em>Environment, Development and Sustainability</em> 8 (1). Kluwer Academic Publishers: 119–37. doi:<a href="https://doi.org/10.1007/s10668-005-1262-8">10.1007/s10668-005-1262-8</a>.</p>
</div>
<div id="ref-pinheiro2009mixed">
<p>Pinheiro, J.C., and D. Bates. 2009. <em>Mixed-Effects Models in S and S-PLUS</em>. Statistics and Computing. Springer.</p>
</div>
<div id="ref-vanReeuwijk1984ISRIC">
<p>Pleijsier, L.K. 1984. <em>Laboratory Methods and Data Quality. Program for Soil Characterization: A Report on the Pilot Round. Part Ii. Exchangeable Bases, Base Saturation and pH</em>. Wageningen: International Soil Reference; Information Centre.</p>
</div>
<div id="ref-Pleijsier1986ISRIC">
<p>———. 1986. <em>The Laboratory Methods and Data Exchange Programme</em>. Interim Report on the Exchange Round 85-2. Wageningen: International Soil Reference; Information Centre.</p>
</div>
<div id="ref-poeplau2017soil">
<p>Poeplau, Christopher, Cora Vos, and Don Axel. 2017. “Soil Organic Carbon Stocks Are Systematically Overestimated by Misuse of the Parameters Bulk Density and Rock Fragment Content.” <em>Soil</em> 3 (1). Copernicus GmbH: 61.</p>
</div>
<div id="ref-poggio2014national">
<p>Poggio, Laura, and Alessandro Gimona. 2014. “National scale 3D modelling of soil organic carbon stocks with uncertainty propagation — An example from Scotland.” <em>Geoderma</em> 232. Elsevier: 284–99.</p>
</div>
<div id="ref-polley2010super">
<p>Polley, Eric C, and Mark J Van Der Laan. 2010. <em>Super Learner in Prediction</em>. Working Paper Series. U.C. Berkeley Division of Biostatistics.</p>
</div>
<div id="ref-Qi2006Geoderma">
<p>Qi, Feng, A-Xing Zhu, Mark Harrower, and James E. Burt. 2006. “Fuzzy Soil Mapping Based on Prototype Category Theory.” <em>Geoderma</em> 136 (3-4): 774–87. doi:<a href="https://doi.org/10.1016/j.geoderma.2006.06.001">10.1016/j.geoderma.2006.06.001</a>.</p>
</div>
<div id="ref-ramcharan2018soil">
<p>Ramcharan, Amanda, Tomislav Hengl, Travis Nauman, Colby Brungard, Sharon Waltman, Skye Wills, and James Thompson. 2018a. “Soil Property and Class Maps of the Conterminous United States at 100-Meter Spatial Resolution.” <em>Soil Science Society of America Journal</em> 82. The Soil Science Society of America, Inc.: 186–201.</p>
</div>
<div id="ref-ramcharan2017soil">
<p>———. 2018b. “Soil Property and Class Maps of the Conterminous Us at 100 Meter Spatial Resolution Based on a Compilation of National Soil Point Observations and Machine Learning.” <em>Soil Science Society of America Journal</em> 82: 186–201. doi:<a href="https://doi.org/10.2136/sssaj2017.04.0122">10.2136/sssaj2017.04.0122</a>.</p>
</div>
<div id="ref-raup2007glims">
<p>Raup, Bruce, Adina Racoviteanu, Siri Jodha Singh Khalsa, Christopher Helm, Richard Armstrong, and Yves Arnaud. 2007. “The GLIMS geospatial glacier database: a new tool for studying glacier change.” <em>Global and Planetary Change</em> 56 (1). Elsevier: 101–10.</p>
</div>
<div id="ref-Rawls1983SS">
<p>Rawls, W.J. 1983. “Estimating soil bulk-density from particle-size analysis and organic matter content.” <em>Soil Science</em> 135: 123–25.</p>
</div>
<div id="ref-Rawls1982JIDDASCE">
<p>Rawls, W.J., and D.L. Brakensiek. 1982. “Estimating soil water retention from soil properties.” <em>J. Irrig. Drainage Div. Am. Soc. Civ. Eng.</em> 108: 166–71.</p>
</div>
<div id="ref-Rawls1989">
<p>———. 1989. “Estimation of soil water retention and hydraulic properties.” In <em>Unsaturated flow in hydrologic modeling; theory and practice</em>, edited by H.J. Morel-Seytoux, 275–300. Proc. Nato Adv. Res. Worksh. Hydrology. Dordrecht, the Netherlands: Kluwer Acad. Publ.</p>
</div>
<div id="ref-Rawls1991AA">
<p>Rawls, W.J., T.J. Gish, and D.L. Brakensiek. 1991. “Estimating soil water retention from soil physical properties and characteristics.” <em>Advances in Agronomy</em> 16: 213–34.</p>
</div>
<div id="ref-Refsgaard2007UEM">
<p>Refsgaard, Jens Christian, Jeroen P. van der Sluijs, Anker Lajer Højberg, and Peter A. Vanrolleghem. 2007. “Uncertainty in the environmental modelling process — A framework and guidance.” <em>Environ. Model. Softw.</em> 22 (11): 1543–56. doi:<a href="https://doi.org/10.1016/j.envsoft.2007.02.004">10.1016/j.envsoft.2007.02.004</a>.</p>
</div>
<div id="ref-reimann2011statistical">
<p>Reimann, C., P. Filzmoser, R. Garrett, and R. Dutter. 2011. <em>Statistical Data Analysis Explained: Applied Environmental Statistics with R</em>. Wiley.</p>
</div>
<div id="ref-richter2015multi">
<p>Richter, Aaron N, Taghi M Khoshgoftaar, Sara Landset, and Tawfiq Hasanin. 2015. “A Multi-Dimensional Comparison of Toolkits for Machine Learning with Big Data.” In <em>Information Reuse and Integration (Iri), 2015 Ieee International Conference</em>, 1–8. IEEE.</p>
</div>
<div id="ref-Richter1995">
<p>Richter, Daniel D., and Daniel Markewitz. 1995. “How Deep Is Soil?” <em>BioScience</em> 45 (9). University of California Press on behalf of the American Institute of Biological Sciences: 600–609. <a href="http://www.jstor.org/stable/1312764" class="uri">http://www.jstor.org/stable/1312764</a>.</p>
</div>
<div id="ref-ridgeway2010gbm">
<p>Ridgeway, G. 2018. <em>Gbm: Generalized Boosted Regression Models</em>. <a href="http://CRAN.R-project.org/package=gbm" class="uri">http://CRAN.R-project.org/package=gbm</a>.</p>
</div>
<div id="ref-rijsberman1985effect">
<p>Rijsberman, Frank R, and M Gordon Wolman. 1985. “Effect of Erosion on Soil Productivity: An International Comparison.” <em>Journal of Soil and Water Conservation</em> 40 (4). Soil; Water Conservation Society: 349–54.</p>
</div>
<div id="ref-Rodriguez-Lado23082013">
<p>Rodríguez-Lado, Luis, Guifan Sun, Michael Berg, Qiang Zhang, Hanbin Xue, Quanmei Zheng, and C. Annette Johnson. 2013. “Groundwater Arsenic Contamination Throughout China.” <em>Science</em> 341 (6148): 866–68. doi:<a href="https://doi.org/10.1126/science.1237484">10.1126/science.1237484</a>.</p>
</div>
<div id="ref-Rosenbaum2012WRCR">
<p>Rosenbaum, U., H. R. Bogena, M. Herbst, J. A. Huisman, T. J. Peterson, A. Weuthen, A. W. Western, and H. Vereecken. 2012. “Seasonal and Event Dynamics of Spatial Soil Moisture Patterns at the Small Catchment Scale.” <em>Water Resources Research</em> 48 (10): 1–22. doi:<a href="https://doi.org/10.1029/2011WR011518">10.1029/2011WR011518</a>.</p>
</div>
<div id="ref-VISCARRAROSSEL2006320">
<p>Rossel, R.A. Viscarra, B. Minasny, P. Roudier, and A.B. McBratney. 2006. “Colour Space Models for Soil Science.” <em>Geoderma</em> 133 (3): 320–37. doi:<a href="https://doi.org/https://doi.org/10.1016/j.geoderma.2005.07.017">https://doi.org/10.1016/j.geoderma.2005.07.017</a>.</p>
</div>
<div id="ref-Rossiter2001">
<p>Rossiter, D.G. 2003. <em>Methodology for Soil Resource Inventories</em>. 3rd ed. ITC Lecture Notes Sol.27. Enschede, the Netherlands: ITC.</p>
</div>
<div id="ref-Rossiter2004SUM">
<p>———. 2004. “Digital Soil Resource Inventories: Status and Prospects.” <em>Soil Use and Management</em> 20 (3). Blackwell Publishing Ltd: 296–301. doi:<a href="https://doi.org/10.1111/j.1475-2743.2004.tb00372.x">10.1111/j.1475-2743.2004.tb00372.x</a>.</p>
</div>
<div id="ref-rowan1990land">
<p>Rowan, James Niall. 1990. <em>Land Systems of Victoria</em>. Land Protection Division.</p>
</div>
<div id="ref-rowe1981ecological">
<p>Rowe, J. Stan, and John W. Sheard. 1981. “Ecological Land Classification: A Survey Approach.” <em>Environmental Management</em> 5 (5). Springer: 451–64.</p>
</div>
<div id="ref-Saini1966N">
<p>Saini, G.R. 1966. “Organic matter as a measure of bulk density of soil.” <em>Nature</em> 210: 1295.</p>
</div>
<div id="ref-Sanchez2009Science">
<p>Sanchez et al. 2009. “Digital Soil Map of the World.” <em>Science</em> 325: 680–81.</p>
</div>
<div id="ref-sanderman2018soil">
<p>Sanderman, Jonathan, Tomislav Hengl, and Gregory J Fiske. 2018. “Soil Carbon Debt of 12,000 Years of Human Land Use.” <em>PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA</em> 115 (7). NATL ACAD SCIENCES 2101 CONSTITUTION AVE NW, WASHINGTON, DC 20418 USA: E1700–E1700.</p>
</div>
<div id="ref-Savtchenko2004ASR">
<p>Savtchenko, A., D. Ouzounov, S. Ahmad, J. Acker, G. Leptoukh, J. Koziana, and D. Nickless. 2004. “Terra and Aqua MODIS products available from NASA GES DAAC.” <em>Advances in Space Research</em> 34 (4): 710–14. doi:<a href="https://doi.org/10.1016/j.asr.2004.03.012">10.1016/j.asr.2004.03.012</a>.</p>
</div>
<div id="ref-Saxton1986SSSAJ">
<p>Saxton, K.E., W.J. Rawls, J.S. Romberger, and R.I. Papendick. 1986. “Estimating generalized soil-water characteristics from texture.” <em>Soil Science Society of America Journal</em> 50: 1031–6.</p>
</div>
<div id="ref-sayre2014new">
<p>Sayre, Roger, Jack Dangermond, Charlie Frye, Randy Vaughan, Peter Aniello, Sean Breyer, Douglas Cribbs, et al. 2014. <em>A New Map of Global Ecological Land Units—an Ecophysiographic Stratification Approach</em>. Washington, DC: USGS / Association of American Geographers.</p>
</div>
<div id="ref-schabenberger2005statistical">
<p>Schabenberger, O., and C. A. Gotway. 2005. <em>Statistical Methods for Spatial Data Analysis</em>. Texts in Statistical Science. Chapman &amp; Hall/CRC.</p>
</div>
<div id="ref-Scharlemann2014CM">
<p>Scharlemann, Jörn P. W., Edmund V. J. Tanner, Roland Hiederer, and Valerie Kapos. 2014. “Global Soil Carbon: Understanding and Managing the Largest Terrestrial Carbon Pool.” <em>Carbon Management</em> 5 (1). Future Science: 81–91. doi:<a href="https://doi.org/10.4155/cmt.13.77">10.4155/cmt.13.77</a>.</p>
</div>
<div id="ref-Schelling1970Geoderma">
<p>Schelling, J. 1970. “Soil Genesis, Soil Classification and Soil Survey.” <em>Geoderma</em> 4 (3): 165–93. doi:<a href="https://doi.org/10.1016/0016-7061(70)90002-9">10.1016/0016-7061(70)90002-9</a>.</p>
</div>
<div id="ref-Schoeneberger1998">
<p>Schoeneberger, P.J., D.A. Wysocki, E.C. Benham, and W.D. Broderson. 1998. <em>Field Book for Describing and Sampling Soils</em>. Lincoln, Nebraska: Natural Resources Conservation Service, USDA, National Soil Survey Centre.</p>
</div>
<div id="ref-Scul01">
<p>Scull, P., J. Franklin, O. A. Chadwick, and D. McArthur. 2003. “Predictive Soil Mapping: A Review.” <em>Progress in Physical Geography</em> 27 (2): 171–97.</p>
</div>
<div id="ref-searle2014australian">
<p>Searle, R. 2014. “The Australian Site Data Collation to Support the Globalsoilmap.” <em>GlobalSoilMap: Basis of the Global Spatial Soil Information System</em>. CRC Press, 127.</p>
</div>
<div id="ref-shangguan2013china">
<p>Shangguan, Wei, Yongjiu Dai, Baoyuan Liu, Axing Zhu, Qingyun Duan, Lizong Wu, Duoying Ji, et al. 2013. “A China Data Set of Soil Properties for Land Surface Modeling.” <em>Journal of Advances in Modeling Earth Systems</em> 5 (2): 212–24. doi:<a href="https://doi.org/10.1002/jame.20026">10.1002/jame.20026</a>.</p>
</div>
<div id="ref-shaw2005ecosystem">
<p>Shaw, C., J.S. Bhatti, and K.J. Sabourin. 2005. <em>An Ecosystem Carbon Database for Canadian Forests</em>. Information Report. Edmonton, Alberta: Canadian Forest Service, Northern Forestry Centre.</p>
</div>
<div id="ref-ShepherdWalsh2007JNIS">
<p>Shepherd, K.D., and M.G. Walsh. 2007. “Infrared Spectroscopy — Enabling an Evidence Based Diagnostic Survellance Approach to Agricultural and Environmental Management in Developing Countries.” <em>Journal of Near Infrared Spectroscopy</em> 15: 1–19.</p>
</div>
<div id="ref-shimada2014new">
<p>Shimada, Masanobu, Takuya Itoh, Takeshi Motooka, Manabu Watanabe, Tomohiro Shiraishi, Rajesh Thapa, and Richard Lucas. 2014. “New Global Forest/Non-Forest Maps from Alos Palsar Data (2007–2010).” <em>Remote Sensing of Environment</em> 155. Elsevier: 13–31.</p>
</div>
<div id="ref-Shirazi2001SSSAJ">
<p>Shirazi, M.A., L. Boersma, and C.B. Johnson. 2001. “Particle size distributions: comparing texture systems, adding rock, and predicting soil properties.” <em>Soil Science Society of America Journal</em> 65: 300–310.</p>
</div>
<div id="ref-Simonson1968AA">
<p>Simonson, Roy W. 1968. “Concept of Soil.” In <em>Prepared Under the Auspices of the American Society of Agronomy</em>, edited by A.G. Norman, 20:1–47. Advances in Agronomy. Academic Press. doi:<a href="https://doi.org/10.1016/S0065-2113(08)60853-6">10.1016/S0065-2113(08)60853-6</a>.</p>
</div>
<div id="ref-Sleutel2007CSSPA">
<p>Sleutel, S., S. De Neve, B. Singier, and G. Hofman. 2007. “Quantification of Organic Carbon in Soils: A Comparison of Methodologies and Assessment of the Carbon Content of Organic Matter.” <em>Communications in Soil Science and Plant Analysis</em> 38: 2647–57.</p>
</div>
<div id="ref-Smith1986SMSS">
<p>Smith, G.D. 1986. <em>The Guy Smith interviews: Rationale for Concepts in Soil Taxonomy</em>. SMSS Technical Monograph No.11. Cornell University: Soil Management Support Services, USDA.</p>
</div>
<div id="ref-Smith2004SUM">
<p>Smith, P., P. Falloon, and L. Kutsch. 2004. “Soils as Carbon Sinks: The Global Context.” <em>Soil Use and Management</em> 20 (2). Blackwell Publishing Ltd: 212–18. doi:<a href="https://doi.org/10.1111/j.1475-2743.2004.tb00361.x">10.1111/j.1475-2743.2004.tb00361.x</a>.</p>
</div>
<div id="ref-snepvangers2003soil">
<p>Snepvangers, J. J. J. C., G. B. M. Heuvelink, and J. A. Huisman. 2003. “Soil Water Content Interpolation Using Spatio-Temporal Kriging with External Drift.” <em>Geoderma</em> 112 (3). Elsevier: 253–71.</p>
</div>
<div id="ref-SSDS1993">
<p>Soil survey Division staff. 1993. <em>Soil Survey Manual</em>. Vol. Handbook 18. Washington: United States Department of Agriculture.</p>
</div>
<div id="ref-SSS1983USDA">
<p>Soil Survey Staff. 1983. <em>Soil Survey Manual</em>. Rev. Vol. Handbook 18. Washington: United States Agriculture, USDA.</p>
</div>
<div id="ref-krogh1996learning">
<p>Sollich, Peter, and Anders Krogh. 1996. “Learning with Ensembles: How over-Fitting Can Be Useful.” In <em>Advances in Neural Information Processing Systems</em>, 8:190.</p>
</div>
<div id="ref-Sommer2008480">
<p>Sommer, M., H.H. Gerke, and D. Deumlich. 2008. “Modelling Soil Landscape Genesis — a ‘Time Split’ Approach for Hummocky Agricultural Landscapes.” <em>Geoderma</em> 145 (3?4): 480–93. doi:<a href="https://doi.org/10.1016/j.geoderma.2008.01.012">10.1016/j.geoderma.2008.01.012</a>.</p>
</div>
<div id="ref-Soon1991CSSPA">
<p>Soon, Y.K., and S. Abboud. 1991. “A comparison of some methods for soil organic carbon determination.” <em>Communications in Soil Science and Plant Analysis</em> 22: 943–54.</p>
</div>
<div id="ref-Statnikov2008">
<p>Statnikov, Alexander, Lily Wang, and Constantin Aliferis. 2008. “A Comprehensive Comparison of Random Forests and Support Vector Machines for Microarray-Based Cancer Classification.” <em>BMC Bioinformatics</em> 9 (1): 319. doi:<a href="https://doi.org/10.1186/1471-2105-9-319">10.1186/1471-2105-9-319</a>.</p>
</div>
<div id="ref-steichen2002note">
<p>Steichen, Thomas J, and Nicholas J Cox. 2002. “A Note on the Concordance Correlation Coefficient.” <em>Stata J</em> 2 (2): 183–89.</p>
</div>
<div id="ref-Stein1999Springer">
<p>Stein, M. L. 1999. <em>Interpolation of Spatial Data: Some Theory for Kriging</em>. Series in Statistics. New York: Springer.</p>
</div>
<div id="ref-tan2005soil">
<p>Tan, K.H. 2005. <em>Soil Sampling, Preparation, and Analysis</em>. Books in Soils, Plants, and the Environment. Taylor &amp; Francis/CRC Press.</p>
</div>
<div id="ref-Timlin1996AS">
<p>Timlin, D.J., Y.A. Pachepsky, B. Acock, and F. Whisler. 1996. “Indirect estimation of soil hydraulic properties to predict soybeanyield using GLYCIM.” <em>Agricultural Systems</em> 52: 331–53.</p>
</div>
<div id="ref-Torri1994C">
<p>Torri, D., J. Poesen, F. Monaci, and E. Busoni. 1994. “Rock fragment content and fine soil bulk density.” <em>Catena</em> 23: 65–71.</p>
</div>
<div id="ref-Toth2013LUCAS">
<p>Tóth, G., A. Jones, and L. Montanarella, eds. 2013. <em>LUCAS Topsoil Survey. Methodology, Data and Results</em>. JRC Technical Reports Eur 26102. Luxembourg: Publications Office of the European Union.</p>
</div>
<div id="ref-Tranter2007SUM">
<p>Tranter, G., B. Minasny, A. B. McBratney, B. Murphy, N.J. McKenzie, M. Grundy, and D. Brough. 2007. “Building and testing conceptual and empirical models for predicting soil bulk density.” <em>Soil Use and Management</em>, 1–6.</p>
</div>
<div id="ref-agriculture2010keys">
<p>U.S. Department of Agriculture. 2014. <em>Keys to Soil Taxonomy</em>. 12th ed. U.S. Government Printing Office.</p>
</div>
<div id="ref-VanEngelen2012">
<p>Van Engelen, V.W.P., and J.A. Dijkshoorn, eds. 2012. <em>Global and National Soils and Terrain Digital Databases (SOTER), Procedures Manual, version 2.0</em>. ISRIC Report 2012/04. Wageningen, the Netherlands: ISRIC - World Soil Information.</p>
</div>
<div id="ref-vanEtten2017r">
<p>van Etten, Jacob. 2017. “R Package gdistance: Distances and Routes on Geographical Grids.” <em>Journal of Statistical Software</em> 76 (13): 1–21.</p>
</div>
<div id="ref-van1980closed">
<p>Van Genuchten, M. Th. 1980. “A Closed-Form Equation for Predicting the Hydraulic Conductivity of Unsaturated Soils.” <em>Soil Science Society of America Journal</em> 44 (5). Soil Science Society of America: 892–98.</p>
</div>
<div id="ref-VanReeuwijk2002">
<p>Van Reeuwijk, L.P., ed. 2002. <em>Procedures for Soil Analysis</em>. Technical Paper 9. Wageningen, the Netherlands: ISRIC - World Soil Information.</p>
</div>
<div id="ref-vanReeuwijk1982">
<p>van Reeuwijk, L.R. 1982. “Laboratory Methods and Data Quality. Program for Soil Characterization: A Report on the Pilot Round. Part I. Cec and Texture.” In <em>Proceedings of 5th International Classification Workshop</em>. Khartoum, Sudan: ISRIC.</p>
</div>
<div id="ref-vanwalleghem2010spatial">
<p>Vanwalleghem, Tom, Jean Poesen, A McBratney, and J Deckers. 2010. “Spatial Variability of Soil Horizon Depth in Natural Loess-Derived Soils.” <em>Geoderma</em> 157 (1). Elsevier: 37–45.</p>
</div>
<div id="ref-Venables2002Springer">
<p>Venables, W. N., and B. D. Ripley. 2002. <em>Modern applied statistics with S</em>. 4th ed. New York: Springer-Verlag.</p>
</div>
<div id="ref-Vereecken1989SS">
<p>Vereecken, H.J., J. Maes, and P.D. Feyen. 1989. “Estimating the Soil Moisture Retention Characteristic from Texture,Bulk Density, and Carbon Content.” <em>Soil Science</em> 148 (6): 389–403.</p>
</div>
<div id="ref-ViscarraRossel2010DSS">
<p>Viscarra Rossel, Raphael A., Alex B. McBratney, and Budiman Minasny, eds. 2010. <em>Proximal Soil Sensing</em>. Progress in Soil Science. Springer.</p>
</div>
<div id="ref-wager2014confidence">
<p>Wager, Stefan, Trevor Hastie, and Bradley Efron. 2014. “Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife.” <em>Journal of Machine Learning Research</em> 15 (1): 1625–51.</p>
</div>
<div id="ref-Walker2003IA">
<p>Walker, W.E., P. Harremoës, J. Rotmans, J.P. van der Sluijs, M.B.A. van Asselt, P. Janssen, and M.P.K von Krauss. 2003. “Defining Uncertainty: A Conceptual Basis for Uncertainty Management in Model-Based Decision Support.” <em>Integrated Assessment</em> 4 (1): 5–17.</p>
</div>
<div id="ref-Walter2006DSS">
<p>Walter, C., P. Lagacherie, and S. Follain. 2006. “Integrating Pedological Knowledge into Digital Soil Mapping.” In <em>Digital Soil Mapping — an Introductory Perspective</em>, edited by P. Lagacherie, A. B. McBratney, and M. Voltz, 31:281–300, 615. Developments in Soil Science. Elsevier. doi:<a href="https://doi.org/10.1016/S0166-2481(06)31022-7">10.1016/S0166-2481(06)31022-7</a>.</p>
</div>
<div id="ref-wan2006modis">
<p>Wan, Zhengming. 2006. <em>MODIS land surface temperature products users’ guide</em>. ICESS, University of California.</p>
</div>
<div id="ref-Wang1996AJSR">
<p>Wang, X.J., P.J. Smethurst, and A.M. Herbed. 1996. “Relationships between three measures of organic matter or carbon in soils of eucalypt plantations in Tasmania.” <em>Australian Journal of Soil Research</em> 34: 545–53.</p>
</div>
<div id="ref-Webster2001Wiley">
<p>Webster, R., and M.A. Oliver. 2001. <em>Geostatistics for Environmental Scientists</em>. Statistics in Practice. Chichester: Wiley.</p>
</div>
<div id="ref-wei2014global">
<p>Wei, Xiaorong, Mingan Shao, William Gale, and Linhai Li. 2014. “Global Pattern of Soil Carbon Losses Due to the Conversion of Forests to Agricultural Land.” <em>Scientific Reports</em> 4. Nature Publishing Group: 4062.</p>
</div>
<div id="ref-white2009principles">
<p>White, R.E. 2009. <em>Principles and Practice of Soil Science: The Soil as a Natural Resource</em>. Wiley.</p>
</div>
<div id="ref-wilkinson2016fair">
<p>Wilkinson, Mark D, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The Fair Guiding Principles for Scientific Data Management and Stewardship.” <em>Scientific Data</em> 3. Nature Publishing Group.</p>
</div>
<div id="ref-Williams1992">
<p>Williams, J., P.J. Ross, and K.L. Bristow. 1992. “Prediction of the Campbell water retention function from texture, structure and organic matter.” In <em>Proceedings of the International Workshop on Indirect Methods for Estimating the Hydraulic Properties of Unsaturated Soils</em>, edited by M.Th. van Genuchten, F.J. Leij, and L.J. Lund, 427–41. Riverside, CA: University of California, Riverside.</p>
</div>
<div id="ref-OGCKML2008">
<p>Wilson, Tim. 2008. “OGC KML.” OGC Standard OGC 07-147r2. Open Geospatial Consortium Inc.</p>
</div>
<div id="ref-Wosten1992">
<p>Wösten, J.H.M., and J. Bouma. 1992. “Applicability of soil survey data to estimate hydraulic properties of unsaturated soils.” In <em>Proceedings of an International Workshop on Indirect Methods for Estimating the Hydraulic Properties of Unsaturated Soils</em>, edited by M.Th. van Genuchten, F.J. Leij, and L.J. Lund, 463–72. University of California, Riverside.</p>
</div>
<div id="ref-Wosten1985SSSAJ">
<p>Wösten, J.H.M., J. Bouma, and G.H. Stoffelsen. 1985. “Use of soil survey data for regional soil water simulation models.” <em>Soil Science Society of America Journal</em> 49: 1238–44.</p>
</div>
<div id="ref-Wosten1995G">
<p>Wösten, J.H.M., P. A. Fi, and M.J.W. Jansen. 1995. “Comparison of class and continuous pedotransfer functions to generate soil hydraulic characteristics.” <em>Geoderma</em> 66: 227–37.</p>
</div>
<div id="ref-Wosten1999G">
<p>Wösten, J.H.M., A. Lilly, A. Nemes, and C. Le Bas. 1999. “Development and use of a database of hydraulic properties of European soils.” <em>Geoderma</em> 90: 169–85.</p>
</div>
<div id="ref-Wosten2001JH">
<p>Wösten, J.H.M., Y.A. Pachepsky, and W.J. Rawls. 2001. “Pedotransfer functions: Bridging the gap between available basic soil data and missing soil hydraulic functions.” <em>Journal of Hydrology</em> 251: 123–50.</p>
</div>
<div id="ref-wosten2013soil">
<p>Wösten, J.H.M., S.J.E. Verzandvoort, J.G.B. Leenaars, T. Hoogland, and J.G. Wesseling. 2013. “Soil Hydraulic Information for River Basin Studies in Semi-Arid Regions.” <em>Geoderma</em> 195. Elsevier: 79–86. doi:<a href="https://doi.org/10.1016/j.geoderma.2012.11.021">10.1016/j.geoderma.2012.11.021</a>.</p>
</div>
<div id="ref-wright2017ranger">
<p>Wright, Marvin N, and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” <em>Journal of Statistical Software</em> 77 (1): 1–17.</p>
</div>
<div id="ref-Wysocki2005Geoderma">
<p>Wysocki, D., P.J. Schoeneberger, and H.E. LaGarry. 2005. “Soil Surveys: A Window to the Subsurface.” <em>Geoderma</em> 126: 167–80.</p>
</div>
<div id="ref-xu2013global">
<p>Xu, Xiaofeng, Peter E Thornton, and Wilfred M Post. 2013. “A Global Analysis of Soil Microbial Biomass Carbon, Nitrogen and Phosphorus in Terrestrial Ecosystems.” <em>Global Ecology and Biogeography</em> 22 (6). Wiley Online Library: 737–49.</p>
</div>
<div id="ref-Yamamoto2008">
<p>Yamamoto, JorgeKazuo. 2008. “Estimation or Simulation? That Is the Question.” <em>Computational Geosciences</em> 12 (4). Springer Netherlands: 573–91. doi:<a href="https://doi.org/10.1007/s10596-008-9096-8">10.1007/s10596-008-9096-8</a>.</p>
</div>
<div id="ref-yegnanarayana2004artificial">
<p>Yegnanarayana, B. 2004. <em>Artificial Neural Networks</em>. PHI Learning Pvt. Ltd.</p>
</div>
<div id="ref-Zacharias2007SSSAJ">
<p>Zacharias, S., and G. Wessolek. 2007. “Excluding Organic Matter Content from Pedotransfer Predictors of Soil Water Retention.” <em>Soil Science Society of America Journal</em> 71: 43–50.</p>
</div>
<div id="ref-ZHONG2011491">
<p>Zhong, B., and Y.J. Xu. 2011. “Scale Effects of Geographical Soil Datasets on Soil Carbon Estimation in Louisiana, Usa: A Comparison of Statsgo and Ssurgo.” <em>Pedosphere</em> 21 (4): 491–501. doi:<a href="https://doi.org/10.1016/S1002-0160(11)60151-3">10.1016/S1002-0160(11)60151-3</a>.</p>
</div>
<div id="ref-Zhou2004JZUS">
<p>Zhou, Bin, Xin-gang Zhang, and Ren-chao Wang. 2004. “Automated Soil Resources Mapping Based on Decision Tree and Bayesian Predictive Modeling.” <em>Journal of Zhejiang University Science</em> 5 (7/2004): 782–95. doi:<a href="https://doi.org/10.1631/jzus.2004.0782">10.1631/jzus.2004.0782</a>.</p>
</div>
<div id="ref-Zhu2010Geoderma">
<p>Zhu, A-Xing, Feng Qi, Amanda Moore, and James E. Burt. 2010. “Prediction of Soil Properties Using Fuzzy Membership Values.” <em>Geoderma</em> 158 (3-4): 199–206. doi:<a href="https://doi.org/10.1016/j.geoderma.2010.05.001">10.1016/j.geoderma.2010.05.001</a>.</p>
</div>
<div id="ref-Zhu2001">
<p>Zhu, A.X., B. Hudson, J. Burt, K. Lubich, and D. Simonson. 2001. “Soil Mapping Using Gis, Expert Knowledge, and Fuzzy Logic.” <em>Soil Science Society of America Journal</em> 65: 1463–72.</p>
</div>
<div id="ref-Zhu2015PSM">
<p>Zhu, A.X., J. Liu, F. Du, S.J. Zhang, C.Z. Qin, J. Burt, T. Behrens, and T. Scholten. 2015. “Predictive Soil Mapping with Limited Sample Data.” <em>European Journal of Soil Science</em>. doi:<a href="https://doi.org/10.1111/ejss.12244">10.1111/ejss.12244</a>.</p>
</div>
</div>
</div>
<!--bookdown:body:end-->
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-envirometrix-github-io-predictivesoilmapping.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
    </div>
  </div>
<!--bookdown:config-->

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
